
Introduction: Cuts, Crisis and Criticisms of the Humanities
The Great Recession beginning in 2008 has resulted in a series of budgetary cuts to many universities, education programs, and cultural institutions that reflect indifference about, and even hostility toward, the humanities and arts in favor of scientific, engineering, business, and other applied fields. Even scientists are now concerned about the perceived legitimacy of their "basic research" when it lacks evident short-term application. However, the sciences have an established tradition of public advocacy and media communication that is quite effective in putting their discoveries before the public. The humanities have no such consistent, planned tradition of advocacy, and in many ways are starting from scratch.

This paper argues for and demonstrates planned humanities advocacy using the special affordances of the digital humanities. In particular, it discusses how the 4Humanities initiative is leveraging DH for next-generation advocacy. In this paper, we will:

Show how 4Humanities uses DH to help analyze public discourse, both pro and con, about the humanities (including text analysis of such discourse as well as crowd-sourced generation of arguments for the humanities).
Show how statistics and other evidence about the contribution of the humanities to society can be analyzed and visualized in support of effective arguments.
Discuss the role of 4Humanities and, more generally, how DH can provide special tools for humanities advocacy, while humanities advocacy can in return incentivize the creation of next-generation DH research and teaching tools with a built-in public engagement dimension.
Analysis of Arguments Against the Humanities
One way to bolster advocacy for the humanities is first to look closely at the arguments made in public against them. We have compiled a small corpus of recent articles (especially from news sources accessed by a broad public) representative of criticisms of the humanities (Auslin, 2012; Bauerlein, 2011; Cohan, 2012a; Cohan, 2012b; Ellouk, 2011; Fendrich, 2009; Fish, 2007; Fish 2008; Fund, 2012; Knapp, 2011; Murdoch, 2011; Pidgeon, 2007; Riley, 2012; Sini, 2011; Stephens, 2012; Wente, 2012; Wood, 2012).


Fig 1:
Voyant Collocate Cluster Visualization (Sinclair and Rockwell)

We find that arguments critical of the humanities cluster around certain ideas. Principally, detractors accuse the humanities of lacking cultural and/or economic relevance. The most nuanced critiques mix or shade the charges of cultural and economic irrelevance. But other arguments refute the social usefulness of the humanities entirely, simply taking for granted their complete economic and social irrelevance. (Interestingly, however, some of these articles also defend irrelevance as a virtue, as in the case of arguments from friends of the humanities who feel that irrelevance is the basic nature of the humanities and is perfectly acceptable.)

A related critique of the professoriate in the humanities is that our work is no longer accessible to a larger educated public. The argument is that we are our own worst enemies because we have descended into theoretical turf battles that no one cares about. For these commentators such cultural irrelevance goes hand in hand with the supposed lack of respect that academics show for the public’s values, as epitomized in Marxism, feminism, post-colonialism, and other approaches that attack iconic ideas. The antidote sometimes proposed is a return to a nebulous concept of the traditional humanities — e.g., to the venerable search for the “beautiful.”

This paper will summarize our reading of the articles as well as present results from some text analysis of other rhetoric about the humanities.

Analysis of Arguments For the Humanities
As important as it is to know the arguments critical of the humanities, it is also important to gather good arguments for the humanities. 4Humanities has taken two approaches to this. The first is to blog good arguments as we come across them with summaries for those who are looking for essays to help them in their advocacy. We have also summarized these in a digestible form for people to review (Bielby, 2012). Finally we ran an All Our Ideas vote on the value of the humanities (http://allourideas.org/4humanities). At the time of writing this proposal there were over 1600 votes and 31 user submitted possible answers (as opposed to 12 seeds that we provided.) The top choices at the time of writing included:


There is obviously overlap in the top choices, but these indicate what the digital humanities community considers important. In the paper we will provide a fuller analysis of the data along with our list of the best arguments.

Looking Closely at the Statistics
“Liberal arts graduates frequently catch or surpass graduates with career-oriented majors in both job quality and compensation.” (Koc, 2011)

In addition to defining and communicating the cultural value of the humanities, the 4Humanities initiative is also committed to gathering data on the economic value of the humanities. One of the most pernicious arguments against the humanities has been the poor job prospects of graduating humanists. For this reason, we review the statistical arguments carefully, especially to highlight the fact that the data on compensation of humanists at mid-career paints a different story. A humanist may find it harder to get a first job with a degree, but she/he will probably rise faster than many with professional certification.

When making arguments for the humanities’ contribution to society, of course, it can be difficult to produce statistics, given that there has been no comprehensive study that gathers together available facts and figures in a usable manner. As part of the 4Humanities project we have been compiling and listing all statistics we can find in the published literature about the benefit of the humanities to society. This was done by collating the literature – including newspaper articles, reports, websites, and op-ed pieces (listed on the 4Humanities website) – and locating numeric references to the humanities. We have compiled such quantitative evidence, and at DH2013 we will present an infographic that sums up the statistical argument that the humanities are relevant to economic and intellectual development.

In addition, we have recently begun our “Infographics Friday” series (http://humanistica.ualberta.ca/category/for-the-public/humanities-infographics/), highlighting a particular statistic or graphical representation on the 4Humanities website once a week, to demonstrate the range of evidence. A core remit of 4Humanities is to gather, analyse, and disseminate this disparate information to provide a knowledge base upon which others can build their opinions and bolster their understanding of the humanities.

Conclusion: 4Humanities and a Digital Humanities response to the Cuts, Crisis, and Criticism
We argue that the mission of public engagement and advocacy in the humanities as embodied by the 4Humanities initiative provides a unique way to consolidate leading technological and methodological directions in DH with outreach to society. The humanities today have an advantage that was not available earlier: the analytical and communication methods of the digital humanities. Not only do the digital humanities provide a strong argument for the relevance of humanities learning in a digital age; they also provide unique, fresh ways of studying the contributions of the humanities to society and then getting the message out. DH research and humanities advocacy can be one, where DH helps advance advocacy, and, reciprocally, the advocacy mission helps drive research in DH. The hunt is now on to develop and extend new generations of digital humanities platforms and tools that can integrate the core research and teaching work of humanists with public visibility and engagement. Such platforms and tools (for publishing, editing, research, pedagogy, etc.) can be designed from the ground up, both to serve the needs of academics and to engage with today's networked public. This paper is a step in that direction.

References
Auslin, M. (2012). Knowledge is Good. National Review Online. 15 March. http://www.aei.org/article/education/higher-education/knowledge-is-good/ (accessed 13 March 2013).
Bauerlein, M. (2011). Oh, the Humanities! The Weekly Standard. 16 May. http://www.weeklystandard.com/articles/oh-humanities_559340.html (accessed 13 March 2013).
Bielby, J. (2012). Arguments for the Humanities. CIRCA Wiki, October. http://circa.cs.ualberta.ca/index.php/CIRCA:Arguments_FOR_the_Humanities (accessed 13 March 2013).
Cohan, P. (2012a). To Boost Post-College Prospects, Cut Humanities Departments. Forbes, 29 May. http://www.forbes.com/sites/petercohan/2012/05/29/to-boost-post-college-prospects-cut-humanities-departments/ (accessed 13 March 2013).
Cohan, P. (2012b). The 13 Most Useless Majors, From Philosophy to Journalism. The Daily Beast, 23 April. http://www.thedailybeast.com/galleries/2012/04/23/the-13-most-useless-majors-from-philosophy-to-journalism.html (accessed 13 March 2013).
Davidson, C. N. (2011). Strangers on a Train. Academe. 97 (5). http://www.aaup.org/AAUP/pubsres/academe/2011/SO/Feat/davi.htm (accessed 13 March 2013).
Davidson, C. N. (2012). Humanities 2.0: Promise, Perils, Predictions. In Gold, M. (ed), Debates in the Digital Humanities. Minneapolis: University of Minnesota Press. 476-489.
Davidson, C. N., and D. T. Goldberg (2004). A Manifesto for the Humanities in a Technological Age. The Chronicle of Higher Education: The Chronicle Review. 13 Feb.
Delblanco, A. (2011). College: What It Was, Is, and Should Be. Princeton, NJ: Princeton University Press.
Ellouk, B. (2011). Do We Still Need the Humanities? The Daily of the University of Washington. 26 July. http://dailyuw.com/news/2011/jul/26/do-we-still-need-humanities/ (accessed 13 March 2013).
Fendrich, L. (2009). The Humanities Have No Purpose. The Chronicle of Higher Education, 20 March. http://chronicle.com/blogs/brainstorm/the-humanities-have-no-purpose/6738 (accessed 13 March 2013).
Fish, S. (2007). Bound For Academic Glory? The New York Times, 23 December. http://opinionator.blogs.nytimes.com/2007/12/23/bound-for-academic-glory/ (accessed 13 March 2013).
Fish, S. (2008). Will the Humanities Save Us? The New York Times, 6 January. http://opinionator.blogs.nytimes.com/2008/01/06/will-the-humanities-save-us/ (accessed 13 March 2013).
Fish, S. (2010). The Crisis of the Humanities Officially Arrives. The New York Times, 11 October. http://opinionator.blogs.nytimes.com/2010/10/11/the-crisis-of-the-humanities-officially-arrives/ (accessed 13 March 2013).
Fund, J. (2012). Censoring Naomi Riley. The National Review Online, 12 May. http://www.nationalreview.com/articles/299765/censoring-naomi-riley-john-fund (accessed 13 May 2013).
Kirschenbaum, M. (2012). Digital Humanities As/Is a Tactical Term. In Gold, M. (ed), Debates in the Digital Humanities. Minneapolis: University of Minnesota Press, 415-428.
Knapp, S. (2011). The Enduring Dilemma of the Humanities. The Phi Beta Kappa Society, 29 March. http://www.pbk.org/home/FocusNews.aspx?id=741 (accessed 13 March 2013).
Koc, E. W. (2011). Just Wait 10 Years. New York Times, 21 March. http://www.nytimes.com/roomfordebate/2011/03/20/career-counselor-bill-gates-or-steve-jobs/your-college-major-matter-less-over-time. (accessed 13 March 2013).
Lakoff, G. (2004). Don’t Think of an Elephant!: Know Your Values and Frame the Debate. White River, VT: Chelsea Green Publishing Company.
Liu, A. (2012). Where Is Cultural Criticism in the Digital Humanities? In Gold, M. (ed), Debates in the Digital Humanities. Minneapolis, MN: University of Minnesota Press, 490-509.
Murdoch, R. (2011). The Steve Jobs Model for Education Reform. The Wall Street Journal, 15 October. http://online.wsj.com/article/SB10001424052970203914304576631100415237430.html (accessed 13 March 2013).
Pidgeon, S. (2007). Commented on Fish, S. Bound For Academic Glory? The New York Times, 23 December. http://opinionator.blogs.nytimes.com/2007/12/23/bound-for-academic-glory/#comment-100883 (accessed 13 March 2013).
Riley, N. S. (2012). The Academic Mob Rules. The Wall Street Journal, 8 May. http://online.wsj.com/article/SB10001424052702304363104577391842133259230.html (accessed 13 May 2013).
Sinclair, S., and G. Rockwell Collocates Cluster, from Voyant Tools. http://docs.voyant-tools.org/tools/links/ (accessed 13 March 2013).
Sini, M. (2011). Oh the Humanities! (OR: A Critique of Crisis). OverLand, 22 February. http://overland.org.au/blogs/loudspeaker/2011/02/%E2%80%98oh-the-humanities%E2%80%99-or-a-critique-of-crisis/ (accessed 13 March 2013).
Stephens, B. (2012). To the Class of 2012. The Wall Street Journal, 9 May. http://online.wsj.com/article/SB10001424052702304451104577389750993890854.html (accessed 13 March 2013).
Wente, M. (2012). Quebec’s University Students are in for a Shock. The Globe and Mail, 1 May. http://www.theglobeandmail.com/commentary/quebecs-university-students-are-in-for-a-shock/article4104304/ (accessed 13 March 2013).
Wood, P. (2012). Rick Santorum is Right. The Chronicle of Higher Education, 29 February. http://chronicle.com/blogs/innovations/rick-santorum-is-right/31769 (accessed 13 March 2013).
Some thirty years ago Donald Knuth, a computer scientist, proposed literate programming as a better way of organizing narrative and code (1984). Knuth argued that more emphasis should be placed on explaining to humans what computers are meant to do, rather than simply instructing computers what to do. Knuth was especially interested in weaving together macrostyle code snippets with prose that provided a larger narrative context, not merely functional comments of specific lines of code that are the distilled remnants of an intellectual process.
Literate programming has been more influential in theory than in practice (Nørmark), despite several utilities and environments including Mathematica, Knuth's (C)WEB, Sweave for R, and Marginalia for Clojure. Perhaps the exigencies of programming in the real world correspond poorly with the vision of Knuth of the programmer as author: "the practitioner of literate programming can be regarded as an essayist, whose main concern is with exposition and excellence of style" (1992, 1). However, that balance of essayist and coder strikes us as perfectly appropriate for the digital humanities, a natural blend of the expression of intellectual process with the exposition of technical methodologies. The prose can gloss the code, or viceversa, in a symbiotic relationship that serves to strengthen an argument and demonstrate its own workings.
One of the most significant potential benefits of the literate programming paradigm is pedagogical: these works can both explain an interpretive insight and present the methodology for reproducing the data or results that were part of the process. Many widely-read digital humanities blogs already present these characteristics of exploration, explanation, interpretation and step-by-step instructions (see for example blogs by Ted Underwood, Benjamin Schmidt, Lisa Rhody and Scott Weingart). Literate programming can be more self-contained and more useful for those learning new methodologies and new programming techniques. This is about the principles of literate programming, but also about the potential for increasing programming literacy.
This poster will introduce Voyant Notebooks, a web-based literate programming environment designed for the digital humanities (see Appendix A). There is already a working prototype and we anticipate having a more feature-rich version available by July 2013. Voyant Notebooks inherits many of the characteristics of the Voyant Tools environment, including a concern for usability and flexibility (researchers and students should be able to use it with minimal or no training and with their own texts of interest). Voyant Notebooks also addresses one of the main weaknesses of Voyant Tools: the fact that most tools are constrained by assumptions about how they would be most commonly used. For instance, the Wordle-like (word cloud) Cirrus tool is designed to show the top frequency terms from a corpus or document; but what if the user instead wants to visualize the top frequency nouns, or people, or repeating phrases? All of that functionality could be built into the tool, but possibly at the cost of usability (endless menus and options), and it could still never address all of the possible use cases. Voyant Notebooks, by contrast, empowers the user to customize some of the functionality by leveraging the analytic capabilities of the Voyant back-end and the visualization interfaces in the front-end (like Cirrus). Our poster will have two parts, a) a usable demonstration on one or more laptops and b) a poster that illustrates how Voyant Notebooks implements Knuth’s concept of literate programming. In addition to these conceptual aspects, the poster will outline technical details about the Voyant Notebooks prototype for those interested, including the technologies used for both client-side (browser) and server-side components. Some of the technical challenges that will be described include:
•        managing the flow of code execution in an asynchronous architecture,
•        using web workers to avoid browser freezes during longer executions,
•        mitigating the security risks of user-defined and persistent Javascript code,
•        code variable scoping across editor instances and window components,
•        embedding of Voyant tool panels (visualizations) and other services,
•        developing a flexible API for different programming levels and styles,
•        developing an API that includes both client-side and server-side operations, and
•        ensuring efficiency of repeated code snippets during writing and viewing.
And of course, visitors to the poster session will be warmly encouraged to play with Voyant Notebooks.
Appendix A: Mockup of Voyant Notebooks (previously called Voyeur Notebooks).
 
Figure 1:
Mockup of Voyant Notebooks
References
Knuth, D. (1984). Literate Programming. The Computer Journal 27(2): 97-111, 1.
Knuth, D. (1992). Literate Programming. Stanford University Center for the Study of Language and Information.
Nørmark, K. (1998). Literate Programming: Issues and Problems. http://www.cs.aau.dk/~normark/litpro/issues-and-problems.html.
Sinclair, S. and G. Rockwell (2012). Teaching Computer-Assisted Text Analysis: Approaches to Learning New Methodologies. in Digital Humanities Pedagogy. Open Book Publishers.
Integrating research and teaching is exciting, time intensive, and a prescription for energizing faculty and students. We present outcomes of a six-year effort in multidisciplinary collaboration centered on the digital humanities as experienced in our teaching and research. Rooted in a set of “connected” courses between English and Computer Science (LeBlanc, et al. 2010) and three summers of NEH-funded research, our Lexomics Research Group has developed a modest set of web-based applications for scholars of digitized texts. We report here on the iterative development of the open-source toolset, how scholars both in and outside our group have used these tools to make significant discoveries, and perhaps most important how our research and teaching collaborations introduce a spirit of experimentation to the digital humanities.

Our current website is both a repository for our tool set as well as an evangelistic platform and teaching resource: http://lexomics.wheatoncollege.edu. We continue to develop online tools for three independent, but logically connected functions that lead scholars through the steps needed for performing hierarchical cluster analyses of texts and/or sections of texts. At this point, our cluster analysis tools are more narrowly focused than other toolsets, c.f. Voyant Tools and the data-intensive flow execution environment of Meandre. Our scrubber tool (PHP, CSS) accepts texts in multiple formats (.txt, .html, .docx) and handles preprocessing steps including stripping tags, removing stop words, and applying lemma lists. A second tool, diviText (ExtJS, PHP), accepts the output from scrubber, cuts texts into “chunks” in one of three ways (fixed size chunks, a specific number of chunks, and/or by manually selecting locations between words for chunk breaks), computes word counts within each chunk, and allows users to merge chunks. The latter functionality has proved valuable for generating “virtual manuscripts”, that is, joining sections from different manuscripts. A third tool, treeView (PHP, R) accepts output from diviText, performs a number of variants of hierarchical cluster analysis, and returns a dendrogram plot in .pdf or phyloXML format.

Based on feedback from scholars who are using our tools, the website now provides video and written tutorials to help new users get started. These tutorials have been especially valuable for introducing these tools to our undergraduates. In the spirit of evangelizing, our website offers a series of “best practices” videos, discussions and step-by-step diagrams that shred insight to the process of how textual analysis at this level of detail can lead to rich new questions. The instructional videos include “The Story of Daniel”, a discussion of one of our initial successes when using the tools where we showed that lexomic methods can accurately characterize the structure and relationships of texts that are already known, for example, identifying Genesis B within the Old English Genesis and the section of Daniel that is paralleled in Azarias (Drout, et al. 2011). Other videos include: “How to Read a Dendrogram”, “How to Create a Dendrogram”, “How to Read a Ribbon Diagram”, “Lexomics for Comparison”, and “Lexomics for Source Detection”. A much longer video, “Editions and Manuscripts,” addresses the challenges of choosing between different kinds of editions that may exist for a text that is found in multiple forms.

We have made what we think are significant discoveries in a number of spaces, including Beowulf, the poems of Cynewulf, Anglo-Saxon prose, a few Old Norse sagas, and Modern English texts including the Harlem Renaissance play Mule Bone (by Zora Neale Hurston and Langston Hughes). Lexomics is both an excellent first step to augment traditional scholarship as well as a rich source of deep analysis.

For example, previous lexomic analysis of several Old English poems suggests that there is a connection between dendrograms with an isolated, single leaf and poems that have an external source for one subsection of the poem different from the source or sources of the main body of the poem. We ﬁnd in the dendrogram of Daniel a single-leaf clade corresponding to lines 299–455 of the poem. This section includes parts of Daniel that have external Latin sources that are different from the source of the rest of the poem (the Latin Bible). Similarly, in the Anglo-Saxon poem Christ III, a single-leaf clade that represents lines 1350–1510 has its source in Sermon 57 of Cæsarius of Arles (lines 1379–1498), and a single-leaf clade in Genesis A (lines 1079–1256) is associated with the genealogical lists from Adam to Noah that give the lineages of both Cain and Seth (lines 1055–1252), material that, for at least some of its content, must have a source different from the biblical text. These relationships were already known to scholars, but our investigation of the Old English poem Guthlac A resolved a century-long critical controversy by demonstrating that a key section of this poem (when demons drag Guthlac to the mouth of hell) has a different proximate source than the rest of the poem and that Guthlac A therefore must have been composed after a separately circulating text similar in content to Vercelli Homily 23 (Downey et al., 2012).

The toolset, instructional materials, and publications are obvious deliverables from our efforts. Yet, we submit that our collaborative experiences with faculty and undergraduate students are even more exciting and provide a significant use-case of how scholarship in the humanities is evolving from the stereotypical solitary scholar to a paradigm of community, collaboration, and experimentation (cf. Unsworth, 1997). In our recent NEH- and locally-funded summer experience, humanities faculty in particular were pleasantly surprised with the intellectual environment that emerged. We got a glimpse of what it must have been like to work at a place like Bell Labs when they were making daily discoveries. This kind of collaborative, fast-moving research is unfortunately largely unknown in the humanities.

So how to continue our own momentum as well as replicate a spirit of experimentation for others? Earhart (2010) rightly notes that “digital projects remain rare, often the product of tenacious participants rather than a supportive academic environment”(emphasis added). We submit that faculty (not administrators nor technologists in the library) are the prime drivers and change must begin with our syllabi. Robust working relationships in the lab are strongest after students have already applied new modes of thinking in the classroom; for example, the importance of exposing undergraduate humanities students to computational thinking: problem decomposition, algorithmic thinking, and the success and failures of experimentation. And we need not overplay the lab metaphor. Our image of the digital humanities lab need not include beakers and soapstone benches, rather, the “new lab” is a room filled with scholars from multiple disciplines and a whiteboard.

Even if we had discovered nothing during our past summers in the lab, the intellectual thrill of the research group would have been a major accomplishment that these students (and we faculty) will never forget. But in fact we made discoveries, so many that there were days when participating faculty got none of their own work done because we were so busy bouncing from student to student seeing what they had found. Most critically, the experience continues to shape the way we share our disciplines with new cohorts of students. The solitary scholar still has a role to be sure, but that is no longer sufficient for the multidisciplinary demands and rewards to be gained from collaborations in the digital humanities: in our teaching, to our research, and back again.

lexomics — The term was originally coined by Betsey Dexter Dyer and ﬁrst appeared in Genome Technology (2002). Since then ‘‘lexomics’’ has appeared on the internet and in some publications without attribution. Some of these appearances could be independent inventions of the term.

References
Downey, S., M. Drout, M. Kahn, and M. D. LeBlanc (2012). 'Books Tell Us': Lexomic and Traditional Evidence for the Sources of Guthlac A. Modern Philology 110: 1-29.
Drout, M., M. Kahn, M. D. LeBlanc, and C. Nelson (2011). Of Dendrogrammatology: Lexomic Methods for Analyzing Relationships among Old English Poems, Journal of English and Germanic Philology 110: 301–36.
Drout, M., M. D. LeBlanc, and M. Kahn (2011-2013). “Lexomic Tools and Methods for Textual Analysis: Providing Deep Access to Digitized Texts.” National Endowment for the Humanities–NEH PR-50112011.
Earhart, A. (2010). Challenging Gaps: Redesigning Collaboration in the Digital Humanities. In Earhart and Jewell (eds),The American Literature Scholar in the Digital Age Ann Arbor: University of Michigan Press. http://hdl.handle.net/2027/spo.9362034.0001.001.
Genome Technology (2002). "In the News." in Genome Technology 1(27), November 1, 2002.
LeBlanc, M. D., M. Gousie, and T. Armstrong (2010). Connecting Across Campus. 'Proceedings of the 41st SIGCSE Technical Symposium on Computer Science Education' held in Milwaukee, WI.
LeBlanc, M. D., M. Drout, and M. Kahn (2008-2010). “Pattern Recognition through Computational Stylistics: Old English and Beyond.” National Endowment for the Humanities–NEH HD-50300-08.
Lexomics Research Group. http://lexomics.wheatoncollege.edu
Meandre http://seasr.org/meandre/
Unsworth, J. (1997). Documenting the Reinvention of Text: The Importance of Failure. Journal of Electronic Publishing, 3:2. http://dx.doi.org/10.3998/3336451.0003.201.
Voyant Tools. http://voyant-tools.org/
GAMS: A Fedora Commons instance

Since 2003 the Centre for Information Modeling - Austrian Centre for Digital Humanities at the University of Graz (Austria) provides an infrastructure for a variety of DH projects. After years of building insular solutions, the Centre introduced a powerful yet flexible new infrastructure, called GAMS (Geisteswissenschaftliches Asset Management System, AMS for the Humanities). It is based on the Fedora Commons architecture. Thus, the infrastructure inherits all features already provided by Fedora: full OAIS-compliance, strict separation of data and metadata, and predefined interfaces like OAI-PMH. A central advantage of the Fedora architecture is its object model: An asset consists of a primary source, some metadata and virtual representations derived from the primary source. The object is completely self-descriptive: It knows about all changes that have been made to it, its version history, datastreams and assigned context objects. Finally, it also knows about all possible representation forms. Each object contains all the necessary information to store, preserve, retrieve and view it.
Cirilo Client: Mass operations in Fedora made easy

Although Fedora is a powerful tool, front-end object management is not always easy, especially with regard to mass operations. The Centre has developed a tool for this use case, complementing Fedora’s built-in Admin Client. Cirilo is a java application developed for data curation and content preservation in Fedora-based repository systems. Content preservation and data curation in our sense include object management and creation, versioning, normalization and standards, and choice of data formats.
Cirilo makes use of Fedora’s management-API (API-M). It offers applications which are particularly prone to being used as tools for mass operations on Fedora repository objects, such as ingest or replacement processes: With Cirilo ingest processes can be performed from the file system, from an eXist database or an Excel spreadsheet. During the ingest metadata is automatically extracted from the source document and written to the newly created object (for instance in DC format).
  The client operates on a collection of predefined content models which can be used without further adjustments for standard workflow scenarios like the management of collections of TEI objects. The content models, which are based on the Fedora object model, are class definitions: On the one hand they define the (MIME-)type of the contained data streams, on the other hand they designate dissemination methods operating on these data streams. Every object in the repository is an instance of one of these class definitions. The advantage of this concept lies in the fact that very complex data sources and workflows can be handled easily.
Currently, the client offers various content models for specific purposes, special emphasis lies on the TEI model. The TEI ingest processes can be flexibly costumized: during ingest policies for the extraction of semantic information can be applied, referenced images can be uploaded simultaneously and ontology concepts can be resolved. A new content model currently in development creates the appropriate ontology objects, especially SKOS objects. A designated query object makes it possible to pose queries with parameters to the Mulgara triplestore. With the help of these ontology and query objects dynamic indices can be created. There is a container object for the creation of collections available, which makes it easy to organize your resources. Finally, there are some models optimized for specified primary sources like METS/MODS, HTML, PDF, BibTeX or external resources accessible via an URL. A content model for linguistic resources is in development (in cooperation with ICLTT, Vienna). Currently, we are testing how controlled vocabularies and thesauri (for instance geonames.org), can be sensibly integrated in the system.
The user can assign numerous virtual representations via the client. The METS/MODS object is designed to be viewed in the DFG-Viewer. TEI objects can be directly used as the input for the Voyant Tools or the Versioning Machine. The members of a context object can be projected on a map using Google Maps. Basically any web-based service can be integrated into the infrastructure. Of course, user- and project-specific stylesheets are often employed.
The Cirilo Client will be made available as an open source software project, including documentation, as a contribution of the Centre for Information Modeling - Austrian Centre for Digital Humanities to DARIAH-AT in 2014.
References

DARIAH-EU: www.dariah.eu [2013-10-28]
DFG-Viewer: dfg-viewer.de/ueber-das-projekt [2013-10-28]
Fedora Commons: www.fedora-commons.org [2013-10-28]
Google Maps: maps.google.at [2013-10-28]
Geisteswissenschaftliches Asset Management System, AMS for the Humanities: gams.uni-graz.at [2013-10-28]
Carl Lagoze, Sandy Payette, Edwin Shin, Chris Wilper, Fedora (2005). An Architecture for Complex Objects and their Relationships. arxiv.org/ftp/cs/papers/0501/0501012.pdf [2013-10-28]
Versioning Machine: v-machine.org [2013-10-28]
Voyant Tools: voyant-tools.org [2013-10-28]

1. Introduction

The English Scientific Text Corpus (SciTex) consists of about 5000 scientific papers with about 34 Mio tokens in two time slots, 1970/80s and 2000s 1, 2. It has been compiled to investigate the construal of scientific disciplinarity, in particular, how interdisciplinary contact disciplines emerge from their seed disciplines. Both time slots consist of nine disciplines: Computer Science (A) as one seed discipline, Linguistics (C1), Biology (C2), Mechanical Engineering (C3), Electrical Engineering (C4) as the other seed disciplines, and Computational Linguistics (B1), Bioinformatics (B2), Digital Construction (B3), and Microelectronics (B4) as the corresponding contact disciplines between A and C1-C4. The individual articles are subdivided into Abstract, Introduction, Main, and Conclusion.
The orthogonal dimensions time, discipline, and logical structure provide for many, potentially interesting setups of variational analysis: We can explore the diachronic evolution of contact disciplines in comparison to their seed disciplines, variation between contact disciplines and their seed disciplines, and genre variation between abstracts and text bodies in individual disciplines. In this paper we present an approach that combines a macroanalytic perspective 3 with the more traditional microanalytic perspective served by concordance search to explore variation along these dimensions.
2. Approach

2.1. Macroanalysis

For supporting explorative macroanalysis, we use well understood visualization techniques – heatmaps and wordclouds – and combine them with intuitive exploration paradigms – drill down and side by side comparison (see Figure 1). The heatmaps and wordclouds are interactive, allowing for a closer inspection at various levels. The leftmost heatmap visualizes the highest level contrast between abstracts and text bodies in the two time slots (1970s/80s and 2000s). The middle and right heatmaps serve for inspecting a chosen contrast at a lower level at the level of individual disciplines. A particular contrast can be chosen by clicking on the respective square, numbers indicating which contrast is displayed in the middle (Selection 1) and right heatmap (Selection 2). In this example, the middle heatmap visualizes the distances between abstracts and text bodies, and the right heatmap visualizes the distances between text bodies and abstracts.
The wordclouds underneath the heatmaps display the most typical words for a chosen contrast. In Figure 1 the wordcloud to the left visualizes the most typical words for abstracts as opposed to text bodies in the 2000s. Unlike in the common use of wordclouds, the size of words is proportional to their contribution to the distance (as defined in Section 2.2), whereas relative frequency is visualized by color, ranging from purple to red.

Fig. 1: Contrast between Abstracts and Text Bodies
Having a closer look at Figure 1, we can observe that the distance between abstracts is generally larger than the distance between text bodies, and that it has increased in the 30 years period. This general trend is mirrored in the individual disciplines (not shown here). Looking at the middle and right heatmaps, we can see that - not surprisingly - the distance between particular disciplines are at a minimum (squares forming the main diagonal), and the distances among the seed disciplines (A and C corpora), are generally larger than the distances among contract disciplines.
The corresponding wordclouds visualize the most typical words for abstracts (middle heatmap) and for text bodies (right heatmap) in the discipline B1 (Computational Linguistics). In this particular contrast, words typical for abstracts are clearly centered around constructions of exposition (we propose, describe, investigate), main topics of B1 (natural, language (generation), machine translation), words describing the methodology (method, statistical, computational, system) and function words (and, of, on). Words typical for text bodies are markedly different: they comprise B1's main entities of topic elaboration (tokens, nouns, object, vp, john, probability), references (see figure, table, section), conjunctions (when, since, because, if), auxiliary and modal verbs (be, is, was, were, do, will, would, may), and prominently, the determiner the. In summary, abstracts exhibit characteristics of an informationally dense text (e.g., omission of determiners) with topic oriented content. In contrast, text bodies are less dense (determiners, references, modality) and more elaborated.
Other contrastive pairs, such as the synchronic comparison between disciplines or the diachronic comparison of the two time slots, corroborate the results derived by means of computationally much more demanding techniques from machine learning [1], [2].
2.2. Corpus Representation and Distance Measures

The individual corpora are tokenized, and tokens are transformed to lower case. Stopwords are deliberately not excluded to inspect all levels of variation: style, lexico-grammar, and theme. On this basis, corpora are represented by means of unigram language models smoothed with Jelinek-Mercer smoothing, which is a linear interpolation between the relative frequency of a word in a subcorpus and its relative frequency in the entire corpus 4. The distance between two corpora P and Q is measured by relative entropy D, also known as Kullback-Leibler Divergence:
D(P||Q)>=Sum_w p(w)*log_2(p(w)/q(w))
Here p(w) is the probability of a word w in P, and q(w) is its probability in Q. Relative entropy thus measures the average amount of additional bits per word needed to encode words distributed according to P by using an encoding optimized for Q. Note that this measure is asymmetric, i.e., D(P||Q) != D(Q||P), and has its minimum at 0 for P = Q5.
The individual word weights are calculated by the pointwise Kullback-Leibler Divergence 6:
D_w(P||Q) = p(w)*log_2(p(w)/q(w))
For all words the statistical significance of a difference is calculated based on an unpaired Welch t-test on the observed word probabilities in the individual documents of a corpus. This is used for discarding words below a given level of significance (p-value). A more detailed comparison with other measures for comparing corpora 7 is beyond the scope of this paper and will appear in another venue.
2.3. Microanalysis

Wordclouds serve as a bridge between the big distance picture of macroanalysis and microanalyis. To this end, they are seamlessly integrated with the IMS Open Corpus Workbench (CQPWeb: http://cwb.sourceforge.net/index.php), which provides for an expressive corpus query language and several summarization tools, such as collocations and comparative word frequency lists. A click on a word sends a query to CQPWeb, which returns the word in the chosen context. For example, clicking on “do” in the right heatmap (B1 (Txt00) vs. B1 (Abs00)) generates the following query shown in Figure 2.

Fig. 2: Concordance for “do” in B1, text bodies, 2000s
This query returns a concordance for “do” in the 2000s slot of SciTex constrained to subcorpus B1 and to the divisions Introduction, Main, and Conclusion. Based on this list one can inspect the larger context of individual hits and get a ranked list of collocations to distinguish the uses of “do” as an auxiliary vs. main verb.
3. Related Work

The need for combining macroanalysis with microanalysis is well recognized in the DH community 8, 9, and there does exist a variety of frameworks with similar goals. Due to space restrictions, we can only give an exemplary selection; for a comprehensive overview see TAPoR 2.0 (http://tapor.ca/). The MONK workbench 10 allows to compare pairs of corpora using Dunning's log-likelihood ratio 11 for word weighting. Apart from the different distance measure, the main difference of our approach is that we combine the macro perspective of overall distance with the micro perspective of individual word weights to allow for an explorative analysis of variation. The Voyant Tools 12 provide a plethora of text visualizations, including word clouds, cooccurrences, and word trends based on frequencies. The focus of these tools, however, lies on summarizing and visualizing one text or corpus, rather than on exploring variation among corpora. Finally, the TXM platform 13 integrates the IMS Corpus Workbench with some macroanalysis R packages such as factorial correspondence analysis, contrastive word specificity, and cooccurrence analysis. While this integration certainly provides a broader set of analysis techniques, it is arguably more complicated to use than the system presented in this paper.
4. Summary and Future Work

We have presented a system that combines macroanalysis with microanalysis to explore language variation, and briefly illustrated its use for analyzing differences along the dimensions time, discipline, and genre in a corpus of scientific text. Future work will be devoted both to technical as well as methodological enhancements. A useful technical extension is the facility to interactively group subcorpora to larger units, maybe with the help of hierarchical clustering based on the distance matrix to form meaningful groups. More generally, the support for importing external corpora and exporting distance matrices and word weights for analysis with other tools is desirable – the presented system has been evaluated based on a number of corpora, but the underlying processing pipeline certainly needs to be generalized and improved. On the methodological side the main challenge lies in supporting a broader variety of feature sets beyond simple unigram language models. This includes latent language models such as topic models 14 and hidden markov models 15, but also enriched representations such as part-of-speech tagging, and other extensions of unigram models. Such richer feature sets allow to focus analysis by means of feature selection, but also bear new challenges in measuring and visualizing the contribution of features to a contrast at hand, and translating features into meaningful queries against the underlying corpus.
References

1. Elke Teich and Peter Fankhauser (2010). Exploring a Corpus of Scientific Texts using Data Mining. In S. Gries, S. Wulff, and M. Davies, editors, Corpus-linguistic applications: Current studies, new directions, pp. 233–247. Rodopi, Amsterdam and New York.
2. Stefania Degaetano-Ortlieb, Hannah Kermes, Ekaterina Lapshinova-Koltunski, and Elke Teich (2013). SciTex: A diachronic corpus for analyzing the development of scientific registers. In Paul Bennett, Martin Durrell, Silke Scheible, and Richard J. Whitt, editors, New Methods in Historical Corpus Linguistics, Corpus Linguistics and Interdisciplinary Perspectives on Language (CLIP), Volume 3, Narr, Tübingen.
3. Matthew L. Jockers (2013). Macroanalysis: Digital Methods & Literary History. University of Illinois Press, Urbana, Chicago, and Springfield.
4. Chengxiang Zhai and John Lafferty (2004). A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems (TOIS), 22(2):179–214.
5. David J. C. MacKay (2002). Information Theory, Inference & Learning Algorithms. Cambridge University Press, New York, NY, USA.
6. Takashi Tomokiyo and Matthew Hurst (2003). A language model approach to keyphrase extraction. Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment (MWE '03), Vol. 18, Association for Computational Linguistics, Stroudsburg, PA, USA, pp. 33–40. DOI=10.3115/1119282.1119287 dx.doi.org/10.3115/1119282.1119287
7. Adam Kilgarriff (2001). Comparing Corpora. International Journal of Corpus Linguistics, 6(1):97–133.
8. Michael Correll and Michael Gleicher (2012). What Shakespeare Taught Us About Text Visualization. IEEE Visualization Workshop Proceedings, 2nd Workshop on Interactive Visual Text Analytics: Task-Driven Analysis of Social Media Content, Seattle, Washington, USA, Oct 2012.
9. Matthew L. Jockers and Julia Flanders (2013). A Matter of Scale. Staged debate at the Boston Area Days of Digital Humanities Conference at Northeastern University, March 18, 2013. digitalcommons.unl.edu/englishfacpubs/106/
10. John Unsworth and Martin Mueller (2009). The MONK Project Final Report. Sep 2009. www.monkproject.org/MONKProjectFinalReport.pdf
11. Ted Dunning (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics 19(1):61–74.
12. Stéfan Sinclair, Geoffrey Rockwell and the Voyant Tools Team (2012). Voyant Tools (web application). http://voyant-tools.org/
13. Serge Heiden (2010). The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme. Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation, Institute for Digital Enhancement of Cognitive Development, Waseda University, Japan, Nov 2010, pp. 389-398.
14. David. M. Blei, Andrew Y. Ng, and Michael I. Jordan (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.
15. Sharon Goldwater and Tom Griffiths (2007). A fully Bayesian approach to unsupervised part-of-speech tagging. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL'07). Association for Computational Linguistics, Prague, Czech Republic, June 2007, pp. 744–751. www.aclweb.org/anthology/P07-1094

Proposal
Culture, Liberal Arts, and Society Scholars (CLASS) is an undergraduate research and fellowship program in the digital humanities awarded to student scholars at Hamilton College’s Digital Humanities Initiative (DHi). Basic literacies for the digital age are critical skills sets for students entering the professional world in the twenty-first century. The Digital Humanities Initiative provides new opportunities for students in the humanities to become fully engaged citizens in this ongoing digital revolution.

CLASS is based on three-broad areas of scholarly inquiry and their intersection with new and emerging digital technologies: 1) Culture, 2) Liberal Arts, and 3) Society. CLASS provides a unique partnership between departments, programs, and units across the liberal arts and humanities at Hamilton in partnership with the College’s Career Center. It begins with course connections in our Cinema and New Media Studies (CNMS) program but then removes the confines of the semester to promote deep understanding of digital humanities research within a specific field of interest. In these experiences, students and their faculty advisor become part of a collaborative working team of experts in DHi.

CLASS provides students with skills training in digital literacies through intensive research and scholarship coupled with two unique internship experiences. In the summer between sophomore and junior years CLASS offers undergraduate students an intensive professional development experience and provides a comprehensive overview of work in their respective field of interest. In the summer immediately after their junior year students enter their second internship off campus leading to employment and/or graduate study as a result of the eighteen-month program. Assistance with job placement, in a professional field, based on their CLASS internship placement, and/or graduate studies occurs in their final year at Hamilton. 


Fig. 1: Fig. 1. CLASS Program 18-month Structure

The coursework for the program begins in the fall of their sophomore year with CNMS 120 or 125 (Fig 1). In the spring semester students can enroll in courses offered in the CNMS minor. Students enroll in either CNMS 200W/Introduction to Digital Humanities or CNMS300/Interdisciplinary Research Methods that provide experiences writing grant proposals in the digital humanities.

The goals for CLASS include:

Collaboration with potential faculty and/or staff mentors to define and develop an interdisciplinary project
Writing a research proposal for their projects.
DHi committee reviews the proposals and recommend possible award opportunities.
Students begin work over a 10-week period in the summer, mid-June to late August.
A two-week intensive training program takes place in June of the first summer. Students survey mature digital humanities projects, participate in discussions of digital humanities readings, interact with invited speakers brought into the program, and explore technologies related to their research project goals. During the academic year following the first summer, students work with their mentor between 4-6 hours a week on their collaborative research project. In the summer between junior and senior years, CLASS offers undergraduate students an intensive professional development experience and provides a comprehensive overview of work in new digital technologies.

CLASS will:

Develop understanding of digital humanities methods
Develop technological expertise for careers and digital scholarship
Through their participation in an undergraduate research project, students will be able to:

Develop a research question, problem, or design;
Apply basic principles and knowledge found in the literature related to the research question;
Develop a research proposal to address or resolve a specific research question or problem;
Apply and evaluate interdisciplinary methodologies throughout the project;
Collect, interpret, and critique data in order to resolve a research question or evaluate a design;
Utilize digital skills (TEI, digital collection development, media object creation, geospatial visualization, etc.) necessary for robust digital scholarship in the humanities
Communicate research findings through oral presentations and digital publications.
Students in research collaborations with Faculty and members of the DHi, develop deep understanding of a specific long-term research agenda. They are expected to conduct collaborative investigation of a specific aspect of the research that is of great interest to them and integrate digital humanities research methods in their process. Deliverables include public presentation and/or publication at milestones in this process.

Accomplishments:
Cohort 2011

Sarah Bither and Melissa Yang worked with Professor Kyoko Omori to develop an understanding of Benshi performance art for contemporary audiences. The outcome of this work is a website, http://courses.hamilton.edu/dhi-class-1/sarah with components that will ultimately be incorporated into Professor Omori’s Japanese Comparative Literature Archive. Bither continued her study of Japanese culture and language by going abroad to Japan in the spring and summer of 2012. Randall Telfer worked with Professor Thomas Wilson to explore Confucian rituals and connections to contemporary religious practices in China. The outcomes of this work were additional edits to two of Professor Wilson’s websites http://academics.hamilton.edu/asian_studies/home/asc_test/index.html and The Cult of Confucius website: http://academics.hamilton.edu/asian_studies/home/coc_test/index.html. Brynna Tomassone worked with Professor Angel David Nieves to explore cultural connections between South Africa and the United States during the time period leading up to the events in 1976 Soweto. The outcome of this work is a series of book chapters currently in forthcoming publications including The Heritage of Iconic Planned Communities: The Challenges of Change (University of Pennsylvania Press, 2014). Tomassone is now a Ph.D. student in Hispanic/Spanish Studies at Syracuse University. 

 

Cohort 2012

Maxwell Lopez (’14) mentored by Professor Nathan Goodale. Continuing aspects of research on the history and culture of the Sinixt Nation in British Columbia, Lopez proposed to work for the 2012 -2013 year creating, “an accurate three dimensional digital representation of the British Columbia site along with some models of artifacts excavated” using the Unity game engine. He believes the project will create a “new way for people to experience and interact with history” and have great capacity for connecting with research with the public. By the end of the two- week CLASS session in June 2012, Lopez had already made several models in Blender (a 3D modeling software) and site maps in Unity (a virtual world platform for models to reside). The following is a screenshot of his initial construction of a pit house in Blender. Please see the folder on CLASS 2012 for screenshots of his work to date and his complete proposal. 


Fig. 2: Figure 2. Sinixt Ritual Pit House model (created in Blender).

Lopez continued work with Professor Nathan Goodale in GIS mapping and archeological data collection for continued development of Sinixt Ritual Pit (Fig. 2) houses at an archeology Field School in British Columbia summer 2013. Lopez has presented aspects of his work with Professor Goodale at several forums on campus in Fall 2012 and also at the 2013 Re:Humanities symposium April, 5, 2013.

Continuing aspects of O’Neill’s development of Beloved Witness: Agha Shahid Ali Archive, Ujjwal Pradham (’14) will explore the use of text analysis tools and TEI in developing aspects of the Agha Shahid Ali Archive. Pradham is also interested in establishing a connection between the archive and current communities of interest in Kashmir. 

Pradham explored the uses of social software to make connections between contemporary Kashmir communities and the developing Beloved Witness archive. The following is a screen shot of the Voyant Tools (Fig. 3) text analysis Ujjwal did to compare theme words (home, waiting, never, Spring, Kashmir) in multiple manuscripts over the development of a poem written by Ali. 


Fig. 3: Figure 3. Voyant Tools.

Cohort 2013

Working with Religious Studies Professor Abhishek Amar on aspects of his Sacred Centers in India Project, students Kenneth Ratliff (’16) and Alex Gioia (’14), embarked on a study of Indian sacred centers -- Buddhist Bodhgaya and Hindu Gaya. The students expanded their understanding of the Indian sacred cities of Gaya and Bodhgaya. They assisted Professor Amar in organizing his research data for these two cities (images, videos, and GPS coordinates) into the metadata schema for his digital research archive. This work of organizing and processing the over 418 data objects from Gaya into survey forms conducive to further analysis is necessary for long term sustainability of the digital archive. It is also the first step in the creation of interactive models of important artifacts and their locations within these religious sites. Appendix “A” is an example of one of the individual data survey forms used in this project and is based on those used by the K.P. Jayaswal Research Institute, with whom Professor Amar collaborates. Ratliff and Gioia have already begun creating an interactive two-dimensional line map of the Vishnupada complex (Fig 4). 


Fig. 4: Figure 4. Interactive line map of Vishnupada Complex with Mahadeva site highlighted (created in Blender).

This interactive map will link to images of the sites (Fig. 5) and 3D models of the artifacts in situ (Fig. 6). They hope that these virtual 3D and geographically correct models will foster greater interest in these religious sites due to the accessibility and interactivity of the maps, photographs, models, and videos.

Ultimately, the plans are to place the models that they produce into an online viewing space, developed from a game engine (Unity), that will make the models easily viewable and web accessible to the public. 


Fig. 5: Figure 5. Image of Mahadeva site and Tablet artifact on far wall.


Fig. 6: Figure 6. Image of Mahadeva Tablet being modeled in Blender (Free Download at Blender.org).

Working with Patricia O'Neill on aspects of her Beloved Witness archive, Kerri Grimaldi (’16) examined the significance of Emily Dickinson’s poetry to Agha Shahid Ali, a poet from Kashmir, whose work is the focus of the archive. Grimaldi’s project traces the depth of Emily Dickinson’s influence in Shahid’s poem, “A Nostalgist’s Map of America,” by placing Shahid’s poem side-by-side with Dickinson’s “A Route of Evanescence” in four stages of analysis, each increasing in level of explication. By analyzing Shahid’s poem, it is possible to read Dickinson’s in a completely different light, while also witnessing the resonating power of her poetry. Grimaldi has started to create a website to present her analysis of the relationship between the work of Shahid and Dickinson. Ultimately, this website will show not only that Dickinson influenced Shahid’s work, but that his work responded to and interpreted hers, such that their works are in conversation with each other. Please review the current status of this project, including the descriptive first layer of the website, the storyboards exploring intertextuality in the second layer and third layers and a draft of Grimaldi’s own creative video interpretation of the two poems in conversation. This project was submitted in September 2013 to the Dickinson Electronic Archives 2.0: CALL FOR PROPOSALS for volume 3 -- Emily Dickinson’s Reading Culture to be published in 2014. Part One of Kerri's website can be found at http://dhinitiative.org/demos/grimaldi/ 

Challenges
Initial Challenges for DHi in developing CLASS included answering, how do we publish in the digital Humanities? Much of collaborative research includes the use of copyrighted material and/or faculty research that is still in early development. These characteristics of the work in CLASS required that we reconsider the amount and type of information (public or not) conveyed to illustrate the progress of the students. We achieved our goal of facilitating discussion with other scholars in the field by including experts from across disciplines in the two-week intensive training program and through the natural association of collaborators on the faculty research projects. CLASS scholars biographies and research descriptions are announced on the DHi website. CLASS scholars give project prospectus presentations and/or example research projects at the end of their two-week training program in the first summer. These presentations and examples are given to an invited audience for feedback on the projects. Ultimately, each student presents or publishes their work off-campus. Several students have presented at Re:Humanities. 

CLASS Program Summary and Future Plans
Students in research collaborations with Faculty and members of the DHi, have been successful in developing deep understanding of a specific long-term research agenda. DHi provides the immersive experiences and ongoing continuity with faculty research necessary to support this engagement. Most of our CLASS scholars have conducted collaborative investigation of a specific aspect of a long-term research agenda, determined their specific interests and contributions to that agenda, and publicly presented their scholarship “in progress” at Hamilton events and professional conferences.

Our future plans are to continue the CLASS program and to collaborate on its development with other liberal arts schools. Several schools have asked us about building similar models at their schools. This would require thinking more about scaling-up the program. One option we are considering is a form of “Summer Institute” for undergraduates in which we bring them together with their mentors and a larger DH community for a two-week program. 

Appendix A: Proof of Concept for a Sustainable Digital Humanities Faculty Collection Infrastructure in the Liberal Arts. 


DHi’s technology infrastructure and research support model is designed to be sustainable. That is, our approach will reduce the need for regular revamping of static faculty research web pages by creating infrastructure and processes that maintain research outcomes as “living” web presences accessible for faculty and student collaborative scholarship over time. To this end we researched best practices in digital collection development and preservation in collaboration with members of our library and decided to develop an institutional warehouse (repository) for digital collections (Fedora Commons). Fedora was chosen for its scalability and ability to be extremely flexible in the way objects can be accessed. Fedora has built-in flexibility to allow creation and maintenance of relationships among objects and across digital collections over time.

After researching open source collaborative tools to interface with collections in Fedora Commons we decided to make use of Islandora. Islandora can be used to create customized themes for faculty collections and projects. Our DHi Collection Development Team is working with the Islandora and Fedora Commons consultants (at Discovery Garden to create our digital scholarship infrastructure. By using experts to help with development we are making efficient use of the Mellon Grant to move this complex project forward. 

2013 Appendix B: KPJR Form Vishnupada Complex Mahadeva Temple 
DOCUMENTATION SHEET OF BUILD HERITAGE/SITE N.M.M.A., ARCHAEOLOGICAL SURVERY OF INDIA

COMPILED AT K.P. JAYASWAL RESEARCH INSTITUTE, PATNA

Sl.No.	 Documentation Parameters 	 
 	 State/Dist./Block 	Gaya
 1	 Name of the monument/built heritage/site	 Mahdava Temple
 2	 Date/Period	 Early Medieval/Medieval?
 3	 Location 	To east and north of 16 Vedis/Padas in Vishnupada Complex 
 	 Geo-coordinate	 
 4	 Approach	East of the Vishnupada Temple, in the Vishnupada Complex 
 	 Airport	Gaya 
 	 Railway Station	Gaya 
 	 Bus Stand	Gaya 
 5	 Topographical features	Slope of the Mundaprishta hill on the western bank of the Phalgu 
 6	 Brief History	 
Temple seems to have origins in early medieval or medieval period. The exact date of the construction of the temple is difficult to determine because of lack of historical sources. It has images and inscriptions, but they may have been moved. 

 7	Local tradition associated with building/structure/site	Gaya Shraddha, place of Pinda-dana as well as Darshana 
 8	 Architectural style	 Inner sanctum, which has a Linga, and there are 20 pillars, which constitute the Mandapa
 9	 Description of the building/structure/site	 
Mahadeva temple: Inner sanctum with a Shiva Linga and a twenty pillared Mandapa. Narasimha Temple (east of Mahadeva): Small rock temple Sarasvati Temple (east of Narasimha): Small rock temple Facing Narasimha are two single chamber shrines. All five are treated as one unit in the Vishnupada complex. 

 10	 Building/Structural material and other	Stone and brick; pillars are stone 
11 	Usage(s) 	Active worship, Darshana 
12 	Ownership 	Same as Vishnupada Main Temple 
13 	Protection status 	Good 
14 	Present condition 	Maintained 
15 	Conservation assessment 	Alright 
16 	Photographs 	See attached 
17 	Plan/elevation, if available 	 
18 	Published references 	 
19 	General Remarks 	There is an inscription 
20 	Name and address of compiler with date elements used 	Abhishek Singh Amar Matthew Sayers 
Images: 

Mahadeva Temple (129):

By opening

1. Vishnu, 16'' (122)

2. Camunda, 16'' (120, 121)

There's no logic to the temple - they have plastered images all over the place; normally, you would not see Bishnu and Camunda next to each other, but in this case we do, in a disorganized fashion. For instance, they are in the same niche, but Camunda is platered higher than Vishnu, which speaks to the disorganization of the collection process. 
South wall

3. Ganesha, 16'' (122)

4. Inscription (123-125)

Appears to be painted more recently. 
5. Eroded Uma-Maheshvaga, 14'' (126)

Inner sanctum (127)

6. Huge Shiva Linga (127)

North wall

7. Uma-Maheshvara, 16'' (128)

By opening, on the north, west-facing niche; left

8. Durga, 36'' (130)

9. New inscription (130)

Outside, left

10. Dasavatara (131)
In this paper, we theorize about the role of the curator – or perhaps it would be more accurate to say “custodian” or even “collection designer” – in preparing electronic texts for use in an online digital environment. This scholarly role is becoming increasingly important to new forms of knowledge dissemination as a result of the growth in aggregating or mashing up existing digital content. The goal of these aggregations is to add value for particular purposes, as seen in initiatives such as the Journal of Digital Humanities, which aims to collect already published materials into quarterly thematic collections, and the related website Digital Humanities Now, which curates weekly the feeds of other digital humanities websites (Digital Humanities Now).

By analogy with the definition of a curator as “The officer in charge of a museum, gallery of art, library, or the like; a keeper, custodian” (“curator,” def. n. 6), the digital scholarly curator performs a similar role with respect to the circulation of digital content, though with a number of significant differences. Gallery or museum curators, for instance, have their own scholarly and professional training and preparation, often with respect to proper handling and display of fine art and other valuable physical objects. There are those who feel that at least some curators should be considered artists themselves (e.g. Ventzislavov). There is also widespread acknowledgment that curators who work with digital materials require a different set of competencies. Melody Madrid, for example, reports a study that resulted in a list of 20, divided into the categories operational and managerial. Digital scholarly curation also harnesses social media technologies (e.g. crowdsourcing, folksonomies) to encourage a new level of user participation in the management and preservation of digital content (Poole). Not quite an editor, but acting as a mediator between the producers of these contents and its audience through such activities as selecting and reframing them, the digital scholarly curator is in a rather unique position. Our discussion considers this role in relation to the Dynamic Table of Contexts (DToC) interface, a generalized tool for the dissemination of digital text that enables the designer of the collection to mediate specifically between the XML encoding of the text and the affordances that such encoding provides in the reading interface (Brown et al.; Dobson et al.).

The Dynamic Table of Contexts (Fig 1) is a joint initiative between the Interface Design research team of the Implementing New Knowledge Environments (INKE) project, the Canadian Writing Research Collaboratory (CWRC), the University of Alberta Press, and the Voyant Tools project. The DToC currently leverages four principal components of a digital text: the actual text of the document, a table of contents, an index, and XML encoding of the document. The goal of the prototype is to provide an online reading environment where the table of contents provides a conventional overview of a book while at the same time incorporating the index terms and XML tags and the text to which they point (Ruecker et al.). The terms and tags can be selected and deselected, providing interactivity between these three means for accessing and navigating the content of the text or collection.


Fig. 1: Dynamic Table of Contexts Interface

The ability to leverage XML markup as part of the navigational interface is a distinguishing feature of the DToC. Customization of that affordance is enabled by what we call the DToC’s “curator mode,” which is distinct from the reading mode in that it allows technically adept superusers to create customized tag lists to serve as navigational aids alongside the index terms, as well as to determine the organization of the table of contents (login is not required, the curated view is expressed through a unique URL). In a print edition, and in particular for anthologies, it is necessary for the editor to decide which of the various alternatives will be used to organize a particular table of contents. For example, a collection of essays might be organized

alphabetically by title
alphabetically by author’s last name
chronologically
by theme
or by some other principle, in order to ensure a certain kind of development or coherence from beginning to end.
A collection of poems, for example, might add organization alphabetically by the first line of the poem, for cases where the poems do not have a title, and other arrangements are also possible, for instance by geographic location, language, or genre. In the case of an instructor preparing a course pack, the arrangement would naturally correspond to the sequence in which the materials will be used in the class. In the history of print, the possibilities for multiple representation of contents were limited.

In addition to the selection and organization of the contents themselves, curators of DToC collections need to make choices with respect to how the encoding works in the interface. Given the more generic and multi-purpose nature of XML encoding, particularly its use to structure a text, curators need to select which tags the DToC interface will display to the reader, and what user-friendly labels to use for those tags. For many purposes (although not all), the structural encoding can be set aside in favor of semantic encoding (when present). For choices among these tags, there is probably some golden mean that’s most appropriate for generic use; the primary benefit of the Dynamic Table of Contexts is to allow variants for more specific uses. Tags that have been rarely used may already be covered by the index, or may be too insignificant to take room on the list. On the other hand, in cases where the tags have been used heavily enough, it may not be useful to include them since it would result in too great a density of hits.

There are also large differences between what kind of curation of tags is required – depending on whether a schema or tagset has instead been applied throughout a collection, such as in the Brown Women Writers Project’s use of the Text Encoding Initiative – as opposed to highly customized versions of the TEI adapted to the needs of a particular text. Finally, the curator is also enabled to label how tags will appear in the interface, so that readers are not asked to decipher cryptic forms such as <biblStruct>, but are instead presented with labels for such tags that are meaningful in the context for which the curated text or collection is being prepared. This functionality is useful in cases where there is a nuanced difference between two similar tags that needs to be conveyed to the readers (Fig 2).


Fig. 2: Dynamic Table of Contents Curator Interface

What this means in terms of the training and qualifications of the DToC curator is that the person needs to hypothesize how the target readers will be dealing with the material, and to use the curatorial functions to customize the view of the text in the DToC interface to meet the anticipated use case(s). In the case of class instructors, to a certain extent the job will be simplified since the course pack has been chosen purposively for the class. In other situations, there may be multiple and possibly conflicting anticipated use cases. The curator also needs to be comfortable enough with XML not only to choose appropriate tags and rename them, but also, if needed, to specify XPath queries to locations in the document that cannot be identified through tag names alone. There will be considerable variation in XML expertise amongst curators, and a previous user study of ours on an earlier version of DToC indicated that a closer relationship to the encoding correlated with more positive experiences of the DToC interface (Dobson et al.).

The paper will frame our understanding of digital scholarly curation in relation to more traditional, historical understandings of curation and representation of contents and will demonstrate the curator mode in the DToC interface. Our previous user studies of the interface found both considerable confusion on the part of users with respect to the role of the encoding or tagging in the DToC (Brown et al.), and, among those who understood it, considerable emphasis on the importance of the XML markup and its ability to shape the reader experience in the interface. As one user said in mousing over the XML Markup pane: “Well, this seems to me the most relevant section, so whoever puts that together is pretty much the wizard in this Oz” (Dobson et al.). Our discussion will incorporate results of the next user study we are conducting on the DToC, with a stress on the curator mode, that will probe these findings which go to the heart of the DToC’s affordances. Our aim will be to gain a fuller understanding of the ways in which users with a range of technical knowledge understand the role of markup in the DToC interface, and their understanding as both readers and curators of the curatorial role. This study will inform our understanding not only of the ways in which reading environments such as the DToC can effectively leverage XML encoding, but more generally of the ways in which the idea of curation is rapidly evolving within the online scholarly environment.

References
Brown, Susan, Brent Nelson, Stan Ruecker, Stéfan Sinclair, Nadine Adelaar, Ruth Knechtel, Jennifer Windsor and the INKE Research Group. “Text Encoding, the Index, and the Dynamic Table of Contexts.” Paper presented at the annual Digital Humanities conference (DH2013), Lincoln, Nebraska. July 16-19, 2013

“Curator.” Def. n. 5. The Oxford English Dictionary. 2013. OED Online. Web. 1 Nov. 2013.

Digital Humanities Now. http://digitalhumanitiesnow.org/

Dobson, Teresa, Brooke Heller, Stan Ruecker, Milena Radzikowska, Mark Bieber, Susan Brown, and the INKE Research Group. “The Dynamic Table of Contexts: User Experience and Future Directions.” Paper presented in the panel “Designing Interactive Reading Environments for the Online Scholarly Edition” at the DH2012 conference, Hamburg, Germany. July 16-20, 2012.

Kholeif, O. The Curator’s New Medium. Art Monthly. February 2013, (363):9-12. Ipswich, MA.

Little, G. Thinking Like Curators. Journal of Academic Librarianship, 2013, 39(2), 123-125. doi:10.1016/j.acalib.2013.01.003

Madrid, Melody M. A study of digital curator competences: A survey of experts. The International Information & Library Review, Volume 45, Issues 3–4, December 2013, pp 149-156.

Poole, Alex H. “Now is the Future? The Urgency of Digital Curation in the Digital Humanities.” Digital Humanities Quarterly 7:2 (2013).

Ruecker, Stan and the INKE Research Group. “Introducing the Dynamic Table of Contexts for Scholarly Editions.” Paper presented at the Modern Language Association (MLA) Conference. Los Angeles, CA. Jan 6-8, 2011.

Ruecker, Stan, Susan Brown, Milena Radzikowska, Stéfan Sinclair, Thomas M. Nelson, Patricia Clements, Isobel Grundy, Sharon Balasz, and Jeff Antoniuk. “The Table of Contexts: A Dynamic Browsing Tool for Digitally Encoded Texts.” In The Charm of a List: From the Sumerians to Computerised Data Processing. Ed. Lucie Dolezalova. Cambridge: Cambridge Scholars Publishing, 2009. pp. 177-187.

Ventzislavov, R. (2014), Idle Arts: Reconsidering the Curator. The Journal of Aesthetics and Art Criticism, 72: 83–93. doi: 10.1111/jaac.12058
>Contexts for Humanities Visualization
Literary scholars are increasingly turning to graphical display of humanistic information as a way to encounter texts in new and engaging ways. Digitally facilitated humanities visualization presents literary critics with opportunities for new insight1, while also opening practitioners to charges of wholesale importation of simplistic scientific methodologies.2 This poster session outlines the rationale, method, and significance of a focused humanities visualization project in order to demonstrate how new techniques of visualization may be undertaken with literary texts to produce new and speculative “aesthetic provocations” in literary studies.3

Materials and Scope
In order to model and theorize how such a project might develop, I am producing a network visualization of the sixteenth-century play Ralph Roister Doister by Nicholas Udall. Using the open-source "interactive visualization and exploration" platform Gephi, I map the relationships between characters in the play based upon dialogue. In other words, this project structurally maps dialogue between characters, producing a network visualization that productively reconfigures the play to provoke new analytical responses. Following the example of Franco Moretti’s work with Hamlet, these models prompt new insight into the “deep structures” of literary works. For Moretti, however, “the most important thing of all” about these reconfigurations is that they can be manipulated: “one can intervene on a model; make experiments.”4 [116, italics in original]. With a baseline visualization of dialogue established, I, following Jerome McGann, selectively intervene and “deform” the re-modeled text to continually reconfigure it. Such processes bring us “to a critical position in which we can imagine things about the texts that we didn’t and perhaps couldn’t otherwise know.”5 [116] Motivated by Drucker and Nowviskie’s call to “engage computing to produce new aesthetic provocations,” I use Roister Doister to understand how humanities visualization may reconfigure our approach to literary inquiry.6 

Methods of Production
In order to map the dialogic relationships between these characters in Gephi, I have created each character as a node in the network; these nodes are connected by directional dialogue originating at a particular node and terminating at another. Thus, the main characters Roister Doister and Merygreeke are nodes 1.0 and 2.0, for example. Within Gephi’s data manipulation environment, a line of dialogue from the former to the latter would be mapped visually as a line [or an “edge”] from node 1.0 to node 2.0. Each exchange is recorded as tabled data in Gephi, which is then used to produce visualizations. Visualizations can be produced for any discrete unit of the text, including scene, act, or the entirety of the play. This project produces visualizations of each of these divisions for comparative purposes. Following Moretti and McGann, I have undertaken selective deformations by, for example, removing various characters at different points, thereby revealing their network centrality. Criticism of Roister Doister has, for the most part, focused heavily on the play’s dramaturgical debt to the classical comedies of Terence and Plautus, the extent to which those formal structures were successfully integrated with “native” elements of English drama, and the play’s debts to the miles gloriosus [“braggart-soldier”] tradition of Classical comedy.78 This project is in part an attempt to revitalize an ossified critical conversation as an example of how new techniques can vigorously re-engage old texts. 

Significance of Anticipated Visualizations
Experimentation in visualization of textual works is a timely one. As can be seen from the growing use of the Voyant suite9 of analysis and visualization tools as well as the popular Mapping the Republic of Lettersproject,10 humanities visualization is a growing area of scholarly concern. Elijah Meeks has argued that “the shift from creating, annotating and analyzing archives to modeling systems can have a profound impact beyond the [admittedly high value of] usability of scholarly material developed during a digital humanities project.”11 [italics mine]. When humanities scholars have reached a certain point of visual literacy, we will begin to engage with such models in profoundly important ways. Indeed, these models may “provide a much more nuanced form of knowledge transmission than the raw datasets or interactive and dynamic applications typically presented as the future of digital scholarly media.”12 This project is an effort to explore how these new forms of knowledge transmission and analysis might impact literary inquiry. 

References
1. Manovich, Lev. (2010). What Is Visualization? Poetess Archive Journal 2(1), n. pag.

2. Drucker, Johanna. (2011). Humanities Approaches to Graphical Display. Digital Humanities Quarterly 5(1), n. page.

3. Drucker, Johanna, and Bethany Nowviskie. (2004). Speculative Computing: Aesthetic Provocations in Humanities Computing. Companion to Digital Humanities. In Schreibman, Susan, Siemens, Ray, & Unsworth, John (eds). Oxford, Blackwell. Accessed 03 April 2012. Available at [journals.tdl.org/paj/index.php/paj/article/view/19/58]

4,5. Moretti, Franco. (2011). Network Theory, Plot Analysis. Stanford, Stanford Literary Lab.

6. Drucker, Johanna, and Bethany Nowviskie. (2004). Speculative Computing: Aesthetic Provocations in Humanities Computing. Companion to Digital Humanities. In Schreibman, Susan, Siemens, Ray, & Unsworth, John (eds). Oxford, Blackwell. Accessed 03 April 2012. Available at [journals.tdl.org/paj/index.php/paj/article/view/19/58]

7. Boas, Frederick S. (1933). An Introduction to Tudor Drama. Oxford, Clarendon.

8. Brooke, C. F. Tucker. (1911). The Tudor Drama: A History of English National Drama to the Retirement of Shakespeare. Boston, Riverside.

9. Voyant Tools. (2012). [voyant-tools.org]. Accessed October 2012.

10. Mapping the Republic of Letters: Navigating Big Data from the Early Modern Period. (2012). Available at [republicofletters.stanford.edu].

11. Meeks, Elijah. (2011). More Networks in the Humanities or Did Books Have DNA? Digital Humanities Specialist. Published 6 December 2011. Accessed 17 April 2012.

12. Meeks, Elijah. (2011). More Networks in the Humanities or Did Books Have DNA? Digital Humanities Specialist. Published 6 December 2011. Accessed 17 April 2012.
How have text analysis tools in the humanities been imagined in the past? What did humanities computing developers think they were addressing with now dated technologies like punch cards, printed concordances and verbose command languages? Whether the analytic functionality is at the surface, as with Voyant Tools, or embedded at deeper levels, as with the Lucene-powered searching and browsing capabilities of the Old Bailey, the web-based text analysis tools that we use today are very different from the first tentative technologies developed by computing humanists. Following Siegfried Zieliniski's exploration of forgotten media technologies, this paper will look at three forgotten text analysis technologies and how they were introduced by their developers at the time. Specifically we will:

Discuss why is it important to recover forgotten tools and the discourse around these instruments,
Look at how punch cards were used in Roberto Busa’s Index Thomisticus project as a way of understanding data entry,
Look at Glickman’s ideas about custom card output from PRORA, as a way of recovering the importance of output,
Discuss the command language developed by John Smith for interacting with ARRAS, and
Conclude with a more general call for digital humanities archaeology. 
Zieliniski and Media Archaeology
Siegfried Zielinski, in Deep Time of the Media, argues that technology does not evolve smoothly and that we therefore need to look at periods of intense development and then look at the dead ends that get overlooked to understand the history of media technology. In particular he shows how important it is to look at technologies that are not in canonical histories as precursors to “successful” technologies, because they provide insight into the thinking at the time. A study of forgotten technologies can help us understand opportunities and challenges as they were perceived at the time and on their own terms rather than imposing our prejudices. From the 1950s until the early 1990s there was just such a period of technology development around mainframe and personal computer text analysis tools. The tools developed, the challenges they addressed, and the debates around these technologies have largely been forgotten in an age of web-mediated digital humanities. For this reason we recover three important mainframe projects that can help us understand how differently data entry, output and interaction were thought through before born- digital content, output to wall-sized screens, and interaction on a touchscreen. 

Busa and Tasman on Literary Data Processing 
The first case study we will present is about the methods that Father Busa and his collaborator Paul Tasman developed for the Index Thomisticus (Busa could hardly be considered a forgotten figure, but he's often referred to metonymically as a founder of the field, with relatively little attention paid to the specifics of his work and his collaborations). Busa, when reflecting back on the project justified his technical approach as supporting a philological method of research aimed at recapturing the way a past author used words, much as we want to recapture past development. He argued in 1980 that, “The reader should not simply attach to the words he reads the significance they have in his mind, but should try to find out what significance they had in the writer’s mind.” (Busa 1980, p. 83) Concordances could help redirect readers towards the “verbal system of an author” or how the author used words in their time and away from the temptation to interpret the text at hand using contemporary conceptual categories. Concording creates a new text that shows the verbal system, not the doctrine.

Busa’s collaborator Paul Tasman, however, presents a much more prosaic picture of their methodology that focuses on data entry using punch cards so you can actually get concordances of words. He published a paper in 1957 on “Literary Data Processing” in the IBM Journal of Research and Development that focuses on how they prepared their texts accounting for human error and other problems. Tasman writes, “It is evident, of course, that the transcription of the documents in these other fields necessitates special sets of ground rules and codes in order to provide for information retrieval, and the results will depend entirely upon the degree and refinement of coding and the variety of cross referencing desired.” (p. 256) This case study takes us back to a forgotten set of problems (representing text using punch cards) which led to more mature issues in text encoding. In the full presentation we will look closely at the data entry challenges faced by Busa’s team and how they were resolved with the card technology of the time. 

Glickman and Stallman on Printed Interfaces
The second case study we will look at is the development of the PRORA programs at the University of Toronto in the 1960s. PRORA was reviewed in the first issue of CHUM and with the publication of the Manual for the Printing of Literary Texts and Concordances by Computer by the University of Toronto Press in 1966 is one of the first academic analytical tools to be formally published in some fashion. What is particularly interesting, for our purposes, is the discussion in the Manual of how concordances might be printed. Glickman had idiosyncratic ideas about how concordances could be printed as cards for 2-ring binders so that they could be taken out and arranged on a table by users. He was combining binder technology with computing to reimagine the concordance text. Today we no longer think about output to paper as important to tools, and yet that is what the early tools were designed to do as they were not interactive. We will use this case study to recover what at the time was one of the most important features of a concording tool – how it could output something that could be published for others to use. 


Fig. 1: Example of PRORA output from the Manual

Smith and Interaction
One of the first text analysis tools designed to support interactive research was John Smith’s ARRAS. In ARRAS Smith developed a number of ideas about analysis that we now take for granted. ARRAS was interactive in the sense that it was not a batch program that you ran for output. It could generate visualizations and it was explicitly designed to be part of a multi-tasking research environment where you might be switching back and forth between analysis and word processing. Many of these ideas influenced the interactive PC concordancing tools that followed like TACT. In this paper, however, we are not going to focus on all the prescient features of ARRAS, but look at the now rather dated command language which Smith was so proud of. Almost no one uses a command language for text analysis any more; we expect our tools to have graphical user interfaces that provide affordances for direct manipulation. If you need to do something more than what Voyant, Tableau, Lucene, Gephi or Weka let you do, then you learn to program in a language like R or Python. John Smith by contrast, spent a lot of time trying to design a natural command language for ARRAS that humanists would find easy to use and this comes through in his publications on the tool (1984 & 1985). Command languages were, for a while, the way you interacted with such systems and attention to their design could make a difference. Smith tried to develop a command language that was conversational so humanists could learn to use it to explore “vast continents of literature or history or other realms of information, much as our ancestors explored new lands.” (Smith 1984, p. 31) Close commanding for distant reading. 

Conclusions
In the 2013 Busa Award lecture Willard McCarty called us to look to our history and specifically to look at the “incunabular” years before the web when humanists and artists were imagining what could be done. One challenge we face in reanimating this history is that so much of the story is in tools, standards and web sites – instruments difficult to interrogate the way we do texts. This paper looks back at one major thread of development - text analysis tools – not for the entertainment of outdated technology, but recover a way of thinking about technology. We will conclude by discussing other ways back including the need for better documentation about past tools, along the lines of what TAPoR 2.0 is supporting, and the need to preserve tools or at least a record of their usage. 

References
Busa, R. (1980). "The Annals of Humanities Computing: The Index Thomisticus." Computers and the Humanities. 14(2): 83-90.

Glickman, Robert Jay, and Gerrit Joseph Staalman. Manual for the Printing of Literary Texts and Concordances by Computer. Toronto: University of Toronto Press, 1966.

Liu, Alan. (2012) “Where is Cultural Criticism in the Digital Humanities.” In Debates in the Digital Humanities. Ed. Matthew K. Gold. University of Minnesota Press. Liu’s essay is online at <http://dhdebates.gc.cuny.edu/debates/part/11>.

Smith, J. B. (1978). "Computer Criticism." STYLE XII(4): 326-356.

Smith, J. B. (1984). "A New Environment For Literary Analysis." Perspectives in Computing 4(2/3): 20-31.

Smith, J. B. (1985). Arras User's Manual: TR85-036. Chapel Hill, NC, The University of North Carolina at Chapel Hill.

Tasman, P. (1957). "Literary Data Processing." IBM Journal of Research and Development 1(3): 249-256.

Zieliniski, Siegfried. (2008) Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. Cambridge, Massachusetts: The MIT Press.


    
        
            
                DREaM: Distant Reading Early Modernity
                
                    
                        Wittek
                        Stephen
                    
                    McGill University, Canada
                    stephen.wittek@mcgill.ca
                
                
                    
                        Sinclair
                        Stéfan
                    
                    McGill University, Canada
                    stefan.sinclair@mcgill.ca
                
                
                    
                        Milner
                        Matthew
                    
                    McGill University, Canada
                    matthew.milner@mcgill.ca
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    early modern
                    eebo
                    voyant
                    topic modeling
                    distant reading
                
                
                    archives
                    repositories
                    sustainability and preservation
                    corpora and corpus activities
                    literary studies
                    content analysis
                    bibliographic methods / textual studies
                    interdisciplinary collaboration
                    digital humanities - pedagogy and curriculum
                    english studies
                    renaissance studies
                    media studies
                    data mining / text mining
                    English
                
            
        
    
    
        
            Our proposed paper will provide an overview of the theory and methodology driving the creation of Distant Reading Early Modernity (DREaM), a digital humanities project that has made a massive corpus of early modern texts amenable for use with macro-scale analytical tools. Key focus areas include the technical challenges deriving from non-standardized spelling, the philosophy of our tutorial program, the argument for our approach to the early modern archive, and the potential benefit to early modern scholarship of distant reading techniques. 
            From Microfilm Library, to EEBO, to EEBO-TCP, to DREaM 
            The foundational work for DREaM began in 1934, when Eugene B. Power used parts of two movie and still cameras to create one of the world’s first microfilm bookcameras, a device he used to photograph thousands of texts in British libraries (Anderson and Power, 1990). In 1998 Power’s microfilm library became the basis for Early English Books Online (EEBO), a database that comprises the images for some 125,000 texts from 1475 to 1700, and has profoundly expanded the horizons of early modern research.
                1 To date, approximately one-third of the documents on EEBO are available as transcribed, full-text editions. Researchers for the EEBO Text Creation Partnership (EEBO-TCP) are currently working to transcribe the remaining 85,000 documents, which are as yet only available as digitized microfilm images.
                2
            
            Although completion of the transcription work is still at least 10 years in the future, the prospect of a full-text library of all documents from the first 225 years of English print points to the need for some careful re-thinking about the relation between scholarship and archival sources. As it now stands, the EEBO-TCP corpus amounts to 8.02 gigabytes of XML-encoded text and contains nearly 45,000 documents, for a grand total of well over a billion words (1,155,264,343 by our count). Confronted by the sheer expanse of a corpus several magnitudes larger than anything one could hope to read in a lifetime, early modern scholarship must now work to incorporate digital methodologies that enable a bird’s-eye view of large corpora, an approach that Franco Moretti has dubbed ‘distant reading’ (Moretti, 2007). DREaM has begun the work of making such a view possible. 
            Unlike EEBO, DREaM enables batch downloading of custom-defined subsets rather than obliging users to download individual texts on a one-by-one basis. In other words, it functions at the level of ‘sets of texts’ (sometimes called 
                worksets) rather than ‘individual texts’. Examples of subsets one might potentially generate include ‘all texts by Ben Jonson’, ‘all texts published in 1623’, or ‘all texts printed by John Wolfe’. A user-friendly interface makes subsets available as either plain text or XML-encoded files, and gives users the option to automatically name individual files by date, author, title, or combinations thereof (this file naming flexibility can be useful when interoperating with other tool suites). 
            
            The ability to generate custom-defined subsets is important because it allows researchers to explore the early modern canon with distant reading techniques, and to capture otherwise intractable data with visualizations such as graphs, charts, or other forms of graphic representation. On this note, another key feature of DREaM is that it allows users to transfer specially tailored subsets directly to the analytic interfaces of Voyant Tools (voyant-tools.org), a suite of textual visualization tools that collectively constitute the leading platform for open-access digital humanities research.
                3 In fact, DREaM is actually implemented within Voyant Tools (version 2.0, not yet released, which provides much better support for very large text collections). DREaM thus provides a compelling example of a bridge between massive full-text repositories (that typically provide faceted searching) and more specialized analytic and visualization environments. By enabling simple transference between the EEBO-TCP archive and Voyant, DREaM has significantly expanded the range and sophistication of technologies currently available to researchers who wish to gain a broad sense of printed matter in early modern England. 
            
            Notably, however, DREaM does not aim to replace EEBO, or to supplant conventional forms of research. Rather, our goal is to simply add a new item to the scholar’s toolbox, and to increase transferability between distant reading methodologies and more fine-grained forms of analysis. 
            
                Negotiating the Complexities of Non-Standardized Spelling
            
            Standardized spelling had yet to emerge in early modernity: writers had the freedom to spell however they pleased. To take a famous example, the name ‘Shakespeare’ has 80 different recorded spellings, including ‘Shaxpere’ and ‘Shaxberd’. As one might imagine, variance on this scale presents a serious challenge for large-scale textual analysis. How is it possible to track the incidence of a specific word, or group of words, if any given word could have an unknown multiplicity of iterations? 
            To address this problem, we enlisted the assistance of VARD 2, a tool that helps to improve the accuracy of textual analysis by finding candidate modern form replacements for spelling variants in historical texts.
                4 As with conventional spellcheckers, a user can choose to process texts manually (selecting a candidate replacement offered by the system), automatically (allowing the system to use the best candidate replacement found), or semi-automatically (training the tool on a sample of the corpora). 
            
            After some preliminary training, we ran the TCP-EEBO corpus through VARD using the default settings (auto normalization at a threshold of 50%). Rather than using the ‘batch’ mode—which proved unreliable for such a big job—we wrote a script that normalized the texts on a one-by-one basis from the command-line. This process took about three days on a commodity machine. VARD normalized 80,676 terms for a grand total of 44,909,676 changes overall. 
            A careful check through the list resulted in 373 term normalizations that we found problematic in one way or another. The problematic normalizations amounted to 462,975 changes overall, or only 1.03% of the total number of changes. These results were satisfactory: our goal was not to make the corpus ‘perfectly normalized’ (an impossibility, not least because perfection is debatable in this context), but, more pragmatically, to make it generally normalized, which is the best one can reasonably expect from an automatic process. On this point, it is important to note that VARD encodes a record of all changes within the output XML file, so scholars will be able to see if the program has made an erroneous normalization. 
            Some of the problematic VARD normalizations seem to have derived from a dictionary error. For example, ‘chan’ became ‘champion’ and ‘ged’ became ‘general’. In other instances, the problematic normalizations were ambiguous or borderline cases that we preferred to simply leave unchanged. Examples include ‘piece’ for ‘peece’, and ‘land’ for ‘iland’. There were also cases where the replacement term was not quite correct: ‘strawberie’ became ‘strawy’ rather than ‘strawberry’, and ‘hoouering’ became ‘hoovering’ rather than ‘hovering’. We fixed as many of these kinks as we could by making adjustments to the VARD training file and running the entire corpus through the normalization process a second time. 
            Of course, it is not difficult to imagine scenarios wherein a researcher may prefer to work with original spellings rather than normalized texts. With such projects in mind, we have kept both normalized and non-normalized versions of the EEBO-TCP corpus. 
            
                The DREaM Tutorial Program
            
            As noted above, one of the central objectives of DREaM is to create an interface that will maximize user-friendliness, allowing scholars with a minimal level of technical expertise to quickly and efficiently create subsets tailored for whatever specific research question they wish to pursue. We are building DREaM for our own research, but we also have a much broader pedagogical perspective in mind. To meet this objective, we have launched a pilot tutorial program, currently under way, that will teach scholars how to use DREaM, but will also point to ways in which DREaM could more effectively serve the demands of scholarly investigation. 
            In a series of tasks that build toward the production of a short case-study report, pilot users must articulate a detailed research question and provide a description of their argument. In addition to establishing a valuable feedback loop for the project, this assignment aims to nudge new users toward a more comprehensive, more practical understanding of how macro-scale textual analysis can complement scholarly practice. The key conceptual challenge, as we see it, hinges on new users’ ability to understand, and learn to negotiate, the gap between distant reading and more conventional means of engaging archival sources. 
            Our pool of pilot users derives from the membership of our parent project, Early Modern Conversions, a five-year interdisciplinary research initiative that has brought together a team of more than 100 scholars, partners, and graduate student associates from universities in Canada, the United States, England, New Zealand, and Australia.
                5 Early Modern Conversions provides a propitious testing ground for DREaM because it is at the vanguard of early modern research, and because it entails a rich diversity of disciplinary approaches. Our presentation for DH2015 will report on the results of the tutorial program and on the progress of the project overall. 
            
            Screenshots 
            
                
            
            Figure 1. The DREaM interface. Search fields in the middle of the screen enable users to define a subset of EEBO-TCP texts by keyword, year, author, and publisher. Below the search field, an ‘Export’ button opens a dialogue box that offers the option of sending the subset directly to Voyant-tools.org, or downloading it as a ZIP archive. Users may also choose to download subsets as either plain text or XML-encoded files. A drag-and-drop mechanism (bottom) enables automatic naming of files within a subset by date, author, title, or combinations thereof. 
            
                
            
            
                
            
            Figure 2. A sample subset transferred to Voyant Tools. Beginning in the top left corner, one sees a word cloud representing the frequency of keywords in terms of font size. At a glance, it shows that the highest frequency words in the subset are ‘good’ and ‘come’. Below the word cloud, there is a summary that lists statistics for basic categories such as word count, vocabulary density, word frequency, etc. In addition, the summary lists words that have a notably high frequency for each year: ‘Rome’ and ‘death’ appeared with particular frequency in 1594, while ‘virtue’ and ‘envy’ stood out in 1612. Moving to the bottom left corner, one sees an ordered list of frequencies for each word in the corpus accompanied by a thumbnail graph that tracks the frequency of words over the 40-year delimitation. At a glance, the tool shows a significant spike for the word ‘knight’ in 1624. In the middle of the screen, a ‘Corpus Reader’ tool enables users to drill down into the corpus to examine the context for particular terms. 
            Notes
            1. See http://eebo.chadwyck.com.
            2. See http://eebo.odl.ox.ac.uk/e/eebo/.
            3. See http://voyant-tools.org.
            4. See http://ucrel.lancs.ac.uk/vard/about/.
            5. See http://earlymodernconversions.com.
        
        
            
                
                    Bibliography
                    
                        Anderson, R. and Power, E. B. (1990). The Autobiography of Eugene B. Power, Founder of University Microfilms. UMI, Ann Arbor, MI.
                    
                    
                        Moretti, F. (2007). Graphs, Maps, Trees: Abstract Models for Literary History. Verso, London.
                    
                
            
        
    



    
        
            
                Talking About Programming in the DIgital Humanities
                
                    
                        Rockwell
                        Geoffrey
                    
                    University of Alberta, Canada
                    geoffrey.rockwell@ualberta.ca
                
                
                    
                        Sinclair
                        Stéfan
                    
                    McGill University
                    stefan.sinclair@mcgill.ca
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    History of DH
                    Programming
                    DH Courses
                
                
                    programming
                    history of Humanities Computing/Digital Humanities
                    English
                
            
        
    
    
        
            In the first months of the HUMANIST discussion list a discussion erupted around the value of programming languages in humanities computing courses. Do computing humanists need to know how to program, or is learning expert use of applications enough? What is the point of learning to program? (McCarthy, 1987). This discussion can be traced back at least as far as the 1986 ACH/Sloan Workshop on Teaching ‘Computers and the Humanities’ Courses, where participants discussed ‘most vigorously of all whether programming should be taught, or only package programs’ (Sperberg-McQueen 1986). The issue has really never gone away as it is about disciplinary formation or ‘what it is we want our students to learn, the nature of the world into which we are sending them, and the relationship both of technology and (more fundamentally) the algorithmic approach to problem-solving’ (McCarty, 1987, 1–2). Based on a reading and analysis of a substantial collection of historical documents (journal articles, association newsletters, books, Listservs, etc.) in this paper we will examine how the humanities computing community discussed programming and how that discussion reflects changing views of the field over time. Specifically we will
             • Outline a three-part history of programming in the digital humanities starting with a concording phase when programming was not important to using computers.
             • Discuss the turn toward teaching programming as a way of teaching computational thinking.
             • Look at how the emergence of the Web changed the discussion. We will argue that as the Web became important, scripting skills suitable for building websites replaced older programming languages and it became possible to imagine the digital humanities being defined by the ability to program. 
             • Conclude with some reflections on how the case for programming has been made more recently. 
            
                Humanities Computing as Concording
            
            I’ve tried to stay free of programming. I’m perfectly innocent of any knowledge of any programming language and I feel I must remain that way if I am going to continue to function as a scholar in literature; my heartfelt advice to any of you younger people embarking on this whole venture is to do the same—to ask programmers to do for you what they know how to do and what would be costly and painful for you to learn—to learn to talk their language but not to get involved in programming research or machine research. I know the best of you will not take this advice and will, therefore, break new barriers and so on, but I persist in offering it. (Parrish, 1969, 24–25) 
            Programming in the early concording years of computing in the humanities was something often left to specialists. Stephen Parrish, who was general editor of the Cornell Concordance Series, saw himself as a ‘scholar in English literature who drifted laterally into the making of concordances’ (1969, 16). He stayed away from programming as a distraction from scholarship. This meant that concording ‘required an organized, teamwork approach, characterized by a substantial budget and a university computing centre’ (McCarty, 1993, 52–53). Parrish is not the only one to worry about programming distracting scholars, and in the presentation we will discuss other examples as a way of teasing out how programming was not considered of scholarly value. 
            
                Programming and Reasoning
            
            By the 1980s, things had changed and programming had become important, as can be seen in the way programming was included in a majority of courses. Surveys like Joseph Rudman’s ‘Teaching Computers and the Humanities Courses: A Survey’ (1987) report 175 courses teaching a programming language while 131 were ‘Applications Only’. In that survey, the most popular languages taught were BASIC (in 75 courses reporting), Pascal (30 courses), Prolog (15 courses), Lisp (13 courses), and SNOBOL (10 courses). What is impressive is the number of responses that indicated they had some sort of course for humanities students. We often assume that computing in the humanities is a recent, post-advent-of-the-Web thing, but given the number of courses, the discussions on HUMANIST, and articles in journals like 
                Computing and the Humanities, it is clear that programming was becoming an acceptable activity, especially for those in support staff positions. 
            
            Despite the popularity of programming languages like BASIC and Pascal for courses, the languages about which humanists wrote in the 1980s tended to be languages like SNOBOL and Icon—all languages that were good for string (text) manipulation, as can be seen in the attention they received in books for teaching programming to humanists like John Abercrombie’s 
                Computer Programs for Literary Analysis (1984)
                , Susan Hockey’s 
                SNOBOL Programming for the Humanities (1985)
                , and Alan Corré’s 
                Icon Programming for Humanists (1990)
                . Then there are the discussions on HUMANIST and reviews of languages like Mark Olsen’s self-explanatory ‘Beyond SNOBOL: The Icon Programming Language’. 
            
            As an aside, one could argue that SGML (Standard Generalized Markup Language) and later XML (Extensible Markup Language) are also forms of programming—meta-languages with which one can create descriptive languages with which to rigorously describe texts for scholarly electronic editions. These were popular in humanities computing, especially after the Text Encoding Initiative began to provide guidelines for the encoding of texts in the late 1980s. How many humanists were first introduced to computing in the humanities when asked to develop a DTD (Document Type Definition) and encode a text? 
            It is interesting that many of the discussions about programming in the 1980s and early 1990s circle around the teaching of it and that this issue is reported as contentious. No one believed that the ability to program was essential for a computing humanist, in part because so many couldn’t, but proponents of programming argued for it to be taught as way of teaching computational reasoning or problem solving. They also argued that there were social and ethical issues that could be understood through learning computing. You learned to program so as to understand what the computer could do or to be able to talk to programmers. In the presentation we will look at summative discussions of the issue like McCarty’s (1987) and a lovely balanced essay by Nancy Ide, ‘Computers and the Humanities Courses: Philosophical Bases and Approach’ that came out of the 1986 ACH/Sloan Workshop. We will also speculate as to why there was this shift towards including programming and arguing for its importance. Whatever the reasons, the discourse had changed and computing humanists were beginning to take an interest in programming and teaching it. 
            
                Programming the Web 
            
            The emergence of the Web changed the languages that humanists were likely to use for programming and, we will argue, made it possible to make programming a defining skill for digital humanists. Moreover, learning to program for the Web provided a more appealing and transferable skillset; web and application development became normalized and desirable. Because the Web provided such a convenient way to show and distribute digital research, it changed which programming languages received attention or were taught. We can see the shift in these histograms of word frequencies in the HUMANIST Archives. The string processing languages popular in the late 1980s taper off and are replaced with languages like PHP with which one can build dynamic websites. 
            
                
            
            
                
            
            Figure 1. Two graphs showing the frequency of programming languages in the annual archives of the HUMANIST Discussion Group Listserv, produced with Voyant Tools. 
            Now programming (and other web skills) went from being about teaching reasoning to being a skill students and humanists could actually use to create humanities products, i.e., websites. Programming became creative and expressive. There was also a convenient on-ramp as students could start creating HTML pages and then learn to use scripting languages like PHP that enhanced the web site until you knew enough to code a scholarly web site. 
            We will further argue that once web development wasn’t something that digital humanists would necessarily be amateurs at, then it could become a defining skill, and a source of some pride. Digital humanists finally had something that they could be good at and a set of competencies unique to the digital humanities. Digital humanists like Stephen Ramsay could provocatively say, ‘Do you have to know how to code? I’m a tenured professor of Digital Humanities and I say “yes”’ (Ramsay 2011). This led to the ‘hack vs. yack’ discussion that followed about programming and other forms of building (see Nowviskie [2014] for more general context). 
            
                Conclusions: Programming in the Humanities and Disciplinary Formation 
            
            This paper traces the ways programming has been discussed in the digital humanities. We believe that the role of programming was important to the way the field of humanities computing (and later digital humanities) conceived of itself. This is not surprising given the importance of programming to computing in general, but it is interesting to follow the particular ways programming was discussed as the discipline emerged. 
            We will end the paper by theorizing, or at least speculating, about where programming in digital humanities is going. One direction (already well under way) is towards software studies and the studying of programmed artifacts as works of human art and expression worthy of the humanities. Matthew Kirschenbaum in a fine essay for the 
                Chronicle of Higher Education talks about ‘procedural literacy’. 
            
            All programming entails world-making, as the ritual act of writing and running Hello World reminds us. . . . 
             Ultimately what’s at stake is not the kind of vocational computer literacy I was taught as an undergraduate, but what a growing number of practitioners in the digital humanities (and related disciplines, like digital art or game studies) have begun to call procedural rhetoric, or procedural literacy. (Kirschenbaum, 2009) 
            Another direction is data science. The opportunities for new insights through large-scale text mining, or what Moretti (2007) calls distant reading, have made programming languages like R attractive. There is a return to text processing languages, but now languages that can analyze large corpora or visualize results.
        
        
            
                
                    Bibliography
                    
                        Ide, N. M. (1987). Computers and the Humanities Courses: Philosophical Bases and Approach. 
                        Computers and the Humanities,
                        21(4): 209–15. 
                    
                    
                        Kirschenbaum, M. (2009). Hello Worlds. 
                        Chronicle of Higher Education,
                        55(20): B10, http://chronicle.com/article/Hello-Worlds/5476. 
                    
                    
                        McCarty, W. (1987). HUMANIST So Far: A Review of the First Two Months. 
                        ACH Newsletter, 
                        9(3): 1–3. 
                    
                    
                        McCarty, W. (1993). Handmade, Computer-Assisted, and Electronic Concordances of Chaucer. 
                        CCH Working Papers,
                        3: 49–65. 
                    
                    
                        Moretti, F. (2007). 
                        Graphs, Maps, Trees: Abstract Models for Literary History. Verso, London. 
                    
                    
                        Nowviskie, B. (2014). On the Origin of ‘Hack’ and ‘Yack’. http://bit.ly/1ttih76.
                    
                    
                        Olsen, M. (1987). Beyond SNOBOL: The Icon Programming Language. 
                        Computers and the Humanities,
                        21(1): 61–66. 
                    
                    
                        Parrish, S. M. (1969). Concordance-Making by Computer: Its Past, Future, Techniques, and Applications. In Burelbach, F. M. (ed.), 
                        Proceedings: Computer Applications to Problems in the Humanities; A Conversation in the Disciplines, Conference at State University College, Brockport, NY, 4–5 April 1969, pp. 16–33. 
                    
                    
                        Ramsay, S. (2011). On Building. http://stephenramsay.us/text/2011/01/11/on-building/. 
                    
                    
                        Rudman, J. (1987). Teaching Computers and the Humanities Courses: A Survey. 
                        Computer and the Humanities, 
                        21(4): 235–43. 
                    
                    
                        Sinclair, S. and G. Rockwell. (2014). 
                        Voyant Tools. http://voyant-tools.org/. 
                    
                    
                        Sperberg-McQueen, C. M.
                         (1986). Report on ACH/Sloan Foundation Workshop on Teaching ‘Computers and the Humanities’ Courses. 
                        ACH Newsletter,
                        8
                        (4): 1–2.
                    
                
            
        
    



    
        
            
                The Trials of Tokenization
                
                    
                        Hoover
                        David L.
                    
                    New York University, United States of America
                    david.hoover@nyu.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Python
                    tokenization
                    word frequency lists
                    programming
                    punctuation
                
                
                    natural language processing
                    software design and development
                    text analysis
                    programming
                    standards and interoperability
                    English
                
            
        
    
    
        
            The process of tokenizing texts is typically out of sight and almost out of mind—often handled invisibly by the analyst’s program or R script, and rarely described, discussed, or even mentioned. For ‘big data’, even if questions did arise about the nature of the word list produced, testing is not feasible. Furthermore, tokenizer accuracy is so critically affected by the state and nature of the texts that probably no general measure of accuracy or appropriateness is possible. Finally, built-in programming functions and libraries are all too often used uncritically with little realization that their output does not conform to the assumptions or expectations of the analyst. I suggest that we should pay a little more attention to the theory and practice of tokenization.
                1
            
            Consider a hypothetical case. Let’s say I want to analyze 5,000 novels, have access to the texts at HathiTrust, download 5,000 novels in plain text, and tokenize them. Below is part of a page from Elizabeth Gaskell’s 
                Cranford, from HathiTrust (Gaskell, 1910 [1851], 107):
            
            
                
            
            Figure 1. 
                Cranford, Elizabeth Gaskell, from page 107.
            
            A human reader would have little trouble tokenizing this passage, and it is not extremely problematic, though minor OCR problems exist (mainly spacing issues around single quotation marks / apostrophes and dashes, and the line-end hyphen). I tokenized this passage with The Intelligent Archive (2012), KWIC (Tsukamoto, 2004) WordSmith Tools (Scott, 2012), Voyant (Sinclair et al., 2012), and Stylo (Eder et al., 2014).
                2 Even on this short text, the five programs identify three different numbers of types and two different numbers of tokens, largely because of the handling of single quotation marks. KWIC and WordSmith produce identical lists, as do Voyant and Stylo, but neither of these match The Intelligent Archive.
            
            Now consider Charles Chesnutt’s 
                The House Behind the Cedars (1900, 13), also from HathiTrust:
            
            
                
            
            Figure 2. 
                The House Behind the Cedars, Charles Chesnutt, from page 13.
            
            The dialect in this passage is challenging even for human readers, and the OCR is more problematic. For example, the printed text (judging from the PDF) had spaced contractions, which explains ‘you 're’ in the fourth line from the bottom and the space in ‘lie 's’ in the first line, where the text reads “he 's.” This classic OCR problem occurs several times in this novel. And in the last line ‘you '11’ has both a space and an erroneous number 11 for the ‘ll’ (double el), another common OCR problem. Those analyzing big data usually rely on the insignificance of random error, but these and many other kinds of error are not random, and systematic error within one text, one author, one genre, or one collection could easily lead to thousands of inaccurate word frequency counts in this hypothetical study of 5,000 texts.
            The use of apostrophes in the Chesnutt passage to indicate dialect pronunciations can also severely affect tokenization. Although The Intelligent Archive, KWIC, and WordSmith Tools produce exactly the same lists for this brief passage, and Voyant has the same number of types and tokens, Voyant removes all initial (but not final) apostrophes, creating different words for eight of the 97 types. Stylo removes all numbers, all initial and final apostrophes, and many internal apostrophes, retaining them only in 
                ain^t, gentleman^s, and 
                spen^s (replaced with a caret). It produces six more tokens and four more types than the other programs, and many more differences in the word list. Unfortunately, in Chesnutt’s short novel, more than 650 words begin and/or end with apostrophes crucial to the identity of the word, so that the word lists produced by Voyant and Stylo are quite inaccurate. Furthermore, only KWIC and WordSmith Tools let the user choose whether apostrophes and hyphens are part of a word, and whether numbers can appear in the word list or not. Only WordSmith Tools allows the user to choose whether to allow apostrophes at the beginnings and/or ends of the word as well as internally.
            
            Obviously, the two texts examined above cause different problems, and different tokenizers are more accurate for one than for the other. Worse yet, these problems are found even in relatively carefully edited texts like those from Project Gutenberg. Although Gutenberg’s 
                The House Behind the Cedars does not have spaced contractions, and correctly has 
                he’s in the first line and 
                you’ll in the final line, the 29 initial and final dialect apostrophes remain problematic. The Gutenberg text also represents dashes as two hyphens without spaces, creating more problems for tokenizers. The Intelligent Archive and Stylo treat these double-hyphen dashes as breaking characters, while retaining single hyphens within compound words, but KWIC, WordSmith Tools, and Voyant treat them like single hyphens, creating compounds with double hyphens where dashes are needed. The situation is still more complex if a double-hyphen is preceded or followed by a breaking character. If this sounds esoteric, consider that this short novel contains nearly 400 double-hyphen dashes (Dickens’ 
                Dombey and Son has more than 2,200). And this problem, too, is highly systematic: words vary considerably in how frequently they are preceded or followed by a dash, and 1,000 dash errors per text would produce 5,000,000 errors in our hypothetical 5,000 novels. (For a practical example of the effects of error, see Matt Jockers’ discussion of topic modeling and several ‘topics’ that arose from OCR error and metadata (Jockers, 2013, 135).
            
            It might seem that we just need more sophisticated tokenizers, but the required level of sophistication to handle double-hyphen dashes correctly is quite high, and the problems caused by apostrophes and single quotation marks cannot be correctly solved computationally at all. In some cases, not even a human reader can tokenize with certainty; in others, a computer can solve problems a human cannot. 
            Let’s consider a few further tokenization questions:
            He said, “That’s ’bout ‘nough, sho’.”
            “That’s ‘bout’, not ‘fight’; ’nough said,” Nough said.
            “John tried that ‘Nough told me to’ on me,” Bill whined.
            He remarked, “John said, ‘Bout starts at nine.’”
            He remarked, “John said, ‘It’s ’bout time.’”
            He remarked, “John said, ‘‘Bout time.’” Can these apostrophes/single quotes be handled correctly computationally? How about the two single quotes before ‘Bout’ in the last example?
            I visited the U.S.S.R. Four tokens? Seven? Is the final period part of the final token?
            I visited the U.S.S.R.! Four tokens? Seven? Is the final period part of the final token?
            Is that C------? Is ‘C------’ the token ‘C’ followed by a dash, or the token ‘C------’? What about ‘C—’? Or ‘C-’?
            C------ is here. Same questions.
            Oh d--n it! Is ‘d--n’ the tokens ‘d’ and ‘n’ separated by a dash, or the token ‘d--n’? How about ‘d---n’? or ‘d-n’? or ‘G-d’?
            I said--never mind. If ‘d--n’ is a token, can we prevent ‘said--never’ from being a token here?
            That’s what I--a mistake, sorry. How do we get ‘d--n’ correct without failing here?
            You’re a real %#@$! Three tokens? Four? Does the last include the final ‘!’? What if there were a period after the ‘!’?
            You’re a real %#@$!. How about now?
            I am working on a Python tokenizer that can handle most of these issues correctly, and some of these problems are fairly rare, but I despair of the possibility of creating a word frequency list that is ‘correct’ even in my own opinion. For many years I have ‘corrected’ the texts before tokenizing, but that is not a practical solution for 5,000 novels and presents its own problems.
            Perhaps in sufficiently big data, the error introduced by tokenizers will not significantly alter the results, and Maciej Eder (2013) has recently shown that some corpora are remarkably resistant to some kinds of intentionally introduced error. And improving the quality of the corpus had a relatively small effect on the attribution of the Federalist Papers (Levitan and Argamon, 2006). More study seems needed before we can be complacent, however, even in large-scale problems involving only authorship or classification. For smaller-scale stylistic studies, tokenization decisions can clearly have serious repercussions. Consider Ramsay’s (2011) analysis of 
                The Waves, where decisions about tokenization significantly alter the lists of men-only and women-only words and words that characterize the six narrative voices (see Hoover [2014a] and Plasek and Hoover [2014], for discussion). Another example that replicates an experience I have had several times is that a Full Spectrum analysis (Hoover, 2014b), based on Craig’s version of Burrows’s Zeta (Burrows, 2007; Craig and Kinney, 2010) can give strange results if uncorrected texts are inadvertently included. For example, in a test of Charlotte Brontë versus Anne and Emily Brontë, 11 of the 100 most distinctive words were words with inappropriate initial “apostrophes” because the novels of Anne and Emily in the analysis both used single quotation marks for dialogue.
            
            Far from being an insignificant tool that can be taken for granted, a tokenizer expresses its author’s theory of text and can significantly affect the results of many kinds of text analysis.
            Notes
            1. As a reviewer of this paper has pointed out, the problems of tokenization have been more widely recognized recently in the NLP community. For example, Dridan and Oepen (2012) and Chiarcos et al. (2012) address and suggest partial solutions for some of the problems discussed here. Even if the problems had all been solved within the NLP community (a fact not in evidence), however, this would not diminish the force of my argument for the DH community, where there has been much less attention paid to them.
            2. These programs represent a variety of those used in DH work (in order): a mature Java program with a database function, a venerable corpus linguistics program with lots of functions and user-options, a highly customizable and powerful commercial program from OUP, a widely used online tool, and a recently developed set of tools written in the currently popular R.
        
        
            
                
                    Bibliography
                    
                        Burrows, J. F. (2007). All the Way Through: Testing for Authorship in Different Frequency Strata. 
                        LLC,
                        22(1): 27–47.
                    
                    
                        Chesnutt, C. W. (1900). 
                        The House Behind the Cedars. Houghton Mifflin, Boston, http://babel.hathitrust.org/cgi/pt?view=plaintext;size=100;id=nc01.ark%3A%2F13960%2Ft7cr7221k;page=root;seq=25;num=13.
                    
                    
                        Chiarcos, C., Ritz, J. and Stede, M. (2012). By All These Lovely Tokens . . . : Merging Conflicting Tokenizations. 
                        Language Resources and Evaluation,
                        46(1): 53–74. 
                    
                    
                        Craig, H. and Kinney, A. F. (2010). 
                        Shakespeare, Computers, and the Mystery of Authorship. Cambridge University Press, Cambridge. 
                    
                    
                        Dridan, R., and Oepen, S. (2012). Tokenization: Returning to a Long Solved Problem: A Survey, Contrastive Experiment, Recommendations, and Toolkit. 
                        Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pp. 378–82.
                    
                    
                        Eder, M. (2013). Mind Your Corpus: Systematic Errors in Authorship Attribution. 
                        LLC,
                        28(4): 603–14.
                    
                    
                        Eder, M., Rybicki, J. and Kestemont, M. (2014). Stylo.
                    
                    
                        Gaskell, E. (1910 [1851]). 
                        Cranford. Houghton Mifflin, Boston, http://babel.hathitrust.org/cgi/pt?q1=twelve;id=hvd.32044097042071;view=plaintext;start=1;sz=10;page=root;size=100;seq=143;num=107.
                    
                    
                        Hoover, D. L. (2014a). Making Waves: Algorithmic Criticism Revisited. 
                        DH2014, Lausanne, Switzerland: EPFL-UNIL, pp. 202–4.
                    
                    
                        Hoover, D. L. (2014b). The Full-Spectrum Text-Analysis Spreadsheet. 
                        Digital Humanities 2013, Center for Digital Research in the Humanities, Lincoln, NE, University of Nebraska, pp. 226–29.
                    
                    
                        The Intelligent Archive. (2012). Centre for Literary and Linguistic Computing, University of Newcastle, Australia.
                    
                    
                        Jockers, M. L. (2013). 
                        Macroanalysis: Digital Methods and Literary History. University of Illinois Press, Urbana-Champaign.
                    
                    
                        Levitan, S. and Argamon, S. (2006). Fixing the Federalist: Correcting Results and Evaluating Editions for Automated Attribution. 
                        Digital Humanities 2006. Paris: Centre de Recherche Cultures Anglophones et Technologies de l’Information, pp. 323–26.
                    
                    
                        Plasek, A. and Hoover, D. L. (2014). Starting the Conversation: Literary Studies, Algorithmic Opacity, and Computer-Assisted Literary Insight. 
                        DH2014, Lausanne: EPFL-UNIL, pp. 305–6.
                    
                    
                        Ramsay, S. (2011). 
                        Reading Machines: Toward an Algorithmic Criticism. University of Illinois Press, Urbana.
                    
                    
                        Scott, M. (2012). WordSmith Tools version 6. Liverpool: Lexical Analysis Software.
                    
                    
                        Sinclair, S., Rockwell, G. and the Voyant Tools Team. (2012). Voyant Tools (web application).
                    
                    
                        Tsukamoto, S. (2004). KWIC Concordance for Windows version 4.7.
                    
                
            
        
    


        
            
                Introduction
                The development of applications in the field of Digital Humanities (DH) does not adequately take into account domain modelling, software design principles and software engineering methodologies (Bozzi, 2013; D'Iorio, 2015; McCarty, 2008; Terras and Crane, 2010). In fact, many systems developed in the context of DH-related projects have not been conceived to be modular, extensible, and scalable: they only tend to solve specific problems such as data-driven and project-oriented tools (Boschetti and Del Grosso, 2015). In addition, most projects focus on the requirements of humanists (as end users), but leave out the needs of software developers.
                This research was motivated by a number of issues emerged from the projects we worked on 
                     (Abrate et al., 2014a; Albanesi et al., 2015; Bellandi et al., 2014; Bozzi, 2015; Del Grosso, 2013; Ruimy et al., 2012) and it fits into an ongoing discussion about textual modelling and research infrastructures (Moulin et al., 2011; Pierazzo, 2015; Schmidt, 2014). In particular, this work aims at providing methodological guidelines for the definition of the core entities of a digital scholarly environment. We chose to adopt an object-oriented approach since it can bring benefits in the definition of efficient and effective digital tools (Boschetti et al., 2014; Del Grosso and Nahli, 2014). To give an analogy, the environment we propose can help developers and scholars as CMS (e.g. Wordpress) can help Web designers and publishers.
                
                The development of the environment follows three criteria: i) adopting an agile process (Ashmore and Runyan, 2014) to define the nature and behavior of the environment through both functional (e.g. user stories) and non-functional requirements (e.g. data model, system architecture) (Cohn, 2004; Collins-Cope et al., 2005); ii) providing well-defined Application Programming Interfaces (APIs) among components (Grill et al., 2012; Tulach, 2008); iii) applying analysis, architectural and design patterns for the sake of abstraction, generalization and flexibility (Ackerman and Gonzalez, 2011; Buschmann et al., 2007; Gamma et al., 1995).
                Following the agile methodology, we are developing a modular environment by starting from the design and implementation of a 
                    microkernel (Buschmann et al., 1996) as the manager of the different components. In addition, the microkernel provides all the operations needed to manipulate the domain basic entities which are described in the section “Domain entities and design patterns”.
                
                
                    Related works
                    Digital humanists have access to several tools for literary studies. TextGrid, for example, provides integrated tools for analyzing texts and gives computer support for digital editing purposes (Hedges et al., 2013). The NINES project offers an environment to support scholars in the creation of long-term digital research materials. It includes three main tools: Collex (Nowviskie, 2007), Juxta, and Ivanhoe. Annotation Studio is a collaborative system to annotate texts and add links to multimedia objects (Paradis et al., 2013). The CULTURA project aims at developing a “corpus agnostic research environment” providing customizable services for a wide range of users (Steiner et al., 2014). The development of an online workspace which helps scholars in the production of critical editions is the main objective of the Workspace for Collaborative Editing framework (Houghton et al., 2014). It uses existing standards and open-source solutions to create an architecture of reusable components. Other platforms worth mentioning are TUSTEP/TXSTEP (Ott, 2000; Ott and Ott, 2014), WebLicht (Hinrichs et al., 2010), Perseids (Almas and Beaulieu, 2013), Muruca/Pundit (Grassi et al., 2013), Textual Communities (Bordalejo and Robinson, 2015), SAWS (Jordanous et al., 2012), Voyant Tools (Sinclair and Rockwell, 2012), Transcribe Bentham (Causer and Terras, 2014) and Alcide (Moretti et al., 2014). However, the aforementioned initiatives allow digital scholars to meet specific needs, but none of them seems to provide, simultaneously, all the following characteristics: i) reusability and extensibility, ii) ease of use and configuration, iii) continuous availability of the services and development over time, iv) a well-grounded software data model.
                
            
            
                Domain entities and design patterns
                One of the main challenges of the DH community is to provide suitable software models and tools (Ciotti, 2014). To model the literary domain and the relative user requirements, we chose to follow the engineering principles of 
                    object-oriented analysis and design (Ackerman and Gonzalez, 2011). The digital representation of a textual resource is a challenge as it involves several theoretical and epistemological issues in semiotics, paleography, philology, linguistics, engineering, and computer science (McCarty, 2005; Meister, 2012; Moretti, 2013; Robinson, 2013; Sahle, 2013).
                
                In this work, we define each textual element by means of four properties: i) the 
                    version allows to select a specific textual element among those available in its history of changes; ii) the 
                    granularity represents a level of a hierarchical structure (e.g. a page composed of lines); iii) the 
                    position provides the location of a textual element within the hierarchical representation (e.g. the second page of a book); iv) the 
                    layer indicates the set of homogeneous information the textual element belongs to (e.g. morphological layer). As pointed outby (Buzzetti, 2002; McGann, 2004; Pierazzo, 2015), the information conveyed by a textual resource is logically organized through multiple layers (also called 
                    dimensions) of information.
                
                
                    
                        
                        Fig. 1: Class diagram of the domain entities
                    On these four properties we have designed and implemented a set of core entities as the fundamental data types shared among all the components of the environment (Fig. 1)
                     The ongoing implementation of the environment is available at: https://github.com/literarycomputinglab. The 
                    Source class is in charge of managing the low-level data: it is composed of a 
                    Payload representing the information conveyed by the textual resource and a
                    SourceType which indicates the nature of the Source (e.g. text, image, audio, etc.). Payload objects (as used in networking) have the only purpose of encapsulating the information. The 
                    Locus and the 
                    P
                    lace
                    OfInterest (POI) classes identify, through a 
                    composition pattern, specific data fragments of the source content, and they are used to establish the boundaries of an 
                    Annotation. A chunk of text, for example, can be addressed to by a locus having a POI (of type Sequence of Interest) representing its start and end coordinates. Similarly, a region of an image can be identified by a locus having a POI (of type Region of Interest) composed of a sequence of coordinates. The Locus and POI provide a stand-off text annotation technique able to tackle, for example, the overlapping hierarchies problem, which cannot be handled easily with inline markup techniques (Schmidt, 2010). As a matter of fact, it is possible, simultaneously, to manipulate a resource on the basis of its documental and textual structure (Renear et al., 1996; Robinson, 2013) (see the example in the following section). However, since stand-off models are affected by the issue of the indexing updating, a dedicated component must be in charge of automatically maintaining the coherence of the annotations each time the underlying text is edited.
                
                An Annotation represents an information associated to a locus and is defined by an 
                    AnnotationType (e.g. a token, a lemma, a named entity, etc.). Since the hierarchical structure of the source may evolve over time, the changes to the relative tree must be managed. For example, a tree structure having tokens as leaves could need to be updated with a finer-grained layer of characters (e.g. to assign annotations to specific letters). In this case, the tokens should become intermediate nodes and the characters would become the leaf nodes. Typically, this kind of editing is unpredictable and it often implies heavy adaptations if the software is not flexible enough to manage changes in the underlying text representation schema. Consequently, we decided to exploit the flexibility of the Object Oriented model by adopting the Role Design Pattern (Fowler, 1997) to switch between leaf and intermediate nodes dynamically. This pattern has been implemented by the 
                    AnnotationRole, 
                    AnnotationRoleElement and 
                    AnnotationRoleStructure classes. Moreover, an annotation is a source in itself (see the inheritance relationship between the Annotation and the Source classes in Fig. 1) and, thus, it can be annotated recursively.
                
            
            
                An Example
                We here introduce an example showing a representation of a snippet of text with annotations. The chosen text is an excerpt of a letter, written in Latin, belonging to the epistolary corpus of the Clavius on the Web project
                     Clavius on the Web is a project funded by Registro.it and partecipated by the Institute of Informatics and Telematics (IIT-CNR), the Institute of Computational Linguistics “A. Zampolli” (ILC-CNR), and the Historical Archives of the Pontifical Gregorian University (APUG). Website: http://claviusontheweb.it/ (Abrate et al., 2014b). Fig. 2 shows a typical way of encoding sentences and lines with a markup language as TEI (Burnard, 2014): the resulting XML hierarchical structure has been broken by the addition of the line anchors () mixing up the textual and documental structure of the text. Indeed, to preserve the integrity of the word “Dinostrati” (spanning across lines 4 and 5), it is necessary to encapsulate it with the element .
                
                
                    
                        
                        Fig. 2: A standard way of encoding a text with TEI-XML
                    
                    
                        
                        Fig. 3: Multi-layered stand-off annotation of text
                    The model we propose solves this problem with the stand-off annotations: as shown in Fig. 3 the document (made of lines) and the textual structure (made of sentences and words) are logically separated. Lines, sentences and words do not overlap and they are structured in separate hierarchies.
                
            
            
                Next Steps
                We plan, in future works, to release a first version of a web environment, called 
                    Omega, built around the core entities that we here described. The environment will allow to load, index, annotate, and query a textual collection. Furthermore, we’ll carry on the development of modules for text analysis and textual scholarship with the related APIs.
                
            
        
        
            
                
                    Bibliography
                    
                        Abrate, M., 
                        Del 
                        Grosso, A. M., Giovannetti, E., 
                        Lo 
                        Duca, A., Luzzi, D., Mancini, L., Marchetti, A., Pedretti, I. and Piccini, S. (2014a). Sharing Cultural Heritage: the Clavius on the Web Project. In Calzolari, N., Choukri, K., Declerck, T., Loftsson, H., Maegaard, B., Mariani, J., Moreno, A., Odijk, J. and Piperidis, S. (eds), 
                        Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC), Reykjavik. European Language Resources Association (ELRA), pp. 627–34.
                    
                    
                        Abrate, M., 
                        Del 
                        Grosso,
                         
                        A. M., Giovannetti, E., 
                        Lo 
                        Duca, A., Marchetti, A., Mancini, L., Pedretti, I. and Piccini, S. (2014b). Il Progetto Clavius on the Web: tecnologie linguistico-semantiche al servizio del patrimonio documentale e degli archivi storici. In Rossi, F. and Tomasi, F. (eds), 
                        Book of Abstracts of 3
                        o
                         AIUCD Conference, Bologna.
                    
                    
                        Ackerman, L. and Gonzalez, C. (2011). 
                        Patterns-Based Engineering: Successfully Delivering Solutions Via Patterns. Addison-Wesley.
                    
                    
                        Albanesi, D., Bellandi, A., Benotto, G., 
                        Di 
                        Segni, G. and Giovannetti, E. (2015). When Translation Requires Interpretation: Collaborative Computer–Assisted Translation of Ancient Texts. 
                        LaTeCH 2015: 84–88.
                    
                    
                        Almas, B. and Beaulieu, M.-C. (2013). Developing a New Integrated Editing Platform for Source Documents in Classics. 
                        Literary and Linguistic Computing, 
                        28(4): 493–503 doi:10.1093/llc/fqt046.
                    
                    
                        Ashmore, S. and Runyan, K. (2014). 
                        Introduction to Agile Methods. Upper Saddle River, NJ: Addison-Wesley Professional, Pearson Education.
                    
                    
                        Bellandi, A., Albanesi, D., Bellusci, A., Bozzi, A. and Giovannetti, E. (2014). The Talmud System: a Collaborative web Application for the Translation of the Babylonian Talmud Into Italian. 
                        The First Italian Conference on Computational Linguistics CLiC-It 2014, pp. 53–57.
                    
                    
                        Bordalejo, B. and Robinson, P. (2015). A new system for collaborative online creation of Scholarly Editions in digital form. 
                        1st Dixit Convension on Technology, Software, Standards for the Digital Scholarly Edition Workshop. The Hague.
                    
                    
                        Boschetti, F. and 
                        Del 
                        Grosso, A. M. (2015). TeiCoPhiLib: A Library of Components for the Domain of Collaborative Philology. 
                        Journal of the Text Encoding Initiative(8). doi:10.4000/jtei.1285. http://jtei.revues.org/1285 (accessed 3 March 2016).
                    
                    
                        Boschetti, F., 
                        Del 
                        Grosso, A. M., Khan, A. F., Lamé, M. and Nahli, O. (2014). A top-down approach to the design of components for the philological domain. 
                        Book of Abstract of Digital Humanities Conference (DH), Lausanne, Switzerland. Alliance of Digital Humanities Organisations, pp. 109–11.
                    
                    
                        Bozzi, A. (2013). G2A: A Web application to study, annotate and scholarly edit ancient texts and their aligned translations. (Ed.) ERC Ideas 249431 
                        Studia Graeco-Arabica, 
                        3: 159–71.
                    
                    
                        Bozzi, A. (2015). Greek into Arabic, a research Infrascructure based on computational modules to annotate and query historical and philosophical digital texts. Part I: Methodological aspects. In Bozzi, A. (ed), 
                        Digital Texts, Translations, Lexicons in a Multi-Modular Web Application: Methods and Samples. Firenze: Leo S. Olschki editore, pp. 27–42.
                    
                    
                        Burnard, L. (2014). TEI P5: Guidelines for Electronic Text Encoding and Interchange. Version 2.9.1. http://www.tei-c.org/Guidelines/P5/index.xml (accessed 3 March 2016).
                    
                    
                        Buschmann, F., Henney, K. and Schmidt, D. C. (2007). 
                        Pattern-Oriented Software Architecture, On Patterns and Pattern Languages. (Pattern-Oriented Software Architecture). Hoboken: John Wiley & Sons.
                    
                    
                        Buschmann, F., Meunier, R., Rohnert, H., Sommerlad, P. and Stal, M. (1996). Pattern-oriented Software Architecture - A System of Patterns. J. Wiley and Sons Ltd., pp. 171–92.
                    
                    
                        Buzzetti, D. (2002). Digital Representation and the Text Model. 
                        New Literary History, 
                        33(1): 61–88.
                    
                    
                        Causer, T. and Terras, M. (2014). “Many hands make light work. Many hands together make merry work”: Transcribe Bentham and crowdsourcing manuscript collections.
                    
                    
                        Ciotti, F. (2014). Digital Literary and Cultural Studies: State of the Art and Perspectives. 
                        Between, 
                        4(8). doi:10.13125/2039-6597/1392. http://dx.doi.org/10.13125/2039-6597/1392 (accessed 3 March 2016).
                    
                    
                        Cohn, M. (2004). 
                        User Stories Applied: For Agile Software Development. Redwood City, CA, USA: Addison Wesley Longman Publishing Co., Inc.
                    
                    
                        Collins-Cope, M., Rosenberg, D. and Stephens, M. (2005). 
                        Agile Development with ICONIX Process: People, Process, and Pragmatism. Berkely, CA, USA: Apress.
                    
                    
                        Fowler, M. (1997). Dealing with roles. 
                        Proceedings of the International Conference on Pattern Languages of Programs, vol. 97, pp. 13–37.
                    
                    
                        Gamma, E., Helm, R., Johnson, R. and Vlissides, J. (1995). 
                        Design Patterns: Elements of Reusable Object-Oriented Software. Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc.
                    
                    
                        Grassi, M., Morbidoni, C., Nucci, M., Fonda, S. and Piazza, F. (2013). Pundit: augmenting web contents with semantics. 
                        Literary and Linguistic Computing, 
                        28(4): 640–59.
                    
                    
                        Grill, T., Polacek, O. and Tscheligi, M. (October 29-312012). Methods Towards API Usability: A Structural Analysis of Usability Problem Categories. 
                        Proceedings of the 4th International Conference on Human-Centered Software Engineering, Toulouse, France. Berlin, Heidelberg: Springer-Verlag, pp. 164–80. doi:10.1007/978-3-642-34347-6_10.
                    
                    
                        Del 
                        Grosso, A. M. (2013). Indexing techniques and variant readings management. (Ed.) D'Ancona, C. 
                        Studia Graeco-Arabica, 
                        3: 209–30.
                    
                    
                        Del 
                        Grosso, A. M. and Nahli, O. (2014). Towards a flexible open-source software library for multi-layered scholarly textual studies: An Arabic case study dealing with semi-automatic language processing. 
                        Proceedings of 3rd IEEE International Colloquium, Information Science and Technology (CIST), Tetouan, Marocco. Washington, DC, USA: IEEE, pp. 285–90. doi:10.1109/CIST.2014.7016633.
                    
                    
                        Hedges, M., Neuroth, H., Smith, K. M., Blanke, T., Romary, L., Küster, M. and Illingworth, M. (2013). TextGrid, TEXTvre, and DARIAH: Sustainability of Infrastructures for Textual Scholarship. 
                        Journal of the Text Encoding Initiative(5). doi:10.4000/jtei.774 (accessed 3 March 2016).
                    
                    
                        Hinrichs, E., Hinrichs, M. and Zastrow, T. (2010). WebLicht: Web-based LRT services for German. 
                        Proceedings of the ACL 2010 System Demonstrations. Association for Computational Linguistics, pp. 25–29.
                    
                    
                        Houghton, H., Sievers, M. and Smith, Catherine (2014). The Workspace for Collaborative Editing. 
                        Digital Humanities 2014. Laussanne: Alliance of Digital Humanities Organisations, pp. 204–05.
                    
                    
                        D'
                        Iorio, P.  (2015). On the scholarly use of the Internet, a conceptual model. In Bozzi, A. (ed), 
                        Digital Texts, Translations, Lexicons in a Multi-Modular Web Application: Methods and Samples. Firenze: Leo S. Olschki editore, pp. 1–25.
                    
                    
                        Jordanous, A., Lawrence, K. F., Hedges, M. and Tupman, C. (June 13-152012). Exploring Manuscripts: Sharing Ancient Wisdoms Across the Semantic Web. 
                        Proceedings of the 2nd International Conference on Web Intelligence, Mining and Semantics (WIMS), Craiova, Romania. New York, NY, USA: ACM, pp. 44:1–44:12. doi:10.1145/2254129.2254184.
                    
                    
                        McCarty, W. (2005). 
                        Humanities Computing. Palgrave Macmillan.
                    
                    
                        McCarty, W. (2008). Signs of times present and future. 
                        Human Discussion Group, 
                        22(218).
                    
                    
                        McGann, J. (2004). Marking Texts of Many Dimensions. In Schreibman, S., Siemens, R. and Unsworth, J. (eds), 
                        A Companion to Digital Humanities. (Blackwell Companions to Literature and Culture). Blackwell Publishing Ltd, pp. 198–217.
                    
                    
                        Meister, J. C. (2012). DH is us or on the unbearable lightness of a shared methodology. 
                        Historical Social Research, 
                        37(3): 77–85.
                    
                    
                        Moretti, F. (2013). 
                        Distant Reading. Verso Books.
                    
                    
                        Moretti, G., Tonelli, S., Menini, S. and Sprugnoli, R. (2014). ALCIDE: An online platform for the Analysis of Language and Content In a Digital Environment. 
                        Proceedings of the First Italian Conference on Computational Linguistics (CLIC-2014). Pisa, Italy.
                    
                    
                        Moulin, C., Nyhan, J., Ciula, A., Kelleher, M., Mittler, E., Tadić, M., Ågren, M., Bozzi, A. and Kuutma, K. (2011). 
                        Research Infrastructures in the Digital Humanities. http://www.esf.org/hosting-experts/scientific-review-groups/humanities-hum/strategic-activities/research-infrastructures-in-the-humanities.html.
                    
                    
                        Nowviskie, B. (2007). Collex: Facets, Folksonomy, and Fashioning the Remixable web. 
                        Book of Abstract of Digital Humanities Conference (DH), University of Illinois at Urbana-Champaign. Alliance of Digital Humanities Organisations.
                    
                    
                        Ott, W. (2000). Strategies and tools for textual scholarship: the Tübingen system of text processing programs (TUSTEP). 
                        Literary and Linguistic Computing, 
                        15(1): 93–108. doi:10.1093/llc/15.1.93. http://llc.oxfordjournals.org/content/15/1/93.abstract.
                    
                    
                        Ott, W. and Ott, T. (2014). Critical Editing with TXSTEP. In Terras, M. (ed), 
                        Book of Abstracts of the Digital Humanities Conference, Lausanne, Switzerland. Alliance of Digital Humanities Organisations, pp. 509–13.
                    
                    
                        Paradis, J., Fendt, K., Kelley, W., Folsom, J., Pankow, J., Graham, E. and Subbaraj, L. (2013). Annotation Studio: Bringing a Time-Honored Learning Practice into the Digital Age. 
                        Whitepaper. http://cmsw.mit.edu/annotation-studio-whitepaper/ (accessed 3 March 2016).
                    
                    
                        Pierazzo, E. (2015). 
                        Digital Scholarly Editing : Theories, Models and Methods. Farnham Surrey: Ashgate.
                    
                    
                        Renear, A. H., Mylonas, E. and Durand, D. (1996). Refining our notion of what text really is: The problem of overlapping hierarchies. (Ed.) Hockey, S. M. 
                        Research in Humanities Computing, 
                        4: 263–80.
                    
                    
                        Robinson, P. (2013). Towards a theory of digital editions. (Ed.) Mierlo, W. V. and Fachard, A. 
                        Variants, (10): 105–31.
                    
                    
                        Ruimy, N., Piccini, S. and Giovannetti, E. (2012). Defining and Structuring Saussure’s Terminology. In Fjeld, R. V. and Torjusen, J. M. (eds), 
                        Proceedings of 15th EURALEX International Congress. Oslo, Norway, Department of Linguistics and Scandinavian Studies, University of Oslo, Reprosentralen: UiO press, pp. 828–33.
                    
                    
                        Sahle, P. (2013). 
                        Digitale Editionsformen: Teil 3: Textbegriffe Und Recodierung; Zum Umgang Mit Der Überlieferung Unter Den Bedingungen Des Medienwandels. Vol. 3. BoD–Books on Demand.
                    
                    
                        Schmidt, D. (2010). The inadequacy of embedded markup for cultural heritage texts. 
                        Literary and Linguistic Computing, 
                        25(3): 337–56. doi:10.1093/llc/fqq007.
                    
                    
                        Schmidt, D. (2014). Towards an Interoperable Digital Scholarly Edition. 
                        Journal of the Text Encoding Initiative(7). doi:10.4000/jtei.979.
                    
                    
                        Sinclair, S. and Rockwell, G. (2012). the Voyant Tools Team (web application) 
                        Voyant Tools. http://voyant-tools.org (accessed 3 March 2016).
                    
                    
                        Steiner, C., Agosti, M., Sweetnam, M., Hillemann, E.-C., Orio, N., Ponchia, C., Hampson, C., et al. (2014). Evaluating a digital humanities research environment: the CULTURA approach. 
                        International Journal on Digital Libraries, 
                        15(1): 53–70. doi:10.1007/s00799-014-0127-x.
                    
                    
                        Terras, M. and Crane, G. (eds). (2010). 
                        Changing the Center of Gravity: Transforming Classical Studies through Cyberinfrastructure. Piscataway: Gorgias Press.
                    
                    
                        Tulach, J. (2008). 
                        Practical API Design: Confessions of a Java Framework Architect. 1st ed. Berkely, CA, USA: Apress.
                    
                
            
        
    


        
            
                Motivation
                In last years, the advancements in computer science brought a global change in the way information is stored, retrieved and analyzed. The digital humanities also benefit from these developments, and now, a vast amount of texts is available in digital form. This information explosion generates interesting research questions for humanities scholars who are capable of deriving new insights from this knowledge bank. In order to support humanities scholars, many visualization techniques – summarized in a survey (Jänicke et al., 2015b) – were developed to aid exploring large texts collections. Most of these techniques are interactive and belong to the category of distant reading (Moretti, 2005). The authors of the mentioned survey observe that less work has been done to improve the close reading capabilities of humanities scholars even though they are often focused on close reading text passages.
                Close reading is the careful interpretation of the text, where the scholar iteratively reads the text in order to explore its meaning, inherent topics and occurring relationships (Boyles, 2013). Traditionally, close reading is done on paper. Several ideas and thoughts are made persistent by annotations written at the margins alongside the text (see Figure 1). But as the margin space is limited, not all observations can be put around the text. So, annotations may become cluttered and confusing for the reader, especially, when obsolete ideas are struck through. Despite its disadvantages, annotating on paper is still quite popular as it benefits the scholars to record observations about the hypothesis and all these changes reappear in front of the scholar’s eyes as soon as he re-reads the text passage. We observed that the way of annotating in close reading resembles the idea of mind maps (Buzan et al., 1993) that are based on a central concept and thoughts are represented around it using lines and text. In the close reading scenario, the text can be considered as the central concept and annotations represent thoughts.
                An important task of computer science is to enhance the original workflows of researchers with computational methods. As most humanities scholars are well trained in close reading and nowadays often work with digital texts, it is necessary to enhance their capabilities for digital close reading. We propose an enhanced close reading design inspired by mind-maps that not only mimics the traditional way of annotating a text on paper, but also helps humanities scholars to perform live visual analyses. Furthermore, we use extendible margins to provide enough space for all thoughts of the scholar.
                
                    
                    
                        
                            
                            Figure 1: Traditional close reading on paper
                        
                    
                    
                        
                            
                            Image reproduced with permission from Kehoe (Kehoe et al., 2013)
                        
                    
                
            
            
                Related Work
                Nancy Boyles (Boyles, 2013) defines close reading, which has become a fundamental method in literary criticism in the 20th century (Hawthorn, 2000), as follows: “Essentially, close reading means reading to uncover layers of meaning that lead to deep comprehension.” Annotating the text in close reading is a strong method for scholars to facilitate the understanding of a text passage. Figure 1 shows the result of a traditional close reading approach. In this example, various annotation methods were used by the scholar to annotate various features of a text passage in Charles Dickens' „David Copperfield“. 
                The availability of digital texts has further awaken the interest of humanities scholars in collaboratively close reading the same texts. There are several annotation tools for such a purpose, such as eMargin (Kehoe et al., 2013), Hypothes.is (Bonn et al., 2014) and NB (Zyto et al., 2012). These tools are beneficial for collaborative research and classroom environments as they provide an excellent paradigm to share thoughts, as well as find collective answers. To avoid clutter, these tools work with popup windows that are only shown on demand. In Figure 2, the eMargin system is shown where colors are used to highlight different text features, and a popup window on demand, lists the comments of collaborating scholars.
                
                    
                    
                        
                            
                            Figure 2: eMargin annotation tool
                        
                    
                    
                        
                            
                            Image reproduced with permission from Kehoe (Kehoe et al., 2013)
                        
                    
                
                Digital Ink Annotations systems (Schilit, 1998, Bargeron et al., 2003, Agrawala et al., 2005, Yoon et al., 2013) also support annotating text, but their use is only limited to pen-based computing devices such as tablets. The systems are designed to work well on smaller screens, and the adaption to larger screens is not appropriately implemented. 
                Close reading tasks can also be assisted via distant reading tools. For example, parallel coordinates, a heatmap and a dot plot are used to analyze the variance of a selected text passage from different German translations of Shakespeare’s Othello (Geng et al., 2013). Heat maps are appropriate visualizations to illustrate the distribution of specific phrases or annotations in a corpus (Muralidharan, 2011, Alex et al., 2015). Voyant Tools allow the user to perform basic text mining functions with selected word statistics shown in linked views (Sinclair et al., 2012). The Voyant Tools interface in Figure 3 shows statistics about Chapter 2 of Oscar Wilde's “David Copperfield”. Goffin's idea to enhance close reading is the integration of small visualizations (e.g., maps or bar charts) besides the words of a text (Goffin et al., 2014).
                
                    
                        
                        Figure 3: Screenshot of web-based Voyant Tools (Sinclair et al., 2012).
                    
                
            
            
                Enhanced Close Reading Design
                In contrast to the tools mentioned above, we combine traditional annotation tasks with distant reading analyses to enhance the close reading capabilities of the scholar. We suggest a design inspired by mind mapping (an example mind map is shown in Figure 4a), a methodology that allows a researcher to work on a central concept, and thoughts and features about that concept are placed around it using figures, lines etc. In a mind map, the associations spread out from a central concept in a free-flowing, yet organized and coherent manner (Budd, 2004) - thus forming a mental map of the central concept. We observe that like in the case of mind maps, fixed annotations around the central text in a traditional close reading process facilitate forming a mental map of the thoughts about the text of interest, and help the scholar to draw conclusions when seeing the whole picture.
                
                    
                        
                            
                                
                                Figure 4a: An example mind map
                            
                             Image reproduced with permission from Kanter (Kanter, 2015) (Figure under CC BY 2.0 license, see 
                                 for details).
                            
                        
                        
                            
                                
                                Figure 4b: Mind-map inspired close reading
                            
                        
                    
                
                Figure 4b illustrates the idea of a mind map inspired interface with multiple types of annotations supporting the scholar in the close reading process. Textual annotations known from the traditional close reading are also necessary in the digital process. In addition, images, videos and charts can facilitate text interpretation and the generation of valuable hypotheses about the text. To support dynamic, multifarious views on a certain text passage or a term of interest, we designed our interface the way that the literary scholar can apply a multitude of visual analyses and generate distant reading visualizations that are placed as annotations alongside the text. This combines the traditional close reading paradigm with elaborated text visualization techniques valuable for exploration purposes. An important feature of our proposed interface design is to support the scholar to „stay in the flow“ (Bederson, 2004), so that the central focus remains on the text, which can be analyzed without interrupting the scholar. The major advantage of our design over existing tools that assist close reading tasks is interface versatility. For example, Voyant Tools (see Figure 3) provide a predefined set of visualizations based on text statistics. On the other hand, our design allows the scholar to choose an appropriate text visualization as an annotation alongside the text, which is based on a user-defined query on the text.. Therefore, the scholar can apply different text visualizations for different passages of the text to support a variety of close reading tasks. 
                An example of the design discussed above is shown in Figure 5. The example from Figure 1 is annotated using different kinds of annotations. Like in other digital tools, certain topics of the text are annotated using colors. In addition, the character(s) Peggotty is marked and a panel shows thumbnail images based on a Google Images search. Also the relative word frequency chart of the term “Peggotty” in Chapter 2 is shown on the bottom left. Furthermore, on the left area, a TagPie (Jänicke et al., 2015a) showing the co-occurrences of both the terms memory and observation helps to investigate the hypothesis of the literary scholar about the similar meaning of both topics. The example depicts how the scholar can use different annotation tools as well as different distant reading tools to enrich the close reading experience.
                
                    
                        
                        Figure 5: Example of our design
                    
                
            
            
                Future Work and Conclusions
                We held discussion with the collaborating humanities scholars about the design as well as the usability of the proposed interface. The scholars remarked that such an interface will help removing fears of using digital humanities tools and that they intend to use the tool as it mimics their existing workflows. They also mentioned that such a tool could help users getting a better big picture of the text, and that it enhances the close reading capabilities of the scholar. Another important point is the capability in supporting teaching activities. They mentioned that various types of annotations (text, pictures, charts) are also used in teaching material, but it is not easy to share these with students. Such a tool could support this process as it generates persistent annotations to be analyzed and discussed collaboratively in courses. 
                We observe that the scholar’s initial reactions after seeing the prototype of the tool, which is still in development, are convincing and encouraging. We think that rigid modeling syntax is inappropriate for annotation. Our final interface will allow the scholar to make annotation styles versatile. At the digital humanities conference, we will demonstrate our prototype and discuss future prospects within the community. An additional user study will compare the viability of our proposed, mind map inspired annotation technique to existing approaches.
            
            
                Acknowledgements
                We thank our colleagues from the humanities department, Judith Blumenstein in particular, who provided insights and expertise that greatly assisted this research.
            
        
        
            
                
                    Bibliography
                    
                        Agrawala, M. and Shilman, M. (2005). DIZI: a digital ink zooming interface for document annotation. Human-Computer Interaction-INTERACT 2005, Springer Berlin Heidelberg, pp. 69-79.
                    
                    
                        Alex, B., Grover, C., Zhou, K., Hinrichs and Palimpsest, U. (2015). Improving Assisted Curation of Loco-specific Literature. Proceedings of the Digital Humanities 2015, pp. 5-7.
                    
                    
                        Bargeron, D. and Moscovich, T. (2003). Reflowing digital ink annotations. Proceedings of the SIGCHI conference on Human factors in computing systems, ACM, pp. 385-93.
                    
                    
                        Bederson, B. B. (2004). Interfaces for staying in the flow. Ubiquity, 1-1.
                    
                    
                        Bonn, M. and McGlone, J. (2014). New Feature: Article Annotation with Hypothesis. Journal of Electronic Publishing, 17(2).
                    
                    
                        Boyles, N. (2013). Closing in on Close Reading. Educational Leadership, 70(4): 36–41.
                    
                    
                        Budd, J. W. (2004). Mind Maps as Classroom Exercises. The Journal of Economic Education, 35(1): 35–46.
                    
                    
                        Buzan, T. and Buzan, B. (1993). The Mind Map Book How to Use Radiant Thinking to Maximise Your Brain's Untapped Potential. New York: Plume.
                    
                    
                        Geng, Z., Cheesman, T., Laramee, R. S., Flanagan, K. and Thiel, S. (2013). ShakerVis: Visual analysis of segment variation of German translations of Shakespeare’s Othello. Information Visualization, 15: 93-116.
                    
                    
                        Goffin, P., Willett, W., Fekete, J. D. and Isenberg, P. (2014). Exploring the placement and design of word-scale visualizations. Visualization and Computer Graphics, IEEE Transactions, 20(12): 2291-300.
                    
                    
                        Hawthorn, J. (2000). A glossary of contemporary literary theory. Oxford University Press.
                    
                    
                        Jänicke, S., Blumenstein, J., Rücker, M., Zeckzer, D. and Scheuermann, G. (2015a). Visualizing the Results of Search Queries on Ancient Text Corpora with Tag Pies. Digital Humanities Quarterly.
                    
                    
                        Jänicke, S., Franzini, G., Cheema, M. F. and Scheuermann, G. (2015b). On Close and Distant Reading in Digital Humanities: A Survey and Future Challenges. In Borgo, R., Ganovelli, F., and Viola, I. (eds.), Eurographics Conference on Visualization (EuroVis) - STARs (2015), The Eurographics Association.
                    
                    
                        Kanter, B. (2015). Cambodia4kids.org, https://www.flickr.com/photos/cambodia4kidsorg/6195211411 (Retrieved 2015-11-25).
                    
                    
                        Kehoe, A. and Gee, M. (2013). eMargin: A Collaborative Textual Annotation Tool. Ariadne, 71.
                    
                    
                        McCabe, M. M. (2015). Platonic Conversations. Oxford University Press.
                    
                    
                        Moretti, F. (2005). Graphs, Maps, Trees: Abstract Models for a Literary History. New York: Verso.
                    
                    
                        Muralidharan, A. (2011). A Visual Interface for Exploring Language Use in Slave Narratives. Proceedings of the Digital Humanities 2011.
                    
                    
                        Schilit, B. N., Golovchinsky, G. and Price, M. N. (1998). Beyond paper: supporting active reading with free form digital ink annotations. Proceedings of the SIGCHI conference on Human factors in computing systems, ACM Press/Addison-Wesley Publishing Co., pp. 249-56.
                    
                    
                        Sinclair, S. and Rockwell, G. (2012). Voyant Tools. Online: http://voyant-tools.org (Retrieved 2015-11-25).
                    
                    
                        Yoon, D., Chen, N. and Guimbretière, F. (2013). TextTearing: Opening white space for digital ink annotation. Proceedings of the 26th annual ACM symposium on User interface software and technology, ACM, pp. 107-12. 
                    
                    
                        Zyto, S., Karger, D., Ackerman, M. and Mahajan, S. (2012). Successful classroom deployment of a social document annotation system. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM, pp. 1883-92.
                    
                
            
        
    


        
            “Remapping Leigh Hunt’s Circles” is an ambitious project that explores Leigh Hunt’s central position in the London literary and critical scene of the first half of the nineteenth century, through the lens of digital humanities tools. Hunt is today considered one of the key figures of the Romantic period in England, known for his work as editor, journalist, poet, and facilitator. Numerous articles, essay collections, biographies, and monographs published in the last fifteen years have made this clear. Hunt's contribution to Romantic and Victorian literature was as extensive as it has proven durable, in matters as various as prosodic experimentation and the modernization of the magazine essay. Yet little work (beyond some biographical notes) has been done on the second half of his life, a period that was as productive as the first, and during which Hunt was intimate with many of the finest writers of the time, and continued to contribute to London’s literary circles through the ongoing publication of critical essays in periodicals and anthologies. This project aims to redress this imbalance/oversight and reassert Hunt’s place in the Romantic and Victorian eras, as well as his continuing significance for understanding the London literary scene between 1805 (publication of his first critical essay) and 1859 (date of his death, with his last article published only a few weeks before). 
            “Remapping Leigh Hunt’s Circles” makes a case for Hunt’s position as a key critical voice in London beyond his already established prominence during the 
                Examiner years. It does so through a careful analysis of his critical reviews and essays (with a specific focus on his drama criticism to underscore Hunt’s ongoing engagement with the public sphere) published during his entire career, which spanned the first half of the nineteenth century. Data mining and textual analysis offer exciting opportunities to bring together different sets of data which, when prepared to the highest standard of text encoding, can yield new and innovative results that encourage reconsideration of preconceived notions regarding the transfer of ideas from one author to another, or one literary genre to another. The results of the research undertaken in “Remapping Leigh Hunt’s Circles” will be presented in a collaborative, visual context that reimagines the digital scholarly edition as a transparent workspace in which established primary objects from existing databases can be gathered, organized, correlated, annotated, and augmented by multiple users in a dynamic environment. 
            
            All the texts prepared for inclusion in our project are encoded to the Text-Encoding-Initiative (TEI) standards. The mark-up language and quality controls for improving metadata in all the resources provide more accurate search and discovery, allow for the presentation of well-supported content on multiple devices and develop tools for assembling, archiving and indexing research objects and artifacts. Ongoing work on this platform will enable researchers to undertake world-class research by providing the means to link data-sets to published content, encouraging data reanalysis, replication studies, and data re-purposing, all of which improve research quality and efficiency.
            Our poster will report on the first year of this project, and the implementation of the latest version of the 
                Voyant Tools to examine the dramatic essays written by Hunt between 1805 and 1813 (when he was sentenced to two years in prison for libel against the Prince Regent). We will showcase in particular two aspects of the integration between the Hunt archives and Voyant Tools. First, the ability to identify and visualize named entity connections and their networks across multiple documents (this a refinement of the previous RezoViz tool in Voyant). The Hunt collection presents an ideal corpus for network exploration given the interconnectedness of the people, locations and events that animate the documents. Second, Voyant provides a generic and customizable way of presenting a web-based corpus catalogue with the same kinds of faceted browsing and advanced querying capabilities we have come to expect from library databases and online stores. A further benefit of this functionality is the ability to create dynamic subsets of a corpus to examine more closely (in other words, using a catalogue skin in Voyant to create worksets destined for Voyant’s more conventional analytic skin).
            
            The “Remapping Leigh Hunt’s Circles” is essentially a project of digital text editing and literary criticism whereas Voyant Tools is essentially a software platform for reading, analyzing and visualizing digital texts. These are separate traditions and separate concerns, but this poster will demonstrate the value of symbiotic development: both projects benefit from the collaboration.
        
        
            
                
                    Bibliography
                    McGann, J. (2014). 
                        A New Republic of Letters. Cambridge, MA: Harvard UP. 
                    
                    Sinatra, M., and Sinclair, S. (2015). Special issue “Repenser le numérique au 21
                        ème siècle”. 
                        Sens public (hiver 2015). 
                    
                    Sinatra, M. (2015). “Representing Leigh Hunt’s Autobiography”.
                        Virtual Victorians: Networks, Connections, Technologies. Eds. Stauffer, A. and Alfano, V. R., Palgrave.
                    
                    Sinclair, S., Rucker, S. and Radzikowska, M. (2011). 
                        Visual Interface Design for Digital Cultural Heritage. Ashgate.
                    
                
            
        
    

Summary
Reproducing experimental results is a hallmark of empirical investigation and serves both to verify and inspire. This paper is a call for more systematic documentation of computational stylistic experiments. Publishing only summaries of the methods and results of empirical work is an artifact of traditional print media. To facilitate experimental reproducibility and to help the growing community who wish to learn how to apply computational methods and subsequently teach the next generation of scholars, the publication of results must include (i) access to the digitized texts, (ii) a clear workflow and most essentially (iii) the source code that led to each and all of the experimental results. By way of example, we present the steps and process in a GitHub repository for computationally probing the unknown and contested authorship of an 1831 short story entitled “A Dream” as we seek evidence if this work is similar to other attributed works by Edgar Allan Poe. The entire framework is intended as a pedagogical jumpstart for others, especially those new to computational stylometry. If Poe did write the story, it would be his first published work.

Introduction
As the Digital Humanities gains access to a wide array of digitized corpora and matures to a discipline that creatively defines new methods for computationally close and distant readings, a growing gap has emerged between those who apply sophisticated programming, e.g., Stylo In R (Eder et al., 2016) and those who are new to the game and need an introduction to the field. Typical of the community spirit in DH, significant efforts are underway to bridge this gap, including web-based tools for entry-level exploration including

Voyant Tools (Sinclair and Rockwell, 2016) and Lexos (Kleinman et al., 2016) and domain-specific introductions to programming, including Jockers' text (2014) and the Programming Historian (Crymble et al., 2016). This paper attempts to narrow the gap by encouraging both sides to document their experimental methods more fully to embrace previous calls for the replication of experimental methods (Rudman, 2012 et al.) and thereby teach effective practices by “leaving a trail” of experimental methods that enable others to execute and extend.

A Good Mystery: Towards Reproducibility
A GitHub repository or “repo” offers a workflow

that explores whether an 1831 story published under

the attribution of only ‘P' might have been written by Edgar Allan Poe. If so, it would be Poe's first published work. In addition to sharing a set of analytical methods applied in this experiment, the broader methodological-pedagogical goals are two-fold: (i) the dissemination of data and code should be championed as a cornerstone of DH research, thereby facilitating the replication of results and (ii) to share a workflow so that others may apply similar analyses to their texts of interest.

The workflow is stored as a set of numbered folders containing the texts and scripts (code) needed to complete each step. The workflow includes: collecting texts, the preprocessing, tokenization, and culling decisions made, unsupervised cluster analyses (k-means, hierarchical-agglomerative, bootstrap consensus tree), and supervised classification methods using Stylo in R's Delta, SVM, and NSC models. Each step represents scaffolding for a “teachable moment” with materials provided so faculty can more easily use them with students.

Scrubbing, Tokenization, Cutting, and
Culling
Lexos, a web-based, open-source workflow of tools (Kleinman, et al., 2016) was used to upload texts and “scrub” them by applying the following options: (i) convert words to lowercase, (ii) all punctuation was removed, (iii) however, a single word-internal hyphen and word-internal apostrophes were kept, and (iv) all digits were removed. Each individual word is considered as its own token. Larger stories were segmented (“cut”) into pieces. We experimented with various culling options, e.g., keeping only the most frequent words that appear in each text at least once.

Cluster Analysis
As a set of initial probes, we compared the contested story “A Dream” to (i) other stories attributed to Poe and (ii) mixed in with stories by other contemporaries. In the repo, we share four variations using cluster analysis:

1. K-means clustering on only Poe's stories (using Lexos)

2. Hierarchical agglomerative clustering on only Poe's stories (uses a Python sklearn module and a script to convert the cluster to ETE and Newick formats)

3. K-means clustering when all stories by each author are concatenated together (Lexos)

4. Bootstrap Consensus Tree (using Stylo in R).

The result from the Bootstrap Consensus Tree is shown in Figure 1. Of interest is that each author's stories cluster consistently together (with the exception that Bird's initial section of “Sheppard Lee” and his “Calavar” are found in different clades, at six and eight o'clock). “A Dream” clusters with the smaller Poe texts. As you'll see, we couldn't resist tossing in the four stories sometimes attributed to Edgar's brother Henry (“Monte Video”, “A Fragment”, “The Pirate”, and “Recollections”). These four stories are found within the cluster of Poe's known works (c.f. Collins, 2013).

A series of cluster analyses often serves well as a preliminary exploration, especially for scholars who are new to this game. Some of the file sizes are very small (e.g., one-half of the Poe stories in this corpus have fewer than 2000 words) and when strict culling is enforced (top-N words that appear at least once in each segment), the available set of words is reduced to only 38 when dealing with “A Dream” and the other eighteen Poe stories. That noted, these exploratory investigations shed some light on why some scholars consider that Poe's “first published tale may have been ‘A Dream'” (Silverman, 1991, p87).


Figure 1. Using Stylo in R Bootstrap Consensus Tree (BCT) showing “A Dream” consistently clustering with other Poe stories. The BCT aggregates results over multiple cluster analyses and shows those texts that satisfy a consensus number of the individual trials. Using 12 different authors

and at least two texts by each author for a total of 46 stories, Stylo formed clusters of the texts for the following frequency bands when using the most-frequent words: 100 to 1000 MFW.

Classification
Three classification models differentiated authorial writing style as implemented in Stylo in R. We scripted in R alongside Stylo to test “A Dream” over N-trials (N=10, 100) using a random selection of files for training sets in each trial. At least one text from each author is also included in the test set for each trial. A followup Python script parses the collected results to build confusion matrices for each author to provide metrics on how well the models predict each author's works. The most-frequently occurring, top-40 words (MFW, 1-grams) that appear in all the texts at least once were used.

Confusion Matrix values for all Poe Stories

Model

Attributions of

“A Dream” to Poe

True+

True-

False+

False-

Delta

9

13

200

0

7

NSC

10

16

170

30

4

SVM

7

14

198

2

6

Table 1: Attributions of the contested story “A Dream” over ten (10) trials with “A Dream” and another randomly selected Poe story in the test set in every trial. Confusion matrix values for results of testing Poe texts over all trials provide overall measures of model effectiveness. In the three cases

where “A Dream” was attributed to a different author, Poe was ranked second.

Summary
We offer a start to an exploration to collect evidence

as to whether Poe may have written the 1831 story “A

Dream” (c.f., Schoberlein (2016) who used the most frequent character 3-grams and attributed the story to Poe using Delta, but not so when using NSC nor SVM models). Evidence and methods aside, a GitHub repo provides a framework to share experimental workflows in a spirit similar to Jupyter notebooks, as well as one that facilitates both reproducible results and opportunities for subsequent contributions.

Notes
Forming an appropriate corpus is hard: thanks to Sam Coale, Ryan Cordell, Cary Gouldin, David Hoover, Shirrel Rhoades, and Ted Underwood. Four undergraduates: Weiqi Feng, Alec Horwitz, Jingxian Liu, and Khaled Sharafaddin worked with us on this problem. Thanks to Maciej Eder for his help with Stylo in R.

Sinclair, S. and Rockwell, G., (2016). Voyant Tools. Web: http://voyant-tools.org/.

Bibliography
Crymble, A., Gibbs, F., Hegel, A., McDaniel, C., Milligan, I., Taparata, E., Visconti, A., and Wieringa, J.,

eds. (2016). The Programming Historian. 2nd ed.. Web: http://programminghistorian.org/.

Eder, M., Kestemont, M. and Rybicki, J. (2016). Stylometry with R: A package for computational text analysis. R Journal, 16(1): 107-121.

GitHub repository:    A Good    Mystery.    .

https://github.com/WheatonCS/aGoodMystery

Jockers, M. (2014). Text Analysis with R for Students of Literature. Springer, New York.

Kleinman, S., LeBlanc, M.D., Drout, M., and Zhang, C.

(2016). Lexos v3.0. Web: http://lexos.wheatoncol-lege.edu.

Rudman, J. (2012). The State of Non-Traditional Authorship Attribution Studies -- 2012: Some Problems and Solutions. English Studies, v93(3), 259-274.

Schoberlein, S. (2016). Poe or Not Poe? A Stylometric Analysis of Edgar Allan Poe's Disputed Writings. Digital Scholarship in the Humanities, July 24, 2016.

Silverman, K. (1991). Edgar A. Poe: Mournful and Never-Ending Remembrance. HarperCollins, New York.
Lexos is a browser-based suite of tools that helps lower barriers of entry to computational text analysis for humanities scholars and students. Situated within a clean and simple interface, Lexos consolidates the common pre-processing operations needed for subsequent analysis, either with Lexos or with external tools. It is especially useful for scholars who wish to engage in research involving computational text analysis and/or wish to teach their students how to do so but lack the time for a manual preparation of texts, the skill sets needed to prepare their texts analysis, or the intellectual contexts for situating computational methods within their work. Lexos is also targeted at researchers studying early texts and texts in non-Western languages, which may involve specialized processing rules. It is thus designed to facilitate advanced research in these fields even for users more familiar with computational techniques. Lexos is developed by the Lexomics research group led by Michael Drout (Wheaton College), Mark LeBlanc (Wheaton College), and Scott Kleinman (California State University, Northridge). It is built on Python 2.7-Flask microframework, with jQuery-Bootstrap UI, and visualizations in d3.js. The Lexomics research group provides access to an public installation of Lexos which does not retain data after a session has expired. Users may also install Lexos locally by cloning the GitHub repository.

Lexos guides users through a workflow of steps that reflects effective practices when working with digitized texts. The workflow includes: (i) uploading Unicode-encoded texts in plain text, HTML, or XML formats; (ii) “scrubbing” functions for consolidating preprocessing decisions such as the handling of punctuation, white-space, and stop words, the use of lemmati-zation rules, and the handling of embedded markup tags and special character entities; (iii) “cutting” texts into segments based on the number of characters, tokens, or lines, or by embedded milestones such as chapter breaks; (iv) tokenization into a Document Term Matrix of raw or proportional counts using character or word n-grams; (v) visualizations such as comparative word clouds per segment (including the ability to visualize topic models generated by MALLET); Rolling Window Analysis that plots the frequency of string, phrase, or regular expression patterns or pattern-pair ratios over the course of a document or collection; and (vi) analysis tools including statistical summaries, hierarchical and k-means clustering, cosine similarity rankings, and Z-tests to identify the relative prominence of terms in documents, document classes, and the collection as whole. At each stage in the workflow the user may download data, visualizations, or the results of the analytical tools, along with metadata about their preprocessing decisions or the parameters selected for their experiments. Lexos thus enables the export of data for use with other tools and facilitates experimental reproducibility.

Lexos {scrubber} An Integrated Lexomics Workflow

Scrubbing Options

Q Remove All Punctuation

B Keep Hyphens ©

Q Make Lowercase

B Keep Word-Internal Apostrophes©

Q Remove Digits

■ Remove Whitespace 0

B Scrub Tags 0

Additional Options

Stop Words/Keep Words O >


Previews of Documents


A1.3_Dan_T00030.txt




Gefr&ae;gn ic Hebreos eadge lifgean in Hierusalem goldhord d&ae;lan cyningdom hab ban swa him gecynde w&ae;s si&d;&d;an &t;urh metodes m&ae;gen on Moyses hand w

ealra gesceafta drihten and waldend se him dom forgeaf unscyndne bl&ae;d eor&d;an rices and &t;u lignest nu &t;&ae;t sie lifgende se ofer deoflum duge&t;um wealde&d;. A3.3_Az_T00130.txt


orn dryhten herede wis in weorcum ond &t;as Word acw&ae;&d;: Meotud allwihta &t;u eart meahtum swi&d; ni&t;as to nerganne. Is &t;in noma m&ae;re wütig ond wul h




Special Characters 0 v


Lemmas 0 v


Consolidations 0 v


Figure 1: The Lexos Scrubber Tool

Lexos addresses three significant challenges for our intended users. The first challenge involves the adoption of computational text analysis methods. Many approaches require proficiency with command line scripting or the use of complex user interfaces that require time to master. Lexos addresses this problem through a simple, browser-based interface that manages workflow through the three major steps of text analysis: pre-processing, generation of statistical data, and visualization. In this, Lexos resembles Voyant Tools (Sinclair and Rockwell, 2016), although Lexos places more emphasis on and providing more tools for preprocessing and segmenting texts. Lexos also shares with tools like Stylometry with R (Eder, et al., 2013; Eder, 2013) and emphasis on cluster analysis, providing both hierarchical and K-Means clustering with silhouette scores as limited form of statistical validation. While Lexos is not a topic modeling tool, it provides a useful “topic cloud” feature for MALLET data that will be useful for beginners since there are few accessible ways to visualize MALLET output that work well out of the box.


Figure 2: The Lexos Multicloud tool showing Chinese "topic clouds"

The second challenge is the opacity of the procedures required to move between computational and traditional forms of text analysis. In order to reduce the “black boxiness” of algorithmic methods, Lexos contains an embedded component called “In the Margins” which provides non-technical explanations of the statistical methods used and effective practices for

handling situations typical of humanities data. “In the

Margins” is a Scalar “book” which can be read separately; however, its individual pages are embedded in Lexos using Scalar's API, making them easily accessible for users of the tool. Lexos shares with tools like Voyant an engagement with the hermeneutics of text analysis and attempts to embed “In the Margins” discussion of these issues in the user interface close to the user's workflow. We hope “In the Margins” will host advice and commentary from contributors with the Digital Humanities community.

A third challenge is the tension between quantitative and computational approaches and the traditions of theoretical and cultural criticism that dominate the humanities in the academy. As Alan Liu (2013) has recently argued, the challenge is to give a better theoretical grounding to the hybrid quantitative-qualitative method of the Digital Humanities by exploring the ways in which we negotiate the difficulties imposed by “the aporia between tabula rasa quantitative interpretation and humanly meaningful qualitative interpretation” (414). The design of Lexos and the discussions in “In the Margins” are intended to open a space for discussion of issues related to the opacity of algorithmic approaches and the limitations and epistemological challenges of computational stylistic analysis and visual representation of humanities data.

This poster presentation provides demonstrations of Lexos using some literature from Old, Middle, and Modern English, as well Chinese, which are in our current test suite. We also discuss use cases and best practices, how to install Lexos locally, and how scholars may contribute to the still growing content of “In the Margins”.

Bibliography

Drout, M., Kleinman, S., and LeBlanc, M. 2016-. “In the Margins.” http://scalar.usc.edu/works/lexos./

Eder, M. (2013). “Mind Your Corpus: Systematic Errors in Authorship Attribution.” Literary and Linguistic Computing 28 (4): 603-14.

Eder, M., Kestemont, M., and Rybiki, J. 2013. “Stylometry with R: A Suite of Tools (Abstract of Poster Session)”. Presented at Digital Humanities 2013, Lincoln, Nebraska. http://dh2013.unl.edu/abstracts/ab-136.html, https://sites.google.com/site/computationalstylistics/

Kleinman, S., LeBlanc, M.D., Drout, M. and Zhang, C. 2016. Lexos v3.0. https://github.com/WheatonCS/Lexos/.

Liu, A. (2013). “The Meaning of the Digital Humanities.” PMLA 128 (2): 409-23.

McCallum, A.K. (2002). MALLET: A Machine Learning for

Language Toolkit. http://mallet.cs.umass.edu.

Sinclair, S., and Rockwell, G. (2016). Voyant Tools. Web.

http://voyant-tools.org/.
Introduction
Voces (from Lat. vox 'voice', 'word') is an analysis

and visualisation dashboard for corpus-based research in lexical semantics. Currently developed as a Shiny application communicating with R session running in the background, Voces provides users with possibly exhaustive account of how selected Latin word is distributed across the corpus and what can be told about its meaning. The application is built around a corpus which currently consists of ca. 200M words from texts dating from the Classical era (1 BCE) to the Middle Ages (14th CE). Although Voces was originally conceived as a tool of historical semantics research, the application - due to its modular design - may be modified and the code basis can be re-used in new research contexts.

Lexical Semantics with R


Information computed on a basis of a CWB-indexed corpus is presented to a user through a single-page interface composed of separate widgets arranged in a clear grid layout. Each widget is responsible for displaying in textual or graphical form a clear-cut property of word's distribution or meaning. A heavy use of data visualisation techniques renders Voces a convenient tool for exploratory analysis of textual corpora, but the grid layout is also reflection of modular architecture of the application. Each widget is implemented as a separate function which can be extended and adopted by researchers with even limited R programming skills.

Use scenarios
A typical use scenario is triggered when the user specifies a lemma to be looked up. If the search fails, a list of lemmas to choose from is provided. In case of success, neatly separated sections of the dashboard are populated with widgets, each of which corresponds to one sense or distributional property of the word under scrutiny.


Fig. 2: Voces. User Interface: Frequency Spectrum Plot (Voces. User Interface (tempus 'time')

Word's frequency is summarised as a number of occurrences in the corpus (both raw and p.m.w. counts) and displayed as a highlighted point on a frequency spectrum plot (Baayen 2001). A barplot is provided for investigating change of frequency in subsequent corpus sections. Study of language variation is enabled through widgets presenting word's frequency as a function of such variables as author, work, genre, and - most importantly - time. Users are, therefore, provided with a list of authors who use the word most frequently or a word cloud summarising terms to be found in the titles of works with a particularly frequent use of the word under scrutiny. Genre variation is presented in form of a pie chart, while diachronic dimension - through a bar plot of frequency counts in partitions of the corpus. Diatopic variation study is still to be implemented.

A word's meaning potential can be investigated by means of a set of widgets presenting its contextual properties. The most frequent co-occurrences are enumerated on a simple count list which may be further analysed according to period and genre criteria. A Distributional Semantics Model (Baroni and Lenci 2010) is built from the corpus in order to enable simple meaning computation. Evert's (2014) wordspace package and a set of Alain Guerreau's scripts is employed in order to cluster co-occurrences. Similar terms of a looked up word are also computed and then presented in both textual and graphical form.

Users are supported in data and visualisation interpretation through hints which accompany every widget. Their role is to explain not only what the data can mean, but also how the figures were computed, how one can interpret the geometrical properties of a plot, and so on. This, along with the availability of data sets, code snippets, and reports generated on the fly, is what makes Voces a tool of reproductive research.

Architecture
Voces was built as a Shiny application (Chang et al. 2016). Its development was greatly facilitated by the availability of a decent documentation and community support (both particularly useful when dealing with framework's complex reactivity model). It turned out soon, however, that it may not be the best choice for web application which has to combine heterogeneous data and non-R code as well. Hence, other solutions are being tested at the moment, those in particular which would provide, for example, more flexible integration of external APIs. The most promising seems to be OpenCPU (Ooms 2014), an application which exposes R session through a RESTful API. This approach allows any application written in some of the less or more popular web development frameworks to easily communicate with an R server instance.

As for the architecture, Voces depends on a CQP server instance running in the background which requires corpora to be indexed with the CWB. Communication of the R server with the CWB is assured through the rcqp package (Desgraupes and Loiseau 2012) which offers a set of useful functions providing access to both positional (token-level) and structural (document-level) attributes. Unfortunately, development of this very helpful tool seems to be less active recently and thus Voces will soon accept also tabular data as input.

Previous research
Nowadays, corpus linguists may chose from a vast array of free, open source and stable corpus query systems (CQS) which not only allow for efficient indexing of large corpora, but also provide a user-friendly concordance interface and offer out-of-the-box a set of such essential functionalities as collocation lists, simple corpus statistics etc. Both web (CQPweb, NoSketchEngine etc.) and desktop applications (TXM etc.) are also usually equipped with a less or more intuitive corpus management interface. Voces, a dashboard for vocabulary research, is not yet another CQS and has no intention to supersede well-established tools which cannot be easily combated in terms of either robustness or speed. Quite the contrary, the application communicates with the CWB engine and adapts some of the design choices and features of the popular CQS, while hopefully does not inherit their drawbacks.

Unlike the case of the well-known CQS, more emphasis has been put on quick access to multifaceted information rather than on close analysis of occurrences. Voces does not attempt, then, to implement some of the features which are traditionally considered an important part of the corpus analytical toolbox, such as concordance sampling, sorting etc. Undoubtedly, the strength of popular CQS lies in their wide applicability: by default, they do not preclude any research scenario. Although agnostic of linguistic theory, Voces was originally built for more specific purposes and focuses on semantic properties of the word and its distribution.

What is believed to be one of the main advantages of the present application is that - thanks to its modular architecture - it can be easily extended or adopted by a researcher with even moderate programming skills. In that Voces attempts to fill the gap that exists between, on the one hand, fully-blown CQS, which are normally quite conservative when it comes to adding

new features, and, on the other hand, single-purpose

research workflows built ad hoc by researchers. What also distinguishes Voces from other CQS is its emphasis on helping users to interpret data. A system of visual and textual hints keeps a researcher informed about where does the data come from, how have they been computed etc.

The grid layout is well-known from analytical environment and is especially popular in finances or engineering (Few 2013); in humanities it was adopted, among others, in the Voyant Tools project. It offers a quick insight into otherwise dispersed data and a coherent account of word's properties.

Further research
Voces is currently in an early stage of development. The work focuses on adding new functionalities and plotting types which may sometimes affect application's efficiency. Future work will focus on: (1) optimising user experience; (2) implementing tools for (a)

comparative (ie. two-lemma) research and (b) tracking language change; (3) better processing user input (multi-word search).

Bibliography
Baayen, R. H. (2001). Word Frequency Distributions. Dordrecht: Kluwer.

Baroni, M., and Lenci, A. (2010). Distributional Memory: A

General Framework for Corpus-Based Semantics. Com-

putationalLinguistics 36 (4): 673-721.

Chang, W., Cheng, J., Allaire, J. J., Xie, Y., and McPherson,

J. (2016). Shiny: Web Application Framework for R.

https://CRAN.R-project.org/package=shiny.

Desgraupes, B., and Loiseau, S. (2012). Rcqp: Interface to the Corpus Query Protocol. http://CRAN.R-pro-ject.org/package=rcqp.

Evert, S. (2014). Distributional Semantics in R with the Wordspace Package. In Proceedings of COLING 2014, the

25th International Conference on Computational Linguistics: System Demonstrations, 110-114. Dublin, Ireland:

Dublin City University and Association for Computational Linguistics.

Few, S. (2013). Information Dashboard Design: Displaying Data for at-a-Glance Monitoring. Burlingame, CA: Analytics Press.

Nowak, K., and Bon, B. (2015). Medialatinitas.eu. Towards

Shallow Integration of Lexical, Textual and Encyclopaedic Resources for Latin. In Electronic Lexicography in the

21st Century: Linking Lexical Data in the Digital Age. Proceedings of the eLex 2015 Conference, edited by Iztok

Kosem, Milos Jakubi'cek, Jelena Kallas, and Simon Krek, 152-69. Ljubljana-Brighton: Trojina, Institute for Applied Slovene Studies - Lexical Computing Ltd.

Ooms, J. (2014). The OpenCPU System: Towards a Universal Interface for Scientific Computing through Separation of Concerns. ArXiv E-Prints, June.
Introduction

Pedagogy in the Digital Humanities is now leaving its “bracketed” state - a term used by HIRSCH 2012 to emphasise the fact that this dimension was not given the consideration its practical importance deserves. As programmes and courses are created on a larger scale and increasingly drive institutional strategies, also in Europe (see Sahle, 2013 and the DARIAH Digital Humanities Course Registry), it becomes essential to make comparisons and shared reflections possible.

Since 2014 all students of Greek and Latin languages and literatures at the Université Paris-Ouest Nanterre (France) have been enrolling in a Master programme entitled “Humanités classiques et humani-tés numériques.” Each semester features a fully fledged course of Digital Humanities: it is therefore an experiment in embedding Digital Humanities into an existing discipline, or rather into the array of disciplines which constitute the field of Classical studies around its philological backbone.

The aim of this poster is to share the approach I take in designing and teaching these courses, and to reflect on what this experience suggests about digital educational models, in Classics and beyond.

The poster will have three components, devoted to situating, describing and comparing the courses. Context and History

I will set out the conditions in which the curriculum was reformed (which involves both national and local contexts), the specific problems encountered (as the heterogeneous levels and motivations of the students, the relationships with the other courses, the available technical options, or the recent introduction of podcasting and distance learning), as well as the rationale and methods which shape the courses, including its main sources of inspiration in the Digital

Humanities community, whether online syllabi or publications like Jockers (2014) and Rockwell and Sinclair (2016).

Overview of the Courses

The courses alternately take the form of more traditional classes and collaborative or personal projects. Across the two years, their contents include theoretical and historical insights, while concentrating on hands-on experience: digital literacy elements are gradually integrated as students go from traditional scholarly editing recreated in Markdown and HTML to critical editing in TEI XML (the focus of year 1) and, beyond text and editing, discover computer-assisted analytical and visualisation methods with the Voyant Tools software environment and then work in a literate programming framework (For which the canonical reference is Knuth, 1984) implemented in R Markdown (the focus of year 2, see Figure 1).


Figure 1: Text analysis in RStudio

The principles of the courses will be expounded: favouring active participation, learning-by-doing and flipped classroom teaching; insisting on the critical, reflexive dimension of digital procedures; promoting free resources like TEI by Example (Van den Branden, Terras, and Vanhoutte) and The Programming Historian (Crymble et al), as well as data reuse; developing an open publication culture through the Classiques et numériques blog maintained by the students (see Figure 2) or a shared Zotero group library; creating an awareness of the surrounding Digital Humanities communities; fostering actual collaboration, both between the students and with other projects or programmes - to date, with another MA specialised in Web design on an online edition prototype, with the Pelagios Commons project on the annotation of place names and with the Sunoikisis

Digital Classics network in its effort to collectively define a core syllabus.


Figure 2: Classiques et numériques, the blog of the MA

Sahle, P. (2013). “DH Studieren! Auf Dem Weg Zu Einem Kern- Und Referenzcurriculum Der Digital Humanities.” DARIAH-DE Working Papers, 1. http://web-

doc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf.

Van den Branden, R., Terras, M. and Vanhoutte, E. (n.d.)

“TEI by Example.” accessed 1 November 2016.

http://teibyexample.org/.

Comparing Models

Finally, drawing on this experience I will address several aspects of the current development of Di-gital Humanities pedagogy: as a separate entreprise or within established disciplines, with or without infrastructural, collegial or cross-departmental support, in various time formats, with different modes of external collaboration, etc. To sketch this broader typology, I will compare this French series of courses with other models, using in particular the data contributed to the aforementioned Digital Humanities Course Registry.

The poster will be in English, but I will naturally interact with the audience of the poster sessions both in English and French.

Bibliography

Crymble, A., Gibbs, F., Hegel, A., McDaniel, C., Milligan, I.,

Taparata, E., Visconti, A. and Wieringa, J. (eds.) (n.d.)

. The Programming Historian. http://programmin-ghistorian.org/.

Hirsch, B. (ed.) (2012). Digital Humanities Pedagogy: Practices, Principles and Politics. Open Book Publishers. http://www.openbookpublishers.com/reader/161.

Jockers, M. (2014). Text Analysis with R for Students of Literature. New York: Springer.

Knuth, D. (1984). “Literate Programming.” The Computer

Journal, 27(2): 97-111. http://comjnl.oxfordjour-

nals.org/content/27/2/97.short.

Rockwell, G. and Sinclair, S. (1984). Hermeneutica. Computer-Assisted Interpretation in the Humanities. Cambridge, Massachusetts: MIT Press.
Following the publication of Franco Moretti’s GrapHs, Maps, Trees, scholars looking to apply digital humanities methods to literature have increasingly been drawn to “distant reading.” The influence of distant reading in digital humanities is apparent not only in the work it has inspired (see, among others, Cordell and Smith; Elson, Dames, and McKeown; Jockers; Long and So; Rhody; and Underwood) but also for its regular inclusion as a method in courses introducing DH. “Teaching digital humanities,” it turns out, often means “teaching distant reading.”

Teaching students the techniques of distant reading can be challenging as it depends on re-framing the familiar object of study. But another difficulty altogether is that this approach depends on a digitized corpus; and such a corpus, in turn, depends on someone, somewhere doing the difficult labor of digitization. One might ask, then: if “teaching digital humanities” means “teaching distant reading,” shouldn’t it also mean “teaching digitization”?

In this paper, I will discuss a collaborative, multiyear assignment that I conducted in two of my “Introduction to Digital Humanities” courses at Emory University: the digitization and analysis of the complete works of Ernest Hemingway (Croxall). With the goal of teaching my students not only how to do distant reading but also about the intense labor that goes into corpus preparation, we digitized the whole of Hemingway’s work in just two weeks. Working from newly purchased copies of the texts, the students and I rapidly scanned hundreds of pages, performed and corrected optical character recognition, and assembled a corpus—with each of us spending no more than 4 hours on the task. Our from-scratch corpus was composed expressly so we could draw important distinctions among Hemingway’s works: individual works vs the whole collection; fiction vs non-fiction; and works published before while Hemingway was alive vs those that appeared after his death in 1961. I will detail what we learned from rapid digitization and how those lessons affected the second iteration of the assignment.

After preparing the corpus, students worked in groups to analyze the many works of Hemingway that they had not had time to read. Making use of Voyant Tools, they identified themes in the corpus and charted patterns that could never have been observed through regular, close reading methods. For example, the class confirmed that while Hemingway insists on writing about “men,” the women to whom they are attached are inevitably just “girls.” In an attempt to chart the patterns of Hemingway’s diction, another group of students investigated the terms he uses to introduce dialogue. Unsurprisingly, the students discovered that “said” is by far the most frequent such term across the entire corpus. What was more surprising, however, was to observe that in late and posthumous writings, the frequency of “said” suddenly drops by 50%. In short, by building our own corpus from scratch, the students were able to conduct original research, something that is relatively rare for many undergraduates in humanities programs.

Building our collection of texts from scratch had two critical advantages. First, we were able to create a small, relatively clean corpus whose provenance we knew. This provided a sense of confidence in the data as we began to distant read. Furthermore, while our analysis of Hemingway’s works was “distant” compared to traditional close reading of a single novel or story, it was not nearly as distant as projects that deal with several thousand texts. We became engaged, in short, in close-distant reading. Second, digitizing the texts ourselves allowed us to skirt a problem that frequently plagues distant reading texts from the twentieth century: copyright. As an educational endeavor focused on teaching the students how to prepare their research materials, this guerilla digitization project fell under the regime of fair use in the United States.

To close, I will discuss how students at Brown University and I have taken further steps with the Hemingway corpus and with their digital humanities education as we have used it as a means to explore the methods and utility of topic modeling. Topic modeling is frequently deployed to come to terms with large and unwieldy corpora (see Jockers; Nelson; Nelson, Mimno, and Brown; Underwood and Goldstone). But working with a small, relatively clean corpus that is created from scratch allows students to better understand what takes place via unsupervised machine learning. At the same time, topic modeling allows us to ask in a new way some of the same questions that my former students had already uncovered: how does Hemingway’s dialog differ from his prose? how different are the topics in Hemingway’s fiction from those of his non-fiction? to what degree does his late—or even posthumous—work differ from what he wrote three decades earlier?

In the end, the process of modeling Hemingway becomes a means by which we can model all of digital humanities—both analysis and corpus creation—in a student-focused environment (see also Brier; Croxall and Singer; Harris; Hirsch; Jewell and Lorang; and Swafford). By doing digital humanities from scratch, students can be engaged in original research and see for themselves, from start to finish, how digital humanities gets done.

Bibliography

Brier, S. (2012). “Where’s the Pedagogy? The Role of

Teaching and Learning in the Digital Humanities.” In

Gold, M. K. (ed), Debates in the Digital Humanities.

Minnesota University Press, pp. 350-367.

Cordell, R. and Smith, D. A. (2017). Viral Texts: Mapping

Networks of Reprinting in 19th-Century Newspapers and

Magazines. http: //viraltexts.org/ (accessed 7 April

2017).

Croxall, B. (2015). “How to NOT Read Hemingway.” Intro to DH.

http://www.briancroxall.net/s15dh/assignments/how

-to-not-read-hemingway/ (accessed 7 April 2017).

Croxall, B. and Singer, K. (2013). “The Future of

Undergraduate Digital Humanities.” Digital Humanities

2013, Lincoln, NE, July 2013.

Elson, D. K., Dames, N. and McKeown, K. R. (2010).

“Extracting Social Networks from Literary Fiction.”

Proceedings of the 48th Annual Meeting of the

Association for Computational Linguisticsi, Uppsala,

Sweden.

http://www.cs.columbia.edu/~delson/pubs/ACL2010-

ElsonDamesMcKeown.pdf (accessed 7 April 2017).

Goldstone, A. and Underwood, T. (2014). “The Quiet

Transformations of Literary Studies: What Thirteen

Thousand Scholars Could Tell Us.” New Literary History

45.3: 359-384.

Harris, K. D. (2011). “Pedagogy & Play: Revising Learning through Digital Humanities.” Digital Humanities 2011, Stanford, CA, June 2011.

Hirsch, B. D. (2012). Digital Humanities Pedagogy:

Practices, Principles and Politics. Open Book Publishers.

Jewell, A. and Lorang, E. (2016). “Teaching Digital Humanities Through a Community-Engaged, Team-Based Pedagogy.” Digital Humanities 2016, Krakow, Poland, July 2016.

Jockers, M. L. (2013). Macroanalysis: Digital Methods and Literary History. Urbana Champaign, IL: University of Illinois Press.

Long, H. and So, R. J. (2016). “Literary Pattern

Recognition: Modernism between Close Reading and Machine Learning.” Critical Inquiry 42.2: 235-267.

Moretti, F. (2013). Distant Reading. London: Verso. Moretti, F. (2007). Graphs, Maps, Trees. London: Verso.

Nelson, R. K. (2011). Mining the Dispatch.

http://dsl.richmond.edu/dispatch/ (accessed 7 April 2017).

Nelson, R. K., Mimno, D. and Brown, T. (2012) “Topic Modeling the Past.” Digital Humanities 2012, Hamburg, Germany, July 2012.

Rhody, L. M. (2013). “Revising Ekphrasis: Methods and Models.” The Association for Computers and the Humanities. http://ach.org/2013/12/30/revising-ekphrasis-methods-and-models/ (accessed 7 April 2017).

Sinclair, S. and Rockwell, G. (2017). Voyant Tools. http://voyant-tools.org/ (accessed 7 April 2017).

Swafford, J. E. (2016). “Read, Play, Build: Teaching Sherlock Holmes through Digital Humanities.” Digital Humanities 2016, Krakow, Poland, July 2016.

Underwood, T. (2013). Why Literary Periods Mattered: Historical Contrast and the Prestige of English Studies. Stanford: Stanford University Press.
Over the past year and a half, the Cyberinfrastructure for Digital Humanities (CyberDH) Group at Indiana University has been developing an open instructional workflow for text analysis that aims to build algorithmic understanding and basic coding skills before scaling up analyses (Gniady et al., 2017). We have chosen to bootstrap in R, a high level and high productivity language, with methods that are open, repeatable, and sustainable. The aim is to provide code templates that can be adapted, remixed, and scaled to fit a wide range of text analysis tasks. This poster presents our approach to teaching computational text analysis and a representative hypothetical case study in which two different users are able to start with the same corpus and adapt code to achieve very different end results in a way not currently possible with black box tools.

This paradigm is fundamentally different from that currently practiced by many in the digital humanities. Black-boxed tools with GUIs that hide computation are very popular for introducing new practitioners of text analysis in the digital humanities to basic algorithms and outputs. In 2012, AntConc was downloaded 120,000 times by users in 80 different countries (Anthony, 2014). Voyant 1.0 had 113 sites linking to it actively in 2012 (Sinclair and Rockwell, 2013) and the week Voyant 2.0 was released the server went down multiple times from excess traffic (@VoyantTools, 2016). However, one of its default corpora is the Shakespearean dramas, with speaker names and stage directions. ((Sinclair and Rockwell, 2016). The inclusion of speaker names skews all algorithms related to frequency counts of characters (e.g. word clouds), which a new user may not even think to take into account. Using AntConc's concordance tool with a Shakespearean corpora including speaker names gives an idea of when a character speaks and when a character is mentioned, but this conflation might not jump out at a new user. If anything, we suggest learning about algorithms first and then moving up to black-box tools when one has the means to critique them.

Having looked at popular “plug-and-play” tools for corpora visualization, it becomes evident that even simple visualizations can lead to inaccurate results if the user is not thinking through how a corpus is being processed to produce a result. We believe that if the user understands how the algorithm is generating visualizations, they can contribute more meaningfully to critiques of sophisticated algorithms when partnered with programmers or even go on to bootstrap themselves with awareness of their domain's particular caveats. Thus, we advocate teaching humanists the basics of coding to create conversant programmers similar to the methodology behind Matthew Jockers' Text Analysis with R for Students of Literature, but with a slightly slower ramp up. To this end we have a three-step process of introducing R: web-deployed Shiny apps, highly marked up RNotebooks, and lightly commented RScripts, both in “regular” and higher performance versions. All are available for download on Github (with associated sample data from Shakespeare and Twitter) (CyberDH Team, 2017). We hope that this simpler bootstrapping method that mixes code and explanation, pedagogy and self-driven inquiry, will be of use to those looking to onramp new practitioners who may go on to partner with programmers if needed or to remix available code to look at their own knowledge domain.

Bibliography

Anthony, L. (2016). Antconc 3.4.4. Software.

http://www.laurenceanthony.net/soft-

ware/antconc/.

Gniady, T. Thomas, G. and Kloster, D. (2017). Text Analysis Github Repository. https://github.com/cyberdh/Text-Analysis.

Jockers, M. (2014). Text Analysis with R for Students of Literature. New York: Springer International Publishing.

Sinclair, S. and Rockwell, G. (2013). “Voyant Notebooks: Literate Programming, Programming Literacy.” Digital

Humanities 2014: Conference Abstracts. Nebraska-Lin-

coin: http://dh2013.unl.edu/abstracts/ab-295.html.

Sinclair, S. and Rockwell, G. (2016). Voyant Tools. http://voyant-tools.org/.

@VoyantTools. Twitter. 8 April 2016.
Hermeneutica
In Hermeneutica, Geoffrey Rockwell and Stéfan Sinclair (2016) argue for an approach to the digital humanities that deemphasizes the tool and positivist notions of proof. Their proposed approach, also called Hermeneutica, champions tool accessibility over tool sophistication. Similarly, scholarly play is legitimated as a useful step in developing research questions and as a means to reconsider established notions within literary disciplines. The aim of Hermeneutica as a methodology seems to be the generation of interesting humanistic questions as much as the resolution of open questions.

Rockwell and Sinclair demonstrate the difference between Hermeneutica and typical DH approaches by quoting from Gary Wong's 2009 blog post:

[Typical DH] takes the worst part of the scientific papers (really really long sets of tabular data in the body of the text) and the worst part of papers from the humanities (really really complicated language where simple language would have done) and puts it in one. If this is what the cooperation of computational text analysis and traditional literary analysis yield, I

am scared.

Because Hermeneutica attempts to join the best parts of these fields, it has the potential to turn DH into a discipline that is more useful for the vast majority of non-DH humanists. It could be the means of accelerating the mainstreaming of DH methods and bringing us to the eventual point where all humanities are digital—a destination Claire Clivaz described succinctly (DARIAH, 2016).

Voyant
One feature that distinguishes Hermeneutica from many other DH approaches is its companion set of tools meant to demonstrate its application. Voyant Tools, now referred to simply as Voyant, is a web-based, modular suite of tools meant to be “worth thinking with” (Rockwell and Sinclair, 2016: 10, original emphasis). The goal is to accommodate playful exploration of text and sharing of corpora across the web. It is not designed as an industrial-grade text analysis tool, but as a “toy” that allows scholars to uncover new questions and gain new appreciation of texts.

Current limitations of Hermeneutica
A fundamental component of Hermeneutica is that the scholar views text through the lens of Voyant (or other computational text analysis tools), and then synthesizes that experience with their prior knowledge of the text and its milieu. A problem that Voyant addresses, but does not solve, is that many scholars who know the most about specific texts lack the technological skills that would be considered pre-novice in DH circles. Voyant allows everyone with a text and a browser to explore word frequencies, collocations, etc., but it presupposes that the text is available and clean enough for use. In order for Hermeneutica to appeal to non-DH humanities scholars, these issues of text availability and the lack of user skill must first be addressed.

On the issue of text availability, it is not often that scholars wish to analyze text that is rare or missing. More often they are interested in text that is protected by various copyright laws, which prohibit posting the text to public websites such as Voyant. Thankfully, in the Unites States at least, Google Books' recent court victory (Stohr, 2016) now permits scholars to publish online the analysis results derived from copyrighted texts, so long as the original text is not recoverable by the user. To this end Rockwell and Sinclair developed Voyant 2's “non-consumptive” mode which restricts access to tools that allow full-text views.

While such developments represent Rockwell and Sinclair's amenability to meet the ever-evolving needs of Hermeneuticans, accommodating users' lack of technology skill is beyond the scope of their involvement. For example, it is not reasonable to expect the Voyant developers to be concerned over issues of text acquisition or text preparation. Rather, those con-cerns—while critical to expanding the pool of potential Hermeneuticans—are issues of local implementation. Similarly, it makes sense that Voyant would offer the ability to link to a corpus after uploading the text, but uploading the text and keeping track of various versions of corpora is beyond the scope of Voyant. A local practice of adding some structure around the

Voyant suite ought to make Hermeneutica useful to a far greater audience than it is now.

Scaffolding
In the field of instructional design, such structure is called scaffolding. Specifically, scaffolding refers to the process of providing learners adequate introduction and examples before allowing them to attempt a task on their own (Bruner, 1978). For scaffolded Her-meneutica, DH-savvy professionals can work to acquire, clean, and upload text to Voyant (and other tools), and then provide public listings of the resulting corpora.

Examples of scaffolded Hermeneutica
We have implemented this scaffolded Hermeneu-tica approach in our Office of Digital Humanities beginning with the Cormac McCarthy Corpus Project (CMCP). The CMCP includes 13 Voyant corpora of McCarthy's 10 novels: one for the complete works, one for each novel, and two for novels (The Orchard Keeper and The Road) where the narration has been segregated from the dialogue. But the linchpin of scaffolded Hermeneutica is the CMCP's publicly-accessible website that organizes these Voyant corpora. The website is built on WordPress with the Pods content management plugin, and contains information on McCarthy's work, descriptions of Voyant (and other tools), and listings of links to the Voyant corpora. An essential feature of the website's structure is the ability to accommodate revisions to the current corpora as well as the addition of other tools in the future. Already, there is a non-Voyant sentence structure search tool attached as a beta-testing option.

A rough version of the Cormac McCarthy Corpus Project was presented at the 2015 conference of the Cormac McCarthy Society. The reaction to these tools being available for public use was strongly positive. One attendee referred to the website as “a game-changer.”

The same scaffolded Hermeneutica is being implemented on two other projects: Machado a longa distancia and The Modernist Short Fiction Project. Preliminary demonstrations of the approach have yielded similar reactions to what we observed with the CMCP. Non-DH scholars become excited rather than anxious when the digital analysis tools are scaffolded to provide them ready access. In fact, these demonstrations turn into play sessions where non-DH scholars repeatedly request for certain words to be added to the frequency charts and other Voyant panels.

Hermeneutica and Voyant represent the greatest potential for growth in DH not because they are the most technologically or theoretically advanced developments, but because they are the most accessible to non-DH scholars. Still, they don't quite reach the ground level of technology skills possessed by most researchers in the humanities. The scaffolded Herme-neutica approach proposed in this paper seems to span that gap to make Hermeneutica more accessible.

Bibliography
Bruner, J. S. (1978). “The role of dialogue in language acquisition.” In Sinclair, A., Jarvelle, R., J., and W. J.M. Levelt (eds), The Child's Concept of Language. New York: Springer-Verlag.

DARIAH (2016). My Digital Humanities - Part 1. YouTube.

https://www.youtube.com/watch?v=I8aRtHW3b6g

(accessed 1 November 2016).

Rockwell, G. and Sinclair, S. (2016). Hermeneutica. Cambridge: MIT Press.

Stohr, G. (2016). Google Book Project Can Proceed as Supreme Court Spurns Appeal. Bloomberg Politics. http://www.bloomberg.com/politics/articles/2016-04-18/google-book-project-can-proceed-as-top-u-s-court-spurns-appeal (accessed 1 November 2016).

Conclusion

        
            Abstract 
            In this workshop for non - coders, participants will be guided through two tasks: the first task will guide participants in creating an application to tap into Twitter’s API, in our case to get Twitter data. The second task will guide participants in the use of a Google spreadsheet to capture streaming (live) data from Twitter in order to archive it, download it and perform text analysis, data visualization and other studies. This workshop will include a brief introduction contextualizing social media data collection good practices including user data privacy issues. 
            
                Keywords: Archiving, Data Collection, Social Media, Twitter, Text Analysis 
            
            Rationale 
            Twitter data can be very valuable for researchers of perhaps all disciplines, not just DH. Given the difficulties to properly collect and analyse Twitter data as viewable from most Twitter Web and mobile clients (as most people use Twitter) and the very limited short - span of search results, there is the danger of losing huge amounts of valuable historical material. 
            Tweets are like butterflies – one can only really look at them for long if one pins them down out of their natural environment. The reason why we have access to Twitter in any form is because of Twitter’s API, which stands for Application Programming Interface. Free access to historic Twitter search results is limited to the last 7 days. This is due to several reasons, including the incredible amount of data that is requested from Twitter’s API, and – this is an educated guess – not disconnected from the fact that Twitter’s business model relies on its data being a commodity that can be resold for research. Twitter’s data is stored and managed by Twitter’s enterprise API platform. 
            For the researcher interested in researching Twitter data, this means that harvesting needs to be do ne not only through automated means but in real time. It also puts scholars without the required coding and data mining skills at a disadvantage. As a researcher, this basically means that there is no way to do proper research of Twitter data without understanding how it works at API level, and this means understanding the limitations and possibilities this imposes on researchers. 
            What’s a n individual researcher without access to pay corporate access to do? The whole butterfly colony cannot be captured with the nets most of us have available. At small scale, however, and collecting in a timely fashion, it is still possible to capture interesting and more - or - less complete specimens using fairly simply, non - coding required methods. (The Library of Congress h s now 12 years’ worth of text - only Tweets. However, as before, the Library of Congress Twitter collection will remain embargoed and there was no projected timetable for providing public access as of 26 December 2017). 
            Most researchers out there are likely not to benefit from access to huge Twitter data dumps. For researchers without much resources that are trying to do the talk whilst doing the walk, and conduct research 
                on Twitter and 
                about Twitter, this workshop and tutorial will guide participants into creating a Twitter application in order to tap into the Twitter API, followed 
            
            by the setting up of a Twitter Google Archiving Spreadsheet. Once a trial archive or dataset has been collected, we will attempt text analysis and basic visualisations using Excel and Voyant Tools. This workshop will include a brief introduction contextualizing social media data collection good practices including user data privacy and research ethics issues. 
            Workshop Requirements 
            • Room with projector and screen 
            • Wifi access 
            • Power plugs for participants to charge devices if required 
            Participants Requirements 
            • Interest in collecting small Twitter datasets and basic Text Analysis 
            • Wifi - enabled Laptop with Excel or similar spreadsheet software 
            • Twitter account, and the login credentials to access it (username and password) 
            
                Tools We’ll Use 
            
            • TAGS 
            https://tags.hawksey.info/ 
            • Voyant Tools 
            https://voyant - tools.org/ 
            El taller se puede dar también en español o bilingüe inglés - español. 
        
        
            
                
                    Bibliography
                    For complete references please follow links in the referenced outputs below and in the body of the text above. 
                    Priego, E. 2018. #rfringe17: Top 230 Terms in Tweetage. 
                    
                        https://epriego.blog/2017/08/05/rfringe17-top 230-terms-in-tweetage/
                    
                    [Accessed 30 January 2018] 
                    Priego, E., 2016. Bar Chart: Number of #DH2016 Tweets in Archive per Conference Day (Sunday 10 to Friday 15 July 2016 GMT). Available from: 
                    
                        https://figshare.com/articles/Bar_Chart_Number_of_DH2016_Tweets_in_Archive_per_Conf erence_Day_Sunday_10_to_Friday_15_July_2016_GMT_/3490001/1 [Accessed 31 Jan 2018]. 
                    
                    Priego, E. 2016. “Stronger In”: Looking Into a Sample Archive of 1,005 StrongerIn Tweets. 
                    
                        https://epriego.blog/2016/06/21/stronger-in-looking-into-a-sample-archive-of-1005- strongerin-tweets/ [Accessed 30 January 2018] 
                    
                    Priego, E. and Zarate, C., 2014. #MLA14 Twitter Archive, 9 - 12 January 2014. Available from: 
                    
                        https://figshare.com/aticles_MLA14_Twitter_Archive_9_12_January_2014/924801/1
                    
                    [Accessed 31 Jan 2018]. 
                    Priego, E. 2014. Some Thoughts on Why You Would Like to Archive and Share [Small] Twitter Data Sets. Available from:
                    
                        https://epriego.blog/2014/05/28/some-thoughts-why-you-would-like-to-archive-and-share-twitter-small-data / [Accessed 30 January 2018] 
                    
                    Priego, E. 2014. Publicly available data from Twitter is public evidence and does not necessarily constitute an “ethical dilemma”. London School of Economics Impact Blog. Available from:
                    
                        http://blogs.lse.ac.uk/impactofsoc ialsciences/2014/05/28/twitter-as-public-evidence/ [Accessed 30 January 2018] 
                    
                
            
        
    

        
            The poster introduces a project to develop a visualization application for a unique data source on Czech sciences. Information Register of R&amp;D Results (RIV) is the Czech Republic’s inventory of the outputs of basic and applied research since 1992. Although it is potentially an important source of data for analyses of various aspects of the intellectual organization and publication culture in Czech sciences, this particular data source has earned itself a pejorative nickname – “a coffee grinder” – for its central role in purely mechanistic science evaluation in the country.
            By employing text-mining technique that are standard in the digital humanities and by getting inspiration from visualization platforms such as 
                
                    Voyant Tools
                 (Sinclair and Rockwell 2012), the project aims to contribute to the shift in the Czech narrative of science evaluation from the exclusively bibliometric perspective to a more diverse one. For example, the hope is that the visual display of the plethora of topics that are discussed in the research outputs registered in RIV will implicitly criticize the myopic vision in which all disciplines are leveled to the singular measure of the number of publications. The latter system is not only intellectually dubious, but it has had documented adverse effects on the quality of research results. Crucially, it stimulates institutions as well as individuals to prioritize quantity over quality (Good et al. 2015; Grančay, Vveinhardt, and Šumilo 2017).
            
            The ill-fated usage of the RIV data to mold nationwide fiscal policies for scientific research reminds us that data analytics is not necessarily a neutral enterprise. A proper treatment of the data is a matter that confronts a data analyst with questions on the borderline of ethics. Although it is perfectly feasible in technical terms, we wish to discourage users from attempts to track individuals researchers; instead we offer features that display institutional or disciplinary dimensions of the data (see Figure 1). Furthermore, the web application will provide a module to visualize textual information from the register. Textual strings, such as abstracts and keywords, have been part and parcel of the recorded entries, but have only served thus far as mere search terms. Meanwhile, the utility of textual data has been demonstrated in studies that strive to map the intellectual organization and relationships within and between disciplines (Leydesdroff 1989; Moody 2004).
             
            
                
                    
                 
            
            Figure 1. Using RIVVIZ to visualize a trend in the publication frequency of research outputs in the “J” (journal) category of the Information Register of R&amp;D Results for the discipline “Philosophy and Religion” [note: the data are only a sample used in the development version] 
             
            The target group of the application are the researchers themselves. Namely, the textual module is intended to serve their needs by providing an overview of the trending topics in research or to identify institutions working on similar problems. The specialist user sub-group is envisaged to come from the fields focusing on social and other studies of science. The accessibility of visualized data and the simplicity of the interface can also attract journalists or other members of the public. The prospective users are also likely to be recruit from among the stakeholders in scientific policy-making and management who may wish to gain quick insights into the quantitatively assessed rates of output per research institutions or funding bodies. 
            The RIVVIZ application is developed in the R language and deployed on the R Server platform using the standard Shiny library. The data are imported from the publicly available repository of the Czech Research, Development and Innovation Information System. The internal setup is also fairly straightforward, relying predominately on the Tidyverse collection of packages, with ggplot2 library being the primary engine for visualization tasks. The underlying principles of the “grammar of graphics”(Wickham 2009) are particularly suitable for programming a user-oriented environment that allows for a control over a wide range of visualization parameters.
            Giving the users more choices should help to make them more engaged with the application, although there is a trade-off between user-friendliness and complexity. Reasonable defaults can partially alleviate this dilemma. The user engagement will be important for the future application development (Galey and Ruecker 2010). In the case of visualization schemes, locking users in a single – no matter how aesthetically pleasing – perspective is problematic. The apparent self-explanatory style and transparent communication of images may draw attention away from the complex and multifaceted nature of the data by making some of their aspects more easily accessible than others (Drucker 2011).
        
        
            
                
                    Bibliography
                    
                        Drucker, J. (2011). Humanities Approaches to Graphical Display. 
                        Digital Humanities Quarterly (DHQ), 5(1).
                    
                    
                        Galey A. and Ruecker, S. (2010). How a Prototype Argues. 
                        Literary and Linguistic Computing, 25 (4): 405-424.
                    
                    
                        Good, B., Vermeulen, N., Tiefenthaler, B. and Arnold, E. (2015). Counting Quality? The Czech Performance-Based Research Funding System. 
                        Research Evaluation 24 (2): 91–105.
                    
                    
                        Grančay, M., Vveinhardt, J. and Šumilo, Ē. (2017). Publish or Perish: How Central and Eastern European Economists Have Dealt with the Ever-Increasing Academic Publishing Requirements 2000–2015. 
                        Scientometrics 111 (3): 1813– 37.
                    
                    
                        Leydesdroff, L. (1989). Words and Co-Words as Indicators of Intellectual Organization. 
                        Research Policy 18 (4): 209–223.
                    
                    
                        Moody, J. (2004). The Structure of a Social Science Collaboration Network: Disciplinary Cohesion from 1963 to 1999. 
                        American Sociological Review 69 (2): 213–238.
                    
                    
                        Sinclair, S., Rockwell, G. and the Voyant Tools Team. (2012). 
                        Voyant Tools (web application).
                    
                    
                        Wickham, H. (2009). 
                        Ggplot2: Elegant Graphics for Data Analysis. Dordrecht: Springer.
                    
                
            
        
    

        
            Overview
            While scholarship on pedagogy in digital humanities has been growing, its focus has largely been on graduate and, to a lesser extent, undergraduate education. Yet, digital humanities pedagogy—namely its value for cultivating 21st century literacies tied to the production of knowledge and the ability to interpret digital media and computation—is as valuable, this panel argues, for middle- and high- school students as it is in higher education. Given that we are pursuing what Matthew Kirschenbaum describes as forms of "scholarship and pedagogy that are bound up with infrastructure in ways that are deeper and more explicit than we are generally accustomed to" (60), this panel examines the work of instructors who are beginning to plant the seeds of these new “customs” early on in humanities and social science training.
            Using digital humanities pedagogy in the middle- and high-school classroom, panelists argue, can redress gaps in these literacies. It enables students, as Mark Sample suggests, “[to think] through their engagement with seemingly incongruous materials, developing a critical thinking practice about process and product” (405). In this way, the approaches to digital humanities pedagogy in middle and high schools articulated by panelists are not an attempt to teach students particular technical skills, applications, or platforms. Rather, this pedagogical approach enables students to envision a relationship between themselves and knowledge production. 
            The approaches to digital humanities voiced in this panel are rooted in digital humanities pedagogies in higher education, particularly project-based approaches to humanities knowledge that foster collaboration. As Tanya Clement has argued:
            
                Like pedagogy intended to teach students to read more critically, project-based learning in digital humanities demonstrates that when students learn how to study digital media, they are learning how to study knowledge production as it is represented in symbolic constructs that circulate within information systems that are themselves a form of knowledge production. (366)
            
            In the case studies and pedagogical approaches discussed by panelists, the project form complements more traditional forms of knowledge production and evaluation in the classroom. As Brett D. Hirsch argues, this “introduces a new mode of work that emphasizes collectivity and collaboration in the pursuit and creation of new knowledge” (16). While these new modes can be linked to participatory forms of culture, made possible by low barriers for civic engagement and creative expression online (Jenkins et al. 9), panelists make the case for greater attention to pedagogies that offer instruction to middle- and high-school students in collaborative production. 
            However, as panelists argue, middle- and high-school pedagogies for digital humanities require attention to the unique needs of students in curricula, the developmental trajectories of the students, and the socio-economic dimensions of these students’ lives. In light of these concerns, what are the biggest challenges to doing digital humanities in middle and high schools? Which methods are most valuable and practically achievable? And how can we effectively prepare teachers to incorporate digital humanities into their teaching practices? In this panel we bring together an international team of researchers and faculty already engaged in answering these questions and implementing curricula in schools of education, digital humanities centers, and high schools. Our goal is both to present models and facilitate discussion with broader digital humanities communities about pedagogical infrastructures, methods, long-term goals, and the exciting possibility of cultivating digital humanities pipelines through intervention in middle and high schools.
            Panel moderator: Alex Gil, Columbia University Libraries
            Designing Digital Humanities Pedagogy Infrastructures for Teachers
            Roopika Risam, Salem State University
            While digital humanities pedagogy has increasingly received attention from practitioners who want to teach their own students more effectively, how do we prepare 
                teachers for the challenging task of engaging with digital humanities in their own classrooms? This talk offers an answer to this question by examining the digital humanities pedagogy infrastructure for middle- and high-school teachers designed at Salem State University. I first discuss findings from a study undertaken with teachers in Massachusetts to identify their attitudes towards digital humanities. The results indicate lack of knowledge about digital humanities but significant interest in incorporating computational approaches to humanities into teaching. Teachers also raised concerns including the time needed to learn technologies and teach them to students, cost of software and hardware, uneven access to computers or the internet in classrooms and for students at home, fear of implementing unsuccessful lessons, and a lack of professional development opportunities for digital humanities.
            
            This talk then considers the interdisciplinary graduate certificate in digital studies that Salem State University designed in response to the study. The program provides professional development while addressing teachers’ perceived obstacles to including digital humanities in their teaching. I discuss the relationship between study results and program design, focusing on development of core courses, selection of elective courses, differentiation of course delivery methods, integration into existing master’s programs, and creation of a directed study for curriculum design. To illustrate the impact of the program, I describe my work advising a team of teachers and administrators in the graduate certificate program who were planning technology needs for a new school building under construction and designing technology-infused curricula in English and History. While core and elective courses gave the teachers and administrators a solid background in digital humanities, a group directed study assisted the team with developing a scaffolded curriculum across middle-school humanities courses, designing classroom technology, and creating a professional learning community to provide in-school pedagogical support for teachers. 
            Finally, this talk discusses a follow-up study with graduates of the certificate programs that assessed program outcomes. These outcomes include assignments implemented by teachers in their classrooms, exemplar student work, and a marked difference in attitudes and perceptions of teachers who completed the certificate in comparison to those who participated in the initial study. Based on the outcomes and the success of the graduate certificate program, Salem State has begun integrating digital humanities pedagogy directly into its teacher training programs. Consequently, this talk argues, this digital humanities pedagogical infrastructure for teachers serves as an effective model for addressing the barriers to incorporating digital humanities into middle- and high-school curricula for teachers who are already in the classroom and those preparing for teaching careers in the humanities. 
            Digital Inquiry: The History of Youth
            Nina Rosenblatt, Trevor Day School
            David Thomas, Trevor Day School
            Stan Golanka, Trevor Day School
            On September 12th, 2017 Trevor Day School, an Independent School on the Upper East Side of New York City, launched two sections of an advanced history course entitled 
                Digital Inquiry: The History of Youth. This course was the culmination of seven years of curriculum development work that began with a November 16th, 2010 article in the 
                New York Times about Humanities 2.0 and the Stanford Republic of Letters Project. After an initial round of research we came to understand that digital projects had a role to play in our High School History curriculum. This realization coincided with our adoption of inquiry-based learning pedagogies. In a fundamental way, we argue, the techniques and disciplines involved in digital humanities allow high school students to conduct their own independent research in digital archives and become producers of history in their own right.
            
            In order to motivate students to collaborate and learn unfamiliar working methods, we developed our course around a subject that would engage all students. We wanted a subject that would not require a textbook, was accessible to juniors and seniors in high school, and would lend itself to seminar style classes. In addition, we wanted to be able to supplement the subject matter with texts illuminating the nature of historical narrative, archives, and the use of digital techniques in academic research such as the paper by Lauren Klein, “The Image of Absence: Archival Silence, Data Visualization, and James Hemings” in 
                American Literature Volume 85, Number 4, December 2013
                .
                The resulting course delved into the history of youth, looking at how being young is experienced and imagined differently in different times and places, and what we can learn about a society from its expectations for and attitude towards its youth, while teaching them production and analysis techniques for them to create new representations of that history.
            
            The final consideration was to craft a series of lesson plans to embed a digital humanities knowledge-production laboratory in the class. The course lab was divided into three modules: Digital editions and markup (an introduction to the fundamentals of plain text and markup), digital collections/exhibits (an introduction to the fundamentals of metadata and databases), and cultural analytics (an introduction to the fundamentals of algorithmic thinking and data mining). Through these modules the students were immersed in the process of selection, digitization, mark-up, the creation of a database/archive, data extraction and cleanup and data analysis, all driven by the imperative to create and interpret history. 
                Technologies taught included, but were not limited to, command line, git, GitHub, plain text editors, Markdown, YAML, Jekyll, Omeka, Python and Voyant Tools. These technologies were directly tied to the variety of ways in which historians collect “data” including using literary, psychological, sociological, statistical, and visual sources, working towards creating our own historical knowledge using the digital tools for collecting, visualizing, mapping, and analyzing the information.
            
            In this panel we will present the results of our two course prototypes, lessons learned, future improvements, and argue for a generalizable model of instruction for high schools in the United States based on our experiences.
            Digital Literary Studies in the High School Environment
            Eric Rettberg, Illinois Mathematics and Science Academy
            What are the challenges of adapting a course in Digital Humanities and Digital Culture from the pedagogical environment of the university to that of the high school classroom? What new challenges arise from asking minors to produce digital and public scholarship, and how can digitally inflected scholars and teachers foster innovative humanities work in school environments bound to pre-existing curricula? In this talk, I use my experience adapting a class in Digital Literary Studies to the high-school level to share unexpected challenges and opportunities and to suggest digital work as a strategy for promoting the humanities to administrators, peers, and students in STEM-oriented high school environments.
            In early 2016, I left higher education to teach in the English department of the Illinois Mathematics and Science Academy, a state-funded boarding school for students talented in math and science. Given the immediate appeal of classes combining humanities with computing for STEM-focused students, I naively expected that I might be able to simply bring a college elective for English majors to my high school students. The actual challenges of doing so, however have been instructive: administrators have been less familiar with the existence of the methods of the Digital Humanities, digital assignments have had to be reframed to accommodate shared practice among teachers in my department, my school’s technology environment has needed to be customized to accommodate the software installations that I took for granted before, oversight from administrators, colleagues, and parents has been more intensive, and without the support staff available to me at my higher education institutions, I’ve had to think creatively around constraints. By demonstrating small-scale digital humanities work in core classes, designing a week-long intersession class on a similar topic, and sharing my knowledge of University-level digital humanities, though, I’ve been able to design a class that has colleagues and students excited.
            Heeding Ryan Cordell’s call to embed digital humanities instruction in larger narratives beyond “recent scholarly revolution,” I treat digital humanities praxis as one of three major components of change in literary production and study in the digital era. In addition to digital humanities projects centered on historical texts of students’ choice, students read and discuss fictive works that represent cultures of technology in the digital era and computationally enabled works of electronic literature. Throughout the class, students sample digital humanities practice in lab sessions and build small-scale web resources and undertake digital-humanities experiments in group projects. By exploring electronic texts, they begin to more fully recognizes the affordances of digital technologies, and by reading print texts that represent digital culture, they think about their own roles as consumers of and creators of digital tools and cultures. While my school’s student population and focus are especially suited to the STEAM focus that a class like this one offers, my experiences suggest that students at a wide variety of high schools would be engaged by these materials and skills.
            Impact od Digital Humanities on High School History and Heritage Teaching and Learning in the Caribbean
            Schuyler K Esprit, Create Caribbean, Inc.
            The experience of Create Caribbean Research Institute, the first Digital Humanities center in the English Speaking Caribbean tells an interesting story of how digital humanities can covertly and explicitly reshape the curriculum in history and literature of the Caribbean without necessarily requiring a massive paradigm shift of the national and regional curriculum requirements.
            In Dominica (where Create Caribbean operates) and the Eastern Caribbean – among other islands – the secondary education curriculum responds to the mandates of the Caribbean Examinations Council (CXC) who sets the CSEC and CAPE syllabi for high school and post-secondary certification in the region. These examinations frame the education curriculum for the five to six years of high school in many islands and many educators in this system find themselves bound to deliver content in limiting and limited methods in order to ensure that students simply meet requirements to excel at subject exams at Caribbean History and English B (Literature), which has a heavy focus on Caribbean Literature.
            However, students leave with an abstract and formalized understand of Caribbean history and culture, without a nuanced understanding of its relevance to their own lived experiences and the implications for their future. Create Caribbean uses digital humanities projects to reframe the conversation and disrupt traditional methods for learning. One of these projects uniquely highlights the potential for technology to change the face of education in Dominica and to get students more invested in Dominica’s history and culture. This project, made by students for students, can be found at 
                www.dominicahistory.org. The college student change-makers of Create Caribbean’s internship program build digital humanities projects with a primary and secondary student audience in mind. The example of dominicahistory.org highlights one way that a collaboration with a national organization has allowed for a broader consideration of the methods of heritage and culture education for students while actually providing solid academic source material for their formal study requirements.
            
            This presentation will discuss the origin, process and impacts of the Dominica History and Imagined Homeland digital projects of Create Caribbean as examples of disruptive secondary education. The presentation will also address the ways in which the projects have attracted the attention of high school teachers and transformed their interests in using technology to revise classroom experiences when they face limitations in adjusting other curricular frameworks.
            Precarity and Practicality: DH, New Media, &amp; Secondary Education
            Matt Appegate, Molloy College
            Jamie Cohen, Molloy College
            In 2015, faculty at Molloy College in Long Island worked with faculty and administrators to found the Baldwin High School New Media Academy, a co-organized effort to bring the study of New Media and Digital Humanities to underserved high school populations in Baldwin, New York. Working collectively, faculty at both institutions have established a curriculum and internship path at Baldwin High School that exposes students to methods of DH praxis and principles of New Media in a variety of means and environments (high school, college, in-person, online).
            Our curriculum is based on five modules and two college-credit bearing courses. Our modules include Critical Making, Digital Storytelling, Multimodal Composition, Online Expression, and Social Media. Our college-credit bearing courses are Introduction to New Media and College Composition (the course is taught entirely on the methods of multimodal composition). Each module is integrated into existing high school courses, i.e., Social Studies, English, Wood Shop, etc., where students take college-credit bearing courses in their junior and senior years. Ultimately, the “academy” concept introduces students to DH methods and New Media in a gradated process--students choose their academy prior to entering their freshman year of high school and are enrolled in courses that employ our modules.
            Our curriculum is based on principles of social good; it emphasizes both civic engagement and social justice, and provides sample assignments with grading rubrics for each module (Ratto). The civic-minded focus of our curriculum was developed in consultation with Baldwin High School, and fleshed out over 18 months of training. Our curriculum attempts to account for the precarious position women and people of color already inhabited in online spaces and demonstrate how DH methods and New Media principles can be mobilized to empower students via digital tools and languages. 
            The focus of this paper is to report on our work with underserved high school populations and relay the challenges of bringing this kind of material to a secondary education setting. We focus on the practicalities of bringing DH methods and New Media principles to high school (i.e., funding, time, expertise, bureaucracy), as well as the necessary training that takes places between high school and college faculty (PT days, on campus conferences, and student events). Finally, we discuss the opportunities that working with underserved high school populations provides both politically and pedagogically. In this context, DH operates on a minimal scale, but addresses communal needs. 
            Bios
            Matt Applegate is an Assistant Professor of English and Digital Humanities at Molloy College. His work focuses on critical theory, digital humanities, digital literacy, and screen studies. His work as appeared in 
                Amodern, 
                Theory &amp; Event, 
                Cultural Politics, 
                Cultural Critique, 
                Telos, and more.
            
            
                Jamie Cohen is the director, co-founder and assistant professor of the New Media program at Molloy College in New York. Jamie is the author of 
                Producing New and Digital Media: Your Guide to Savvy Use of the Web
                 (Routledge 2015) and his published and presented research focuses on memes, YouTubers, populism, VR/AR/MR, and digital media literacy. He is a fellow of the Salzburg Academy on Media and Global Change and the Academy of Television Arts and Sciences.
            
            Schuyler K Esprit is the Director of Create Caribbean Research Institute at Dominica State College, the first Digital Humanities center in the Caribbean. Dr. Esprit holds a PhD in English literature from University of Maryland – College Park. She is a scholar of Caribbean literature and cultural studies, and postcolonial theory. She is now completing her book entitled 
                West Indian Readers: A Social History and its digital companion, both of which are historical explorations of reading culture in the Caribbean. She is currently Dean of Academic Affairs at Dominica State College. 
            
            Stan Golanka is Director of Academic Technology at Trevor Day School. He teaches computer programming and co-teaches Advanced History: Digital Inquiry. He holds a MA in Computing in Education from Teachers College at Columbia University. 
            Eric Rettberg teaches English at the Illinois Mathematics and Science Academy. He remains an active scholar of modernism, experimental poetry, sound studies, and the digital humanities. His work has appeared in 
                Comparative Literature Studies and 
                Jacket 2. 
            
            Roopika Risam is an assistant professor of English and English education at Salem State University. Her research considers the intersections of postcolonial cultures, African diaspora studies, and digital humanities. She is the author of 
                New Digital Worlds: Postcolonial Digital Humanities in Theory, Praxis, and Pedagogy (Northwestern UP 2018) and her work has recently appeared in 
                Debates in the Digital Humanities 2016, 
                Digital Scholarship in the Humanities, and 
                South Asian Review. 
            
            Nina Rosenblatt teaches US History, Art History, and Advanced History: Digital Inquiry at Trevor Day School. She holds a PhD in Art History from Columbia University. 
            David Thomas is Chair of the History Department at Trevor Day School, he teaches European History, Advanced European History, The History of China, and Advanced History: Digital Inquiry.
        
        
            
                
                    Bibliography
                    Clement, Tanya. “Multiliteracies in the Undergraduate Digital Humanities Curriculum: Skills, Principles, and Habits of Mind,” in 
                        Digital Humanities Pedagogy: Practices, Principles, and Politics, ed. Brett D. Hirsch (Cambridge: Open Book Publishers, 2012), 365-88.
                    
                    Cordell, Ryan. "How Not to Teach Digital Humanities." 
                        Ryancordell.org, 1 Feb. 2015, 
                        http://ryancordell.org/teaching/how-not-to-teach-digital-humanities/.
                    
                    Hirsch, Brett D. “&lt;/Parentheses&gt;: Digital Humanities and the Place of Pedagogy.” 
                        Digital Humanities Pedagogy: Practices, Principles, and Politics, ed. Brett D. Hirsch (Cambridge: Open Book Publishers, 2012), 3-30.
                    
                    Jenkins, Henry and Ravi Purushotma, Margaret Weigel, Katie Clinton, Alice J. Robison. 
                        Confronting the Challenges of Participatory Culture: Media Education for the 21
                        st
                         Century (Cambridge, MA: MIT Press, 2009).
                    
                    Kirschenbaum, Matthew. “What Is Digital Humanities and What’s It Doing in English Departments?” 
                        ADE Bulletin 150 (2010): 55-61.
                    
                    Ratto, Matt. “OPEN DESIGN NOW.” 
                        Open Design Now, Netherlands Institute for Design and Fashion and Waag Society, opendesignnow.org/index.html%3Fp=434.html.
                    
                    Sample, Mark. “What’s Wrong with Writing Essays?” 
                        Debates in the Digital Humanities, ed. Matthew K. Gold (Minneapolis: University of Minnesota Press, 2012), 404-5.
                    
                
            
        
    


    
        
            
                Voyant Tools 2.0: The New, The Neat &amp; the Gnarly
                
                    
                        Sinclair
                        Stéfan
                    
                    McGill University, Canada
                    sgsinclair@gmail.com
                
                
                    
                        Rockwell
                        Geoffrey
                    
                    University of Alberta, Canada
                    grockwel@ualberta.ca
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Pre-Conference Workshop and Tutorial (Round 2)
                
                
                    text analysis
                    Voyant
                
                
                    text analysis
                    data mining / text mining
                    English
                
            
        
    
    
        
            Voyant Tools (voyant-tools.org) is a web-based reading and analysis environment for digital texts. Users can create their own corpus of texts to study by pointing to URLs or uploading files in a variety of formats (plain text, XML, HTML, PDF, MS Word, RTF, etc.). Voyant allows users to navigate between macro views of the corpus (e.g., a word cloud visualization of the entire corpus) and micro views (e.g., a reading individual occurrences of a specific term in context). The default interface provides access to a basic set of tools for reading texts and studying word frequency and distribution. There are also more tools available in various pre-defined or user-defined ‘skins’ (a layout of tools that are coupled). 
            Voyant Tools is deliberately designed to be user-friendly and welcoming for text analysis. Voyant currently averages nearly 50,000 visits and about 750,000 tool invocations per month (not counting the downloadable instances of VoyantServer). This will be the seventh consecutive workshop of Voyant, with past sessions focusing on different aspects (pedagogy, multilingualism, customizability, standalone version, etc.). 
            The 2015 workshop will focus on the second major release of Voyant Tools (2.0), which represents an entire rewrite of the codebase to address several of the major shortcomings and irritants of the currently available version 1.0. Version 2.0 is currently available in a beta version online with a major release due in early spring 2015. In addition to performance improvements throughout, the search and filtering functionality have been vastly enhanced, and Voyant now supports proximity and n-gram operations. Voyant 2.0 also has improved corpus handling. Documents can be reordered or added to corpora on the fly, and there is a lightweight access management layer that differentiates between full access, full-text access, and expressive/consumptive access. 
            We have designed this workshop to be of interest both to new users of Voyant, who will get an introduction to the platform, and to existing users, who will discover all the new functionality 2.0 has to offer. As always, a crucial aspect of the workshop will be to get feedback from the community. 
            Workshop Outline 
            
                1. Introduction to Text Analysis with Voyant (1 hour) 
            
            We will begin with a general introduction to text analysis using Voyant aimed at those who haven’t used it before. We will provide a brief overview of Voyant’s user interface and discuss its strengths and weaknesses. We will provide initial text collections that users can use with Voyant, with a view to having participants experiment subsequently with their own text collections. 
            
                2. Voyant 2.0: What’s New? (1 hour) 
            
            This second part of the workshop will focus on what’s new in Voyant 2.0. Examples of changes include more powerful proximity and fuzzy searching of terms, infinite scrolling instead of paginated scrolling for tabular data, in-place modifications of corpora (adding documents or re-ordering them), and new tools (collocate networks, n-gram wordtrees, etc.). This part of the workshop will be useful both for users familiar with the old Voyant (to understand the changes and enhancements) and also to newcomers who will get a better sense of the variety of available tools. 
            
                3. Voyant: Text Repository or Analytic Platform? (1 hour) 
            
            One benefit of the enhanced scalability of Voyant Tools 2.0 is the ability to bridge the gap between existing text repositories (typically focused on searching for documents) and analytic platforms (for text mining). We are collaborating with several large-scale content providers (like TCP-EEBO and Érudit.org) to create custom Voyant skins that allow users to search and filter within very large text collections in order to create smaller worksets of relevant documents for analysis. Because everything is happening in Voyant, the jump from text repository to text analysis is smooth and efficient (very few text repositories allow mass downloading of worksets, but even when they do, additional steps are typically required for re-ingesting the workset into an analytic platform). This component of the workshop will demonstrate some of our existing collaborations and describe how other content providers and projects might be able to leverage this hybrid functionality. 
            Workshop Leaders 
            
                Stéfan Sinclair, sgsinclair@gmail.com, is an associate professor in digital humanities at McGill University. His research focuses primarily on the design, development, and theorization of tools for the digital humanities, especially for text analysis and visualization. He has led or contributed significantly to projects such as Voyant Tools, the Text Analysis Portal for Research (TAPoR), and BonPatron. Other professional activities include serving as associate editor of 
                Digital Humanities Quarterly, as well as serving on the executive boards of ACH, CSDH/SCHN, ADHO, and centerNET. 
            
            
                Geoffrey Rockwell, grockwel@ualberta.ca, is a professor of philosophy and humanities computing at the University of Alberta, Canada. He has published and presented papers in the area of philosophical dialogue, textual visualization and analysis, humanities computing, instructional technology, computer games, and multimedia. He was the project leader for the CFI (Canada Foundation for Innovation)–funded project TAPoR, a Text Analysis Portal for Research (tapor.ca), which has developed a text tool portal for researchers who work with electronic texts. He is the author of 
                Defining Dialogue: From Socrates to the Internet (Humanity Books).
            
            Target Audience 
            A wide range of DH practitioners interested in text analysis, particularly for research, teaching, or technical support. Voyant Tools workshops are typically fully subscribed; we prefer to limit registration to about 25 people to allow us to help participants as needed. 
            Format
            Half-day.
        
    


        
            
                What is computational thinking in the digital humanities?
                The question whether digital humanists should learn to code has been highly-contested (Ramsay, 2011: 243–45). The ‘hack’ versus ‘yack’ debate has lost its edge as scholars concede that theory and programming praxis can be brought together productively through what Davidson terms “collaboration by difference” (Davidson, 2015: 134). Given that the majority of digital humanists will not need to program professionally, to what extent ought computation be taught in the digital humanities? Jeannette M. Wing coined the term “computational thinking” to address the teaching of digital literacy beyond computer science. She contended that “computational thinking is a fundamental skill for everyone, not just for computer scientists” (Wing, 2006: 33). In 
                    Digital Humanities, Berry and Fagerjord comment at length on Wing’s definition, concluding that “a critical understanding of computing at its different levels is a prerequisite for a digital humanist...” (Berry and Fagerjord, 2017: 59). There is little agreement, however, about the best way to teach computational thinking to humanists. We review the potential of visual or “block-based” programming languages for teaching computational literacy in the digital humanities. We argue that digital humanists should learn from these tools’ emphasis on the ludic over the pragmatic. We also offer suggestions about how digital humanists might adapt and critically adopt block-based programming as they seek to expand their understanding of fundamental concepts of computer science.
                
            
            
                Review of educational programming environments
                The use of visual or “block-based” programming has become a mainstay for computer science education in the K-12 arena. Block-based programming involves the manipulation of graphical elements to create units of computation. The authors of 
                    Learnable Programming: Blocks and Beyond argue that block-based programming makes it easier to learn to program for three primary reasons: emphasizing recognition over recall, “chunking code,” and constraining options (Bau et al., 2017: 72–80). These pedagogical advantages would also seem to apply in the digital humanities though, as a quick review of the evolution of these languages demonstrates, they were not created to teach computational thinking to adults.
                
                
                    
                        Logo. The origins of block-based programming stretch back to Logo, a programming language and graphical environment for computer science education. While not a visual programming language, when paired with Turtle Graphics Logo provides students with the ability to visualize their computations (Papert, 1980: 16–20). Logo has gained renewed popularity among the elementary age set due to Gene Luen Yang and Mike Holmes’ 
                        Secret Coders, a series of graphic novels that employs Logo to teach basic computational literacy (Yang and Holmes, 2015).
                        
                    
                    
                        Scratch. The designers of Scratch sought to create a computational environment for kids and teens to become active manipulators rather than passive consumers of digital media. The designers stripped away many of the complexities of software development (e.g., linking libraries and compiling binaries) to create what they term a “tinkerable” environment, pioneering the use of blocks for syntax (Maloney et al., 2010: 16:4).
                    
                    
                        Snap! While Scratch succeeded in developing an extensive community of users in the K-12 arena, its emphasis on semantic simplicity inhibits its usefulness for teaching students computer science at the postsecondary level. The Snap! programming environment emerged from a collaboration between Brian Harvey at Berkeley and Jens Mönig, a software developer currently at SAP. Drawing on long experience teaching functional programming in Scheme, the authors created a semantics with lambda expressions, recursion, and high-order functions using a Scratch-like syntax (Harvey and Mönig, 2015: 35–38).
                    
                    
                        NetsBlox. NetsBlox is an adaptation of Snap! that makes it straightforward for users to communicate with internet services and to communicate peer-to-peer (Broll et al., 2017: 81–86). Students can draw on these features of NetsBlox, for example, to place markers representing art museums in the vicinity on a Google Map or to create a shared digital whiteboard. By fostering the ability to communicate beyond the boundaries of the programmer’s laptop, NetsBlox paves the way for creating data-driven digital humanities projects. Data may also be persisted in the cloud, making it possible to preserve state. Currently, NetsBlox comes with the ability to call out to a select number of services. However, the developers envision “adding a lot of new services and data sources to NetsBlox...”(Broll et al., 2017: 86). This raises the question whether a version of NetsBlox could be developed specifically for digital humanists, integrating web-based application programming interfaces (or APIs) for platforms like the DPLA, the HathiTrust, and Europeana, among others.
                    
                
                
                    
                
                NetsBlox with prototype RPC block for Wikidata
                The digital humanities community also embraces visual programming models. Voyant Tools, for instance, provides a graphical interface for scholars seeking to study textual corpora (Sinclair et al., 2016). To date, there appears to be little to no scholarship about the pedagogical effectiveness of using visual programming environments in the digital humanities.
            
            
                Programming as ludic rather than pragmatic
                Is learning block-based programming a means to an end or an end in itself? While computer science students will inevitably move from block-based to text-based programming (Kölling et al., 2015: 29–38), the designers of Scratch claim that many students will fruitfully remain within its environment (Resnick et al., 2009: 66f). At the secondary and post-secondary level, “The Beauty and Joy of Computing” curriculum likewise promotes the enjoyment of programming within the Snap! environment: “having fun is an explicit course goal” (Garcia et al., 2015: 71). Leading digital humanists also acknowledge the playful aspects of programing. In “On Building,” Stephen Ramsay remarks, “Learn to code because it’s fun and because it will change the way you look at the world” (Ramsay, 2011: 245). Nick Montfort argues that motivations for learning programming go beyond the “merely instrumental” (Montfort, 2016: 268), remarking “it is enjoyable to write computer programs and to use them to create and discover” (Montfort, 2016: 277). By customizing the visual representations and selecting domain-specific exercises, block-based programming could find wide application in the digital humanities, promoting the joy of learning computation for its own sake while providing humanists with a better conceptual grounding for the evaluation and application of algorithms and software in their digital research.
            
            
                Prolegomena to any future visual programming environment for the digital humanities
                What would a digital humanities version of a block-based programing environment look like? By way of conclusion, we suggest how NetsBlox might evolve past its origins in Scratch to provide a shared platform for teaching computational thinking in the digital humanities. We propose three developments: 1. creating default sprites that represent the domains of digital humanities research (i.e. representing books rather than basketballs); 2. establishing libraries of blocks to call commonly-used web-based APIs in the digital humanities; 3. providing a curriculum focusing on major research areas in the digital humanities, including distant reading, educational gaming, geospatial analysis, and steganography, among other topics. By developing a block-based environment for the digital humanities, we hope not only to advance computational thinking in our field, but also to provide resources for introducing the digital humanities into secondary and postsecondary courses on computational thinking.
            
        
        
            
                
                    Bibliography
                    
                        Bau, D., Gray, J., Kelleher, C., Sheldon, J. and Turbak, F. (2017). Learnable programming: Blocks and beyond. 
                        Commun. ACM, 
                        60(6). New York, NY, USA: ACM: 72–80.
                    
                    
                        Berry, D. M. and Fagerjord, A. (2017). 
                        Digital Humanities: Knowledge and Critique in a Digital Age. Cambridge: Polity.
                    
                    
                        Broll, B., Lédeczi, A., Volgyesi, P., Sallai, J., Maroti, M., Carrillo, A., Weeden-Wright, S. L., Vanags, C., Swartz, J. D. and Lu, M. (2017). A visual programming environment for learning distributed programming. In, 
                        Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education. ACM, pp. 81–86.
                    
                    
                        Davidson, C. (2015). Why yack needs hack (and vice versa): From digital humanities to digital literacy. 
                        Svensson, P. , Goldberg, DT, Ed: 131–45.
                    
                    
                        Garcia, D., Harvey, B. and Barnes, T. (2015). The beauty and joy of computing. 
                        ACM Inroads, 
                        6(4). New York, NY, USA: ACM: 71–79.
                    
                    
                        Harvey, B. and Mönig, J. (2015). Lambda in blocks languages: Lessons learned. In, 
                        2015 IEEE Blocks and Beyond Workshop (Blocks and Beyond). pp. 35–38.
                    
                    
                        Kölling, M., Brown, N. C. C. and Altadmri, A. (2015). Frame-based editing: Easing the transition from blocks to text-based programming. In, 
                        Proceedings of the Workshop in Primary and Secondary Computing Education. (WiPSCE ’15). New York, NY, USA: ACM, pp. 29–38.
                    
                    
                        Maloney, J., Resnick, M., Rusk, N., Silverman, B. and Eastmond, E. (2010). The scratch programming language and environment. 
                        Trans. Comput. Educ., 
                        10(4). New York, NY, USA: ACM: 16:1–16:15.
                    
                    
                        Montfort, N. (2016). 
                        Exploratory Programming for the Arts and Humanities. Cambridge: MIT Press.
                    
                    
                        Papert, S. (1980). 
                        Mindstorms: Children, Computers, and Powerful Ideas. New York, NY, USA: Basic Books, Inc.
                    
                    
                        Ramsay, S. (2011). On building. In Melissa Terras, Julianne Nyhan, (ed), 
                        Defining Digital Humanities: A Reader. London: Routledge, pp. 243–45.
                    
                    
                        Resnick, M., Maloney, J., Monroy-Hernández, A., Rusk, N., Eastmond, E., Brennan, K., Millner, A., et al. (2009). Scratch: Programming for all. 
                        Commun. ACM, 
                        52(11). New York, NY, USA: ACM: 60–67.
                    
                    
                        Sinclair, S., Rockwell, G. and Others (2016). Voyant tools. 
                        URL: Http://Voyant-Tools. Org/[September 5, 2016].
                    
                    
                        Wing, J. M. (2006). Computational thinking. 
                        Commun. ACM, 
                        49(3): 33–35.
                    
                    
                        Yang, G. L. and Holmes, M. (2015). 
                        Secret Coders. New York: Macmillan.
                    
                
            
        
    

        
            IsiXhosa is an Nguni language classified in the south-eastern geographical zone of South Africa (Guthrie, 1971:33). It is one of the official South African languages, and one of the most widely spoken (after isiZulu) with approximately eight million mother-tongue speakers. In terms of natural language processing, particularly computational morphology, the Nguni languages including isiXhosa belong to the lesser-studied languages of the world and can be classified as under-resourced languages. Nguni languages are characterised by a rich agglutinating morphological structure, based on two principles: the nominal classification system and the concordial agreement system (Bosch &amp; Pretorius, 2008:97).
            The principal author’s masters study focused on the representation of women protagonists by male and female authors in isiXhosa dramas. The whole analysis process was done manually, mainly because digitised isiXhosa literature books were not available. This limited the study to only four books. The analysis focused on gender inequality in the way women are represented by male authors, as opposed to the way in which women authors represent women protagonists, and also on patriarchal traces found in isiXhosa dramas.
            
                When examining the representation of female protagonists in isiXhosa dramas, similar works from other scholars are noteworthy. The first contribution that narrates the same viewpoint as the one investigated here is by Ngqase (2002), in which she examines the representations of women in four isiXhosa drama books. The study highlights the interplay between culture and women's social space. The second contribution by Peter (2010) expresses female character portrayal in various drama works written by males. He concludes that many male writers are unwilling to portray female characters in their totality and true complexity, which is evident in the way some writers have resorted to the use of stereotypes (Peter, 2010:15).
            
            As an isiXhosa language researcher at the South African Centre for Digital Language Resources (SADiLaR), the principal author has been introduced to computational methods which could afford new ways to approach the research topic described. 
            Assessing and reporting on the usability of computational tools when analysing isiXhosa texts 
            
                This presentation reports on the same research topic, with the focus on computational methods to analyse the texts instead of manual approaches. The computational tools which were utilised include, Voyant Tools and regular expressions (regular expressions) as well as testing the feasibility of BookNLP when used conjunctively with written languages. 
            
            
                The creators of 
                Voyant Tools
                 note that it supports analysis in any language since it mostly operates on character sequences; however, limited language-specific support is available (Sinclair &amp; Rockwell, 2019). Capitalisation in isiXhosa is of special importance in the proposed study as the language follows a pattern where the second letter of a word is capitalised instead of the first. For instance, the “Context” tool in Voyant Tools produces search terms only in lower case.
            
            
                With 
                BookNLP
                , which is specifically built for English texts, the authors will now focus on how successfully the sub-processes in its pipeline fare with a non-Western language and how it could be adapted and/or how a similar pipeline could be developed for isiXhosa using tools developed by SADiLaR. BookNLP was developed by Bamman, Underwood and Smith (2014). The study follows similar approaches to those utilised by Algee-Hewitt, Porter and Walser (2016).
            
            
                Finally,
                 regular expressions
                 will be used as well, as it allows to match patterns and search for very specific character sequences more effectively.
            
            Operationalisation of the research questions
            
                The study has two parts. First, by reporting on the performance of the computational tools on the isiXhosa drama corpus versus an English equivalent. The specific steps for Voyant Tools and differences when using regular expressions will be provided and compared. Second, in terms of research questions focusing on the representation of Xhosa protagonists by male and female authors, regular expressions was used as the main investigation tool. 
            
            
                The paper reports on:
            
            
                How can computational tools used to analyse Western languages be used for conjunctively written South African languages? 
                Are authors of isiXhosa literature influenced or led by their gender when writing? 
                
                    Do authors conceptualise their work with the intention to uplift one gender while diminishing the other?
                
                How can the gap caused by inequality between sexes be bridged through written literature? 
            
            A practical example:
            
                If data from an English corpus is analysed using Voyant Tools, the tool would automatically be able to provide word frequencies and links between words. However, with a conjunctive language like isiXhosa, this would only be possible by making use of special search options, because the generic frequency table will be skewed owing to the difference in semantic properties of the words. For example, gender association in isiXhosa depends solely on the prefix. Only through the prefix will one be able to confirm whether a noun is referring to a single person or a group of people. Furthermore, only through contextualisation will one know whether that person is male or female, as isiXhosa prefixes are also unisex. 
            
            
                E.g.:
                 uPeter 
                u
                sela amanzi
            
            Peter (he) is drinking water
            
                uSammy 
                u
                phunga iti.
            
            Sammy (she) is drinking tea.
            This research also aims to provide a point of departure for new scholars interested in analysing isiXhosa literary works using computational approaches.
            
                Key words
                : Conjunctive language, Nguni language, computational methodologies, voyant tools, regular expressions, BookNLB
            
        
        
            
                
                    Bibliography
                    
                        Algee-Hewitt, M., Porter, J., Walser, H
                        . (2016). Representations Of Race: Mining Identity In American Fiction, 1789-1964. In Digital Humanities 2016: Conference Abstracts. Jagiellonian University &amp; Pedagogical University, Kraków, pp. 111-112.
                    
                    
                        Bamman, D., Underwood, T. and Smith, N. A.
                         (2014). A Bayesian Mixed Effects Model of Literary Character. 
                        Proceedings of the 52nd Annual Meeting of the Association for Computation Linguistics.
                         Baltimore, Maryland, pp. 370-79.
                    
                    
                        Bosch, S., Pretorius, L. and Fleisch, A.
                         (2008). Experimental Bootstrapping of Morphological Analysers for Nguni Languages. 
                        Nordic Journal of African Studies
                         17(2):66-88.
                    
                    
                        Guthrie, M.,
                         (1969). 
                        Comparative Bantu, Farnborough
                        : Gregg, vol. 4.
                    
                    
                        Guthrie, M.,
                         (1970). Contributions from Comparative Bantu studies to the prehistory of Africa.  
                        Language and history in Africa.
                         (Dalby ed.), 1:1-27.
                    
                    
                        Ngqase, F.F.,
                         (2002). 
                        The way in which women are portrayed in isiXhosa dramas. 
                        B.A Thesis. University of Stellenbosch. Available at: (Accessed: 24 April 2019)
                    
                    
                        Peter, Z.W., 
                        (2010). 
                        The depiction of female characters by male writers in selected isiXhosa drama works. 
                        BA Thesis. Nelson Mandela Metropolitan University. Available at:
                        
                            http://hdl.handle.net/10948/1482
                        
                         (Accessed: 24 April 2019)
                    
                    
                        Sinclair, S. and Rockwell, G.
                         (2019) Languages.-
                        
                            https://voyant-tools.org/docs/#!/guide/languages Date of access: 24 April 2019.
                        
                    
                
            
        
    

        
            
                Introduction and motivation
                In a 2020 talk entitled “A Hornbook for Digital Book History”, Whitney Trettien weaves together many of the strands that have led book history, bibliography, media studies, and the digital humanities to have become deeply entangled in recent years. She convincingly argues for the potential of Book History done digitally “to build connective tissue across scattered collections” and advocates “using the digital tools at our disposal in order to see the big picture of the past”.
                    
                        
                            https://rarebookschool.org/rbs-online/a-hornbook-for-digital-book-history/. She shares this vision of a continuum of print and digital with other influential voices at the intersection of book history, media studies, and the digital humanities, among them Henrike Laehnemann, Sarah Werner, and Matt Kirschenbaum to name but a few.
                        
                    
                
                It is in this vein that this paper presents the motivation for and realisation of a new open-access open scholarship platform (currently in public beta) named PRISMS.
                    
                        
                            https://www.prisms.digital/
                        
                     The aim of the PRISMS Open Scholarship platform is two-fold:
                
                
                    It offers a publication platform for digital scholarly editions, with full-text (preferably encoded in TEI) and facsimiles, and any accompanying materials, such as introduction, editorial statement, critical apparatus, contextual source materials, bibliography, and indices;
                    It facilitates the semantic annotation of these editions and their related scholarship (in any format) by enabling easy-to-perform formal ontological modelling (based on the CIDOC-CRM family of ontologies
                        
                            
                                http://www.cidoc-crm.org/
                            
                        ), and thus hopes to contribute to providing the abovementioned “connective tissue” not only for scattered collections, but to overcome the artificial print/digital divide.
                    
                
                PRISMS was born out of the realization that digital editions do not break with the historicity or materiality of the sources they organize and present, but instead remediate and extend them in ways that enable new forms of access, engagement, presentation, and analysis. PRISMS conceptualizes digital editions as living entities that perform rather than merely document the remediation they engage in.
                The scholarship that underpins each digital edition provides the essential context for these remediation processes, and collectively they sustain the knowledge network that supports all academic engagement with the texts from any disciplinary viewpoint. PRISMS is designed to allow for the collaborative and collective modelling of this continuum of digital editions and scholarship by placing digital editions, their material and contextual basis, and the resulting academic engagement in a linked context, building on the standards and tools provided by the Semantic Web.
                We believe that this type of formalization is beneficial for the purposes of this project in at least three ways: firstly, ontologies facilitate modelling with reduced reliance on implicit knowledge through an explicit, shared conceptualization of the domain. Secondly, formal models encourage collaboration as they can be shared, re-used, adapted (forked), enhanced, aggregated, and developed collaboratively. Thirdly, as a form of knowledge representation, visualization, and preservation, formal models support computational processing and ultimately reasoning, and can develop alongside the mental models and human reasoning we engage in as scholars. PRISMS facilitates scholarship that is based on these principles
                    
                         Ground-breaking research projects in this domain include the 
                            ResearchSpace platform and the 
                            Sphaera CorpusTracer project.
                        
                    .
                
            
            
                Approach and implementation
                The PRISMS Open Scholarship platform integrates the task of publishing digital editions with the need for analytical and modelling tools to perform the type of knowledge representation that connects the material, digital, and the scholarship that builds on them. PRISMS aims to support digital editors, book historians, experts in media and cultural studies, librarians, literary scholars, and of course digital humanists, to ensure a wide range of domain expertise, disciplinary practices, and methodological approaches are reflected in the platform. To this end, the PRISMS platform hosts a variety of tools alongside the digital editions, which can be categorized as component tools (such as text-based tools, image-based tools, XML-based tools, etc.) and workbench tools (those available across document types and editions).
                
                    
                    Figure 1 
                        Some of the built-in analysis and visualization tools in PRISMS. Voyant-Tools is shown alongside a relation being made between two editions, and some highlighted annotations
                    
                
                The former category of tools is useful for any type of close scholarly work, and in PRISMS these tools include a bookmarking tool, an annotation tool (initially focussing on texts and images, but with a vision to extend annotation capabilities across all media types), the ability to keep research records (and other forms of note-taking, e.g. transcriptions, translations etc.) in the form of notebooks, and integration of Voyant Tools
                    
                        
                            https://voyant-tools.org/
                        
                     for statistical analysis and a variety of visualisations of texts. The latter category includes the ability to participate in the shaping of the knowledge graph by modelling concepts and relationships, an easy way to organize research materials, and the ability to download, share, and publish contributions for the benefit of all.
                
                
                    
                    Figure 2 
                        Modelling both the material legacies and digital remediation processes with Linked Data technologies and Cytoscape.js
                    
                
                All semantic modelling work, e.g. with regard to the provenance of the material and digital manifestations of an edition, can be performed both directly in a visual representation of the graph using Cytoscape.js or via a set of customizable HTML forms. All resulting triples are stored in an RDF-native graph database (using the abovementioned ontologies) for long-term preservation, collaboration, and re-use. Every user of PRISMS has both read access to this global graph that underpins the platform and unlimited access (via SPARQL Update operations) to a private graph. By default, everything in PRISMS is private. Everything contributors add is immediately visible to them, and they can conduct their scholarship in complete privacy, without delay or interference. Only when the user decides to publish their contributions will they be made available to everyone. Depending on the type of contribution made, there are options to share, download, and/or publish them. All contributions made to the PRISMS platform are stored in standard formats, e.g. the W3C Web Annotation Data Model for annotations and relations.
            
            
                Contribution and further work
                PRISMS has been launched with corpora from the EEBO-TCP
                    
                        
                            https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/
                        
                    , ECCO-TCP
                    
                        
                            https://textcreationpartnership.org/tcp-texts/ecco-tcp-eighteenth-century-collections-online/
                        
                    , EVANS-TCP
                    
                        
                            https://textcreationpartnership.org/tcp-texts/evans-tcp-evans-early-american-imprints/
                        
                    , the DTA extended core corpus
                    
                        
                            https://www.deutschestextarchiv.de/
                        
                    , and the Taylor Editions
                    
                        
                            https://editions.mml.ox.ac.uk/
                        
                     scholarly editions platform. And it is easy to add new editions to the platform either as part of a dedicated digital scholarly editing process, for which training is provided, or by simply adding a IIIF manifest and using a built-in XML-aware or standard text-editor to start transcribing and adding contextual materials. The platform already supports the addition and semantic annotation of a wide range of primary and secondary materials, such as facsimiles in IIIF, a transcription of a source text, a PDF of a journal article, a video of a theatrical performance, an audio book, an image, or a 3D-model of a sculpture mentioned in a text, etc. 
                
                
                    
                    Figure 3 
                        A view of the PRISMS workbench, with three editions of Faust loaded, and an aggregation of primary and research materials in support of a performance analysis, with a facsimile, two videos, and an audio book
                    
                
                Moving forward, we will continue to work on integrating the digital research and tools important to PRISMS’ users (e.g. reference manager, images taken in reading rooms, items deposited in institutional repositories). With end of the beta phase, the project also intends to provide access to the entire PRISMS knowledge graph through regular data dumps and a public SPARQL endpoint. We envision that over time, PRISMS will evolve both as a powerful discovery tool and a personal research tool.
            
        
        
            
                
                    Bibliography
                    Ciotti, F. (2015) “Digital methods for Literary Criticism.” Lecture slides. University of Rome Tor Vergata, 
                        http://didattica.uniroma2.it/files/scarica/insegnamento/161783-Informatica-Umanistica-Lm-Per-Il-Llea/37175-Slide
                    
                    Ciula, A. and Marras, C. (2016) “Circling around texts and language: towards 'pragmatic modelling' in Digital Humanities.” 
                        
                            Digital Humanities Quarterly (DHQ)
                         10.3 
                        http://www.digitalhumanities.org/dhq/vol/10/3/000258/000258.html
                    
                    Ciula, A. and Eide, Ø. (2107) “Modelling in digital humanities: Signs in context.” 
                        
                            Digital Scholarship in the Humanities
                         32: i33–i46. 
                        https://doi.org/10.1093/llc/fqw045
                    
                    Ciula, A., Eide, Ø, Marras, C. and Sahle, P. (2018) 
                        Models and Modelling between Digital and Humanities — A Multidisciplinary Perspective. 
                        Historical Social Research (HSR) Supplement 31.
                    
                    Eide, Ø. (2015)
                         Media Boundaries and Conceptual Modelling: Between Texts and Maps. Pre-print manuscript, 
                        https://www.oeide.no/research/eideBetween.pdf
                    
                    Kirschenbaum, M. and Werner, S. (2014) “Digital Scholarship and Digital Studies: The State of the Discipline.” 
                        Book History 17, 406-458 
                        https://www.academia.edu/15995371/Digital_​Studies_​and_​Digital_​Scholarship_​The_​State_​of_​the_​Discipline
                    
                    Kräutli, F. and Valleriani, M. (2018) “CorpusTracer: A CIDOC database for tracing knowledge networks.” 
                        Digital Scholarship in the Humanities 33(2): 336-346. 
                        https://pure.mpg.de/rest/items/item_2472866_10/component/file_3002633/content
                    
                    Laehnemann, H. (2022) “History of the Book blog.” 
                        https://historyofthebook.mml.ox.ac.uk/
                    
                    Oldman, D., Doerr, M. and Gradmann, S. (2016) “Zen and the Art of Linked Data: New Strategies for a Semantic Web of Humanist Knowledge.” In Schreibman, S., Siemens, R., Unsworth, J. (eds.) 
                        A New Companion to Digital Humanities. Malden, MA: Wiley Blackwell, 251-273.
                    
                
            
        
    



        
            
                Introduction
                It is difficult to identify named entities like people and places in long texts and even more difficult to connect the entities that you find to the rich network of information available on the web. In this paper we describe work supported by the LINCS (Linked Infrastructure for Networked Cultural Scholarship) project to make named entity recognition available to scholars through Voyant and its extension Spyral. In this talk we will:
                First, describe the development of NSSI, a set of named entity recognition (NER) tools that are also available as web services for other tools like Voyant to use.
                Second, describe how Voyant can use NSSI as a web service to process a text by adding named entity recognition.
                Third, describe how Spyral, the notebook programming extension of Voyant, can be used for more sophisticated control of the process of named entity recognition, extraction, and use in Voyant. 
                Finally, we will conclude by discussing how NSSI and Spyral will be linked into the LINCS infrastructure to allow scholars to connect their enriched data to that of others.
                Background on LINCS
                Humanists tend to be interested in named people, named places and particular organizations over time. NER tools let humanists identify mentions in text referring to the people, places, organizations and other entities discussed in large collections without having to manually comb through them. Good tools like the Stanford Named Entity Recognizer (Finkel et al. 2005) have been available for some time, but are difficult to use if you are not familiar with command line tools and not connected with other resources.
                The LINCS project, led by Susan Brown at the University of Guelph, is funded by the Canadian Foundation for Innovation to develop shared infrastructure for linked open data. To that end LINCS is working with teams at the University of Alberta and McGill University to develop new NER tools and to connect them to easy-to-use text analysis environments like Voyant.
            
            
                NSSI
                NSSI, or NERVE Secure Scalable Infrastructure, is an application that bundles natural language processing tools, making them simple to use and combine into workflows common to the digital humanities (Zafar 2021). This framework was developed as part of the LINCS project, with the intent to decouple the backend NER tools from the existing Named Entity Recognition Vetting Environment (NERVE) user interface developed by the Canadian Writing Research Collaboratory. This separation allows us to continue using those NER services for NERVE, while making them accessible to other tools such as Voyant and Spyral.
                NSSI’s design focuses on modularity, with each tool connected as a service that can be used individually or within a larger set of steps. For NER in particular, we have integrated Stanford NER which otherwise requires programming knowledge to use, since it does not come with its own API. With NSSI, a tool such as Spyral can make an API call that includes input text or XML and retrieve the named entities when processing completes. In the presentation we will briefly describe the NSSI infrastructure.
                
                    
                    Figure 1: Experimental RezoViz NER Interface in Voyant
                
            
            
                Voyant and Spyral
                Voyant Tools is a suite of text analysis and visualization tools that are widely used with over 100,000 users in the last six months. The tools are available in the browser so they don’t need to be installed, though you can download them and run them locally (Rockwell & Sinclair 2016). In the presentation we will show how Voyant can call the NER tools in NSSI and display the found entities as a list for further use. We will also describe the usability testing conducted on ResoViz through the LINCS project.
                
                    
                    Figure 2: ResoViz Social Network Visualization
                
                Voyant is also being extended with a notebook programming environment called Spyral (Land et al. 2021; Rockwell et al. 2021). Spyral is, like Observable, an in-browser notebook programming environment that uses JavaScript as the programming language. The difference between Spyral and other notebook environments like Mathematica or Google Colab is that a) the notebooks are maintained on the server so that, again, there is no installation needed and b) Spyral is an extension to Voyant. This means that you can save what you see in Voyant as a notebook with an interactive panel of results embedded in the notebook. Then you can document your results, add more interactive panels, and process the results. In the presentation we will show how Spyral can be used to extend the work with NSSI possible with Voyant and to edit and document results.
            
            
                Next Steps
                The paper will conclude by describing the next steps in the larger project, and those are to allow users to connect named entities in their texts to other data about the entities available through the LINCS triple store and other open data resources like Wikidata (Vrandečić 2012). The ultimate goal is to provide scholars with linked infrastructure where data about entities like people or novels can be annotated and connected with that of other projects.
            
            
                Links
                Google Colaboratory (Colab): https://colab.research.google.com/ 
                LINCS project: https://lincsproject.ca/
                Stanford Named Entity Recognizer: https://nlp.stanford.edu/software/CRF-NER.html 
                Voyant Tools: https://voyant-tools.org and Spyral: https://voyant-tools.org/spyral
            
        
        
            
                
                    Bibliography
                    Finkel, J. R., Grenager, T., and Manning C. (2005). Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf (accessed 21 May 2022).
                    Zafar, H. (2021). Linked Data Conversion using Microservices [video file]. Zenodo. https://doi.org/10.5281/zenodo.6551465 (accessed 21 May 2022).
                    Land, K., MacDonald, A. and Rockwell, G. (2021). Spyral Notebooks as a Supplement to Voyant Tools. CSDH-SCHN 2021 conference online. http://dx.doi.org/10.17613/2bsr-xp53 (accessed 21 May 2022).
                    Rockwell, G. and Sinclair, S. (2016). Hermeneutica: Computer-Assisted Interpretation in the Humanities. Cambridge, Massachusetts, MIT Press.
                    Rockwell, G., Land, K., and MacDonald, A. (2021). Social Analytics Through Spyral. Pop! Public. Open. Participatory. no. 3 (2021-10-31). https://popjournal.ca/issue03/rockwell (accessed 21 May 2022).
                    
                        Vrandečić, D. (2012). Wikidata: A new platform for collaborative data collection. In Proceedings of the 21st international conference on world wide web, pp. 1063-1064.
                    
                
            
        
    



        
            Digital literary text analysis is increasingly becoming an integral part of literary studies. However, many tools designed for performing such analysis remain inaccessible to researchers without significant coding and computing skills. Voyant Tools was designed in part to address this gap. Spyral Notebooks are an extension of Voyant Tools and allow researchers to expand upon their findings from Voyant in a notebook environment. Unlike other notebook environments, Spyral Notebooks are accessible without downloading any programs or advanced set-up. Spyral Notebooks are available in an entirely online format. To use Spyral Notebooks, one needs only a connection to the Internet. The notebooks are easily adaptable, shareable, and editable. 
            Spyral is a notebook development environment that is integrated into Voyant Tools. Notebook environments can be thought of as both extensions of traditional research notebooks and as novel tools that integrate documentation, active analysis and presentation of results. At their core, notebooks are made up of three types of blocks or cells that a user can add or delete in a sequence. 
            
                There are text cells that can contain headings and other text elements found in word processors or browser editors (usually based in HTML) for typing unstructured text. Depending on the notebook environment, the text blocks can be simple or more sophisticated. Spyral Notebooks use HTML for text and offer an in- browser WYSIWYG HTML editor for the text blocks. 
                There are code cells where the user inputs code, be it Python, the Wolfram language used in Mathematica, or JavaScript, which is used in Spyral. The code cells can be run in sequence or individually as you debug your code. Code cells can contain as much or as little code as the user desires. 
                There are output cells which produce the output of the code you input in the associated code cell. It is important to recognize that the output of the code is dependent on what you have instructed the computer to do; that is, it is not a printout of the code cell but the results of running your code. You thus have to instruct the computer to print out the desired results. 
            
            In our tutorial we introduce participants to Spyral Notebooks. We illustrate how to create a corpus for textual analysis from Voyant Tools or directly in Spyral Notebooks. After walking through the basic mechanisms for using Spyral Notebooks including saving, editing, and sharing notebooks, we move on to more specific features available in Spyral. Participants will learn how to enhance the capabilities of Voyant and go deeper with their textual analysis using Spyral. Finally we provide participants with several tutorial notebooks designed to highlight some of Spyral’s advanced features such as categories for use in sentiment analysis. 
            Spyral Notebooks are a welcome addition to the field of digital humanities as they provide an accessible notebook environment specifically designed for literary text analysis. Spyral Notebooks are thoughtfully designed to serve researchers with limited coding skills who want to take their analysis from Voyant one step further. We especially envisage Spyral proving useful for digital humanities instructors. Spyral provides a useful platform for student work, allowing students to embed their analysis from Voyant, perform more complex analysis using JavaScript, and annotate their code with their thought processes.
        
    



        
            The introduction of digital text analysis tools and methodologies in (non-digital) humanities undergraduate courses has been sparsely documented in the literature. Furthermore, most of the times we encounter it, it is done in the context of semester-long or mid-term projects (Boyle and Hall 2016; Ficke 2014), where the stakes for the students are very high. Other times, they include a session on text analysis but no practical application of the tools and methodologies discussed in the course, other than a follow along demonstration.
            This short paper introduces a middle point between these two extremes through the introduction of low stakes activities and assignments to help student discover and use digital text analysis tools and methodologies.
            Besides giving students the opportunity to interact with the material in a safe and relaxed manner, low stakes activities help with student retention, confidence, and relationship building (Hamilton 2020; Meer and Chapman 2014). Low stakes activities are also a useful tool to assess comprehension and instruction when the person delivering the lesson is not the regular or official instructor in the course, such as the case of a librarian or a guest speaker. Furthermore, these types of activities are particularly useful for digital humanities instruction because they contribute to scaffolding, a method that has been identified as ideal in this type of instruction (Griffin and Taylor 2017; Isuster 2020; Sample and Schrum 2013; Tracy and Hoiem, 2018).
            In the context of a Hispanic Studies course, a librarian offered a workshop series on digital text analysis and the web-based reading and analysis environment Voyant Tools. Interspersed with instruction there were a series of low stakes assessments that helped students understand and apply the content of the workshops. Working with the class readings, the librarian created activities that did not rely on having a single answer but encouraged students to discuss and interrogate both the methods and the information used. For example, when preparing a text for text analysis, students debated how different research questions necessitate different text preparation. The activities were completed in groups and were not graded. Results were discussed within the class.
            The short paper presentation will explore the process of creating and implementing low stakes activities for digital text analysis and other digital humanities instruction. It will discuss the benefits of these types of activities as they pertain to digital humanities instruction and engagement and will share best practices and tips to help attendees create these kinds of activities in their own classrooms, including assignment design and sourcing materials.
        
        
            
                
                    Bibliography
                    Boyle, M. and Hall, C. (2016) ‘Teaching “Don Quixote” in the Digital Age: Page and Screen, Visual and Tactile’, 
                        Hispania, 99(4), pp. 600–614.
                    
                    Ficke, S.H. (2014) ‘From Text to Tags: The Digital Humanities in an Introductory Literature Course’, 
                        CEA Critic, 76(2), pp. 200–210. 
                        10.1353/cea.2014.0012.
                    
                    Griffin, M. and Taylor, T.I. (2017) ‘Shifting expectations: Revisiting core concepts of academic librarianship in undergraduate classes with a digital humanities focus’, 
                        College & Undergraduate Libraries, 24(2–4), pp. 452–466. 
                        10.1080/10691316.2017.1325346.
                    
                    Hamilton, M. (2020) ‘Implementation of a low-stakes daily assessment in a large introductory LAC course’, 
                        Teaching and Assessment Symposium [Preprint]. Available at: 
                        https://digscholarship.unco.edu/posters_2020/4.
                    
                    Isuster, M.Y. (2020) ‘From students to authors: Fostering student content creation with Scalar’, 
                        College & Undergraduate Libraries, 27(2-4), pp. 133–148. 
                        10.1080/10691316.2020.1830908.
                    
                    Meer, N.M. and Chapman, A. (2014) ‘Assessment for confidence: Exploring the impact that low-stakes assessment design has on student retention’, 
                        The International Journal of Management Education, 12(2), pp. 186–192. 
                        10.1016/j.ijme.2014.01.003.
                    
                    Sample, M. and Schrum, K. (2013) ‘What’s Wrong with Writing Essays: A Conversation’, in Cohen, D.J. and Scheinfedlt, J.T. (eds) 
                        Hacking the academy : new approaches to scholarship and teaching from digital humanities. Ann Arbor, MI: University of Michigan Press, pp. 87–96.
                    
                    Tracy, D.G. and Hoiem, E.M. (2018) ‘Scaffolding and Play Approaches to Digital Humanities Pedagogy: Assessment and Iteration in Topically-Driven Courses’, 
                        Digital Humanities Quarterly, 11(4). Available at: 
                        http://digitalhumanities.org:8081/dhq/vol/11/4/000358/000358.html.
                    
                
            
        
    



        
            This short paper introduces LEAF (the Linked Editorial Academic Framework virtual research environment), an enhanced and expanded collaborative editorial platform that supports a variety of digital scholarly projects through a pipeline of integrated tools for collaborative production and publication of scholarly and documentary collections. Funded through the Canada Foundation for Innovation and the Andrew W. Mellon Foundation, LEAF aims to address the challenges that face many who undertake and maintain large-scale collaborative DH projects now: namely, the need to ensure that these projects can remain operational and available to editors and audiences over the long-haul. It is only by sharing physical, software, and human infrastructures across institutions that this can be accomplished. In so doing we can support scalability, interoperability, and preservation while allowing for dynamic, iterative, and collaborative editing, and therefore ensure that our materials, collections, and editions will remain viable and accessible. The LEAF team aims to do this by integrating best practices for text encoding, annotation, and metadata standards. This short paper will report on the development of LEAF and the functionalities that it will provide. 
            The implementation, and dissemination of LEAF is built upon a collaboration to extend the Canadian Writing Research Collaboratory (CWRC) built by the Universities of Alberta and Guelph (Susan Brown) with Bucknell University (Diane Jakacki), and Newcastle University (James Cummings) as founding partners. This work enhances CWRC’s functionality through collaborative software development that will ultimately support multiple instances of the LEAF platform in Canada, the US, and the UK. At Bucknell, this work will inform the Liberal Arts Based Digital Edition Publishing Cooperative and the Bucknell Digital Press, funded by an Andrew W. Mellon Digital Publishing Cooperative Implementation grant that will support an expanding portfolio of peer-reviewed digital editions and edition clusters. 
            
                The LEAF platform combines hardware, software, and personnel. LEAF is being built on a solid foundation in terms of its data models, core functionality, and code management, so that it is positioned for extension and long-term sustainability. The platform is based on the Islandora 8 framework, which combines Drupal 8 with a Fedora 5 repository for long-term preservation. The LEAF repository will customize and enhance Islandora to enable digital humanities workflows and publication needs. Enhancements include an innovative web-based editing tool that allows users to employ TEI XML along with Web Annotation and IIIF standards-compatible Linked Open Data annotations that enhance discoverability and interoperability. 
            
            
                The founding LEAF institutions are collaborating to upgrade the existing CWRC environment and produce a fully modular platform that will also be hosted on Bucknell’s servers, further tested at Newcastle University, and offered as containerized open-source code freely available for download and installation by other institutions. In particular, LEAF will facilitate the production and publication of dynamic digital scholarly editions and collections, offering multilingual transcription, translation, and image markup. Entirely browser-based, its functionality includes an in-browser XML markup editor, XML rendering tools, built-in text and data visualization tools including the Voyant Tools suite and its Dynamic Table of Contexts Browser. Overall the LEAF platform will provide a sophisticated interface for digital editions in which the XML markup is leveraged for navigation and active reading, and enhanced with Linked Open Data.
            
        
    

