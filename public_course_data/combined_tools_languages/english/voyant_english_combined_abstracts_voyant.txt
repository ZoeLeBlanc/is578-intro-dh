
Introduction: Cuts, Crisis and Criticisms of the Humanities
The Great Recession beginning in 2008 has resulted in a series of budgetary cuts to many universities, education programs, and cultural institutions that reflect indifference about, and even hostility toward, the humanities and arts in favor of scientific, engineering, business, and other applied fields. Even scientists are now concerned about the perceived legitimacy of their "basic research" when it lacks evident short-term application. However, the sciences have an established tradition of public advocacy and media communication that is quite effective in putting their discoveries before the public. The humanities have no such consistent, planned tradition of advocacy, and in many ways are starting from scratch.

This paper argues for and demonstrates planned humanities advocacy using the special affordances of the digital humanities. In particular, it discusses how the 4Humanities initiative is leveraging DH for next-generation advocacy. In this paper, we will:

Show how 4Humanities uses DH to help analyze public discourse, both pro and con, about the humanities (including text analysis of such discourse as well as crowd-sourced generation of arguments for the humanities).
Show how statistics and other evidence about the contribution of the humanities to society can be analyzed and visualized in support of effective arguments.
Discuss the role of 4Humanities and, more generally, how DH can provide special tools for humanities advocacy, while humanities advocacy can in return incentivize the creation of next-generation DH research and teaching tools with a built-in public engagement dimension.
Analysis of Arguments Against the Humanities
One way to bolster advocacy for the humanities is first to look closely at the arguments made in public against them. We have compiled a small corpus of recent articles (especially from news sources accessed by a broad public) representative of criticisms of the humanities (Auslin, 2012; Bauerlein, 2011; Cohan, 2012a; Cohan, 2012b; Ellouk, 2011; Fendrich, 2009; Fish, 2007; Fish 2008; Fund, 2012; Knapp, 2011; Murdoch, 2011; Pidgeon, 2007; Riley, 2012; Sini, 2011; Stephens, 2012; Wente, 2012; Wood, 2012).


Fig 1:
Voyant Collocate Cluster Visualization (Sinclair and Rockwell)

We find that arguments critical of the humanities cluster around certain ideas. Principally, detractors accuse the humanities of lacking cultural and/or economic relevance. The most nuanced critiques mix or shade the charges of cultural and economic irrelevance. But other arguments refute the social usefulness of the humanities entirely, simply taking for granted their complete economic and social irrelevance. (Interestingly, however, some of these articles also defend irrelevance as a virtue, as in the case of arguments from friends of the humanities who feel that irrelevance is the basic nature of the humanities and is perfectly acceptable.)

A related critique of the professoriate in the humanities is that our work is no longer accessible to a larger educated public. The argument is that we are our own worst enemies because we have descended into theoretical turf battles that no one cares about. For these commentators such cultural irrelevance goes hand in hand with the supposed lack of respect that academics show for the public’s values, as epitomized in Marxism, feminism, post-colonialism, and other approaches that attack iconic ideas. The antidote sometimes proposed is a return to a nebulous concept of the traditional humanities — e.g., to the venerable search for the “beautiful.”

This paper will summarize our reading of the articles as well as present results from some text analysis of other rhetoric about the humanities.

Analysis of Arguments For the Humanities
As important as it is to know the arguments critical of the humanities, it is also important to gather good arguments for the humanities. 4Humanities has taken two approaches to this. The first is to blog good arguments as we come across them with summaries for those who are looking for essays to help them in their advocacy. We have also summarized these in a digestible form for people to review (Bielby, 2012). Finally we ran an All Our Ideas vote on the value of the humanities (http://allourideas.org/4humanities). At the time of writing this proposal there were over 1600 votes and 31 user submitted possible answers (as opposed to 12 seeds that we provided.) The top choices at the time of writing included:


There is obviously overlap in the top choices, but these indicate what the digital humanities community considers important. In the paper we will provide a fuller analysis of the data along with our list of the best arguments.

Looking Closely at the Statistics
“Liberal arts graduates frequently catch or surpass graduates with career-oriented majors in both job quality and compensation.” (Koc, 2011)

In addition to defining and communicating the cultural value of the humanities, the 4Humanities initiative is also committed to gathering data on the economic value of the humanities. One of the most pernicious arguments against the humanities has been the poor job prospects of graduating humanists. For this reason, we review the statistical arguments carefully, especially to highlight the fact that the data on compensation of humanists at mid-career paints a different story. A humanist may find it harder to get a first job with a degree, but she/he will probably rise faster than many with professional certification.

When making arguments for the humanities’ contribution to society, of course, it can be difficult to produce statistics, given that there has been no comprehensive study that gathers together available facts and figures in a usable manner. As part of the 4Humanities project we have been compiling and listing all statistics we can find in the published literature about the benefit of the humanities to society. This was done by collating the literature – including newspaper articles, reports, websites, and op-ed pieces (listed on the 4Humanities website) – and locating numeric references to the humanities. We have compiled such quantitative evidence, and at DH2013 we will present an infographic that sums up the statistical argument that the humanities are relevant to economic and intellectual development.

In addition, we have recently begun our “Infographics Friday” series (http://humanistica.ualberta.ca/category/for-the-public/humanities-infographics/), highlighting a particular statistic or graphical representation on the 4Humanities website once a week, to demonstrate the range of evidence. A core remit of 4Humanities is to gather, analyse, and disseminate this disparate information to provide a knowledge base upon which others can build their opinions and bolster their understanding of the humanities.

Conclusion: 4Humanities and a Digital Humanities response to the Cuts, Crisis, and Criticism
We argue that the mission of public engagement and advocacy in the humanities as embodied by the 4Humanities initiative provides a unique way to consolidate leading technological and methodological directions in DH with outreach to society. The humanities today have an advantage that was not available earlier: the analytical and communication methods of the digital humanities. Not only do the digital humanities provide a strong argument for the relevance of humanities learning in a digital age; they also provide unique, fresh ways of studying the contributions of the humanities to society and then getting the message out. DH research and humanities advocacy can be one, where DH helps advance advocacy, and, reciprocally, the advocacy mission helps drive research in DH. The hunt is now on to develop and extend new generations of digital humanities platforms and tools that can integrate the core research and teaching work of humanists with public visibility and engagement. Such platforms and tools (for publishing, editing, research, pedagogy, etc.) can be designed from the ground up, both to serve the needs of academics and to engage with today's networked public. This paper is a step in that direction.

References
Auslin, M. (2012). Knowledge is Good. National Review Online. 15 March. http://www.aei.org/article/education/higher-education/knowledge-is-good/ (accessed 13 March 2013).
Bauerlein, M. (2011). Oh, the Humanities! The Weekly Standard. 16 May. http://www.weeklystandard.com/articles/oh-humanities_559340.html (accessed 13 March 2013).
Bielby, J. (2012). Arguments for the Humanities. CIRCA Wiki, October. http://circa.cs.ualberta.ca/index.php/CIRCA:Arguments_FOR_the_Humanities (accessed 13 March 2013).
Cohan, P. (2012a). To Boost Post-College Prospects, Cut Humanities Departments. Forbes, 29 May. http://www.forbes.com/sites/petercohan/2012/05/29/to-boost-post-college-prospects-cut-humanities-departments/ (accessed 13 March 2013).
Cohan, P. (2012b). The 13 Most Useless Majors, From Philosophy to Journalism. The Daily Beast, 23 April. http://www.thedailybeast.com/galleries/2012/04/23/the-13-most-useless-majors-from-philosophy-to-journalism.html (accessed 13 March 2013).
Davidson, C. N. (2011). Strangers on a Train. Academe. 97 (5). http://www.aaup.org/AAUP/pubsres/academe/2011/SO/Feat/davi.htm (accessed 13 March 2013).
Davidson, C. N. (2012). Humanities 2.0: Promise, Perils, Predictions. In Gold, M. (ed), Debates in the Digital Humanities. Minneapolis: University of Minnesota Press. 476-489.
Davidson, C. N., and D. T. Goldberg (2004). A Manifesto for the Humanities in a Technological Age. The Chronicle of Higher Education: The Chronicle Review. 13 Feb.
Delblanco, A. (2011). College: What It Was, Is, and Should Be. Princeton, NJ: Princeton University Press.
Ellouk, B. (2011). Do We Still Need the Humanities? The Daily of the University of Washington. 26 July. http://dailyuw.com/news/2011/jul/26/do-we-still-need-humanities/ (accessed 13 March 2013).
Fendrich, L. (2009). The Humanities Have No Purpose. The Chronicle of Higher Education, 20 March. http://chronicle.com/blogs/brainstorm/the-humanities-have-no-purpose/6738 (accessed 13 March 2013).
Fish, S. (2007). Bound For Academic Glory? The New York Times, 23 December. http://opinionator.blogs.nytimes.com/2007/12/23/bound-for-academic-glory/ (accessed 13 March 2013).
Fish, S. (2008). Will the Humanities Save Us? The New York Times, 6 January. http://opinionator.blogs.nytimes.com/2008/01/06/will-the-humanities-save-us/ (accessed 13 March 2013).
Fish, S. (2010). The Crisis of the Humanities Officially Arrives. The New York Times, 11 October. http://opinionator.blogs.nytimes.com/2010/10/11/the-crisis-of-the-humanities-officially-arrives/ (accessed 13 March 2013).
Fund, J. (2012). Censoring Naomi Riley. The National Review Online, 12 May. http://www.nationalreview.com/articles/299765/censoring-naomi-riley-john-fund (accessed 13 May 2013).
Kirschenbaum, M. (2012). Digital Humanities As/Is a Tactical Term. In Gold, M. (ed), Debates in the Digital Humanities. Minneapolis: University of Minnesota Press, 415-428.
Knapp, S. (2011). The Enduring Dilemma of the Humanities. The Phi Beta Kappa Society, 29 March. http://www.pbk.org/home/FocusNews.aspx?id=741 (accessed 13 March 2013).
Koc, E. W. (2011). Just Wait 10 Years. New York Times, 21 March. http://www.nytimes.com/roomfordebate/2011/03/20/career-counselor-bill-gates-or-steve-jobs/your-college-major-matter-less-over-time. (accessed 13 March 2013).
Lakoff, G. (2004). Don’t Think of an Elephant!: Know Your Values and Frame the Debate. White River, VT: Chelsea Green Publishing Company.
Liu, A. (2012). Where Is Cultural Criticism in the Digital Humanities? In Gold, M. (ed), Debates in the Digital Humanities. Minneapolis, MN: University of Minnesota Press, 490-509.
Murdoch, R. (2011). The Steve Jobs Model for Education Reform. The Wall Street Journal, 15 October. http://online.wsj.com/article/SB10001424052970203914304576631100415237430.html (accessed 13 March 2013).
Pidgeon, S. (2007). Commented on Fish, S. Bound For Academic Glory? The New York Times, 23 December. http://opinionator.blogs.nytimes.com/2007/12/23/bound-for-academic-glory/#comment-100883 (accessed 13 March 2013).
Riley, N. S. (2012). The Academic Mob Rules. The Wall Street Journal, 8 May. http://online.wsj.com/article/SB10001424052702304363104577391842133259230.html (accessed 13 May 2013).
Sinclair, S., and G. Rockwell Collocates Cluster, from Voyant Tools. http://docs.voyant-tools.org/tools/links/ (accessed 13 March 2013).
Sini, M. (2011). Oh the Humanities! (OR: A Critique of Crisis). OverLand, 22 February. http://overland.org.au/blogs/loudspeaker/2011/02/%E2%80%98oh-the-humanities%E2%80%99-or-a-critique-of-crisis/ (accessed 13 March 2013).
Stephens, B. (2012). To the Class of 2012. The Wall Street Journal, 9 May. http://online.wsj.com/article/SB10001424052702304451104577389750993890854.html (accessed 13 March 2013).
Wente, M. (2012). Quebec’s University Students are in for a Shock. The Globe and Mail, 1 May. http://www.theglobeandmail.com/commentary/quebecs-university-students-are-in-for-a-shock/article4104304/ (accessed 13 March 2013).
Wood, P. (2012). Rick Santorum is Right. The Chronicle of Higher Education, 29 February. http://chronicle.com/blogs/innovations/rick-santorum-is-right/31769 (accessed 13 March 2013).
Some thirty years ago Donald Knuth, a computer scientist, proposed literate programming as a better way of organizing narrative and code (1984). Knuth argued that more emphasis should be placed on explaining to humans what computers are meant to do, rather than simply instructing computers what to do. Knuth was especially interested in weaving together macrostyle code snippets with prose that provided a larger narrative context, not merely functional comments of specific lines of code that are the distilled remnants of an intellectual process.
Literate programming has been more influential in theory than in practice (Nørmark), despite several utilities and environments including Mathematica, Knuth's (C)WEB, Sweave for R, and Marginalia for Clojure. Perhaps the exigencies of programming in the real world correspond poorly with the vision of Knuth of the programmer as author: "the practitioner of literate programming can be regarded as an essayist, whose main concern is with exposition and excellence of style" (1992, 1). However, that balance of essayist and coder strikes us as perfectly appropriate for the digital humanities, a natural blend of the expression of intellectual process with the exposition of technical methodologies. The prose can gloss the code, or viceversa, in a symbiotic relationship that serves to strengthen an argument and demonstrate its own workings.
One of the most significant potential benefits of the literate programming paradigm is pedagogical: these works can both explain an interpretive insight and present the methodology for reproducing the data or results that were part of the process. Many widely-read digital humanities blogs already present these characteristics of exploration, explanation, interpretation and step-by-step instructions (see for example blogs by Ted Underwood, Benjamin Schmidt, Lisa Rhody and Scott Weingart). Literate programming can be more self-contained and more useful for those learning new methodologies and new programming techniques. This is about the principles of literate programming, but also about the potential for increasing programming literacy.
This poster will introduce Voyant Notebooks, a web-based literate programming environment designed for the digital humanities (see Appendix A). There is already a working prototype and we anticipate having a more feature-rich version available by July 2013. Voyant Notebooks inherits many of the characteristics of the Voyant Tools environment, including a concern for usability and flexibility (researchers and students should be able to use it with minimal or no training and with their own texts of interest). Voyant Notebooks also addresses one of the main weaknesses of Voyant Tools: the fact that most tools are constrained by assumptions about how they would be most commonly used. For instance, the Wordle-like (word cloud) Cirrus tool is designed to show the top frequency terms from a corpus or document; but what if the user instead wants to visualize the top frequency nouns, or people, or repeating phrases? All of that functionality could be built into the tool, but possibly at the cost of usability (endless menus and options), and it could still never address all of the possible use cases. Voyant Notebooks, by contrast, empowers the user to customize some of the functionality by leveraging the analytic capabilities of the Voyant back-end and the visualization interfaces in the front-end (like Cirrus). Our poster will have two parts, a) a usable demonstration on one or more laptops and b) a poster that illustrates how Voyant Notebooks implements Knuth’s concept of literate programming. In addition to these conceptual aspects, the poster will outline technical details about the Voyant Notebooks prototype for those interested, including the technologies used for both client-side (browser) and server-side components. Some of the technical challenges that will be described include:
•        managing the flow of code execution in an asynchronous architecture,
•        using web workers to avoid browser freezes during longer executions,
•        mitigating the security risks of user-defined and persistent Javascript code,
•        code variable scoping across editor instances and window components,
•        embedding of Voyant tool panels (visualizations) and other services,
•        developing a flexible API for different programming levels and styles,
•        developing an API that includes both client-side and server-side operations, and
•        ensuring efficiency of repeated code snippets during writing and viewing.
And of course, visitors to the poster session will be warmly encouraged to play with Voyant Notebooks.
Appendix A: Mockup of Voyant Notebooks (previously called Voyeur Notebooks).
 
Figure 1:
Mockup of Voyant Notebooks
References
Knuth, D. (1984). Literate Programming. The Computer Journal 27(2): 97-111, 1.
Knuth, D. (1992). Literate Programming. Stanford University Center for the Study of Language and Information.
Nørmark, K. (1998). Literate Programming: Issues and Problems. http://www.cs.aau.dk/~normark/litpro/issues-and-problems.html.
Sinclair, S. and G. Rockwell (2012). Teaching Computer-Assisted Text Analysis: Approaches to Learning New Methodologies. in Digital Humanities Pedagogy. Open Book Publishers.
1. An Introduction to the Design of New Knowledge Environments
Ruecker, Stan|sruecker@id.iit.edu|IIT Institute of Design
Rockwell, Geoffrey|grockwel@ualberta.ca|University of Alberta
INKE Research Group||
In this panel, we report on our year 4 work in the Interface Design (ID) research team of the Implementing New Knowledge Environments (INKE) project. INKE is a 7-year major collaborative research initiative (MCRI) project funded in Canada by the Social Sciences and Humanities Research Council (SSHRC). INKE is led by Ray Siemens at the University of Victoria, and includes dozens of researchers worldwide. Each year, we have had a different focus for our work. The schedule is as follows:

Year 1: interdisciplinary citations
Year 2: corpora
Years 3 and 4: the scholarly edition
Year 5: the monograph and journal
Year 6: born-digital literature
Year 7: wrap-up and dissemination
For the first three years of the project, our focus has been on a wide range of experimental prototypes. Beginning in year 4, we attempted to aggregate these prototypes into a smaller set of “new knowledge environments”, where the goal is no longer to have a single piece of functionality that we can experiment with, but instead to begin imagining how the various pieces of functionality, as represented by our prototypes, can work in concert to produce an environment for people working with electronic books.

What makes these environments “new” is that they are comprised of a set of experiments into working with digital text. More often than not, these experiments involve the design and programming of a prototype, which may exist at any one of a range of levels of fidelity. Ideally, the lowest level of fidelity is developed that is necessary in order to come to grips with the central idea. To develop further is sometimes required in order to achieve an adequate user experience study, but in any case it is important to keep in mind that the end goal is the extension of our understanding rather than the production of a piece of stable software.

So the process is to conceive of a concept and produce a prototype to help us better understand the concept. Typically a prototype will generate some new knowledge itself, and that knowledge can contribute to the next iteration of the idea. In some cases, the prototype teaches us enough that there is no need for a further prototype. If what we’ve learned is of potential use, then we can consider a more robust implementation of the idea. If what we’ve learned is that the line of thought we’ve been pursuing may not be fruitful after all, then we can at that point abandon the trajectory and move to another idea.

In the case of the year 4 projects in INKE, we have selected some prototypes that we think deserve to be aggregated into an environment. We are at the same time trying to learn what we can about the theoretical and technical issues involved in this kind of redesign and reprogramming for the purposes of aggregation. These are environments rather than simpler tools in the sense that they afford more than a single task or set of tasks.

In the papers that follow, we describe each of the environments, examine the state of the art in scholarly editions for tablets, and discuss the user experience study of two of the prototypes that have gone on to inform one of our new knowledge environments.

2. Reading Skins: Voyant and Tool Aggregation
Rockwell, Geoffrey|grockwel@ualberta.ca|University of Alberta
Sinclair, Stéfan|stefan.sinclair@mcgill.ca|McMaster University
INKE Research Group||
Tools are not just ways to ask questions - they are also reading skins. Interface designers have known this for some time. They design interfaces to present affordances and views that facilitate different types of reading. A dictionary is designed for consultation reading (Blair 2010), a manual for training. Likewise an e-book minimizes distractions, presenting “just the text”, while text analysis environments embed the texts in alternative ways of reading, from visualizations to interactive controls. These design choices are based on a model of the reader and the tasks they are engaged in. In this paper we will look at the types of interfaces or “skins” presented by text analysis environments and end by discussing the decisions taken in the design of Voyant. We will do that in the following ways:

1 First, we will look back at one of the first computing humanists to consider visualization and the interface to text analysis tools, John Smith.
2 Second, we will survey different types of text analysis interfaces.
3 Third, we will close by discussing how the architecture of Voyant is designed to allow for different reading skins that aggregate different tools to suit different uses.
1. First Thoughts on the Interface
One of the first computing humanists to think about the interface to text analysis tools was John Smith. John Smith in “Computer Criticism” and other articles proposed a way that analytical tools like his ARRAS could fit into interpretative research practices. He saw the computer as a tool to help identify and then trace structures through a text and gave an example of how this can help rereading a text in his article “Image and Imagery in Joyce's Portrait.” In “A New Environment For Literary Analysis” he explicitly discussed how the analytical tool ARRAS was not meant to replace the inquirer but to amplify them, how it could provide what we today call visualizations, and how it should be thought of not as a program, but as an environment for work where one could switch from text analysis to editing the text or sending a message. In the presentation we will go into more detail about how Smith articulated the relationship between tool design and interpretative practices.

2. Survey of Type of Interface
In the second part of the paper we will survey various interface paradigms for text analysis tools including:

ARRAS: We will start with the interface John Smith developed for ARRAS, and discuss his ideas for a humanities accessible command line language.
TACT and TACTweb: We will then discuss TACT, which was released in 1989 and designed by John Bradley and others (Lancashire 1996). TACT, while running in MS DOS, was influenced by the then new idea of a Graphical User Interface (GUI). It had a primitive windowing model that let you split the screen to see multiple displays and use one to drive the other(s). TACTweb, which came later, brought TACT functionality to the web, and illustrated for the first time how the web separated interface from text database so that multiple interfaces could be built.
HyperPo: While HyperPo (hyperpo.org) wasn’t the first text analysis environment on the web, it was one of the first to fully exploit the web. It let you upload a text and it provided a number of innovative features including making displays themselves affordances for further interaction. It was also explicitly designed as a reading environment.
TextArc: One of the most beautiful interfaces to text analysis is the TextArc (textarc.org) visual concordance designed by W. Bradford Paley. This work pushes the idea of interface in interesting directions as it can be considered a work of art or design meant to be appreciated in and of itself rather than as a window onto something else.
Other interfaces could be mentioned like the visual programming interface idea of Eye-ConTACT that is also available in SEASR or the library interface of the MONK project, but these are more management interfaces than reading ones.

3. Voyant and Skins
In the last part of the presentation we will discuss the layered architecture of Voyant that allows one to create different combinations of tools into “skins” that aggregate different tools. A Skin Builder tool for creating your own skins will be demonstrated and different examples of skins for different reading purposes will be shown. Different uses call for different combinations of tools.

All of these text analysis interfaces are presented from the perspective that they are views on a static object that is studied, but what if the object of study is changing? What if we think of text analysis tools performing the text rather than skin it? The presentation will end with an alternative prototype interface designed not for reading, but for animating the text as if it were a performance.


Figure 1 -
Voyant Skin Builder

References
Blair, A. (2010). Too Much to Know: Managing Scholarly Information Before the Modern Age. New Haven: Yale University Press.
Galey, A., and S. Ruecker (2010). How a Prototype Argues. Literary and Linguistic Computing 25(4): 405–424.
Lancashire, I., et al. (1996). Using TACT with Electronic Texts: a Guide to Text-analysis Computing Tools, Version 2.1 for MS-DOS and PC DOS. Modern Language Association of America.
Parunak, H. V. D. (1981). Prolegomena to Pictorial Concordances. Computers and the Humanities 15(1): 15–36.
Rockwell, G., S. Sinclair, et al. (2010). Ubiquitous Text Analysis. The Journal of the Initiative for Digital Humanities, Media, and Culture 2(1).
Sinclair, S. (2003). Computer-Assisted Reading: Reconceiving Text Analysis. Literary and Linguistic Computing 18(2): 175–184.
Sinclair, S., and G. Rockwell (2009). Between Language and Literature: Digital Text Exploration. Teaching Literature and Language Online. In Lancashire, I. (ed. and introd.). vii, 460 pp. New York, NY: Modern Language Association of America. 104–117. Options for Teaching (OfT): 26.
Smith, J. (1984). A New Environment for Literary Analysis. Perspectives in Computing: Applications in the Academic and Scientific Community 4(2): 20–31.
Smith, J. (1989). Computer Criticism. Literary Computing and Literary Criticism: Theoretical and Practical Essays on Theme and Rhetoric. University of Pennsylvania Press. 326–356.
Smith, J. (1973). Image and Imagery in Joyce’s Portrait: A Computer-Assisted Analysis. Directions in Literary Criticism: Contemporary Approaches to Literature. Festschrift for Henry W. Sams, (ed.) Stanley Weintraub and Philip Young (University Park and London: Pennsylvania State University Press): 220–227.
Smith, J. (1985). Arras User's Manual: TR85-036. Chapel Hill, NC, The University of North Carolina at Chapel Hill.
3. Designing the interface within the interface: legibility and readability in the Dynamic Table of Contexts
Windsor, Jennifer|jwindsor@ualberta.ca|University of Alberta
Brown, Susan|sbrown@uoguelph.ca|University of Guelph
Nelson, Brent|brent.nelson@usask.ca|University of Saskatchewan
Radzikowska, Milena|mradzikowska@gmail.com|Mount Royal University
Sinclair, Stéfan|stefan.sinclair@mcgill.ca|McMaster University
INKE Research Group||
“Typography is to literature as musical performance is to composition: an essential act of interpretation, full of endless opportunities for insight or obtuseness.”

— Robert Bringhurst, The Elements of Typographic Style
As humanities scholars transition from reading traditional print texts to reading on computer screens, ebooks and tablets, we are discovering that the visible word has taken a step backwards in quality. Typographic considerations are often the most challenging aspects of user experience development of digital reading environments. Ereaders are still in their infancy, and thus far little attention has been paid to textual design beyond very basic choices of typeface and font size.

We have developed the Dynamic Table of Contexts, a text analysis environment that combines the traditional concepts of the table of contents and index to create new methods for Humanities scholars to read and interact with digital text. The main interface consists of four interactive panes that perform dynamically with each other: the table of contents, the index, the xml tag list and a large pane to read the text itself (see Figure 1). In addition, there is a pane for reader notes. The Dynamic Table of Contexts interface is predominantly textual in nature, but the text at the centre of it is itself an interface – the point of interaction between the reader and that which is read. This study examines the appearance and arrangement of the textual interface within the larger interface. We asked: how can we typographically optimize the ereading experience?

The guiding fundamentals of typography are legibility and readability. Legibility concerns the ease with which a letterform can be recognized and readability refers to the ease with which text can be understood (Lupton 2004). Many problems with legibility have arisen in the transition from print to screen. For example, rather than black ink on a paper page, black text on a screen is an absence in the glowing pixels that surround it, and the result is perceived as blurry regardless of screen resolution. Anti-aliasing is used to compensate for low resolution on most computer monitors but can't be applied consistently from character to character because of where the letter may land on the physical pixel grid. In an effort to offset this problem font size is often increased, which in itself can become a reading irritant and navigationally awkward as larger type creates shorter lines of text which in turn requires more left-right eye movements from the end of one line to the beginning of the next. Fewer lines of text also necessitate more scrolling which is visually uncomfortable and requires additional time and visual energy for the reader to relocate himself in the text after each movement. While we wait for technology to catch up in these areas, this study outlines the methods we used to minimize legibility issues and discusses which existing typefaces (most of which, it must be remembered, were never intended for the screen) best counteract blurring and movement. We identify optimal type size and line length combinations for comfortable extended on-screen reading in the Dynamic Table of Contexts.

Compared to legibility, issues of readability are less often addressed in discussions of digital reading environments. Readability is comprised not only of the arrangement of type on a page or screen, but also of attention to the entire visual entity and all the complex relationships between levels of type, symbols and images (Berryman 1984). Typography imparts semantic meaning as it interprets content and influences meaning by creating hierarchies, assigning values, and manipulating emphasis (Bachfischer, Robertson and Zmijewska 2007). Our investigations into optimal readability design for the Dynamic Table of Contexts raised interesting questions about the representation of text and the process of reading on-screen. In the remediation from print to screen, do we still read the same way? Do the rules that apply to the readability of print typography apply to on-screen typography? Do conventions that govern readability of websites also apply to extended reading of scholarly materials?

In this study we have also examined other components of the textual interface that affect readability but that are not necessarily typographic in nature. Often on-screen paging representations (such as a simulated left and right sides of a double page spread or an animated page turning) give the illusion of a traditional print book. We examine theories of whether these provide valuable perceptual cues to the reader or are simply a vestigial device that has lost its significance in a changing environment.

This research represents a move from an adequate to an optimized reading environment.


Figure 2 -
Dynamic Table of Contexts

References
Bachfischer, G., T. Robertson, and A. Zmijewska (2007). “Understanding Influences of the Typographic Quality of Text.” Journal of Internet Commerce 6(2): 97–122.
Berryman, G. (1984). Notes on Graphic Design and Visual Communication. Los Altos, California: W. Kaufmann.
Bringhurst, R. (2004). The Elements of Typographic Style. 3rd ed. Hartley and Marks Publishers.
Larson, K. (2004). “The Science of Word Recognition: Or How I Learned to Stop Worrying and Love the Bouma.” July. Microsoft Typography site. Retrieved August 20, 2006. http://www.microsoft.com/typography/ctfonts/WordRecognition.aspx
Lupton, E. (2010). Thinking with Type: A Critical Guide for Designers, Writers, Editors, & Students. New York. Princeton Architectural Press.
Mangen, A. (2008). “Hypertext Fiction Reading: Haptics and Immersion.” Journal of Research in Reading 31(4): 404–419.
Weinzettelova, S. (2012). “Traditional Type in the Digital Era.” Bulletin - Prague College Centre for Research and Interdisciplinary Studies 2012(2): 5–24.
4. The Tablet as a New Medium for Scholarly Editions
Mohseni, Atefeh|amohseni@ualberta.ca|University of Alberta
Sondheim, Daniel|sondheim@uvic.ca|University of Victoria
Frizzera, Luciano|dosreisf@ualberta.ca|University of Alberta
Rockwell, Geoffrey|grockwel@ualberta.ca|University of Alberta
Ruecker, Stan|sruecker@id.iit.edu|IIT Institute of Design
INKE Research Group||
How have scholarly editions been implemented on tablets? In the past few years, a significant number of scholarly editions have seen deployment on the Web, and the differences between printed editions and Web-based ones has become a matter of attention for many scholars. In this paper, we will investigate scholarly editions as they appear in tablets and other mobile devices.

Web-based scholarly editions offer many new features that do not and cannot exist in printed ones. Such features include opportunities for collaboration with other users or with the editors of the work, innovative methods to search and browse, specialized tools and visualizations for text analysis, means to customize the layout of the interface and the content, options to change the language of the interface or to see translations of the text, high resolution zoomable images, multimedia elements such as video and audio, and a wealth of material that would be too costly or cumbersome to include in a printed edition.

Despite these advantages, some users still prefer their scholarly editions to be in paper form. Reasons may include a lack of physical distance between reader and book, and the ability to interact directly with the material. Tablets have solved these problems to some extent, emulating the physicality of paper books, and allowing users an opportunity to touch and feel them. As Elena Pierazzo notes, “Usability studies have demonstrated that reading on tablets is more enjoyable than reading on the screen of computers and, in some cases, more than reading print” (Pierazzo 2011). Additionally, the fact that an app is an independent program housed on a particular machine results in increased speed and stability over Web-based editions, which also serves to reproduce some of the positive features of paper-based editions (McDayter 2012).

Tablets represent a return to printed editions in some more negative respects as well. For instance, users cannot create shared annotations or collaborate with each other in other ways; the app on the tablet is isolated from references and related material available on the Web; tools for text analysis are absent; and no options are provided to show the interface in different languages. Part of the reason for these omissions in tablets may be due to the fact that editions that are currently available on tablets have generally been produced more for a popular audience than a scholarly one. This is reflected not only in terms of functionalities of the interface, but with respect to the content as well. Bibliographies, glossaries, and textual apparatuses are typically missing, variants tend not to be included, and notes and annotations have been recycled from older print-based editions, rather than being the result of new scholarship. Tablet-based editions are often more focused on making an edition attractive and amusing than on making it scholarly, and may include games or quizzes, methods of sharing passages or images via social media sites, and other innovative but potentially distracting features likely to be of interest to keen fans of the material rather than to scholars.

So, tablets are in a way mediating between print and the Web, sharing some of the advantages and disadvantages of both. But what effects will these advantages and disadvantages have on scholarly editions? Do scholarly editions have a future in tablets? These are questions that we will discuss in this paper.

At this point, very few tablet-based scholarly editions have been released, and as discussed above, those that have are decidedly unscholarly in some respects. Touch Press has produced some notable examples, including editions of T.S Eliot’s The Waste Land, Leonardo da Vinci's notebooks, and Shakespeare’s sonnets. The Waste Land is the first edition produced by Touch Press, and continues to stand up as a nice example of the genre. This edition allows the poem to be viewed as plain text, and includes annotations, audio recordings, facsimile images of the original typescript, filmed performance of the poem, interviews about the poem and some image galleries.

Many of the features provided in The Waste Land would make using scholarly editions easier and more pleasurable. As is the case with most works of philosophy, art, and literature, The Waste Land is multi-layered, and multimedia is of great help in exploring, finding and analyzing the different existing layers. Study is also eased and enhanced by being able to quickly and easily reveal or hide information, switch between viewing multiple encodings of the text, and simply by having the ability to manually scroll through the text, rather than having to use a device.

With what has been discussed, some questions require further exploration. Will the advantages of tablet-based editions win out over their disadvantages? Will the tablet become accepted as an appropriate medium in which to produce and study scholarly editions? In this paper, we will consider whether scholarly editions have a future on tablets and how such a future might look.

References
Eliot, T. S. (2011). The Waste Land for iPad, ed. Justin Badger and Charles Chabot, Touch Press LLP, and Faber and Faber. http://touchpress.com/titles/thewasteland/
Pierazzo, E. (2011). Tablets Apps, or the future of the Scholarly Editions? Random Thoughts of a Digital Humanist with a Passion for Cookery, 27 November, 2011. http://epierazzo.blogspot.ca/2011/11/tablets-apps-or-future-of-scholarly.html
McDayter, M. (2012). Are We There Yet? Touch Press’s The Waste Land for iPad. February 25, 2012. http://clickherefordigitalhumanities.wordpress.com/2012/02/25/are-we-there-yet-touch-presss-the-waste-land-for-ipad/
5. Designing for Multi-Touch Surfaces as Social Reading Environments
Frizzera, Luciano|dosreisf@ualberta.ca|University of Alberta
Vela, Sarah|svela@ualberta.ca|University of Alberta
Ilovan, Mihaela|ilovan@ualberta.ca|University of Alberta
Michura, Piotr|pmichura@asp.krakow.pl|Academy of Fine Arts in Krakow
Sondheim, Daniel|sondheim@uvic.ca|University of Victoria
Rockwell, Geoffrey|grockwel@ualberta.ca|University of Alberta
Ruecker, Stan|sruecker@id.iit.edu|IIT Institute of Design
INKE Research Group||
The INKE Interface Design group has been exploring the application of multi-touch table technology for improving the comprehension, manipulation, and analysis of variorum editions beyond what has been accomplished in previous digital variorums. These volumes consist of three general components: the text of the work itself as selected by an editor; a list of variations between this ‘base-text’ and other manuscript or printed versions; and a comprehensive anthology of previous scholarly annotations on a particular line, passage, or the entire work. For many texts the existing notes are so extensive that it is difficult in a print medium to present “…simultaneous access to text and relevant commentary”, leading to an effort since the mid-nineteen-nineties to produce digital variorum editions (Werstine 2011).

Scholarly editions, especially variorums, raise interesting questions about the representation of elements and the act of reading. The richness of this type of edition creates dilemmas related to the organization of different pieces of information, as well as interacting with the text. A print version presents physical space constraints, such as the two-page spread and the necessity of linear presentation. This constraint become less of an issue in a digital environment, where space, time, and dimensionality in general are more fluid.

Although current iterations of digital variora (for example, the Online Chopin Variorum Edition, and the Electronic Variorum Edition of Don Quixote) attempt to take advantage of this flexibility of the medium, they present themselves as flat webpages, losing physical engagement with the materiality of the book. In order to bring back fuller physicality, we have used multi-touch surface technologies to simulate the “real space”, and to return to full gestures as opposed to clicks. A few projects focusing on eBooks and tablets have begun to emerge with the same idea (for example, The Wasteland, Shakespeare's Sonnets and Kerouac's On the Road), but this technology has not yet been applied to variorum editions.

Our project, The Comedy of Errors Tangible Variorum, involves creating a tangible user interface representing the Modern Language Association’s variorum edition of Shakespeare’s Comedy of Errors in order to explore the affordances of this technology to increase collaborative scholarly research and interpretation of the material.

Existing digital variorums have attempted to encourage collaboration and interaction by incorporating features into their web page interfaces. In the Electronic Variorum Edition of Don Quixote (EVE-DQ) from Texas A&M’s Cervantes project, for example, users can add their own commentary and annotations which can then be accessed by future users. Similarly the Online Chopin Variorum Edition (OCVE) in progress at King’s College London allows the attachment of notes to the base text with an option to share them publicly. For both projects, however, adding comments is an individual activity, with a single user at a workstation inputting notes with a mouse and keyboard. There is little room for group interaction, and any that exists must be sequential rather than communal. Most previous electronic New Variorum Shakespeare (eNVS) projects, particularly the recent Winter’s Tale version, have no digital annotation function at all, limiting them to use as a reference tool, rather than an interactive platform.

Furthermore, it can be exceptionally difficult to allow simultaneous visual comparison of the numerous features of a variorum on these web page formats given the small size of most screens. A study by Wästlund, Norlander, and Archer on the effect of page layout on mental workload shows that “manipulating an onscreen text document via scrolling necessitates a shift of focus from the text to the action of controlling the page movement,” (Wästlund, Norlander, and Archer, 1243) leading to decreased performance on reading comprehension tasks. Vandendorpe (2009) writes that “navigation by means of a mouse tends to give rise to chaotic, extremely rapid movement that is not very favorable to reading” (133).

By using a multi-touch table as a display device this project attempts to solve the problems of poor collaboration and broken comprehension. There has been significant research supporting the use of touch screens in improving reader focus, for example a study by Eva Siegenthaler et al. (2012) found that subjects performed better at various tasks involved in reading when using devices with tangible inputs, concluding that “a touch screen allows for an easier and more intuitive interaction” than a non-touch screen (Eva Siegenthaler et al. 2012, 94). The sizes of both the screen and the table perimeter of a multi-touch device, meanwhile, are conducive to multiple users working together. We thus believe that the application of multi-touch technology will have two effects. First, it represents another stage in the remediation of variorums, one that will better allow us to implement Unsworth’s (2000) scholarly primitives: to sample, compare, discover, represent, annotate and reference different versions of the base text. Simultaneously, it encourages these tasks to be performed socially, deepening understanding by incorporating multiple viewpoints.

The collaborative uses of tangible devices in research situations is one of the main goals of this research. In driving towards this end, however, our group has faced issues in two camps: the ability of users to adapt to multiple people concurrently using touch controls; and, less expectedly, the ability of a designer to structure elements on an unconventional screen.

From the perspective of users, much of the challenge is about breaking habits. Since the introduction of personal computers, users have learned how to interact with the machine in individual work spaces. The adoption of this concept is so intense that one is liable to think that group work is less effective than work alone with the computer. However, it is noticeable that the technology has determined this situation: people cannot work together because the machine allows just one input at a time. The questions that we raise are: what happens when more than one person can interact with the machine? What kind of operations and collaborations can a group perform when all of them are engaged with the same machine in the same environment?

For designers, learning to think beyond the confines of a screen and mouse proved to be a major obstacle. The original design conceived of the mutli-touch screen as being suspended on the wall, and the elements were placed accordingly (Figure 1). As we built and tested the interface it became clear that the layout would be effective for a single user, but was not ideal for the collaboration we were trying to achieve. While flipping the screen to a table was a logical choice to allow more users to work simultaneously, reassessing the design from that perspective resulted in a number of questions: where do you place touchstone elements when there is no clear top or bottom? How can items being used by different people overlap without disrupting anyone’s work?

This paper explores the problems faced when building a social reading environment, both for users and for designers. The technology to allow such interaction exists: a multi-touch table has a size that allows for the display of multiple documents side-by-side, and its status as a touch screen enables easy and intuitive operation, lessening the mental workload required to operate it and permitting users to focus on the content rather than the interface. The system accommodates familiar gestures such as touch, pinch and flick to let the user move, select, grab and scroll through information on the screen, and since more than one point of interaction is possible, multiple people are able to work at the same time in the examination of the material, improving collaborative work. Designing for these features, however, is a mental challenge that requires an upset of standards in the minds of all those involved, users and builders, and facing these problems is a necessary step in moving towards the future of collaboration.


Figure 1 -
Initial view showing chosen copy text in a full view and a reading panel. All variants and commentaries on the chosen page are marked. More to the right there are views of all textual notes and commentaries connected to the chosen part of the copy text. At the top there is a representation of all editions in chronological order, in which the dark bars shows the overall difference of a particular edition to the chosen copy text.

References
Bradley, J., and P. Vetch (2006). Supporting Annotation as a Scholarly Tool–Experiences From the Online Chopin Variorum Edition. Literary and Linguistic Computing 22(2): 225–241.
Siegenthaler, E., et al. (2012). The Effects of Touch Screen Technology on the Usability of E-Reading Devices. Journal of Usability Studies 7(3): 94–104.
Furuta, R., et al. (2001). Towards an Electronic Variorum Dition of Don Quixote. Proceedings of the 1st ACM/IEEE-CS Joint Conference on Digital Libraries. 444–445.
Galey, A. (2005). ‘Alms for Oblivion’: Bringing an Electronic New Variorum Shakespeare to the Screen. Shakespeare Association of America, Bermuda http://pear.hcmc.uvic.ca:8080/ach/site/xhtml.xq?id=98
Hinckley, K., et al. (1997). Cooperative Bimanual Action. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 27–34.
Unsworth, J. (2000). Scholarly Primitives: What Method
This paper describes an experimental approach to designing and teaching an introductory digital humanities course for graduate students. In 2011 Kevin Kee was asked to create and teach a class as part of a new interdisciplinary Humanities Ph.D. program. The graduate students taking the course would be largely unfamiliar with the digital humanities.

Kee began his preparation for the course by asking several questions. The first was "what should an introductory digital humanities course attempt to accomplish"? As he searched for answers, another important question emerged: "How could digital methods be used to design and deliver the course?" In this paper, Kee and Spencer Roberts, a Research Assistant who worked with Kee, describe first their method for researching and designing the course. They then sketch the structure and content of the course that resulted from their research. Finally, they provide examples of student responses to the material and methods covered in the course (including Roberts's perspective as a graduate student). The collected responses and their reflections on the process suggest particular ways in which future courses of this kind might be designed, implemented, and improved. Most importantly, they found that an effective way to design and teach an introductory digital humanities course is to think about the discipline through discussions about its topics and to think with the discipline by using digital tools and methods in the classroom.

Overviews of digital humanities course offerings have been conducted throughout the past fifteen years. In 1999, Willard McCarty and Matthew Kirschenbaum identified only fourteen institutions that offered courses in humanities computing. In 2006, Melissa Terras conducted another survey of digital humanities curriculum, and in 2011, Lisa Spiro undertook to collect and analyze syllabi from digital humanities courses. Of these previous surveys, Spiro’s was the most comprehensive; she collected over 134 syllabi from various levels of study in the digital humanities. Although Spiro’s work parallels and was helpful to that of Kee and Roberts, the latter were unaware of her project when they began, and had no way to replicate her research method. As a result, Kee and Roberts drew on the results of their own analyses while designing the course.

For Kee and Roberts' research, Roberts designed a method by which syllabi were converted into sets of data representing reading lists, assignments, assessment methods, and digital tools used. Commonly occurring items from within those sets were highlighted and identified as items deemed important by the statistical consensus of instructors represented in their sample. For example, their results showed that seven authors of digital humanities-related articles and books appeared on reading lists at a significantly higher frequency than others; the data also showed that other instructors found these authors most useful. Kee drew on these results when deciding on readings for his course list.

Topics covered in the course included text encoding and markup, distant reading, building, mapping, modelling and simulating, playing and gaming, teaching, and collaborating. Each of these was paired with a practical application, usually drawn from a modified version of William Turkel’s "Method". For example, students learned the theory of text markup and were asked to create pages on the course wiki using the basic wiki standard markup. Franco Moretti’s theory of distant reading made more sense for students once they experimented with text mining and analysis tools such as Voyant. Kee’s assessment strategy required students to use blogs and Twitter to comment on the theories and tools they encountered; Kee also encouraged them to participate in scholarly discourse that occurs on the Web. Although most of the students studied history, Kee aimed to create an environment in which the digital humanities were understood as both theory and practice that could be incorporated into any humanities discipline.

Because Kee and Roberts hoped to learn from the experiences of graduate students new to the digital humanities, Kee built feedback mechanisms into the course assignments, and asked students to reflect on the course before and after completion. Nearly all of the students were challenged by the dual responsibility of learning theory and skills simultaneously. Although some students were relieved to finish experimenting, others were pleased with their progress and the opportunities for future research. One student commented, “Not only do I now have some new tools to use while I’m doing research… I’m also more open-minded towards using them in the first place and really trying to engage with them, rather than brushing them off.” While students readily adopted some of the tools, such as Zotero and Evernote, they found more complex tools such as DevonThink or Voyant required a level of commitment and time they did not want to make. In short, these students were not willing to commit to a new, digital research method at a time when they were simultaneously taking graduate courses rooted in conventional research methods. For some students, however, patience led to late or accidental discoveries that improved their methods; in at least one case, a student who was skeptical throughout the course became an enthusiastic supporter of digital methods and now avidly attends DH conferences and events. At the conclusion of the course, most students were open to the various theories and approaches used in the digital humanities, and were enthusiastic about trying new tools and experimenting with new methods that might improve their research and scholarship.

From the outset of the project, Kee and Roberts understood that they were asking questions for which there were several feasible answers. Some graduate level digital humanities courses focus on topics within the digital humanities; others primarily train students to develop digital skills using computational tools. Kee's approach was to combine these two approaches into one course that provided opportunity for theoretical discussion while also showcasing practical applications, so that students could see the potential beneDits of digital humanities methods without having to master sophisticated tools. The research method used to build the course syllabus employed the same theories and tools that were later discussed in the course, creating an iterative loop through which student feedback and developments in the discipline can be incorporated into future versions. Already there are new tools to improve the collection and analysis of digital humanities syllabi, and new methods being explored by instructors. Through the experimental approach described in this paper, Kee and Roberts have found that thinking about and thinking with the discipline, a method that many digital scholars employ in their research, is also an effective way to design a course, and appeals to students who are new to the discipline, fostering enthusiasm for its use in their own often conventional humanities scholarship. The authors hope that this approach contributes to the growing conversation about teaching digital humanities, while also reflecting and adapting to the dynamic topics within the field.
Integrating research and teaching is exciting, time intensive, and a prescription for energizing faculty and students. We present outcomes of a six-year effort in multidisciplinary collaboration centered on the digital humanities as experienced in our teaching and research. Rooted in a set of “connected” courses between English and Computer Science (LeBlanc, et al. 2010) and three summers of NEH-funded research, our Lexomics Research Group has developed a modest set of web-based applications for scholars of digitized texts. We report here on the iterative development of the open-source toolset, how scholars both in and outside our group have used these tools to make significant discoveries, and perhaps most important how our research and teaching collaborations introduce a spirit of experimentation to the digital humanities.

Our current website is both a repository for our tool set as well as an evangelistic platform and teaching resource: http://lexomics.wheatoncollege.edu. We continue to develop online tools for three independent, but logically connected functions that lead scholars through the steps needed for performing hierarchical cluster analyses of texts and/or sections of texts. At this point, our cluster analysis tools are more narrowly focused than other toolsets, c.f. Voyant Tools and the data-intensive flow execution environment of Meandre. Our scrubber tool (PHP, CSS) accepts texts in multiple formats (.txt, .html, .docx) and handles preprocessing steps including stripping tags, removing stop words, and applying lemma lists. A second tool, diviText (ExtJS, PHP), accepts the output from scrubber, cuts texts into “chunks” in one of three ways (fixed size chunks, a specific number of chunks, and/or by manually selecting locations between words for chunk breaks), computes word counts within each chunk, and allows users to merge chunks. The latter functionality has proved valuable for generating “virtual manuscripts”, that is, joining sections from different manuscripts. A third tool, treeView (PHP, R) accepts output from diviText, performs a number of variants of hierarchical cluster analysis, and returns a dendrogram plot in .pdf or phyloXML format.

Based on feedback from scholars who are using our tools, the website now provides video and written tutorials to help new users get started. These tutorials have been especially valuable for introducing these tools to our undergraduates. In the spirit of evangelizing, our website offers a series of “best practices” videos, discussions and step-by-step diagrams that shred insight to the process of how textual analysis at this level of detail can lead to rich new questions. The instructional videos include “The Story of Daniel”, a discussion of one of our initial successes when using the tools where we showed that lexomic methods can accurately characterize the structure and relationships of texts that are already known, for example, identifying Genesis B within the Old English Genesis and the section of Daniel that is paralleled in Azarias (Drout, et al. 2011). Other videos include: “How to Read a Dendrogram”, “How to Create a Dendrogram”, “How to Read a Ribbon Diagram”, “Lexomics for Comparison”, and “Lexomics for Source Detection”. A much longer video, “Editions and Manuscripts,” addresses the challenges of choosing between different kinds of editions that may exist for a text that is found in multiple forms.

We have made what we think are significant discoveries in a number of spaces, including Beowulf, the poems of Cynewulf, Anglo-Saxon prose, a few Old Norse sagas, and Modern English texts including the Harlem Renaissance play Mule Bone (by Zora Neale Hurston and Langston Hughes). Lexomics is both an excellent first step to augment traditional scholarship as well as a rich source of deep analysis.

For example, previous lexomic analysis of several Old English poems suggests that there is a connection between dendrograms with an isolated, single leaf and poems that have an external source for one subsection of the poem different from the source or sources of the main body of the poem. We ﬁnd in the dendrogram of Daniel a single-leaf clade corresponding to lines 299–455 of the poem. This section includes parts of Daniel that have external Latin sources that are different from the source of the rest of the poem (the Latin Bible). Similarly, in the Anglo-Saxon poem Christ III, a single-leaf clade that represents lines 1350–1510 has its source in Sermon 57 of Cæsarius of Arles (lines 1379–1498), and a single-leaf clade in Genesis A (lines 1079–1256) is associated with the genealogical lists from Adam to Noah that give the lineages of both Cain and Seth (lines 1055–1252), material that, for at least some of its content, must have a source different from the biblical text. These relationships were already known to scholars, but our investigation of the Old English poem Guthlac A resolved a century-long critical controversy by demonstrating that a key section of this poem (when demons drag Guthlac to the mouth of hell) has a different proximate source than the rest of the poem and that Guthlac A therefore must have been composed after a separately circulating text similar in content to Vercelli Homily 23 (Downey et al., 2012).

The toolset, instructional materials, and publications are obvious deliverables from our efforts. Yet, we submit that our collaborative experiences with faculty and undergraduate students are even more exciting and provide a significant use-case of how scholarship in the humanities is evolving from the stereotypical solitary scholar to a paradigm of community, collaboration, and experimentation (cf. Unsworth, 1997). In our recent NEH- and locally-funded summer experience, humanities faculty in particular were pleasantly surprised with the intellectual environment that emerged. We got a glimpse of what it must have been like to work at a place like Bell Labs when they were making daily discoveries. This kind of collaborative, fast-moving research is unfortunately largely unknown in the humanities.

So how to continue our own momentum as well as replicate a spirit of experimentation for others? Earhart (2010) rightly notes that “digital projects remain rare, often the product of tenacious participants rather than a supportive academic environment”(emphasis added). We submit that faculty (not administrators nor technologists in the library) are the prime drivers and change must begin with our syllabi. Robust working relationships in the lab are strongest after students have already applied new modes of thinking in the classroom; for example, the importance of exposing undergraduate humanities students to computational thinking: problem decomposition, algorithmic thinking, and the success and failures of experimentation. And we need not overplay the lab metaphor. Our image of the digital humanities lab need not include beakers and soapstone benches, rather, the “new lab” is a room filled with scholars from multiple disciplines and a whiteboard.

Even if we had discovered nothing during our past summers in the lab, the intellectual thrill of the research group would have been a major accomplishment that these students (and we faculty) will never forget. But in fact we made discoveries, so many that there were days when participating faculty got none of their own work done because we were so busy bouncing from student to student seeing what they had found. Most critically, the experience continues to shape the way we share our disciplines with new cohorts of students. The solitary scholar still has a role to be sure, but that is no longer sufficient for the multidisciplinary demands and rewards to be gained from collaborations in the digital humanities: in our teaching, to our research, and back again.

lexomics — The term was originally coined by Betsey Dexter Dyer and ﬁrst appeared in Genome Technology (2002). Since then ‘‘lexomics’’ has appeared on the internet and in some publications without attribution. Some of these appearances could be independent inventions of the term.

References
Downey, S., M. Drout, M. Kahn, and M. D. LeBlanc (2012). 'Books Tell Us': Lexomic and Traditional Evidence for the Sources of Guthlac A. Modern Philology 110: 1-29.
Drout, M., M. Kahn, M. D. LeBlanc, and C. Nelson (2011). Of Dendrogrammatology: Lexomic Methods for Analyzing Relationships among Old English Poems, Journal of English and Germanic Philology 110: 301–36.
Drout, M., M. D. LeBlanc, and M. Kahn (2011-2013). “Lexomic Tools and Methods for Textual Analysis: Providing Deep Access to Digitized Texts.” National Endowment for the Humanities–NEH PR-50112011.
Earhart, A. (2010). Challenging Gaps: Redesigning Collaboration in the Digital Humanities. In Earhart and Jewell (eds),The American Literature Scholar in the Digital Age Ann Arbor: University of Michigan Press. http://hdl.handle.net/2027/spo.9362034.0001.001.
Genome Technology (2002). "In the News." in Genome Technology 1(27), November 1, 2002.
LeBlanc, M. D., M. Gousie, and T. Armstrong (2010). Connecting Across Campus. 'Proceedings of the 41st SIGCSE Technical Symposium on Computer Science Education' held in Milwaukee, WI.
LeBlanc, M. D., M. Drout, and M. Kahn (2008-2010). “Pattern Recognition through Computational Stylistics: Old English and Beyond.” National Endowment for the Humanities–NEH HD-50300-08.
Lexomics Research Group. http://lexomics.wheatoncollege.edu
Meandre http://seasr.org/meandre/
Unsworth, J. (1997). Documenting the Reinvention of Text: The Importance of Failure. Journal of Electronic Publishing, 3:2. http://dx.doi.org/10.3998/3336451.0003.201.
Voyant Tools. http://voyant-tools.org/
GAMS: A Fedora Commons instance

Since 2003 the Centre for Information Modeling - Austrian Centre for Digital Humanities at the University of Graz (Austria) provides an infrastructure for a variety of DH projects. After years of building insular solutions, the Centre introduced a powerful yet flexible new infrastructure, called GAMS (Geisteswissenschaftliches Asset Management System, AMS for the Humanities). It is based on the Fedora Commons architecture. Thus, the infrastructure inherits all features already provided by Fedora: full OAIS-compliance, strict separation of data and metadata, and predefined interfaces like OAI-PMH. A central advantage of the Fedora architecture is its object model: An asset consists of a primary source, some metadata and virtual representations derived from the primary source. The object is completely self-descriptive: It knows about all changes that have been made to it, its version history, datastreams and assigned context objects. Finally, it also knows about all possible representation forms. Each object contains all the necessary information to store, preserve, retrieve and view it.
Cirilo Client: Mass operations in Fedora made easy

Although Fedora is a powerful tool, front-end object management is not always easy, especially with regard to mass operations. The Centre has developed a tool for this use case, complementing Fedora’s built-in Admin Client. Cirilo is a java application developed for data curation and content preservation in Fedora-based repository systems. Content preservation and data curation in our sense include object management and creation, versioning, normalization and standards, and choice of data formats.
Cirilo makes use of Fedora’s management-API (API-M). It offers applications which are particularly prone to being used as tools for mass operations on Fedora repository objects, such as ingest or replacement processes: With Cirilo ingest processes can be performed from the file system, from an eXist database or an Excel spreadsheet. During the ingest metadata is automatically extracted from the source document and written to the newly created object (for instance in DC format).
  The client operates on a collection of predefined content models which can be used without further adjustments for standard workflow scenarios like the management of collections of TEI objects. The content models, which are based on the Fedora object model, are class definitions: On the one hand they define the (MIME-)type of the contained data streams, on the other hand they designate dissemination methods operating on these data streams. Every object in the repository is an instance of one of these class definitions. The advantage of this concept lies in the fact that very complex data sources and workflows can be handled easily.
Currently, the client offers various content models for specific purposes, special emphasis lies on the TEI model. The TEI ingest processes can be flexibly costumized: during ingest policies for the extraction of semantic information can be applied, referenced images can be uploaded simultaneously and ontology concepts can be resolved. A new content model currently in development creates the appropriate ontology objects, especially SKOS objects. A designated query object makes it possible to pose queries with parameters to the Mulgara triplestore. With the help of these ontology and query objects dynamic indices can be created. There is a container object for the creation of collections available, which makes it easy to organize your resources. Finally, there are some models optimized for specified primary sources like METS/MODS, HTML, PDF, BibTeX or external resources accessible via an URL. A content model for linguistic resources is in development (in cooperation with ICLTT, Vienna). Currently, we are testing how controlled vocabularies and thesauri (for instance geonames.org), can be sensibly integrated in the system.
The user can assign numerous virtual representations via the client. The METS/MODS object is designed to be viewed in the DFG-Viewer. TEI objects can be directly used as the input for the Voyant Tools or the Versioning Machine. The members of a context object can be projected on a map using Google Maps. Basically any web-based service can be integrated into the infrastructure. Of course, user- and project-specific stylesheets are often employed.
The Cirilo Client will be made available as an open source software project, including documentation, as a contribution of the Centre for Information Modeling - Austrian Centre for Digital Humanities to DARIAH-AT in 2014.
References

DARIAH-EU: www.dariah.eu [2013-10-28]
DFG-Viewer: dfg-viewer.de/ueber-das-projekt [2013-10-28]
Fedora Commons: www.fedora-commons.org [2013-10-28]
Google Maps: maps.google.at [2013-10-28]
Geisteswissenschaftliches Asset Management System, AMS for the Humanities: gams.uni-graz.at [2013-10-28]
Carl Lagoze, Sandy Payette, Edwin Shin, Chris Wilper, Fedora (2005). An Architecture for Complex Objects and their Relationships. arxiv.org/ftp/cs/papers/0501/0501012.pdf [2013-10-28]
Versioning Machine: v-machine.org [2013-10-28]
Voyant Tools: voyant-tools.org [2013-10-28]

1. Introduction

The English Scientific Text Corpus (SciTex) consists of about 5000 scientific papers with about 34 Mio tokens in two time slots, 1970/80s and 2000s 1, 2. It has been compiled to investigate the construal of scientific disciplinarity, in particular, how interdisciplinary contact disciplines emerge from their seed disciplines. Both time slots consist of nine disciplines: Computer Science (A) as one seed discipline, Linguistics (C1), Biology (C2), Mechanical Engineering (C3), Electrical Engineering (C4) as the other seed disciplines, and Computational Linguistics (B1), Bioinformatics (B2), Digital Construction (B3), and Microelectronics (B4) as the corresponding contact disciplines between A and C1-C4. The individual articles are subdivided into Abstract, Introduction, Main, and Conclusion.
The orthogonal dimensions time, discipline, and logical structure provide for many, potentially interesting setups of variational analysis: We can explore the diachronic evolution of contact disciplines in comparison to their seed disciplines, variation between contact disciplines and their seed disciplines, and genre variation between abstracts and text bodies in individual disciplines. In this paper we present an approach that combines a macroanalytic perspective 3 with the more traditional microanalytic perspective served by concordance search to explore variation along these dimensions.
2. Approach

2.1. Macroanalysis

For supporting explorative macroanalysis, we use well understood visualization techniques – heatmaps and wordclouds – and combine them with intuitive exploration paradigms – drill down and side by side comparison (see Figure 1). The heatmaps and wordclouds are interactive, allowing for a closer inspection at various levels. The leftmost heatmap visualizes the highest level contrast between abstracts and text bodies in the two time slots (1970s/80s and 2000s). The middle and right heatmaps serve for inspecting a chosen contrast at a lower level at the level of individual disciplines. A particular contrast can be chosen by clicking on the respective square, numbers indicating which contrast is displayed in the middle (Selection 1) and right heatmap (Selection 2). In this example, the middle heatmap visualizes the distances between abstracts and text bodies, and the right heatmap visualizes the distances between text bodies and abstracts.
The wordclouds underneath the heatmaps display the most typical words for a chosen contrast. In Figure 1 the wordcloud to the left visualizes the most typical words for abstracts as opposed to text bodies in the 2000s. Unlike in the common use of wordclouds, the size of words is proportional to their contribution to the distance (as defined in Section 2.2), whereas relative frequency is visualized by color, ranging from purple to red.

Fig. 1: Contrast between Abstracts and Text Bodies
Having a closer look at Figure 1, we can observe that the distance between abstracts is generally larger than the distance between text bodies, and that it has increased in the 30 years period. This general trend is mirrored in the individual disciplines (not shown here). Looking at the middle and right heatmaps, we can see that - not surprisingly - the distance between particular disciplines are at a minimum (squares forming the main diagonal), and the distances among the seed disciplines (A and C corpora), are generally larger than the distances among contract disciplines.
The corresponding wordclouds visualize the most typical words for abstracts (middle heatmap) and for text bodies (right heatmap) in the discipline B1 (Computational Linguistics). In this particular contrast, words typical for abstracts are clearly centered around constructions of exposition (we propose, describe, investigate), main topics of B1 (natural, language (generation), machine translation), words describing the methodology (method, statistical, computational, system) and function words (and, of, on). Words typical for text bodies are markedly different: they comprise B1's main entities of topic elaboration (tokens, nouns, object, vp, john, probability), references (see figure, table, section), conjunctions (when, since, because, if), auxiliary and modal verbs (be, is, was, were, do, will, would, may), and prominently, the determiner the. In summary, abstracts exhibit characteristics of an informationally dense text (e.g., omission of determiners) with topic oriented content. In contrast, text bodies are less dense (determiners, references, modality) and more elaborated.
Other contrastive pairs, such as the synchronic comparison between disciplines or the diachronic comparison of the two time slots, corroborate the results derived by means of computationally much more demanding techniques from machine learning [1], [2].
2.2. Corpus Representation and Distance Measures

The individual corpora are tokenized, and tokens are transformed to lower case. Stopwords are deliberately not excluded to inspect all levels of variation: style, lexico-grammar, and theme. On this basis, corpora are represented by means of unigram language models smoothed with Jelinek-Mercer smoothing, which is a linear interpolation between the relative frequency of a word in a subcorpus and its relative frequency in the entire corpus 4. The distance between two corpora P and Q is measured by relative entropy D, also known as Kullback-Leibler Divergence:
D(P||Q)>=Sum_w p(w)*log_2(p(w)/q(w))
Here p(w) is the probability of a word w in P, and q(w) is its probability in Q. Relative entropy thus measures the average amount of additional bits per word needed to encode words distributed according to P by using an encoding optimized for Q. Note that this measure is asymmetric, i.e., D(P||Q) != D(Q||P), and has its minimum at 0 for P = Q5.
The individual word weights are calculated by the pointwise Kullback-Leibler Divergence 6:
D_w(P||Q) = p(w)*log_2(p(w)/q(w))
For all words the statistical significance of a difference is calculated based on an unpaired Welch t-test on the observed word probabilities in the individual documents of a corpus. This is used for discarding words below a given level of significance (p-value). A more detailed comparison with other measures for comparing corpora 7 is beyond the scope of this paper and will appear in another venue.
2.3. Microanalysis

Wordclouds serve as a bridge between the big distance picture of macroanalysis and microanalyis. To this end, they are seamlessly integrated with the IMS Open Corpus Workbench (CQPWeb: http://cwb.sourceforge.net/index.php), which provides for an expressive corpus query language and several summarization tools, such as collocations and comparative word frequency lists. A click on a word sends a query to CQPWeb, which returns the word in the chosen context. For example, clicking on “do” in the right heatmap (B1 (Txt00) vs. B1 (Abs00)) generates the following query shown in Figure 2.

Fig. 2: Concordance for “do” in B1, text bodies, 2000s
This query returns a concordance for “do” in the 2000s slot of SciTex constrained to subcorpus B1 and to the divisions Introduction, Main, and Conclusion. Based on this list one can inspect the larger context of individual hits and get a ranked list of collocations to distinguish the uses of “do” as an auxiliary vs. main verb.
3. Related Work

The need for combining macroanalysis with microanalysis is well recognized in the DH community 8, 9, and there does exist a variety of frameworks with similar goals. Due to space restrictions, we can only give an exemplary selection; for a comprehensive overview see TAPoR 2.0 (http://tapor.ca/). The MONK workbench 10 allows to compare pairs of corpora using Dunning's log-likelihood ratio 11 for word weighting. Apart from the different distance measure, the main difference of our approach is that we combine the macro perspective of overall distance with the micro perspective of individual word weights to allow for an explorative analysis of variation. The Voyant Tools 12 provide a plethora of text visualizations, including word clouds, cooccurrences, and word trends based on frequencies. The focus of these tools, however, lies on summarizing and visualizing one text or corpus, rather than on exploring variation among corpora. Finally, the TXM platform 13 integrates the IMS Corpus Workbench with some macroanalysis R packages such as factorial correspondence analysis, contrastive word specificity, and cooccurrence analysis. While this integration certainly provides a broader set of analysis techniques, it is arguably more complicated to use than the system presented in this paper.
4. Summary and Future Work

We have presented a system that combines macroanalysis with microanalysis to explore language variation, and briefly illustrated its use for analyzing differences along the dimensions time, discipline, and genre in a corpus of scientific text. Future work will be devoted both to technical as well as methodological enhancements. A useful technical extension is the facility to interactively group subcorpora to larger units, maybe with the help of hierarchical clustering based on the distance matrix to form meaningful groups. More generally, the support for importing external corpora and exporting distance matrices and word weights for analysis with other tools is desirable – the presented system has been evaluated based on a number of corpora, but the underlying processing pipeline certainly needs to be generalized and improved. On the methodological side the main challenge lies in supporting a broader variety of feature sets beyond simple unigram language models. This includes latent language models such as topic models 14 and hidden markov models 15, but also enriched representations such as part-of-speech tagging, and other extensions of unigram models. Such richer feature sets allow to focus analysis by means of feature selection, but also bear new challenges in measuring and visualizing the contribution of features to a contrast at hand, and translating features into meaningful queries against the underlying corpus.
References

1. Elke Teich and Peter Fankhauser (2010). Exploring a Corpus of Scientific Texts using Data Mining. In S. Gries, S. Wulff, and M. Davies, editors, Corpus-linguistic applications: Current studies, new directions, pp. 233–247. Rodopi, Amsterdam and New York.
2. Stefania Degaetano-Ortlieb, Hannah Kermes, Ekaterina Lapshinova-Koltunski, and Elke Teich (2013). SciTex: A diachronic corpus for analyzing the development of scientific registers. In Paul Bennett, Martin Durrell, Silke Scheible, and Richard J. Whitt, editors, New Methods in Historical Corpus Linguistics, Corpus Linguistics and Interdisciplinary Perspectives on Language (CLIP), Volume 3, Narr, Tübingen.
3. Matthew L. Jockers (2013). Macroanalysis: Digital Methods & Literary History. University of Illinois Press, Urbana, Chicago, and Springfield.
4. Chengxiang Zhai and John Lafferty (2004). A study of smoothing methods for language models applied to information retrieval. ACM Transactions on Information Systems (TOIS), 22(2):179–214.
5. David J. C. MacKay (2002). Information Theory, Inference & Learning Algorithms. Cambridge University Press, New York, NY, USA.
6. Takashi Tomokiyo and Matthew Hurst (2003). A language model approach to keyphrase extraction. Proceedings of the ACL 2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment (MWE '03), Vol. 18, Association for Computational Linguistics, Stroudsburg, PA, USA, pp. 33–40. DOI=10.3115/1119282.1119287 dx.doi.org/10.3115/1119282.1119287
7. Adam Kilgarriff (2001). Comparing Corpora. International Journal of Corpus Linguistics, 6(1):97–133.
8. Michael Correll and Michael Gleicher (2012). What Shakespeare Taught Us About Text Visualization. IEEE Visualization Workshop Proceedings, 2nd Workshop on Interactive Visual Text Analytics: Task-Driven Analysis of Social Media Content, Seattle, Washington, USA, Oct 2012.
9. Matthew L. Jockers and Julia Flanders (2013). A Matter of Scale. Staged debate at the Boston Area Days of Digital Humanities Conference at Northeastern University, March 18, 2013. digitalcommons.unl.edu/englishfacpubs/106/
10. John Unsworth and Martin Mueller (2009). The MONK Project Final Report. Sep 2009. www.monkproject.org/MONKProjectFinalReport.pdf
11. Ted Dunning (1993). Accurate methods for the statistics of surprise and coincidence. Computational Linguistics 19(1):61–74.
12. Stéfan Sinclair, Geoffrey Rockwell and the Voyant Tools Team (2012). Voyant Tools (web application). http://voyant-tools.org/
13. Serge Heiden (2010). The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme. Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation, Institute for Digital Enhancement of Cognitive Development, Waseda University, Japan, Nov 2010, pp. 389-398.
14. David. M. Blei, Andrew Y. Ng, and Michael I. Jordan (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022.
15. Sharon Goldwater and Tom Griffiths (2007). A fully Bayesian approach to unsupervised part-of-speech tagging. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL'07). Association for Computational Linguistics, Prague, Czech Republic, June 2007, pp. 744–751. www.aclweb.org/anthology/P07-1094

Proposal
Culture, Liberal Arts, and Society Scholars (CLASS) is an undergraduate research and fellowship program in the digital humanities awarded to student scholars at Hamilton College’s Digital Humanities Initiative (DHi). Basic literacies for the digital age are critical skills sets for students entering the professional world in the twenty-first century. The Digital Humanities Initiative provides new opportunities for students in the humanities to become fully engaged citizens in this ongoing digital revolution.

CLASS is based on three-broad areas of scholarly inquiry and their intersection with new and emerging digital technologies: 1) Culture, 2) Liberal Arts, and 3) Society. CLASS provides a unique partnership between departments, programs, and units across the liberal arts and humanities at Hamilton in partnership with the College’s Career Center. It begins with course connections in our Cinema and New Media Studies (CNMS) program but then removes the confines of the semester to promote deep understanding of digital humanities research within a specific field of interest. In these experiences, students and their faculty advisor become part of a collaborative working team of experts in DHi.

CLASS provides students with skills training in digital literacies through intensive research and scholarship coupled with two unique internship experiences. In the summer between sophomore and junior years CLASS offers undergraduate students an intensive professional development experience and provides a comprehensive overview of work in their respective field of interest. In the summer immediately after their junior year students enter their second internship off campus leading to employment and/or graduate study as a result of the eighteen-month program. Assistance with job placement, in a professional field, based on their CLASS internship placement, and/or graduate studies occurs in their final year at Hamilton. 


Fig. 1: Fig. 1. CLASS Program 18-month Structure

The coursework for the program begins in the fall of their sophomore year with CNMS 120 or 125 (Fig 1). In the spring semester students can enroll in courses offered in the CNMS minor. Students enroll in either CNMS 200W/Introduction to Digital Humanities or CNMS300/Interdisciplinary Research Methods that provide experiences writing grant proposals in the digital humanities.

The goals for CLASS include:

Collaboration with potential faculty and/or staff mentors to define and develop an interdisciplinary project
Writing a research proposal for their projects.
DHi committee reviews the proposals and recommend possible award opportunities.
Students begin work over a 10-week period in the summer, mid-June to late August.
A two-week intensive training program takes place in June of the first summer. Students survey mature digital humanities projects, participate in discussions of digital humanities readings, interact with invited speakers brought into the program, and explore technologies related to their research project goals. During the academic year following the first summer, students work with their mentor between 4-6 hours a week on their collaborative research project. In the summer between junior and senior years, CLASS offers undergraduate students an intensive professional development experience and provides a comprehensive overview of work in new digital technologies.

CLASS will:

Develop understanding of digital humanities methods
Develop technological expertise for careers and digital scholarship
Through their participation in an undergraduate research project, students will be able to:

Develop a research question, problem, or design;
Apply basic principles and knowledge found in the literature related to the research question;
Develop a research proposal to address or resolve a specific research question or problem;
Apply and evaluate interdisciplinary methodologies throughout the project;
Collect, interpret, and critique data in order to resolve a research question or evaluate a design;
Utilize digital skills (TEI, digital collection development, media object creation, geospatial visualization, etc.) necessary for robust digital scholarship in the humanities
Communicate research findings through oral presentations and digital publications.
Students in research collaborations with Faculty and members of the DHi, develop deep understanding of a specific long-term research agenda. They are expected to conduct collaborative investigation of a specific aspect of the research that is of great interest to them and integrate digital humanities research methods in their process. Deliverables include public presentation and/or publication at milestones in this process.

Accomplishments:
Cohort 2011

Sarah Bither and Melissa Yang worked with Professor Kyoko Omori to develop an understanding of Benshi performance art for contemporary audiences. The outcome of this work is a website, http://courses.hamilton.edu/dhi-class-1/sarah with components that will ultimately be incorporated into Professor Omori’s Japanese Comparative Literature Archive. Bither continued her study of Japanese culture and language by going abroad to Japan in the spring and summer of 2012. Randall Telfer worked with Professor Thomas Wilson to explore Confucian rituals and connections to contemporary religious practices in China. The outcomes of this work were additional edits to two of Professor Wilson’s websites http://academics.hamilton.edu/asian_studies/home/asc_test/index.html and The Cult of Confucius website: http://academics.hamilton.edu/asian_studies/home/coc_test/index.html. Brynna Tomassone worked with Professor Angel David Nieves to explore cultural connections between South Africa and the United States during the time period leading up to the events in 1976 Soweto. The outcome of this work is a series of book chapters currently in forthcoming publications including The Heritage of Iconic Planned Communities: The Challenges of Change (University of Pennsylvania Press, 2014). Tomassone is now a Ph.D. student in Hispanic/Spanish Studies at Syracuse University. 

 

Cohort 2012

Maxwell Lopez (’14) mentored by Professor Nathan Goodale. Continuing aspects of research on the history and culture of the Sinixt Nation in British Columbia, Lopez proposed to work for the 2012 -2013 year creating, “an accurate three dimensional digital representation of the British Columbia site along with some models of artifacts excavated” using the Unity game engine. He believes the project will create a “new way for people to experience and interact with history” and have great capacity for connecting with research with the public. By the end of the two- week CLASS session in June 2012, Lopez had already made several models in Blender (a 3D modeling software) and site maps in Unity (a virtual world platform for models to reside). The following is a screenshot of his initial construction of a pit house in Blender. Please see the folder on CLASS 2012 for screenshots of his work to date and his complete proposal. 


Fig. 2: Figure 2. Sinixt Ritual Pit House model (created in Blender).

Lopez continued work with Professor Nathan Goodale in GIS mapping and archeological data collection for continued development of Sinixt Ritual Pit (Fig. 2) houses at an archeology Field School in British Columbia summer 2013. Lopez has presented aspects of his work with Professor Goodale at several forums on campus in Fall 2012 and also at the 2013 Re:Humanities symposium April, 5, 2013.

Continuing aspects of O’Neill’s development of Beloved Witness: Agha Shahid Ali Archive, Ujjwal Pradham (’14) will explore the use of text analysis tools and TEI in developing aspects of the Agha Shahid Ali Archive. Pradham is also interested in establishing a connection between the archive and current communities of interest in Kashmir. 

Pradham explored the uses of social software to make connections between contemporary Kashmir communities and the developing Beloved Witness archive. The following is a screen shot of the Voyant Tools (Fig. 3) text analysis Ujjwal did to compare theme words (home, waiting, never, Spring, Kashmir) in multiple manuscripts over the development of a poem written by Ali. 


Fig. 3: Figure 3. Voyant Tools.

Cohort 2013

Working with Religious Studies Professor Abhishek Amar on aspects of his Sacred Centers in India Project, students Kenneth Ratliff (’16) and Alex Gioia (’14), embarked on a study of Indian sacred centers -- Buddhist Bodhgaya and Hindu Gaya. The students expanded their understanding of the Indian sacred cities of Gaya and Bodhgaya. They assisted Professor Amar in organizing his research data for these two cities (images, videos, and GPS coordinates) into the metadata schema for his digital research archive. This work of organizing and processing the over 418 data objects from Gaya into survey forms conducive to further analysis is necessary for long term sustainability of the digital archive. It is also the first step in the creation of interactive models of important artifacts and their locations within these religious sites. Appendix “A” is an example of one of the individual data survey forms used in this project and is based on those used by the K.P. Jayaswal Research Institute, with whom Professor Amar collaborates. Ratliff and Gioia have already begun creating an interactive two-dimensional line map of the Vishnupada complex (Fig 4). 


Fig. 4: Figure 4. Interactive line map of Vishnupada Complex with Mahadeva site highlighted (created in Blender).

This interactive map will link to images of the sites (Fig. 5) and 3D models of the artifacts in situ (Fig. 6). They hope that these virtual 3D and geographically correct models will foster greater interest in these religious sites due to the accessibility and interactivity of the maps, photographs, models, and videos.

Ultimately, the plans are to place the models that they produce into an online viewing space, developed from a game engine (Unity), that will make the models easily viewable and web accessible to the public. 


Fig. 5: Figure 5. Image of Mahadeva site and Tablet artifact on far wall.


Fig. 6: Figure 6. Image of Mahadeva Tablet being modeled in Blender (Free Download at Blender.org).

Working with Patricia O'Neill on aspects of her Beloved Witness archive, Kerri Grimaldi (’16) examined the significance of Emily Dickinson’s poetry to Agha Shahid Ali, a poet from Kashmir, whose work is the focus of the archive. Grimaldi’s project traces the depth of Emily Dickinson’s influence in Shahid’s poem, “A Nostalgist’s Map of America,” by placing Shahid’s poem side-by-side with Dickinson’s “A Route of Evanescence” in four stages of analysis, each increasing in level of explication. By analyzing Shahid’s poem, it is possible to read Dickinson’s in a completely different light, while also witnessing the resonating power of her poetry. Grimaldi has started to create a website to present her analysis of the relationship between the work of Shahid and Dickinson. Ultimately, this website will show not only that Dickinson influenced Shahid’s work, but that his work responded to and interpreted hers, such that their works are in conversation with each other. Please review the current status of this project, including the descriptive first layer of the website, the storyboards exploring intertextuality in the second layer and third layers and a draft of Grimaldi’s own creative video interpretation of the two poems in conversation. This project was submitted in September 2013 to the Dickinson Electronic Archives 2.0: CALL FOR PROPOSALS for volume 3 -- Emily Dickinson’s Reading Culture to be published in 2014. Part One of Kerri's website can be found at http://dhinitiative.org/demos/grimaldi/ 

Challenges
Initial Challenges for DHi in developing CLASS included answering, how do we publish in the digital Humanities? Much of collaborative research includes the use of copyrighted material and/or faculty research that is still in early development. These characteristics of the work in CLASS required that we reconsider the amount and type of information (public or not) conveyed to illustrate the progress of the students. We achieved our goal of facilitating discussion with other scholars in the field by including experts from across disciplines in the two-week intensive training program and through the natural association of collaborators on the faculty research projects. CLASS scholars biographies and research descriptions are announced on the DHi website. CLASS scholars give project prospectus presentations and/or example research projects at the end of their two-week training program in the first summer. These presentations and examples are given to an invited audience for feedback on the projects. Ultimately, each student presents or publishes their work off-campus. Several students have presented at Re:Humanities. 

CLASS Program Summary and Future Plans
Students in research collaborations with Faculty and members of the DHi, have been successful in developing deep understanding of a specific long-term research agenda. DHi provides the immersive experiences and ongoing continuity with faculty research necessary to support this engagement. Most of our CLASS scholars have conducted collaborative investigation of a specific aspect of a long-term research agenda, determined their specific interests and contributions to that agenda, and publicly presented their scholarship “in progress” at Hamilton events and professional conferences.

Our future plans are to continue the CLASS program and to collaborate on its development with other liberal arts schools. Several schools have asked us about building similar models at their schools. This would require thinking more about scaling-up the program. One option we are considering is a form of “Summer Institute” for undergraduates in which we bring them together with their mentors and a larger DH community for a two-week program. 

Appendix A: Proof of Concept for a Sustainable Digital Humanities Faculty Collection Infrastructure in the Liberal Arts. 


DHi’s technology infrastructure and research support model is designed to be sustainable. That is, our approach will reduce the need for regular revamping of static faculty research web pages by creating infrastructure and processes that maintain research outcomes as “living” web presences accessible for faculty and student collaborative scholarship over time. To this end we researched best practices in digital collection development and preservation in collaboration with members of our library and decided to develop an institutional warehouse (repository) for digital collections (Fedora Commons). Fedora was chosen for its scalability and ability to be extremely flexible in the way objects can be accessed. Fedora has built-in flexibility to allow creation and maintenance of relationships among objects and across digital collections over time.

After researching open source collaborative tools to interface with collections in Fedora Commons we decided to make use of Islandora. Islandora can be used to create customized themes for faculty collections and projects. Our DHi Collection Development Team is working with the Islandora and Fedora Commons consultants (at Discovery Garden to create our digital scholarship infrastructure. By using experts to help with development we are making efficient use of the Mellon Grant to move this complex project forward. 

2013 Appendix B: KPJR Form Vishnupada Complex Mahadeva Temple 
DOCUMENTATION SHEET OF BUILD HERITAGE/SITE N.M.M.A., ARCHAEOLOGICAL SURVERY OF INDIA

COMPILED AT K.P. JAYASWAL RESEARCH INSTITUTE, PATNA

Sl.No.	 Documentation Parameters 	 
 	 State/Dist./Block 	Gaya
 1	 Name of the monument/built heritage/site	 Mahdava Temple
 2	 Date/Period	 Early Medieval/Medieval?
 3	 Location 	To east and north of 16 Vedis/Padas in Vishnupada Complex 
 	 Geo-coordinate	 
 4	 Approach	East of the Vishnupada Temple, in the Vishnupada Complex 
 	 Airport	Gaya 
 	 Railway Station	Gaya 
 	 Bus Stand	Gaya 
 5	 Topographical features	Slope of the Mundaprishta hill on the western bank of the Phalgu 
 6	 Brief History	 
Temple seems to have origins in early medieval or medieval period. The exact date of the construction of the temple is difficult to determine because of lack of historical sources. It has images and inscriptions, but they may have been moved. 

 7	Local tradition associated with building/structure/site	Gaya Shraddha, place of Pinda-dana as well as Darshana 
 8	 Architectural style	 Inner sanctum, which has a Linga, and there are 20 pillars, which constitute the Mandapa
 9	 Description of the building/structure/site	 
Mahadeva temple: Inner sanctum with a Shiva Linga and a twenty pillared Mandapa. Narasimha Temple (east of Mahadeva): Small rock temple Sarasvati Temple (east of Narasimha): Small rock temple Facing Narasimha are two single chamber shrines. All five are treated as one unit in the Vishnupada complex. 

 10	 Building/Structural material and other	Stone and brick; pillars are stone 
11 	Usage(s) 	Active worship, Darshana 
12 	Ownership 	Same as Vishnupada Main Temple 
13 	Protection status 	Good 
14 	Present condition 	Maintained 
15 	Conservation assessment 	Alright 
16 	Photographs 	See attached 
17 	Plan/elevation, if available 	 
18 	Published references 	 
19 	General Remarks 	There is an inscription 
20 	Name and address of compiler with date elements used 	Abhishek Singh Amar Matthew Sayers 
Images: 

Mahadeva Temple (129):

By opening

1. Vishnu, 16'' (122)

2. Camunda, 16'' (120, 121)

There's no logic to the temple - they have plastered images all over the place; normally, you would not see Bishnu and Camunda next to each other, but in this case we do, in a disorganized fashion. For instance, they are in the same niche, but Camunda is platered higher than Vishnu, which speaks to the disorganization of the collection process. 
South wall

3. Ganesha, 16'' (122)

4. Inscription (123-125)

Appears to be painted more recently. 
5. Eroded Uma-Maheshvaga, 14'' (126)

Inner sanctum (127)

6. Huge Shiva Linga (127)

North wall

7. Uma-Maheshvara, 16'' (128)

By opening, on the north, west-facing niche; left

8. Durga, 36'' (130)

9. New inscription (130)

Outside, left

10. Dasavatara (131)
Walt Whitman’s Leaves of Grass is one of the most famous and often-studied works of American literature. In the century since Van Wyck Brooks declared Whitman the originator of “the sense of something organic in American life”—the first to combine high art and rude experience—Whitman’s masterwork has been thoroughly digested into a series of critical truisms that gives even new readers of the poems a sense of familiarity. Whether we have his poems committed to memory or have never actually read one of them, we “all know” that Whitman eschewed traditionally poetic diction, that his is a poetry of inclusiveness, that the first edition of his text in 1855 is more daring, lively, and experimental than later editions, etc. 

Such axioms are comforting in the face of what is on many levels a difficult text (actually, a set of texts) to assimilate. Because Whitman applied the title “Leaves of Grass” to more than ten distinctly different volumes over the course of three and a half decades—not only adding poems but also retitling, cancelling, drastically revising, combining, and re-grouping existing ones—the goal of accurately tracing the book’s evolution has consistently frustrated scholars. Recognizing that “for the reader to understand how Leaves of Grass grew from edition to edition, some sense had to be made of these often bewildering textual permutations,” a group of late-twentieth century scholars labored for over a decade to produce a variorum edition, a tremendous accomplishment that has, unfortunately, done little to alleviate the bewilderment of permutations. 

The hope that digital technologies might offer a way, at last, to lucidly represent the various stages in the evolution of Leaves of Grass was one of the early motivations for the creators of the Whitman Archive in the late 1990s. We have often revisited the question of how to convey visually the information represented in the arcane coding of the 3-volume print variorum and inherent in the separate digitized editions. Nearly two decades later, however, we haven’t made much progress. 

Though they cannot provide the kind of detailed, objective understanding that might be conveyed by the schematic, interactive interfaces that we’ve sometimes (very hazily) imagined—ones that somehow collate whole texts, poems, lines, and phrases—we have begun to experiment with distant reading strategies that provide a different sort of view. So while collation tools do not cope well with the scope of transformation involved in Whitman’s reworking the first edition’s 10,000-word prose preface into the 4,000-word poem “By Blue Ontario’s Shore,” text analysis tools such as Voyant offer a number of potentially enlightening prospects on the two works and their relationship. 

Likewise, such tools can begin to offer ways to assay and quantify some of the critical commonplaces that have grown up around Leaves of Grass: Is Whitman’s diction, in fact, innovative and what makes it so? How do Whitman’s early poems compare to his later poems? What basis might be found for claims that Whitman is the great poet of America, women, the body, male homoeroticism, or democracy?

At the University of Nebraska–Lincoln's Center for Digital Research in the Humanities we have been experimenting with a new way of visualizing phenomena in TEI corpora and have created Indigo, an experimental XSLT-based tool that queries TEI files and generates animated videos of the results. Using XPath and XQuery techniques, this tool makes it possible to ask specific or general questions of a corpus. The data are then output as scalable vector graphic (SVG) files that are converted to raster images and rendered in high definition H.264 video at 30 frames per second. At its core, Indigo is a program for performing scripted stop-motion animation, arranged in one or more scenes. What each scene contains is up to the user: it might include letters, numbers, shapes, colors, gradients, patterns, lines, paths, or imported raster images, each moving or not moving. The only requirement is that a scene must be modeled in XSLT, with SVG structures as the initial output. For the user wishing to visualize aspects of TEI text corpora, the news is good, for that format shares membership with XSLT and SVG in the XML ecosystem. Indigo provides a method for presenting, in fresh and unexpected ways, quantitative data relevant to scholarly questions in a way that is open-ended, making the user a co-creator with Whitman in the “meaning” of his texts. 

Our experiment involves such activities as creating quantitive analyses of some of the linguistic characteristics of Whitman's poetic corpus, comparing them to those of some of his popular contemporaries, and then "presenting" the results as a video sequence. Such a procedure is admittedly outside the mainstream of critical methodology in the humanities, but it is entirely in keeping with Whitman’s own theories of the proper relationships among authors, readers, and texts. “The process of reading,” he said, “is not a half-sleep, but, in highest sense, an exercise, a gymnast’s struggle; . . . the reader is to do something for himself, . . . must himself or herself construct indeed the poem, argument, history, metaphysical essay—the text furnishing the hints, the clue, the start or frame-work.”1

As Tanya Clement has recently observed, "sometimes the view facilitated by digital tools generates the same data human beings . . . could generate by hand, but more quickly," and sometimes "these vantage points are remarkably different . . . and provide us with a new perspective on texts."2 And as Dana Solomon has written, "due in large part to its often powerful and aesthetically pleasing visual impact, relatively quick learning curve … and overall 'cool,' the practice of visualizing textual data has been widely adopted by the digital humanities."3

In representing the literary work as an absorbing performance, one that comprises both "data" and "art," the method we are presenting is calculated to provoke responses in both informational and aesthetic registers. It is, in the terms of Jerome McGann and Lisa Samuels, an act of “interpretive deformance,” whereby “we are brought to a critical position in which we can imagine things about the text that we didn’t and perhaps couldn’t otherwise know.”4

References
1. Whitman, Walt (1892). Democratic Vistas in Complete Prose Works, (Philadelphia: David McKay), p. 257.

2. Clement, T (2013). Text Analysis, Data Mining, and Visualizations in Literary Scholarship in Literary Studies in the Digital Age: An Evolving Anthology (eds., Kenneth M. Price, Ray Siemens). Modern Language Association.

3. Solomon, D. (2013). Building the Infrastructural Layer: Reading Data Visualization in the Digital Humanities. MLA 2013 Conference Presentation. url: danaryansolomon.wordpress.com/2013/01/08/mla-2013-conference-presentation-from-sunday-162013/

4. McGann, Jerome and Lisa Samuels.Deformance and Interpretation url: www2.iath.virginia.edu/jjm2f/old/deform.html
In this paper, we theorize about the role of the curator – or perhaps it would be more accurate to say “custodian” or even “collection designer” – in preparing electronic texts for use in an online digital environment. This scholarly role is becoming increasingly important to new forms of knowledge dissemination as a result of the growth in aggregating or mashing up existing digital content. The goal of these aggregations is to add value for particular purposes, as seen in initiatives such as the Journal of Digital Humanities, which aims to collect already published materials into quarterly thematic collections, and the related website Digital Humanities Now, which curates weekly the feeds of other digital humanities websites (Digital Humanities Now).

By analogy with the definition of a curator as “The officer in charge of a museum, gallery of art, library, or the like; a keeper, custodian” (“curator,” def. n. 6), the digital scholarly curator performs a similar role with respect to the circulation of digital content, though with a number of significant differences. Gallery or museum curators, for instance, have their own scholarly and professional training and preparation, often with respect to proper handling and display of fine art and other valuable physical objects. There are those who feel that at least some curators should be considered artists themselves (e.g. Ventzislavov). There is also widespread acknowledgment that curators who work with digital materials require a different set of competencies. Melody Madrid, for example, reports a study that resulted in a list of 20, divided into the categories operational and managerial. Digital scholarly curation also harnesses social media technologies (e.g. crowdsourcing, folksonomies) to encourage a new level of user participation in the management and preservation of digital content (Poole). Not quite an editor, but acting as a mediator between the producers of these contents and its audience through such activities as selecting and reframing them, the digital scholarly curator is in a rather unique position. Our discussion considers this role in relation to the Dynamic Table of Contexts (DToC) interface, a generalized tool for the dissemination of digital text that enables the designer of the collection to mediate specifically between the XML encoding of the text and the affordances that such encoding provides in the reading interface (Brown et al.; Dobson et al.).

The Dynamic Table of Contexts (Fig 1) is a joint initiative between the Interface Design research team of the Implementing New Knowledge Environments (INKE) project, the Canadian Writing Research Collaboratory (CWRC), the University of Alberta Press, and the Voyant Tools project. The DToC currently leverages four principal components of a digital text: the actual text of the document, a table of contents, an index, and XML encoding of the document. The goal of the prototype is to provide an online reading environment where the table of contents provides a conventional overview of a book while at the same time incorporating the index terms and XML tags and the text to which they point (Ruecker et al.). The terms and tags can be selected and deselected, providing interactivity between these three means for accessing and navigating the content of the text or collection.


Fig. 1: Dynamic Table of Contexts Interface

The ability to leverage XML markup as part of the navigational interface is a distinguishing feature of the DToC. Customization of that affordance is enabled by what we call the DToC’s “curator mode,” which is distinct from the reading mode in that it allows technically adept superusers to create customized tag lists to serve as navigational aids alongside the index terms, as well as to determine the organization of the table of contents (login is not required, the curated view is expressed through a unique URL). In a print edition, and in particular for anthologies, it is necessary for the editor to decide which of the various alternatives will be used to organize a particular table of contents. For example, a collection of essays might be organized

alphabetically by title
alphabetically by author’s last name
chronologically
by theme
or by some other principle, in order to ensure a certain kind of development or coherence from beginning to end.
A collection of poems, for example, might add organization alphabetically by the first line of the poem, for cases where the poems do not have a title, and other arrangements are also possible, for instance by geographic location, language, or genre. In the case of an instructor preparing a course pack, the arrangement would naturally correspond to the sequence in which the materials will be used in the class. In the history of print, the possibilities for multiple representation of contents were limited.

In addition to the selection and organization of the contents themselves, curators of DToC collections need to make choices with respect to how the encoding works in the interface. Given the more generic and multi-purpose nature of XML encoding, particularly its use to structure a text, curators need to select which tags the DToC interface will display to the reader, and what user-friendly labels to use for those tags. For many purposes (although not all), the structural encoding can be set aside in favor of semantic encoding (when present). For choices among these tags, there is probably some golden mean that’s most appropriate for generic use; the primary benefit of the Dynamic Table of Contexts is to allow variants for more specific uses. Tags that have been rarely used may already be covered by the index, or may be too insignificant to take room on the list. On the other hand, in cases where the tags have been used heavily enough, it may not be useful to include them since it would result in too great a density of hits.

There are also large differences between what kind of curation of tags is required – depending on whether a schema or tagset has instead been applied throughout a collection, such as in the Brown Women Writers Project’s use of the Text Encoding Initiative – as opposed to highly customized versions of the TEI adapted to the needs of a particular text. Finally, the curator is also enabled to label how tags will appear in the interface, so that readers are not asked to decipher cryptic forms such as <biblStruct>, but are instead presented with labels for such tags that are meaningful in the context for which the curated text or collection is being prepared. This functionality is useful in cases where there is a nuanced difference between two similar tags that needs to be conveyed to the readers (Fig 2).


Fig. 2: Dynamic Table of Contents Curator Interface

What this means in terms of the training and qualifications of the DToC curator is that the person needs to hypothesize how the target readers will be dealing with the material, and to use the curatorial functions to customize the view of the text in the DToC interface to meet the anticipated use case(s). In the case of class instructors, to a certain extent the job will be simplified since the course pack has been chosen purposively for the class. In other situations, there may be multiple and possibly conflicting anticipated use cases. The curator also needs to be comfortable enough with XML not only to choose appropriate tags and rename them, but also, if needed, to specify XPath queries to locations in the document that cannot be identified through tag names alone. There will be considerable variation in XML expertise amongst curators, and a previous user study of ours on an earlier version of DToC indicated that a closer relationship to the encoding correlated with more positive experiences of the DToC interface (Dobson et al.).

The paper will frame our understanding of digital scholarly curation in relation to more traditional, historical understandings of curation and representation of contents and will demonstrate the curator mode in the DToC interface. Our previous user studies of the interface found both considerable confusion on the part of users with respect to the role of the encoding or tagging in the DToC (Brown et al.), and, among those who understood it, considerable emphasis on the importance of the XML markup and its ability to shape the reader experience in the interface. As one user said in mousing over the XML Markup pane: “Well, this seems to me the most relevant section, so whoever puts that together is pretty much the wizard in this Oz” (Dobson et al.). Our discussion will incorporate results of the next user study we are conducting on the DToC, with a stress on the curator mode, that will probe these findings which go to the heart of the DToC’s affordances. Our aim will be to gain a fuller understanding of the ways in which users with a range of technical knowledge understand the role of markup in the DToC interface, and their understanding as both readers and curators of the curatorial role. This study will inform our understanding not only of the ways in which reading environments such as the DToC can effectively leverage XML encoding, but more generally of the ways in which the idea of curation is rapidly evolving within the online scholarly environment.

References
Brown, Susan, Brent Nelson, Stan Ruecker, Stéfan Sinclair, Nadine Adelaar, Ruth Knechtel, Jennifer Windsor and the INKE Research Group. “Text Encoding, the Index, and the Dynamic Table of Contexts.” Paper presented at the annual Digital Humanities conference (DH2013), Lincoln, Nebraska. July 16-19, 2013

“Curator.” Def. n. 5. The Oxford English Dictionary. 2013. OED Online. Web. 1 Nov. 2013.

Digital Humanities Now. http://digitalhumanitiesnow.org/

Dobson, Teresa, Brooke Heller, Stan Ruecker, Milena Radzikowska, Mark Bieber, Susan Brown, and the INKE Research Group. “The Dynamic Table of Contexts: User Experience and Future Directions.” Paper presented in the panel “Designing Interactive Reading Environments for the Online Scholarly Edition” at the DH2012 conference, Hamburg, Germany. July 16-20, 2012.

Kholeif, O. The Curator’s New Medium. Art Monthly. February 2013, (363):9-12. Ipswich, MA.

Little, G. Thinking Like Curators. Journal of Academic Librarianship, 2013, 39(2), 123-125. doi:10.1016/j.acalib.2013.01.003

Madrid, Melody M. A study of digital curator competences: A survey of experts. The International Information & Library Review, Volume 45, Issues 3–4, December 2013, pp 149-156.

Poole, Alex H. “Now is the Future? The Urgency of Digital Curation in the Digital Humanities.” Digital Humanities Quarterly 7:2 (2013).

Ruecker, Stan and the INKE Research Group. “Introducing the Dynamic Table of Contexts for Scholarly Editions.” Paper presented at the Modern Language Association (MLA) Conference. Los Angeles, CA. Jan 6-8, 2011.

Ruecker, Stan, Susan Brown, Milena Radzikowska, Stéfan Sinclair, Thomas M. Nelson, Patricia Clements, Isobel Grundy, Sharon Balasz, and Jeff Antoniuk. “The Table of Contexts: A Dynamic Browsing Tool for Digitally Encoded Texts.” In The Charm of a List: From the Sumerians to Computerised Data Processing. Ed. Lucie Dolezalova. Cambridge: Cambridge Scholars Publishing, 2009. pp. 177-187.

Ventzislavov, R. (2014), Idle Arts: Reconsidering the Curator. The Journal of Aesthetics and Art Criticism, 72: 83–93. doi: 10.1111/jaac.12058
>Contexts for Humanities Visualization
Literary scholars are increasingly turning to graphical display of humanistic information as a way to encounter texts in new and engaging ways. Digitally facilitated humanities visualization presents literary critics with opportunities for new insight1, while also opening practitioners to charges of wholesale importation of simplistic scientific methodologies.2 This poster session outlines the rationale, method, and significance of a focused humanities visualization project in order to demonstrate how new techniques of visualization may be undertaken with literary texts to produce new and speculative “aesthetic provocations” in literary studies.3

Materials and Scope
In order to model and theorize how such a project might develop, I am producing a network visualization of the sixteenth-century play Ralph Roister Doister by Nicholas Udall. Using the open-source "interactive visualization and exploration" platform Gephi, I map the relationships between characters in the play based upon dialogue. In other words, this project structurally maps dialogue between characters, producing a network visualization that productively reconfigures the play to provoke new analytical responses. Following the example of Franco Moretti’s work with Hamlet, these models prompt new insight into the “deep structures” of literary works. For Moretti, however, “the most important thing of all” about these reconfigurations is that they can be manipulated: “one can intervene on a model; make experiments.”4 [116, italics in original]. With a baseline visualization of dialogue established, I, following Jerome McGann, selectively intervene and “deform” the re-modeled text to continually reconfigure it. Such processes bring us “to a critical position in which we can imagine things about the texts that we didn’t and perhaps couldn’t otherwise know.”5 [116] Motivated by Drucker and Nowviskie’s call to “engage computing to produce new aesthetic provocations,” I use Roister Doister to understand how humanities visualization may reconfigure our approach to literary inquiry.6 

Methods of Production
In order to map the dialogic relationships between these characters in Gephi, I have created each character as a node in the network; these nodes are connected by directional dialogue originating at a particular node and terminating at another. Thus, the main characters Roister Doister and Merygreeke are nodes 1.0 and 2.0, for example. Within Gephi’s data manipulation environment, a line of dialogue from the former to the latter would be mapped visually as a line [or an “edge”] from node 1.0 to node 2.0. Each exchange is recorded as tabled data in Gephi, which is then used to produce visualizations. Visualizations can be produced for any discrete unit of the text, including scene, act, or the entirety of the play. This project produces visualizations of each of these divisions for comparative purposes. Following Moretti and McGann, I have undertaken selective deformations by, for example, removing various characters at different points, thereby revealing their network centrality. Criticism of Roister Doister has, for the most part, focused heavily on the play’s dramaturgical debt to the classical comedies of Terence and Plautus, the extent to which those formal structures were successfully integrated with “native” elements of English drama, and the play’s debts to the miles gloriosus [“braggart-soldier”] tradition of Classical comedy.78 This project is in part an attempt to revitalize an ossified critical conversation as an example of how new techniques can vigorously re-engage old texts. 

Significance of Anticipated Visualizations
Experimentation in visualization of textual works is a timely one. As can be seen from the growing use of the Voyant suite9 of analysis and visualization tools as well as the popular Mapping the Republic of Lettersproject,10 humanities visualization is a growing area of scholarly concern. Elijah Meeks has argued that “the shift from creating, annotating and analyzing archives to modeling systems can have a profound impact beyond the [admittedly high value of] usability of scholarly material developed during a digital humanities project.”11 [italics mine]. When humanities scholars have reached a certain point of visual literacy, we will begin to engage with such models in profoundly important ways. Indeed, these models may “provide a much more nuanced form of knowledge transmission than the raw datasets or interactive and dynamic applications typically presented as the future of digital scholarly media.”12 This project is an effort to explore how these new forms of knowledge transmission and analysis might impact literary inquiry. 

References
1. Manovich, Lev. (2010). What Is Visualization? Poetess Archive Journal 2(1), n. pag.

2. Drucker, Johanna. (2011). Humanities Approaches to Graphical Display. Digital Humanities Quarterly 5(1), n. page.

3. Drucker, Johanna, and Bethany Nowviskie. (2004). Speculative Computing: Aesthetic Provocations in Humanities Computing. Companion to Digital Humanities. In Schreibman, Susan, Siemens, Ray, & Unsworth, John (eds). Oxford, Blackwell. Accessed 03 April 2012. Available at [journals.tdl.org/paj/index.php/paj/article/view/19/58]

4,5. Moretti, Franco. (2011). Network Theory, Plot Analysis. Stanford, Stanford Literary Lab.

6. Drucker, Johanna, and Bethany Nowviskie. (2004). Speculative Computing: Aesthetic Provocations in Humanities Computing. Companion to Digital Humanities. In Schreibman, Susan, Siemens, Ray, & Unsworth, John (eds). Oxford, Blackwell. Accessed 03 April 2012. Available at [journals.tdl.org/paj/index.php/paj/article/view/19/58]

7. Boas, Frederick S. (1933). An Introduction to Tudor Drama. Oxford, Clarendon.

8. Brooke, C. F. Tucker. (1911). The Tudor Drama: A History of English National Drama to the Retirement of Shakespeare. Boston, Riverside.

9. Voyant Tools. (2012). [voyant-tools.org]. Accessed October 2012.

10. Mapping the Republic of Letters: Navigating Big Data from the Early Modern Period. (2012). Available at [republicofletters.stanford.edu].

11. Meeks, Elijah. (2011). More Networks in the Humanities or Did Books Have DNA? Digital Humanities Specialist. Published 6 December 2011. Accessed 17 April 2012.

12. Meeks, Elijah. (2011). More Networks in the Humanities or Did Books Have DNA? Digital Humanities Specialist. Published 6 December 2011. Accessed 17 April 2012.
“Humanists have always been explorers. They sail not on seas of water but on seas of color, sound, and, most especially, words.” — John B. Smith, 1984

Why is big data such a big deal? Modern digital communications are producing and recording data at a feverish rate, with 90% of the world’s recorded data, most of it unstructured, having been produced in just the last two years (Dragland, 2013). In addition, more traditional texts are being digitized all the time, and Crane (2006) tells us that while the largest academic digital libraries now hold tens of thousands of books, a completed Google Library will likely have more than ten million. To further Smith’s analogy, we are adrift in a sea of data, and we are at risk of floundering.

Front page news events like Edward Snowden’s May 2013 revelations disclosing the secret NSA (America’s National Security Agency) collection and analysis of massive amounts of ostensibly private data both domestically and internationally, are making big data analytics part of the public lexicon.  Major corporations, led by IBM with $1.3 billion in big data revenue in 2012, are scrambling to adopt practices that will allow them to capitalize on the volume of information now being generated.  Global big data related revenues in 2012 were $11.6 billion, and are projected to break $18 billion in 2013 (Kelly et al.).

While one might shudder to think what the NSA will do with all that information, outside of the world of politics and espionage, what can researchers and academics do with the volume of information now, or at least soon to be at our disposal? Matthew Jockers asks, “How do we mine them [texts] to find something we don’t already know?” (University of Nebraska-Lincoln) To utilize such massive corpora and avoid succumbing to the dilemma of what McCormick et al (1987) refer to as “information without interpretation” we need to find the most effective ways for end-users to explore, visualize and interact with big data such that it is both meaningful and understandable, if possible by both the trained and untrained eye. 

Big data analytics and computer-supported visualization offer ways to read collections as our cognitive abilities are stretched to their limit with the sheer volume of data available (Araya, 2003). However, as Franco Moretti has pointed out, what we are reading when we use text mining methods and visualizations are really models of the collections. (Moretti 2013, p. 157) It is important therefore to survey the visual models emerging and question if they can better be designed to suit humanities exploration. In this paper we therefore propose to look at visualization for text mining in the following ways:

Survey text mining visualization in the humanities. Who is using text mining and why? What kinds of visualizations do they find compelling and why?
Identify and Combine commonly presented visualizations for modeling. What visual models could be used for the exploration of large corpora? How could they be combined?  
Model interactive prototypes of different combinations of visualizations for exploration. 
Surveying: In a poster at DH 2013 we presented a framework of text mining tools that are useful in the humanities. Now we will survey the variety of visualizations used to present mining results. We will begin with early discussions of visualization like Smith’s “Computer Criticism” (Style 1978, p. 326), where he notes with agreement Paul de Man’s observation that as late as 1973 there had been no evolution beyond the close reading “techniques of description and interpretation” being used by literary critics since the 1930s or 40s, and suggests “pictorial representation” as one of the potential uses of computer aided text analysis. Brunet in a 1989 article talks about exploiting large corpora and provides a number of examples of visualizations. Our survey will examine advances in practice and understanding in the intervening thirty plus years, leading to contemporary works like Franco Moretti’s Graphs, Maps, and Trees, which, as the title suggests, introduces visual models for literary exploration.

Our survey pays particular attention to recent text mining projects and tools including “The Proceedings of the Old Bailey”, David L. Hoover’s work with cluster analyses at NYU using “MiniTab”, Matt Jockers’ topic modeling work in “Macroanalysis”, the University of Waikato’s “WEKA”, the open-source visualization tool “Gephi”, the UMass machine learning tool “MALLET”, the research tools for textual study reviewed on “TAPoR”as well as some of our own INKE related projects such as “Dynamic Table of Contents”, “CiteLens”, “TextTiles” and “dialR”. 

Identifying and Combining: While we recognize that output will assume a variety of formats including for instance heat maps, topographic plots or scatter plots, our survey suggests that text-mining projects commonly use five principal types of visualization:

1. Dendrograms, showing data clustering within sets
2. Histograms, showing change over time
3. Networkdiagrams, showing how entities are connected within a network
4. Wordclouds, representing topics of words
5. Scatterplots, showing words or parts in an abstract space
Visualizations that are presented in print are typically Spartan, focusing the attention on the results through careful design. All affordances are removed. The same types of visualizations automatically generated from large data sets, however, tend to be too dense to be useful and have to include affordances if meant to be interactive. Simple representative visualizations, like histograms for instance, are insufficient to display complex interrelationships. Dendrograms, especially if you are working with massive data sets, quickly become an illegible mass of inter-connectivity. Word clouds are good at showing the relative frequency of words in a text or topic, but not at comparing one text or topic to another. Network diagrams produce some beautiful results, but suffer from the same difficulties as dendrograms; large data sets quickly lead to illegibility.

“By visualizing information, we turn it into a landscape that you can explore with your eyes, a sort of information map. And when you’re lost in information, an information map is kind of useful.” — David McCandless, 2010

Modeling: Our research now is looking at ways to increase the exploratory power of visualization of large data sets. Used in combination, visualizations can provide otherwise elusive insight and clarity. They can also provide affordances for each other – a histogram can be used to explore a dendrogram. We have developed interactive prototypes using combinations of visualizations, and focusing on not simply allowing, but even encouraging the user to truly explore the (big) data, “function[ing] almost instinctively”, as McCullough (1996) stated, “to serve the process of development”. The more we can encourage users to explore and play with the data, the more likely they are to develop useful insights.


Fig. 1: Combination of a modified dendrogram showing clustering, and a diachronic timeline of 500 philosophy papers.

Figure 1 shows a prototype of a combination of dendrogram and histogram that we developed to visualize the clustering of 500 papers published between 1966 and 2004. The prototype is a combination of a modified dendrogram showing the clusters, and a diachronic visualization displaying the subjects over time. The displayed data is intuitively explorable, and the modified dendrogram is designed to encourage exploration. We developed the R code to prepare the data for interactivity. In Voyant we have developed skins that combine scatter plots and histograms, word clouds and histograms, and network diagrams with other tools. Again, the tools are open for others to recombine.

To conclude, surveying commonly used graphical representations allowed us to identify commonly used visualizations that humanists find useful. To scale these so that they can be used interactively to explore large data sets we have prototyped combinations, where one visualization can be used to explore another and vice versa. The goal is visualizations that help researchers make sense of big data; visualizations that let us explore the forest, not just the trees so that we can draw accurate and appropriate inferences from the data.

References
The Proceedings of the Old Bailey - http://www.oldbaileyonline.org/

Cluster Analysis, Principal Components Analysis (PCA), and T-testing in Minitab - https://files.nyu.edu/dh3/public/ClusterAnalysis-PCA-T-testingInMinitab.html

Jockers, M. Macroanalysis.

Weka 3: Data Mining Software in Java - http://www.cs.waikato.ac.nz/ml/weka/

Gephi - The Open Graph Viz Platform - https://gephi.org/

Mallet - MAchine Learning for LanguagE Toolkit - http://mallet.cs.umass.edu/

TAPoR - Research Tools for Textual Study - http://tapor.ca/

Dynamic Table of Contents - http://www.ualbertaprojects.info/dyntoc/dyntoc_v3_5/Main.html

CiteLens - http://labs.fluxo.art.br/CiteLens/

TextTiles - http://dev.giacometti.me/textTiles/trunk/

dialR - http://research.artsrn.ualberta.ca/~dialr/drMain.html

Allison, S., Heuser, R., Jockers, M., Moretti, F. and Witmore, M. (2012 ). Quantitative Formalism: An Experiment.Pamphlet 1, Stanford Literary Lab. Print.

Araya, A. A. (2003). The Hidden Side of Visualization. Techné: Research in Philosophy and Technology; Vol 7, No 2, Print.

Brunet, É. (1989). L'Exploitation des Grands Corpus: Le Bestiare de la Littérature Française. Literary and Linguistic Computing, Vol. 4, No. 2, p. 121-134. Print.

Crane, G. (2006). What Do You Do with a Million Books? D-Lib MagazineVol. 12 No. 3, Print.

Dragland, Å. (2013). Big Data, for better or worse. SINTEF.no. 22 May 2013. Web. 27 Oct. 2013.

Jockers, M. (2013). Macroanalysis : digital methods and literary history - University of Illinois Press, Urbana, Chicago & Springfield.

Kelly, J., Floyer, D., Vellante, D. and Miniman, S. (2013). Big Data Vendor Revenue and Market Forecast 2012-2017. Wikibon.org. 19 Feb. 2013. Web. 28 Oct. 2013.

McCandless, D. (2010). David McCandless: The beauty of data visualization. TED Talks. Web Video.  25 Oct.  2013. 

McCormick, Bruce H., DeFanti, Thomas A., and Brown, Maxine D. (1987). Visualization in Scientific Computing. Computer Graphics 21, 6 (November). New York: Association for Computing Machinery, SIGGRAPH, Print.

McCullough, M. (1996).  Abstracting Craft: The Practiced Digital Hand; Cambridge, MIT Press, Print.

Moretti, F. (2013). The End of the Beginning: A Reply to Christopher Prendergast.Distant Reading. London: Verso, p. 137-158. Print.

Risen, J. and Poitras, L. (2013). NSA Gathers Data on Social Connections of U.S. Citizens, New York Times, 28 Sep. 2013. Web, 27 Oct, 2013.

Simpson J.; Rockwell, G.; Sinclair, S.; Uszkalo, K.; Brown, S.; Dyrbye, A.; Chartier, R.. (2013). Framework for Testing Text Analysis and Mining Tools.Poster presented at the Digital Humanities 2013 conference at the University of Nebraska-Lincoln. Lincoln, Nebraska, USA.

Smith, J. (1978). Computer Criticism.Style. Vol XII, No 4. Print.

Smith, J. (1984). A New Environment For Literary Analysis.Perspectives in Computing 4. 2/3, (1984): 20-31. Print.

Tufte, Edward. (1983). The Visual Display of Quantitative Information; Cheshire, CT: Graphics Press, Print.

University of Nebraska-Lincoln. (2012). By text-mining the classics, UNL prof unearths new literary insights. UNL News Blog. 23 Aug. 2012. Web. 27 Oct. 2013.
How have text analysis tools in the humanities been imagined in the past? What did humanities computing developers think they were addressing with now dated technologies like punch cards, printed concordances and verbose command languages? Whether the analytic functionality is at the surface, as with Voyant Tools, or embedded at deeper levels, as with the Lucene-powered searching and browsing capabilities of the Old Bailey, the web-based text analysis tools that we use today are very different from the first tentative technologies developed by computing humanists. Following Siegfried Zieliniski's exploration of forgotten media technologies, this paper will look at three forgotten text analysis technologies and how they were introduced by their developers at the time. Specifically we will:

Discuss why is it important to recover forgotten tools and the discourse around these instruments,
Look at how punch cards were used in Roberto Busa’s Index Thomisticus project as a way of understanding data entry,
Look at Glickman’s ideas about custom card output from PRORA, as a way of recovering the importance of output,
Discuss the command language developed by John Smith for interacting with ARRAS, and
Conclude with a more general call for digital humanities archaeology. 
Zieliniski and Media Archaeology
Siegfried Zielinski, in Deep Time of the Media, argues that technology does not evolve smoothly and that we therefore need to look at periods of intense development and then look at the dead ends that get overlooked to understand the history of media technology. In particular he shows how important it is to look at technologies that are not in canonical histories as precursors to “successful” technologies, because they provide insight into the thinking at the time. A study of forgotten technologies can help us understand opportunities and challenges as they were perceived at the time and on their own terms rather than imposing our prejudices. From the 1950s until the early 1990s there was just such a period of technology development around mainframe and personal computer text analysis tools. The tools developed, the challenges they addressed, and the debates around these technologies have largely been forgotten in an age of web-mediated digital humanities. For this reason we recover three important mainframe projects that can help us understand how differently data entry, output and interaction were thought through before born- digital content, output to wall-sized screens, and interaction on a touchscreen. 

Busa and Tasman on Literary Data Processing 
The first case study we will present is about the methods that Father Busa and his collaborator Paul Tasman developed for the Index Thomisticus (Busa could hardly be considered a forgotten figure, but he's often referred to metonymically as a founder of the field, with relatively little attention paid to the specifics of his work and his collaborations). Busa, when reflecting back on the project justified his technical approach as supporting a philological method of research aimed at recapturing the way a past author used words, much as we want to recapture past development. He argued in 1980 that, “The reader should not simply attach to the words he reads the significance they have in his mind, but should try to find out what significance they had in the writer’s mind.” (Busa 1980, p. 83) Concordances could help redirect readers towards the “verbal system of an author” or how the author used words in their time and away from the temptation to interpret the text at hand using contemporary conceptual categories. Concording creates a new text that shows the verbal system, not the doctrine.

Busa’s collaborator Paul Tasman, however, presents a much more prosaic picture of their methodology that focuses on data entry using punch cards so you can actually get concordances of words. He published a paper in 1957 on “Literary Data Processing” in the IBM Journal of Research and Development that focuses on how they prepared their texts accounting for human error and other problems. Tasman writes, “It is evident, of course, that the transcription of the documents in these other fields necessitates special sets of ground rules and codes in order to provide for information retrieval, and the results will depend entirely upon the degree and refinement of coding and the variety of cross referencing desired.” (p. 256) This case study takes us back to a forgotten set of problems (representing text using punch cards) which led to more mature issues in text encoding. In the full presentation we will look closely at the data entry challenges faced by Busa’s team and how they were resolved with the card technology of the time. 

Glickman and Stallman on Printed Interfaces
The second case study we will look at is the development of the PRORA programs at the University of Toronto in the 1960s. PRORA was reviewed in the first issue of CHUM and with the publication of the Manual for the Printing of Literary Texts and Concordances by Computer by the University of Toronto Press in 1966 is one of the first academic analytical tools to be formally published in some fashion. What is particularly interesting, for our purposes, is the discussion in the Manual of how concordances might be printed. Glickman had idiosyncratic ideas about how concordances could be printed as cards for 2-ring binders so that they could be taken out and arranged on a table by users. He was combining binder technology with computing to reimagine the concordance text. Today we no longer think about output to paper as important to tools, and yet that is what the early tools were designed to do as they were not interactive. We will use this case study to recover what at the time was one of the most important features of a concording tool – how it could output something that could be published for others to use. 


Fig. 1: Example of PRORA output from the Manual

Smith and Interaction
One of the first text analysis tools designed to support interactive research was John Smith’s ARRAS. In ARRAS Smith developed a number of ideas about analysis that we now take for granted. ARRAS was interactive in the sense that it was not a batch program that you ran for output. It could generate visualizations and it was explicitly designed to be part of a multi-tasking research environment where you might be switching back and forth between analysis and word processing. Many of these ideas influenced the interactive PC concordancing tools that followed like TACT. In this paper, however, we are not going to focus on all the prescient features of ARRAS, but look at the now rather dated command language which Smith was so proud of. Almost no one uses a command language for text analysis any more; we expect our tools to have graphical user interfaces that provide affordances for direct manipulation. If you need to do something more than what Voyant, Tableau, Lucene, Gephi or Weka let you do, then you learn to program in a language like R or Python. John Smith by contrast, spent a lot of time trying to design a natural command language for ARRAS that humanists would find easy to use and this comes through in his publications on the tool (1984 & 1985). Command languages were, for a while, the way you interacted with such systems and attention to their design could make a difference. Smith tried to develop a command language that was conversational so humanists could learn to use it to explore “vast continents of literature or history or other realms of information, much as our ancestors explored new lands.” (Smith 1984, p. 31) Close commanding for distant reading. 

Conclusions
In the 2013 Busa Award lecture Willard McCarty called us to look to our history and specifically to look at the “incunabular” years before the web when humanists and artists were imagining what could be done. One challenge we face in reanimating this history is that so much of the story is in tools, standards and web sites – instruments difficult to interrogate the way we do texts. This paper looks back at one major thread of development - text analysis tools – not for the entertainment of outdated technology, but recover a way of thinking about technology. We will conclude by discussing other ways back including the need for better documentation about past tools, along the lines of what TAPoR 2.0 is supporting, and the need to preserve tools or at least a record of their usage. 

References
Busa, R. (1980). "The Annals of Humanities Computing: The Index Thomisticus." Computers and the Humanities. 14(2): 83-90.

Glickman, Robert Jay, and Gerrit Joseph Staalman. Manual for the Printing of Literary Texts and Concordances by Computer. Toronto: University of Toronto Press, 1966.

Liu, Alan. (2012) “Where is Cultural Criticism in the Digital Humanities.” In Debates in the Digital Humanities. Ed. Matthew K. Gold. University of Minnesota Press. Liu’s essay is online at <http://dhdebates.gc.cuny.edu/debates/part/11>.

Smith, J. B. (1978). "Computer Criticism." STYLE XII(4): 326-356.

Smith, J. B. (1984). "A New Environment For Literary Analysis." Perspectives in Computing 4(2/3): 20-31.

Smith, J. B. (1985). Arras User's Manual: TR85-036. Chapel Hill, NC, The University of North Carolina at Chapel Hill.

Tasman, P. (1957). "Literary Data Processing." IBM Journal of Research and Development 1(3): 249-256.

Zieliniski, Siegfried. (2008) Deep Time of the Media: Toward an Archaeology of Hearing and Seeing by Technical Means. Cambridge, Massachusetts: The MIT Press.
In A Companion to Digital Humanities1, Willard McCarty cites Nelson Goodman in saying that the term 'model' can be used to denote "almost anything from a naked blonde to a quadratic equation". Indeed the terms 'model' and 'modeling' seem almost painfully polysemous. Nevertheless within Digital Humanities we cannot ignore the terms or the concepts behind them—the notions are inextricably linked to what is one of the core objectives of humanities computing2, namely to render humanities data computationally tractable3 and processable45 to enhance our abilities for analysis. 
In light of the renewed debate on modeling in Digital Humanities6 this panel proposes to investigate how humanists currently understand the role and meaning of modeling, and how we may arrive at an understanding of the term appropriate for humanities research and pedagogy.
McCarty stated a decade ago that the humanities lack a disciplined way of talking about modeling7 which makes it extremely difficult to define the properties and uses of appropriate models for humanities research. Modeling is a commonplace implicit activity in digital humanities, yet our modeling activities are almost never explicitly discussed as such, and it is rarely pointed out that many of our results are in fact models: charts, probabilistic methods, interfaces to the information we structure in databases. This implicitness is attested by our language use. We do not speak of "modeling an analogy" or of "modeling a chart". We "make" or "create" them as concrete representations of an implicit and abstract model.
Yet, given the concrete applications and results that can already be seen within the humanities, modeling needs to be a humanities praxis to the same extent as it already is in other scientific fields such as biology and physics. As the social sciences –more specifically the ethnography practices in Science & Technology Studies for instance– show us, praxis by definition can be studied and interrogated for its properties by observing and following its practitioners8. This panel provides a first step in such observant interrogation. 
In the computational domain modeling can be delineated in a narrow mathematical sense where model theory9 defines Turing complete languages as models or instantiations of logic constructed from formulas (i.e. syntax or rules) and signatures (i.e. vocabulary or objects). Thus, computer languages are themselves mathematical models of logic. They provide a layer of expressive logic that in turn allows us to compositionally model data, objects and their relations10. Analogous to the statement made by Peter Robinson about interfaces11, we can argue that such a composition or model expresses an intellectual argument about the real world entities and relations they mimic, capture, or simulate—an intellectual argument that is made on several levels through the computational model and that eventually is communicated to an observer (or user) by way of its interface.
In recent years we find most notably the application of modeling in order to create maps, graphs, trees1213, analogies, diagrams, charts, simulations14, and stylometric analyses15, as well as in discourse analysis, topic modeling, and narrative modeling16. If the successful computational analytical models are quantitatively and statistically founded, does that mean that humanities modeling must necessarily be anchored in the somewhat narrowly defined models that are generally associated with quantification and computer science
More generally, must the concepts of ‘model’ and ‘modeling’ appropriate for Digital Humanities be bound solely by parameters of the mathematical foundations of binary logic? Modeling as activity and concept applies more widely to the humanities than merely in its computational applications. Is it possible to turn around the dynamic of the computational 'stack', so that rather than having mathematics drive humanities computability, the properties of humanistic problems and the data behind them might drive models of computation? We can argue that the goal of any computational approach within the humanities is to render computable the complexity, the abstraction, the ambiguity, the subjectivity, and multiplicity of perspective of the humanities17. Similarly: how do we encompass aspects of modeling present in simulation and (serious) gaming18 of which the humanistic aspects seem to transcend the narrow mathematical connotation of ‘model’? And how does modeling relate to the continuing history of developing and redeveloping digital humanities tools that–rather than merely representing infrastructure–creates a record of intellectual theorizing humanistic computational models?19 How do we break out of the mathematical sandbox defined by first-order logic to do justice to the modalities of humanities? Does this require completely new models for data, logic, and representation? Does it require a general theory of modeling? Even a new symbolic language inspired by the humanities?
This panel brings together some of the most visible practitioners of computational methods within the humanities who have captured analytic models in software code, as well as some of the most influential figures of what might be called 'tacit modeling theory in digital humanities'. We invite them to consider the characteristics of humanities modeling and how those contrast with computational modeling and mathematical modeling, so as to determine what idiosyncrasies modeling might have in a humanities domain. Do these idiosyncrasies allow us to delineate a computationally tractable vocabulary at all? To investigate these questions the panel will discuss and reflect on matters such as…
How do we address the role of modeling and models in the humanities?
How do we ensure that existing mathematical logic does not confine our ability to represent and manipulate humanistic evidence?
What benefits does a definition of modeling appropriated for the humanities hold?
What would a symbolic language for the humanities look like?
What are the standards of evaluation in modeling and do we need specific ones in the Humanities?
What is a useful vocabulary to talk about modeling in a humanities sense?
Panelists

Joris van Zundert (Chair) is in charge of methodological research at the Huygens Institute for the History of the Netherlands. Next to his research in computational humanities he is interested in exchanges between digital humanities and science and technology studies (STS).
Tara L. Andrews has implemented a digital workbench for the fully computational stemmatic analysis of text traditions (http://www.digitalbyzantinist.org/2012/09/announcing-stemmaweb.html). As an assistant professor of digital humanities she is currently developing and teaching a curriculum that emphasizes modeling and algorithmic approaches to humanistic analysis
Johanna Drucker vehemently called attention to the properties of humanities data that are normally neglected by mathematical and conventional computational models and analyses. She has argued that all data are in fact capta and that naïve approaches to statistics are at risk of defining all data as intrinsically quantitative
Fotis Jannidis is developing a white paper on modeling in digital humanities, a version of which will be included in the new edition of the Companion to Digital Humanities. He is a  member of the TEI consortium–most notably as the Chair of the Genetic Edition Encoding Special Interest Group. TEI can be designated the only de facto standard for text structure modeling and encoding
Mike Kestemont specializes in stylometry and together with the Computational Stylistics Group (https://sites.google.com/site/computationalstylistics/home) has developed "Stylo", a software package in the R statistical programming language. He is an expert of statistical models expressed through computer algorithms and applied to literature stud
Geoffrey Rockwell conceptualized a number of highly visible tools for text analyses (e.g. Voyant: http://voyant-tools.org). He is finalizing a book demonstrating amongst others how the hermeneutic and theoretical aspects of text analysis models in the form of tool development transcends mere IT mathematics and infrastructure.
Michael Sperberg-McQueen is a markup specialist by profession and was co-editor of the Extensible Markup Language (XML) specification, chair of the XML Schema working group, as well as heavily involved with the Text Encoding Initiative (TEI)
Ted Underwood works at the interface of literary history and machine learning and is particularly interested in using Bayesian statistics to develop models that reason about uncertainty in a principled way. He maintains an influential blog on his experiences in computational humanities (http://tedunderwood.com/).  
Organization of the panel

The primary selection criterion for the panelists is their expertise, but care has been taken to balance the panel as much as possible for age, gender, field, and region. The panel session will be organized as follows
The Chair will introduce the panel’s topic, discussion questions, and the panelists (10 minutes);
Each of the panelists will give a definition of modeling as a 1 minute provocative pitch (10 minutes);
An open forum between the panelists and the audience follows (60 minutes);
A circular setting of seats with panelists distributed among the attendees will be used to enhance audience participation in the discussion;
The panel discussion will be audio recorded, concise conclusions will be published to the web.
Further Reading

Checkland, P. & Holwell, S., 1998. Information, Systems, and Information Systems: Making Sense of the Field. Chichester: John Wiley & Sons, Ltd.
Davis, M., 2012. The Universal Computer: The Road From Leibniz to Turing. New York: CRC Press.
Mahoney, M.S., 2011. Histories of Computing. T. Haigh (ed.),Cambridge: Harvard University Press.
Hayles, K.N., 2012. How We Think: Digital Media and Contemporary Technogenesis. Chicago: University of Chicago Press.
Ramsay, Stephen, 2011. Reading Machines: Toward an Algorithmic Criticism (Topics in the Digital Humanities). Chicago: University of Illinois Press.
References

1. Schreibman, Susan, Raymond George Siemens, and John M. Unsworth (2004). A Companion to Digital Humanities. Wiley-Blackwell.
2. Unsworth, J., (2002). What Is Humanities Computing And What Is Not? G. Braungart, P. Gendolla, & F. Jannidis, eds. Jahrbuch für Computerphilologie, 4. Available at: computerphilologie.digital-humanities.de/jg02/unsworth.html (Accessed July 8, 2013).
3. Mccarty, W. (2005). Humanities Computing, New York: Palgrave MacMillan.
4. Unsworth, J., (2002). What Is Humanities Computing And What Is Not? G. Braungart, P. Gendolla, & F. Jannidis, eds. Jahrbuch für Computerphilologie, 4. Available at: computerphilologie.digital-humanities.de/jg02/unsworth.html (Accessed July 8, 2013).
5. Orlandi, T., The Scholarly Environment of Humanities Computing, A Reaction to Willard McCarty’s talk on The computational transformation of the humanities. Available at: rmcisadu.let.uniroma1.it/~orlandi/mccarty1.html (Accessed May 7, 2012).
6. Flanders, J. & Jannidis, F., (2012). Panel Discussion: Data Models in Humanities Theory and Practice, Providence (US). Available at: youtu.be/lHJmPT-VjPE (Accessed November 1, 2013).
7. McCarty, W., (2004). Modeling: A Study in Words and Meanings. In S. Schreibman, R. Siemens, & J. Unsworth, eds. A Companion to Digital Humanities. Oxford: Blackwell. Available at: www.digitalhumanities.org/companion/.
8. Kaptelinin, V. & Nardi, B.A., (2006). Acting with technology: activity theory and interaction design, Cambridge, MA, USA/London UK: MIT Press.
9. Rautenberg, W., (2009). A Concise Introduction to Mathematical Logic 3rd ed., Available at: page.mi.fu-berlin.de/raut/logic3/announce.pdf.
10. Forbus, K.D., (2008). Qualitative Modeling. In F. van Harmelen, V. Lifschitz, & B. Porter, eds. Handbook of Knowledge Representation. Foundations of Artificial Intelligence. Amsterdam, Boston, Heidelberg etc.: Elsevier, pp. 361–394.
11. Robinson, P., (2013). Five desiderata for scholarly editions in digital form. In Digital Humanities Conference 2013. Lincoln (NB, USA). Available at: dh2013.unl.edu/abstracts/ab-314.html.
12. Moretti, F. (2007). Maps, Graphs, and Trees: Abstract Models for Literary History. London: Verso.
13. Jockers, M. (2013). Macroanalysis: Digital Methods and Literary History. University of Illinois Press.
14. Mccarty, W. (2005). Humanities Computing, New York: Palgrave MacMillan.
15. Hoover, D.L., (2012). The Excel Text-Analysis Page: A Collection of Microsoft Excel © spreadsheets with macros, in the service of text-analysis. Available at: files.nyu.edu/dh3/public/The%20Excel%20Text-Analysis%20Pages.html (Accessed October 14, 2013).
16. Meister, J.C. & Gertz, M., (2013). heureCLÉA, collaborative literature exploration & annotation. heureCLÉA | Tools. Available at: heureclea.de/tools/.
17. Drucker, J., (2011). Humanities Approaches to Graphical Display. Digital Humanities Quarterly, 5(1). Available at: digitalhumanities.org/dhq/vol/5/1/000091/000091.html (Accessed August 24, 2012).
18. Bogdanovych, A., Cohen, A. & Roper, M., (2009). The City of Uruk: Virtual Instituions in Cultural Heritage. In Proceedings of the HCSNet 2009 Workshop on Interacting with Intelligent Virtual Characters. HCSNet 2009 Workshop on Interacting with Intelligent Virtual Characters. Sydney. Available at: www-staff.it.uts.edu.au/~anton/Publications/HCSNet09.pdf.
19. Ramsay, S. & Rockwell, G., (2012). Developing Things: Notes toward an Epistemology of Building in the Digital Humanities. In Debates in Digital Humanities. University of Minnesota Press. Available at: dhdebates.gc.cuny.edu/debates/text/11.


    
        
            
                DREaM: Distant Reading Early Modernity
                
                    
                        Wittek
                        Stephen
                    
                    McGill University, Canada
                    stephen.wittek@mcgill.ca
                
                
                    
                        Sinclair
                        Stéfan
                    
                    McGill University, Canada
                    stefan.sinclair@mcgill.ca
                
                
                    
                        Milner
                        Matthew
                    
                    McGill University, Canada
                    matthew.milner@mcgill.ca
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    early modern
                    eebo
                    voyant
                    topic modeling
                    distant reading
                
                
                    archives
                    repositories
                    sustainability and preservation
                    corpora and corpus activities
                    literary studies
                    content analysis
                    bibliographic methods / textual studies
                    interdisciplinary collaboration
                    digital humanities - pedagogy and curriculum
                    english studies
                    renaissance studies
                    media studies
                    data mining / text mining
                    English
                
            
        
    
    
        
            Our proposed paper will provide an overview of the theory and methodology driving the creation of Distant Reading Early Modernity (DREaM), a digital humanities project that has made a massive corpus of early modern texts amenable for use with macro-scale analytical tools. Key focus areas include the technical challenges deriving from non-standardized spelling, the philosophy of our tutorial program, the argument for our approach to the early modern archive, and the potential benefit to early modern scholarship of distant reading techniques. 
            From Microfilm Library, to EEBO, to EEBO-TCP, to DREaM 
            The foundational work for DREaM began in 1934, when Eugene B. Power used parts of two movie and still cameras to create one of the world’s first microfilm bookcameras, a device he used to photograph thousands of texts in British libraries (Anderson and Power, 1990). In 1998 Power’s microfilm library became the basis for Early English Books Online (EEBO), a database that comprises the images for some 125,000 texts from 1475 to 1700, and has profoundly expanded the horizons of early modern research.
                1 To date, approximately one-third of the documents on EEBO are available as transcribed, full-text editions. Researchers for the EEBO Text Creation Partnership (EEBO-TCP) are currently working to transcribe the remaining 85,000 documents, which are as yet only available as digitized microfilm images.
                2
            
            Although completion of the transcription work is still at least 10 years in the future, the prospect of a full-text library of all documents from the first 225 years of English print points to the need for some careful re-thinking about the relation between scholarship and archival sources. As it now stands, the EEBO-TCP corpus amounts to 8.02 gigabytes of XML-encoded text and contains nearly 45,000 documents, for a grand total of well over a billion words (1,155,264,343 by our count). Confronted by the sheer expanse of a corpus several magnitudes larger than anything one could hope to read in a lifetime, early modern scholarship must now work to incorporate digital methodologies that enable a bird’s-eye view of large corpora, an approach that Franco Moretti has dubbed ‘distant reading’ (Moretti, 2007). DREaM has begun the work of making such a view possible. 
            Unlike EEBO, DREaM enables batch downloading of custom-defined subsets rather than obliging users to download individual texts on a one-by-one basis. In other words, it functions at the level of ‘sets of texts’ (sometimes called 
                worksets) rather than ‘individual texts’. Examples of subsets one might potentially generate include ‘all texts by Ben Jonson’, ‘all texts published in 1623’, or ‘all texts printed by John Wolfe’. A user-friendly interface makes subsets available as either plain text or XML-encoded files, and gives users the option to automatically name individual files by date, author, title, or combinations thereof (this file naming flexibility can be useful when interoperating with other tool suites). 
            
            The ability to generate custom-defined subsets is important because it allows researchers to explore the early modern canon with distant reading techniques, and to capture otherwise intractable data with visualizations such as graphs, charts, or other forms of graphic representation. On this note, another key feature of DREaM is that it allows users to transfer specially tailored subsets directly to the analytic interfaces of Voyant Tools (voyant-tools.org), a suite of textual visualization tools that collectively constitute the leading platform for open-access digital humanities research.
                3 In fact, DREaM is actually implemented within Voyant Tools (version 2.0, not yet released, which provides much better support for very large text collections). DREaM thus provides a compelling example of a bridge between massive full-text repositories (that typically provide faceted searching) and more specialized analytic and visualization environments. By enabling simple transference between the EEBO-TCP archive and Voyant, DREaM has significantly expanded the range and sophistication of technologies currently available to researchers who wish to gain a broad sense of printed matter in early modern England. 
            
            Notably, however, DREaM does not aim to replace EEBO, or to supplant conventional forms of research. Rather, our goal is to simply add a new item to the scholar’s toolbox, and to increase transferability between distant reading methodologies and more fine-grained forms of analysis. 
            
                Negotiating the Complexities of Non-Standardized Spelling
            
            Standardized spelling had yet to emerge in early modernity: writers had the freedom to spell however they pleased. To take a famous example, the name ‘Shakespeare’ has 80 different recorded spellings, including ‘Shaxpere’ and ‘Shaxberd’. As one might imagine, variance on this scale presents a serious challenge for large-scale textual analysis. How is it possible to track the incidence of a specific word, or group of words, if any given word could have an unknown multiplicity of iterations? 
            To address this problem, we enlisted the assistance of VARD 2, a tool that helps to improve the accuracy of textual analysis by finding candidate modern form replacements for spelling variants in historical texts.
                4 As with conventional spellcheckers, a user can choose to process texts manually (selecting a candidate replacement offered by the system), automatically (allowing the system to use the best candidate replacement found), or semi-automatically (training the tool on a sample of the corpora). 
            
            After some preliminary training, we ran the TCP-EEBO corpus through VARD using the default settings (auto normalization at a threshold of 50%). Rather than using the ‘batch’ mode—which proved unreliable for such a big job—we wrote a script that normalized the texts on a one-by-one basis from the command-line. This process took about three days on a commodity machine. VARD normalized 80,676 terms for a grand total of 44,909,676 changes overall. 
            A careful check through the list resulted in 373 term normalizations that we found problematic in one way or another. The problematic normalizations amounted to 462,975 changes overall, or only 1.03% of the total number of changes. These results were satisfactory: our goal was not to make the corpus ‘perfectly normalized’ (an impossibility, not least because perfection is debatable in this context), but, more pragmatically, to make it generally normalized, which is the best one can reasonably expect from an automatic process. On this point, it is important to note that VARD encodes a record of all changes within the output XML file, so scholars will be able to see if the program has made an erroneous normalization. 
            Some of the problematic VARD normalizations seem to have derived from a dictionary error. For example, ‘chan’ became ‘champion’ and ‘ged’ became ‘general’. In other instances, the problematic normalizations were ambiguous or borderline cases that we preferred to simply leave unchanged. Examples include ‘piece’ for ‘peece’, and ‘land’ for ‘iland’. There were also cases where the replacement term was not quite correct: ‘strawberie’ became ‘strawy’ rather than ‘strawberry’, and ‘hoouering’ became ‘hoovering’ rather than ‘hovering’. We fixed as many of these kinks as we could by making adjustments to the VARD training file and running the entire corpus through the normalization process a second time. 
            Of course, it is not difficult to imagine scenarios wherein a researcher may prefer to work with original spellings rather than normalized texts. With such projects in mind, we have kept both normalized and non-normalized versions of the EEBO-TCP corpus. 
            
                The DREaM Tutorial Program
            
            As noted above, one of the central objectives of DREaM is to create an interface that will maximize user-friendliness, allowing scholars with a minimal level of technical expertise to quickly and efficiently create subsets tailored for whatever specific research question they wish to pursue. We are building DREaM for our own research, but we also have a much broader pedagogical perspective in mind. To meet this objective, we have launched a pilot tutorial program, currently under way, that will teach scholars how to use DREaM, but will also point to ways in which DREaM could more effectively serve the demands of scholarly investigation. 
            In a series of tasks that build toward the production of a short case-study report, pilot users must articulate a detailed research question and provide a description of their argument. In addition to establishing a valuable feedback loop for the project, this assignment aims to nudge new users toward a more comprehensive, more practical understanding of how macro-scale textual analysis can complement scholarly practice. The key conceptual challenge, as we see it, hinges on new users’ ability to understand, and learn to negotiate, the gap between distant reading and more conventional means of engaging archival sources. 
            Our pool of pilot users derives from the membership of our parent project, Early Modern Conversions, a five-year interdisciplinary research initiative that has brought together a team of more than 100 scholars, partners, and graduate student associates from universities in Canada, the United States, England, New Zealand, and Australia.
                5 Early Modern Conversions provides a propitious testing ground for DREaM because it is at the vanguard of early modern research, and because it entails a rich diversity of disciplinary approaches. Our presentation for DH2015 will report on the results of the tutorial program and on the progress of the project overall. 
            
            Screenshots 
            
                
            
            Figure 1. The DREaM interface. Search fields in the middle of the screen enable users to define a subset of EEBO-TCP texts by keyword, year, author, and publisher. Below the search field, an ‘Export’ button opens a dialogue box that offers the option of sending the subset directly to Voyant-tools.org, or downloading it as a ZIP archive. Users may also choose to download subsets as either plain text or XML-encoded files. A drag-and-drop mechanism (bottom) enables automatic naming of files within a subset by date, author, title, or combinations thereof. 
            
                
            
            
                
            
            Figure 2. A sample subset transferred to Voyant Tools. Beginning in the top left corner, one sees a word cloud representing the frequency of keywords in terms of font size. At a glance, it shows that the highest frequency words in the subset are ‘good’ and ‘come’. Below the word cloud, there is a summary that lists statistics for basic categories such as word count, vocabulary density, word frequency, etc. In addition, the summary lists words that have a notably high frequency for each year: ‘Rome’ and ‘death’ appeared with particular frequency in 1594, while ‘virtue’ and ‘envy’ stood out in 1612. Moving to the bottom left corner, one sees an ordered list of frequencies for each word in the corpus accompanied by a thumbnail graph that tracks the frequency of words over the 40-year delimitation. At a glance, the tool shows a significant spike for the word ‘knight’ in 1624. In the middle of the screen, a ‘Corpus Reader’ tool enables users to drill down into the corpus to examine the context for particular terms. 
            Notes
            1. See http://eebo.chadwyck.com.
            2. See http://eebo.odl.ox.ac.uk/e/eebo/.
            3. See http://voyant-tools.org.
            4. See http://ucrel.lancs.ac.uk/vard/about/.
            5. See http://earlymodernconversions.com.
        
        
            
                
                    Bibliography
                    
                        Anderson, R. and Power, E. B. (1990). The Autobiography of Eugene B. Power, Founder of University Microfilms. UMI, Ann Arbor, MI.
                    
                    
                        Moretti, F. (2007). Graphs, Maps, Trees: Abstract Models for Literary History. Verso, London.
                    
                
            
        
    



    
        
            
                From Mapping the Republic of Letters to Humanities +Design Research Lab: Creating Visualization Tools for Humanistic Inquiry
                
                    
                        Coleman
                        Catherine Nicole
                    
                    Stanford University, United States of America
                    cncoleman@stanford.edu
                
                
                    
                        Caviglia
                        Giorgio
                    
                    Stanford University, United States of America
                    giorgio.caviglia@gmail.com
                
                
                    
                        Comsa
                        Maria
                    
                    Stanford University, United States of America
                    mcomsa@stanford.edu
                
                
                    
                        Braude
                        Mark
                    
                    Stanford University, United States of America
                    braude@stanford.edu
                
                
                    
                        Edelstein
                        Dan
                    
                    Stanford University, United States of America
                    danedels@stanford.edu
                
                
                    
                        Ceserani
                        Giovanna
                    
                    Stanford University, United States of America
                    ceserani@stanford.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    knowledge design
                    interface design
                    open design
                    visualization
                
                
                    interface and user experience design
                    software design and development
                    knowledge representation
                    visualisation
                    networks
                    relationships
                    graphs
                    spatio-temporal modeling
                    analysis and visualisation
                    English
                
            
        
    
    
        
            What does it mean to build visualization tools that support the research process in the humanities? In this paper we will trace the evolution of our thinking about data-driven tools beginning with case studies in early modern intellectual history and eventually including a wide range of projects from classics, social history, performance studies, and other fields. We will give concrete examples of how individual tools were designed and whether those tools ultimately failed or succeeded to provide scholars with a means to gain insights into historical data. Through these examples, this paper argues for the role of an open design process in the development of visualization tools for humanities research that brings designers, developers, and scholars into deep collaboration to build nuanced and rigorous tools for humanities research. 
            Mapping the Republic of Letters
            Mapping the Republic of Letters was formed on the assumption that intellectual history is one of the fields that stands the most to gain from the influx of big data. By combining metadata from library catalogues and large-scale digitization projects, the project seeks to maximize the transformative effect of all this information. The cartographic, chronological, and network visualizations ultimately produced allow researchers to examine some of the big questions that intellectual historians have long struggled with: How do intellectual networks function? How interconnected are they? How independent are these networks from other social networks? 
            The 2009 Digging Into Data Challenge grant award launched an active tool development phase in the Mapping the Republic of Letters (MRofL) project at Stanford. In partnership with DensityDesign Research Lab in Milan, the team began to engage in a tool design process in response to concrete research questions. The data—based on individuals and their correspondence, travel, and publications—were multidimensional and qualitatively rich. It became clear that historians who wish to bring data visualization tools to bear on the study of the past face a number of challenges. Many available tools had a steep learning curve and were ultimately of limited help for humanists. These tools rest on assumptions about the completeness and empirical value of data that often do not hold true for humanities research. Historical data can be incomplete and messy: statistical analysis can be a helpful to a limited extent, but interpretation at the most fundamental level is required to uncover meaning. Humanists also ask questions about the data that cannot be answered by numerical analysis. We needed tools that help us filter, contextualize, compare, and see the gaps in our data. 
            Humanities + Design
            
                Humanities + Design, a research lab founded in 2012 by Dan Edelstein, Paula Findlen, and Nicole Coleman, emerged directly out of lessons learned and opportunities for humanities data analysis discovered through MRofL. The mission of the lab is to produce, through the lens of humanistic inquiry, new modes of thinking in design and computer science to serve data-driven research in the humanities. We believe that humanistic inquiry, grounded in interpretation, has much to contribute to the development of technologies if they are to help us reveal ambiguity and paradox, allowing human-scale exploration of complex systems. In the laboratory environment, theoretical and methodological discussions happen side by side with hands-on work with digital materials. Humanities scholars and students, designers, engineers, and computer scientists engage together in ongoing tool design as defined by the specific needs of participating humanities projects.
            
            Palladio Project
            The award of the 2012 NEH Implementation Grant for Networks in History allowed the lab to pursue the development of visualization techniques and rich interaction with data that supports ‘thinking through data’ rather than using prescribed algorithms for data analysis. Palladio is a web-based demonstration application that allows any researcher to upload, visualize, and explore complex and multidimensional data, directly in a web browser. 
                It has been designed for humanistic inquiry, with a special focus on historical research. 
                The Palladio visualization system combines a primary view (for example, Map, Network Graph, and Tabular views) with filters to make it easy to query a dataset. 
                There is no need to create an account, nor do we store any data
                . 
                Researchers can save and shared the work they have done in the browser as a Palladio Project. Palladio’s TimeLine and TimeSpan filters encourage filtering and sorting temporal data, and allows the filtering of two or more discontinuous time periods. A Facet filter is also particularly useful when exploring multidimensional datasets and drilling down to specific aspects of one’s data. Using case studies (examples listed later in this document) we will discuss how scholars have used Palladio, highlighting those instances when uses of the tool diverged from our expectations or led us toward new insights that we incorporated (or plan to incorporate) in future versions.
            
            Open Design
            
                The development of Palladio has been an iterative process. We have been eliciting and incorporating feedback from the academic community concerning Palladio’s current and potential features and uses. Most specifically, we have engaged in sustained discussion with a small and inter-disciplinary group of scholars, known as Open Design Contributors. Our paper will offer insight into this design process and the ways that it has directly influenced current and future iterations of Palladio, as well as other tools.
            
            Summary
            The core innovation of our project is the design of visualization techniques that emphasize the contextualization and interpretation of data in cases where we lack the metrics for useful quantitative analysis. The two other key innovations both involve the leveraging of novel technologies that are particularly important to the study of cultural heritage data: we use new flexible data models to let individual scholars create and apply their own data categorizations, and we use open linked data sources to reconcile datasets against established authority files, in order to link entities across datasets and thereby explore networks across collections.
            
                Additional Case Studies to Be Discussed
            
            
                Case Study: Toward More Complex Ways of Displaying Travel
            
            
                Kate Elswit, Lecturer in Theatre and Performance Studies at the University of Bristol, ‘Ballet, Digital History, and the Cold War: Visualizing the Labor of Dance Touring’
            
            Dance scholar Kate Elswit has been using Palladio in her research on the labor of dance touring. She writes, ‘Such [visualization] techniques enable us to feel the passage of time differently.’ Following discussion with Elswit and other scholars interested in tracing travel routes, we have been thinking about how to display point-to-point travel in ways that go beyond simple flight-path-like visualizations. How to account for the differences in traveling at night rather than in the day? How to represent different levels of comfort, safety, and efficiency in travel?
            Case Study: Questions of Scale and Incomplete Data
            
                Molly Taylor-Poleskey, PhD Candidate, Department of History, Stanford: Food Culture in Brandenburg-Prussia
            
            
                Taylor-Poleskey uses a large base of manuscript sources detailing the yearly consumption of one of the palaces of Prince-Elector Friedrich Wilhelm of Brandenburg-Prussia. She argues that the elector’s cultural agenda helped transform his territories over the course of his reign from dilapidated and war-torn to stable and powerful. To support her argument, she wants to see how tastes and consumption patterns changed over time, to consider how such changes might reveal the court’s aesthetic values and cultural ambitions. Creating visualizations in Palladio have helped her analyze what proportions of different foods or food groups were consumed. We have worked with Taylor-Poleskey toward creating visualizations that privilege the display of relative magnitude, and that are especially sensitive to working in different registers and scales. As some of the years in the sources she studies have incomplete or missing data, her use case has also aided us in thinking about how best to work with and represent incomplete data in ways that are not misleading or overly simplistic.
            
            Case Study: Toward New Palladio Data-Visualization Iterations
            
                Office of the Historian, US State Department, Foreign Relations of the United States
            
            We will share results from our ongoing work with Thomas Faith at the Office of the Historian at the US Department of State, with whom we have been working toward the goal of producing an integrated version of Palladio that would function as a visual browser for extant online data concerning the foreign relations of the United States. The State Department project is one of many we are working on, as we look to help other researchers to implement customized versions of Palladio that can be used as search, analysis, and visualization exploratory tools within extant large-scale research projects.
        
        
            
                
                    Bibliography
                    
                        Balsamo, A. 
                        (2009). Design. 
                        International Journal of Learning and Media,
                        1
                        (4): 1–10.
                    
                    
                        Berry, D. M. 
                        (2012). 
                        Understanding Digital Humanities.
                         Palgrave Macmillan, New York.
                    
                    
                        Buchanan, R. 
                        (2001). Design Research and the New Learning. 
                        Design Issues,
                        17
                        (4): 3–23.
                    
                    
                        Burdick, A. 
                        (2009). Design Without Designers. 
                        Conference on the Future of Art and Design Education in the 21st Century
                        , University of Brighton, England, 29 April 2009.
                    
                    
                        Burdick, A. and Willis, H. 
                        (2011). Digital Learning, Digital Scholarship, and Design Thinking.
                        Design Studies,
                        32
                        (6): 546–56.
                    
                    
                        Drucker, J. 
                        (2009). SpecLab. In 
                        Digital Aesthetics and Projects in Speculative Computing.
                         University of Chicago Press, Chicago.
                    
                    
                        Drucker, J. 
                        (2011). Humanities Approach to Interface Theory. 
                        Culture Machine,
                        12
                        : 1–20.
                    
                    
                        Friedman, K. 
                        (2003). Theory Construction in Design Research: Criteria, Approaches, and Methods. 
                        Design Studies,
                        24
                        (6): 16.
                    
                    
                        Fuller, M.
                        (2008). Software Studies. MIT Press, Cambridge, MA.
                    
                    
                        Ivanhoe.
                         (n.d.). http://www2.iath.virginia.edu/jjm2f/old/IGamehtm.html. 
                    
                    
                        Lunenfeld, P., Burdick, A., Drucker, J., Presner, T. and Schnapp, J. P.
                        (2012).
                        Digital_Humanities
                        . MIT Press, Cambridge, MA.
                    
                    
                        Mandala Browser. 
                        (n.d.). http://mandala.humviz.org/.
                    
                    
                        Masud, L., Valsecchi, F., Ciuccarelli, P., Ricci, D. and Caviglia, G.
                        (2010). From Data to Knowledge: Visualizations as Transformation Processes within the Data-Information-Knowledge Continuum. In Banissi, E., Bertschi, S., Burkhard, R., Counsell, J., Dastbaz, M., Eppler, M., Forsell, C., et al. (eds),
                        Information Visualisation IV, 2010 14th International Conference
                        , pp. 445–49.
                    
                    
                        McCarty, W. 
                        (2003). 
                        Encyclopedia of Library and Information Science.
                         Vol. 2. 2nd ed. New York: Dekker, pp. 1224–35.
                    
                    
                        McGann, J. and Samuels, L. 
                        (2004). Deformance and Interpretation. In 
                        Radiant Textuality.
                        New York: Palgrave Macmillan.
                    
                    
                        Moretti, F. (
                        2005). 
                        Graphs, Maps, Trees.
                         Verso Books, New York.
                    
                    
                        Nowviskie, B.
                        (2004). 
                        Speculative Computing: Instruments for Interpretative Scholarship.
                         Ph.D. thesis, University of Virginia.
                    
                    
                        Orbis.
                        (n.d.). http://orbis.stanford.edu/.
                    
                    
                        Pope, R. (
                        1995). 
                        Textual Intervention: Critical and Creative Strategies for Literary Studies.
                        Routledge, London.
                    
                    
                        Ramsay, S. 
                        (2011a). On Building, 11 January, http://stephenramsay.us/text/2011/01/11/onNbuilding.html.
                    
                    
                        Ramsay, S. 
                        (2011b). Who’s In and Who’s Out, 8 January, http://stephenramsay.us/text/2011/01/08/whosNinNandNwhosNout.html.
                    
                    
                        Ruecker, S., Radzikowska, M. and Sinclair, S.
                         (2011). 
                        Visual Interface Design for Digital Cultural Heritage.
                         Ashgate.
                    
                    
                        Schnapp, J. and Presner, T. 
                        (2009). The Digital Humanities Manifesto 2.0, 17 June, www.humanitiesblast.com/manifesto/Manifesto_V2.pdf.
                    
                    
                        Schon, D. A. 
                        (1983). 
                        The Reflective Practitioner: How Professionals Think in Action.
                        1st ed. Basic Books, New York.
                    
                    
                        Temporal Modeling. 
                        (n.d.). http://www2.iath.virginia.edu/time/time.html.
                    
                    
                        Voyant.
                         (n.d.). http://voyant-tools.org.
                    
                
            
        
    



    
        
            
                Nocht: An Open Source Tool for Text Analysis
                
                    
                        O'Sullivan
                        James Christopher
                    
                    Pennsylvania State University
                    josullivan@psu.edu
                
                
                    
                        Hswe
                        Patricia
                    
                    Pennsylvania State University
                    phswe@psu.edu
                
                
                    
                        Long
                        Christopher P.
                    
                    Pennsylvania State University
                    cplong@psu.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    Tools
                    Text Analysis
                    Python
                    Digital Literary Studies
                
                
                    software design and development
                    text analysis
                    English
                
            
        
    
    
        
            Computational approaches to text analysis have revolutionised the ways in which scholarly research is being conducted. A number of tools exist that help scholars, from a variety of disciplines, analyse textual data, whether literary, historical, or otherwise, using scientific methodologies. However, many of these tools are either proprietary, present a steep learning curve, or are constructed without much transparency, often leaving users with results whose means of production they do not understand. This poster will outline the development of a tool that is intuitive and completely free and open-source, so that scholars in literary studies, and indeed the broader humanities, can leverage computational methods and big data analytics in their research.
            Nocht
            Nocht (trans.: to reveal, uncover), is developed, primarily, in Python, so that it is flexible, scalable, and cross-platform. It has been developed in accordance with the following principles:
             • It offers users a low-barrier means of using computational approaches to text analysis.
             • It is designed and developed in a humanities / arts / social sciences context.
             • It is completely open-source, removing the ‘black-box’, closed-code issues.
             • It brings together existing libraries and code-sets, acting as a ‘script portal’ of sorts.
            At present, Nocht supports the following methodologies, though with some limitations:
             • Wordcount and most frequent wordlists.
             • Word / wordlist frequency plotting.
             • Syntax and sentence analysis.
             • Sentiment analysis.
             • Topic modeling.
             • Zeta analysis.
            It is hoped that Nocht will further contribute to our field’s ongoing commitment to open and sustainable research tools, complementing highly regarded projects like Voyant
                1 and Stylo (Eder et al., 2013). Its name is an obvious tribute to the former, which has for so long been one of our field’s fundamental tools. It is hoped that Nocht will add further to the DH toolkit, as well as complement the ongoing work of Voyant’s creators in leveraging the iPython architecture. 
            
            From a technical perspective, Nocht is scalable and robust, and satisfies the needs of a wide range of scholars, many of whom wish to conduct this form of research but lack the expertise or resources to do so. In this respect, it enables scholars, both emerging and established, to engage with cutting-edge analyses across a variety of disciplines. In many cases, it draws on a series of existing libraries and proven methodologies, such as NLTK
                2 and matplotlib,
                3 and so acts as a set of original scripts as well as a portal to existing tools. A complete technical overview of the project’s features, as well as the components utilised in its modular development, will be provided at the session. 
            
            Discussion
            This poster proposes to introduce Nocht to the field, discussing possible future development directions, as well as issues to date. Some of the disciplinary particularities identified by Gibbs and Owens (2012), such as our need to enhance the usability of our tools, will be addressed. The tension between having an intuitive interface and the need for scholarly tools to produce verifiable results is particularly clear in this project. While there is a long-established requirement that such tools be user-friendly (Krug 2005), one might argue that this must be balanced with a commitment to avoiding ‘black-box’ projects; usability does not necessarily equate to understanding. 
            Measuring the value and success of development projects also remains problematic for scholars and practitioners working across the digital humanities. Schreibman and Hanlon’s survey (2010) finds that the majority of respondents were satisfied that their tools had been ‘successful’, enabling themselves and others to further their research. However, respondents also outlined that they had measured this success from a ‘controlled list’. As our methods continue to gain prominence beyond the core digital humanities community, we must find new metrics through which we can reliably measure the impact of our tools, not just in terms of user volumes, but in relation to the quality of research output. As a project that has sacrificed some aspects of usability and marketability in favour of broad functionality and a commitment to open principles, perhaps to its detriment, Nocht is an ideal catalyst for this debate. It is a small development with limited financial support, so it will be interesting to see if projects of this scale have a future in our discipline.
            Notes
            1. See Stéfan Sinclair and Geoffrey Rockwell, http://voyant-tools.org/.
            2. Natural Language Toolkit, http://www.nltk.org/.
            3. matplotlib, http://matplotlib.org/.
        
        
            
                
                    Bibliography
                    
                        Eder, M., Kestemont, M. and Rybicki, J. (2013). Stylometry with R: A Suite of Tools. 
                        Digital Humanities 2013: Conference Abstracts, University of Nebraska–Lincoln, pp. 487–89.
                    
                    
                        Gibbs, F. and Owens, T. Building Better Digital Humanities Tools: Toward Broader Audiences and User-Centered Designs. 
                        Digital Humanities Quarterly,
                        6(2).
                    
                    
                        Krug, S. (2005). 
                        Don’t Make Me Think! A Common Sense Approach to Web Usability. 2nd ed. New Riders Press, New York.
                    
                    
                        Schreibman, S. and Hanlon, A. M. (2010). Determining Value for Digital Humanities Tools: Report on a Survey of Tool Developers. 
                        Digital Humanities Quarterly,
                        4(2).
                    
                
            
        
    



    
        
            
                Queen Luise of Prussia, a Digital Hagiography
                
                    
                        Askey
                        Jennifer D
                    
                    McMaster University, Canada
                    askeyj@mcmaster.ca
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Short Paper
                
                
                    German history
                    digitization
                    nineteenth century
                    fraktur
                
                
                    literary studies
                    digitisation
                    resource creation
                    and discovery
                    text analysis
                    authorship attribution / authority
                    german studies
                    English
                
            
        
    
    
        
            Popular biographies of Queen Luise of Prussia (1776–1810, r. 1797–1810) written in the second half of the nineteenth century uniformly adhere to what Wulf Wülfing has called a ‘stations of the cross’ structure (Wülfing, 1984). Independent of whether the biography focuses on her domestic life, her personal character and temperament, or her influence at court, it relates and pays homage to canonical moments in Luise’s life story: her childhood encounter with Goethe’s mother, her presence at the coronation of the last Holy Roman Emperor, her meeting with her future husband, and several instances of informal, uncourtly conduct upon becoming princess. Biographies written for a variety of intended audiences (the German people, young Germans, young ladies, etc.) all adhere to the same narrative thread. While biographies on the same subject might be expected to follow a similar time line, popular treatments of Luise’s life and times tend to treat the same set of life anecdotes using the same language (Askey, 2013). While Luise’s function as a secular Prussian saint is one reason for the similarity of these biographies, another is their grounding in the same foundational texts. After Luise’s death in 1810, two biographies written by members of the Prussian court reached the public. King Friedrich Wilhelm III’s court pastor, Ruleman Friedrich Eylert, published 
                Charakter-Züge und historische Fragmente aus dem Leben des Königs von Preussen Friedrich Wilhelm III (Characteristics and Historical Fragments form the Life of King Friedrich Wilhelm III of Prussia), a multivolume court history that contained over one volume of information on Queen Luise. And Luise’s lady in waiting, Countess Sophie Marie von Voss, published 
                Neunundsechzig Jahre am preussischen Hofe; aus den Erinnerungen der Oberhofmeisterin Sophie Marie Gräfin von Voss (Sixty-Nine Years at the Prussian Court: The Memories of the Senior Lady in Waiting Countess Sophie Marie von Voss)
                . This poster will introduce a digitization and textual analysis project that brings court and popular biographies into conversation with one another. 
            
             While the court biographies are available digitally, most of the popular biographies, especially those for children, are not. Libraries in the nineteenth century did not generally collect children’s literature, and so my personal collection of around a dozen Luise biographies for young readers will be digitized. Our team at the Sherman Centre for Digital Scholarship will OCR the biographies, experimenting with ABBYY, OmniPage, and Tesseract to achieve optimal results with the notoriously difficult German 
                Fraktur typescript. Once the corpus of official court and popular biographies has been created, we will perform standard text mining on the body of texts for young people to confirm or refute personal close readings. Our text mining queries will focus on the frequency of selected key terms relating to Luise’s life story, as well as the frequency of certain gendered textual markers (such as the word 
                Gemüt—temperament, or words associated with clothing or fashion). The information gleaned from text mining will contribute to a scholarly discussion not only on the queen but on exemplarity, female childhood, and the gender discourse of the public sphere in the long nineteenth century. 
            
             The final step involves running subsets of the popular biographies (for the general public, for young people) and the court biographies through a comparison engine such as Juxta to determine adherence or deviation from a supposed ur-text. In the context of my previous work on Queen Luise and her literary function as exemplar for young German women, my hopes for the comparison step of the process are quite high. I use Voyant and Juxta to focus on specific language tokens that provide insight into gender norms and expectations, references to Luise’s physicality, and the development of nationalist discourse (apparent where the queen’s use of French at the court is set against her domestic use of German). 
             The poster will illustrate the resource creation process (digitization and OCR of Fraktur texts, accessibility of that corpus for other scholars) as well as the first stages of textual analysis using the corpus. 
        
        
            
                
                    Bibliography
                    
                        Askey, J. (2013). Good Girls, Good Germans: Girls’ Education and Emotional Nationalism in Wilhelminan Germany. Camden House, Rochester, NY. 
                    
                    
                        Eylert, R. (1844). Charakter-Züge und historische Fragmente aus dem Leben des Königs von Preußen Friedrich Wilhelm III: Gesammelt nach eigenen Beobachtungen und selbstgemachten Erfarhungen; wohlfeile Ausgabe für das Volk. Heinrichshofenschen Buchhandlung, Magdeburg.
                    
                    
                        Voß, S. (1876). Neunundsechsig Jahre am Preußischem Hofe: Aus den Erinnerungen der Oberhofmeisterin Sophie Marie Gräfin von Voß. Dunker und Humbolt, Leipzig.
                    
                    
                        Wülfing, W. (1984). ‘Die Heilige Luise von Preußen: Zur Mythisierung einer figure der Geschichte in der deutschen Literatur des 19. Jahrhunderts’: Bewegung und Stillstand in Metaphern und Mythen: Fallstudien zum Verhältnis von elementarem Wissen und Literatur im 19. Jahrhundert. Stuttgart: Klett-Cotta. 
                    
                
            
        
    



    
        
            
                Talking About Programming in the DIgital Humanities
                
                    
                        Rockwell
                        Geoffrey
                    
                    University of Alberta, Canada
                    geoffrey.rockwell@ualberta.ca
                
                
                    
                        Sinclair
                        Stéfan
                    
                    McGill University
                    stefan.sinclair@mcgill.ca
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    History of DH
                    Programming
                    DH Courses
                
                
                    programming
                    history of Humanities Computing/Digital Humanities
                    English
                
            
        
    
    
        
            In the first months of the HUMANIST discussion list a discussion erupted around the value of programming languages in humanities computing courses. Do computing humanists need to know how to program, or is learning expert use of applications enough? What is the point of learning to program? (McCarthy, 1987). This discussion can be traced back at least as far as the 1986 ACH/Sloan Workshop on Teaching ‘Computers and the Humanities’ Courses, where participants discussed ‘most vigorously of all whether programming should be taught, or only package programs’ (Sperberg-McQueen 1986). The issue has really never gone away as it is about disciplinary formation or ‘what it is we want our students to learn, the nature of the world into which we are sending them, and the relationship both of technology and (more fundamentally) the algorithmic approach to problem-solving’ (McCarty, 1987, 1–2). Based on a reading and analysis of a substantial collection of historical documents (journal articles, association newsletters, books, Listservs, etc.) in this paper we will examine how the humanities computing community discussed programming and how that discussion reflects changing views of the field over time. Specifically we will
             • Outline a three-part history of programming in the digital humanities starting with a concording phase when programming was not important to using computers.
             • Discuss the turn toward teaching programming as a way of teaching computational thinking.
             • Look at how the emergence of the Web changed the discussion. We will argue that as the Web became important, scripting skills suitable for building websites replaced older programming languages and it became possible to imagine the digital humanities being defined by the ability to program. 
             • Conclude with some reflections on how the case for programming has been made more recently. 
            
                Humanities Computing as Concording
            
            I’ve tried to stay free of programming. I’m perfectly innocent of any knowledge of any programming language and I feel I must remain that way if I am going to continue to function as a scholar in literature; my heartfelt advice to any of you younger people embarking on this whole venture is to do the same—to ask programmers to do for you what they know how to do and what would be costly and painful for you to learn—to learn to talk their language but not to get involved in programming research or machine research. I know the best of you will not take this advice and will, therefore, break new barriers and so on, but I persist in offering it. (Parrish, 1969, 24–25) 
            Programming in the early concording years of computing in the humanities was something often left to specialists. Stephen Parrish, who was general editor of the Cornell Concordance Series, saw himself as a ‘scholar in English literature who drifted laterally into the making of concordances’ (1969, 16). He stayed away from programming as a distraction from scholarship. This meant that concording ‘required an organized, teamwork approach, characterized by a substantial budget and a university computing centre’ (McCarty, 1993, 52–53). Parrish is not the only one to worry about programming distracting scholars, and in the presentation we will discuss other examples as a way of teasing out how programming was not considered of scholarly value. 
            
                Programming and Reasoning
            
            By the 1980s, things had changed and programming had become important, as can be seen in the way programming was included in a majority of courses. Surveys like Joseph Rudman’s ‘Teaching Computers and the Humanities Courses: A Survey’ (1987) report 175 courses teaching a programming language while 131 were ‘Applications Only’. In that survey, the most popular languages taught were BASIC (in 75 courses reporting), Pascal (30 courses), Prolog (15 courses), Lisp (13 courses), and SNOBOL (10 courses). What is impressive is the number of responses that indicated they had some sort of course for humanities students. We often assume that computing in the humanities is a recent, post-advent-of-the-Web thing, but given the number of courses, the discussions on HUMANIST, and articles in journals like 
                Computing and the Humanities, it is clear that programming was becoming an acceptable activity, especially for those in support staff positions. 
            
            Despite the popularity of programming languages like BASIC and Pascal for courses, the languages about which humanists wrote in the 1980s tended to be languages like SNOBOL and Icon—all languages that were good for string (text) manipulation, as can be seen in the attention they received in books for teaching programming to humanists like John Abercrombie’s 
                Computer Programs for Literary Analysis (1984)
                , Susan Hockey’s 
                SNOBOL Programming for the Humanities (1985)
                , and Alan Corré’s 
                Icon Programming for Humanists (1990)
                . Then there are the discussions on HUMANIST and reviews of languages like Mark Olsen’s self-explanatory ‘Beyond SNOBOL: The Icon Programming Language’. 
            
            As an aside, one could argue that SGML (Standard Generalized Markup Language) and later XML (Extensible Markup Language) are also forms of programming—meta-languages with which one can create descriptive languages with which to rigorously describe texts for scholarly electronic editions. These were popular in humanities computing, especially after the Text Encoding Initiative began to provide guidelines for the encoding of texts in the late 1980s. How many humanists were first introduced to computing in the humanities when asked to develop a DTD (Document Type Definition) and encode a text? 
            It is interesting that many of the discussions about programming in the 1980s and early 1990s circle around the teaching of it and that this issue is reported as contentious. No one believed that the ability to program was essential for a computing humanist, in part because so many couldn’t, but proponents of programming argued for it to be taught as way of teaching computational reasoning or problem solving. They also argued that there were social and ethical issues that could be understood through learning computing. You learned to program so as to understand what the computer could do or to be able to talk to programmers. In the presentation we will look at summative discussions of the issue like McCarty’s (1987) and a lovely balanced essay by Nancy Ide, ‘Computers and the Humanities Courses: Philosophical Bases and Approach’ that came out of the 1986 ACH/Sloan Workshop. We will also speculate as to why there was this shift towards including programming and arguing for its importance. Whatever the reasons, the discourse had changed and computing humanists were beginning to take an interest in programming and teaching it. 
            
                Programming the Web 
            
            The emergence of the Web changed the languages that humanists were likely to use for programming and, we will argue, made it possible to make programming a defining skill for digital humanists. Moreover, learning to program for the Web provided a more appealing and transferable skillset; web and application development became normalized and desirable. Because the Web provided such a convenient way to show and distribute digital research, it changed which programming languages received attention or were taught. We can see the shift in these histograms of word frequencies in the HUMANIST Archives. The string processing languages popular in the late 1980s taper off and are replaced with languages like PHP with which one can build dynamic websites. 
            
                
            
            
                
            
            Figure 1. Two graphs showing the frequency of programming languages in the annual archives of the HUMANIST Discussion Group Listserv, produced with Voyant Tools. 
            Now programming (and other web skills) went from being about teaching reasoning to being a skill students and humanists could actually use to create humanities products, i.e., websites. Programming became creative and expressive. There was also a convenient on-ramp as students could start creating HTML pages and then learn to use scripting languages like PHP that enhanced the web site until you knew enough to code a scholarly web site. 
            We will further argue that once web development wasn’t something that digital humanists would necessarily be amateurs at, then it could become a defining skill, and a source of some pride. Digital humanists finally had something that they could be good at and a set of competencies unique to the digital humanities. Digital humanists like Stephen Ramsay could provocatively say, ‘Do you have to know how to code? I’m a tenured professor of Digital Humanities and I say “yes”’ (Ramsay 2011). This led to the ‘hack vs. yack’ discussion that followed about programming and other forms of building (see Nowviskie [2014] for more general context). 
            
                Conclusions: Programming in the Humanities and Disciplinary Formation 
            
            This paper traces the ways programming has been discussed in the digital humanities. We believe that the role of programming was important to the way the field of humanities computing (and later digital humanities) conceived of itself. This is not surprising given the importance of programming to computing in general, but it is interesting to follow the particular ways programming was discussed as the discipline emerged. 
            We will end the paper by theorizing, or at least speculating, about where programming in digital humanities is going. One direction (already well under way) is towards software studies and the studying of programmed artifacts as works of human art and expression worthy of the humanities. Matthew Kirschenbaum in a fine essay for the 
                Chronicle of Higher Education talks about ‘procedural literacy’. 
            
            All programming entails world-making, as the ritual act of writing and running Hello World reminds us. . . . 
             Ultimately what’s at stake is not the kind of vocational computer literacy I was taught as an undergraduate, but what a growing number of practitioners in the digital humanities (and related disciplines, like digital art or game studies) have begun to call procedural rhetoric, or procedural literacy. (Kirschenbaum, 2009) 
            Another direction is data science. The opportunities for new insights through large-scale text mining, or what Moretti (2007) calls distant reading, have made programming languages like R attractive. There is a return to text processing languages, but now languages that can analyze large corpora or visualize results.
        
        
            
                
                    Bibliography
                    
                        Ide, N. M. (1987). Computers and the Humanities Courses: Philosophical Bases and Approach. 
                        Computers and the Humanities,
                        21(4): 209–15. 
                    
                    
                        Kirschenbaum, M. (2009). Hello Worlds. 
                        Chronicle of Higher Education,
                        55(20): B10, http://chronicle.com/article/Hello-Worlds/5476. 
                    
                    
                        McCarty, W. (1987). HUMANIST So Far: A Review of the First Two Months. 
                        ACH Newsletter, 
                        9(3): 1–3. 
                    
                    
                        McCarty, W. (1993). Handmade, Computer-Assisted, and Electronic Concordances of Chaucer. 
                        CCH Working Papers,
                        3: 49–65. 
                    
                    
                        Moretti, F. (2007). 
                        Graphs, Maps, Trees: Abstract Models for Literary History. Verso, London. 
                    
                    
                        Nowviskie, B. (2014). On the Origin of ‘Hack’ and ‘Yack’. http://bit.ly/1ttih76.
                    
                    
                        Olsen, M. (1987). Beyond SNOBOL: The Icon Programming Language. 
                        Computers and the Humanities,
                        21(1): 61–66. 
                    
                    
                        Parrish, S. M. (1969). Concordance-Making by Computer: Its Past, Future, Techniques, and Applications. In Burelbach, F. M. (ed.), 
                        Proceedings: Computer Applications to Problems in the Humanities; A Conversation in the Disciplines, Conference at State University College, Brockport, NY, 4–5 April 1969, pp. 16–33. 
                    
                    
                        Ramsay, S. (2011). On Building. http://stephenramsay.us/text/2011/01/11/on-building/. 
                    
                    
                        Rudman, J. (1987). Teaching Computers and the Humanities Courses: A Survey. 
                        Computer and the Humanities, 
                        21(4): 235–43. 
                    
                    
                        Sinclair, S. and G. Rockwell. (2014). 
                        Voyant Tools. http://voyant-tools.org/. 
                    
                    
                        Sperberg-McQueen, C. M.
                         (1986). Report on ACH/Sloan Foundation Workshop on Teaching ‘Computers and the Humanities’ Courses. 
                        ACH Newsletter,
                        8
                        (4): 1–2.
                    
                
            
        
    



    
        
            
                The Trials of Tokenization
                
                    
                        Hoover
                        David L.
                    
                    New York University, United States of America
                    david.hoover@nyu.edu
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Long Paper
                
                
                    Python
                    tokenization
                    word frequency lists
                    programming
                    punctuation
                
                
                    natural language processing
                    software design and development
                    text analysis
                    programming
                    standards and interoperability
                    English
                
            
        
    
    
        
            The process of tokenizing texts is typically out of sight and almost out of mind—often handled invisibly by the analyst’s program or R script, and rarely described, discussed, or even mentioned. For ‘big data’, even if questions did arise about the nature of the word list produced, testing is not feasible. Furthermore, tokenizer accuracy is so critically affected by the state and nature of the texts that probably no general measure of accuracy or appropriateness is possible. Finally, built-in programming functions and libraries are all too often used uncritically with little realization that their output does not conform to the assumptions or expectations of the analyst. I suggest that we should pay a little more attention to the theory and practice of tokenization.
                1
            
            Consider a hypothetical case. Let’s say I want to analyze 5,000 novels, have access to the texts at HathiTrust, download 5,000 novels in plain text, and tokenize them. Below is part of a page from Elizabeth Gaskell’s 
                Cranford, from HathiTrust (Gaskell, 1910 [1851], 107):
            
            
                
            
            Figure 1. 
                Cranford, Elizabeth Gaskell, from page 107.
            
            A human reader would have little trouble tokenizing this passage, and it is not extremely problematic, though minor OCR problems exist (mainly spacing issues around single quotation marks / apostrophes and dashes, and the line-end hyphen). I tokenized this passage with The Intelligent Archive (2012), KWIC (Tsukamoto, 2004) WordSmith Tools (Scott, 2012), Voyant (Sinclair et al., 2012), and Stylo (Eder et al., 2014).
                2 Even on this short text, the five programs identify three different numbers of types and two different numbers of tokens, largely because of the handling of single quotation marks. KWIC and WordSmith produce identical lists, as do Voyant and Stylo, but neither of these match The Intelligent Archive.
            
            Now consider Charles Chesnutt’s 
                The House Behind the Cedars (1900, 13), also from HathiTrust:
            
            
                
            
            Figure 2. 
                The House Behind the Cedars, Charles Chesnutt, from page 13.
            
            The dialect in this passage is challenging even for human readers, and the OCR is more problematic. For example, the printed text (judging from the PDF) had spaced contractions, which explains ‘you 're’ in the fourth line from the bottom and the space in ‘lie 's’ in the first line, where the text reads “he 's.” This classic OCR problem occurs several times in this novel. And in the last line ‘you '11’ has both a space and an erroneous number 11 for the ‘ll’ (double el), another common OCR problem. Those analyzing big data usually rely on the insignificance of random error, but these and many other kinds of error are not random, and systematic error within one text, one author, one genre, or one collection could easily lead to thousands of inaccurate word frequency counts in this hypothetical study of 5,000 texts.
            The use of apostrophes in the Chesnutt passage to indicate dialect pronunciations can also severely affect tokenization. Although The Intelligent Archive, KWIC, and WordSmith Tools produce exactly the same lists for this brief passage, and Voyant has the same number of types and tokens, Voyant removes all initial (but not final) apostrophes, creating different words for eight of the 97 types. Stylo removes all numbers, all initial and final apostrophes, and many internal apostrophes, retaining them only in 
                ain^t, gentleman^s, and 
                spen^s (replaced with a caret). It produces six more tokens and four more types than the other programs, and many more differences in the word list. Unfortunately, in Chesnutt’s short novel, more than 650 words begin and/or end with apostrophes crucial to the identity of the word, so that the word lists produced by Voyant and Stylo are quite inaccurate. Furthermore, only KWIC and WordSmith Tools let the user choose whether apostrophes and hyphens are part of a word, and whether numbers can appear in the word list or not. Only WordSmith Tools allows the user to choose whether to allow apostrophes at the beginnings and/or ends of the word as well as internally.
            
            Obviously, the two texts examined above cause different problems, and different tokenizers are more accurate for one than for the other. Worse yet, these problems are found even in relatively carefully edited texts like those from Project Gutenberg. Although Gutenberg’s 
                The House Behind the Cedars does not have spaced contractions, and correctly has 
                he’s in the first line and 
                you’ll in the final line, the 29 initial and final dialect apostrophes remain problematic. The Gutenberg text also represents dashes as two hyphens without spaces, creating more problems for tokenizers. The Intelligent Archive and Stylo treat these double-hyphen dashes as breaking characters, while retaining single hyphens within compound words, but KWIC, WordSmith Tools, and Voyant treat them like single hyphens, creating compounds with double hyphens where dashes are needed. The situation is still more complex if a double-hyphen is preceded or followed by a breaking character. If this sounds esoteric, consider that this short novel contains nearly 400 double-hyphen dashes (Dickens’ 
                Dombey and Son has more than 2,200). And this problem, too, is highly systematic: words vary considerably in how frequently they are preceded or followed by a dash, and 1,000 dash errors per text would produce 5,000,000 errors in our hypothetical 5,000 novels. (For a practical example of the effects of error, see Matt Jockers’ discussion of topic modeling and several ‘topics’ that arose from OCR error and metadata (Jockers, 2013, 135).
            
            It might seem that we just need more sophisticated tokenizers, but the required level of sophistication to handle double-hyphen dashes correctly is quite high, and the problems caused by apostrophes and single quotation marks cannot be correctly solved computationally at all. In some cases, not even a human reader can tokenize with certainty; in others, a computer can solve problems a human cannot. 
            Let’s consider a few further tokenization questions:
            He said, “That’s ’bout ‘nough, sho’.”
            “That’s ‘bout’, not ‘fight’; ’nough said,” Nough said.
            “John tried that ‘Nough told me to’ on me,” Bill whined.
            He remarked, “John said, ‘Bout starts at nine.’”
            He remarked, “John said, ‘It’s ’bout time.’”
            He remarked, “John said, ‘‘Bout time.’” Can these apostrophes/single quotes be handled correctly computationally? How about the two single quotes before ‘Bout’ in the last example?
            I visited the U.S.S.R. Four tokens? Seven? Is the final period part of the final token?
            I visited the U.S.S.R.! Four tokens? Seven? Is the final period part of the final token?
            Is that C------? Is ‘C------’ the token ‘C’ followed by a dash, or the token ‘C------’? What about ‘C—’? Or ‘C-’?
            C------ is here. Same questions.
            Oh d--n it! Is ‘d--n’ the tokens ‘d’ and ‘n’ separated by a dash, or the token ‘d--n’? How about ‘d---n’? or ‘d-n’? or ‘G-d’?
            I said--never mind. If ‘d--n’ is a token, can we prevent ‘said--never’ from being a token here?
            That’s what I--a mistake, sorry. How do we get ‘d--n’ correct without failing here?
            You’re a real %#@$! Three tokens? Four? Does the last include the final ‘!’? What if there were a period after the ‘!’?
            You’re a real %#@$!. How about now?
            I am working on a Python tokenizer that can handle most of these issues correctly, and some of these problems are fairly rare, but I despair of the possibility of creating a word frequency list that is ‘correct’ even in my own opinion. For many years I have ‘corrected’ the texts before tokenizing, but that is not a practical solution for 5,000 novels and presents its own problems.
            Perhaps in sufficiently big data, the error introduced by tokenizers will not significantly alter the results, and Maciej Eder (2013) has recently shown that some corpora are remarkably resistant to some kinds of intentionally introduced error. And improving the quality of the corpus had a relatively small effect on the attribution of the Federalist Papers (Levitan and Argamon, 2006). More study seems needed before we can be complacent, however, even in large-scale problems involving only authorship or classification. For smaller-scale stylistic studies, tokenization decisions can clearly have serious repercussions. Consider Ramsay’s (2011) analysis of 
                The Waves, where decisions about tokenization significantly alter the lists of men-only and women-only words and words that characterize the six narrative voices (see Hoover [2014a] and Plasek and Hoover [2014], for discussion). Another example that replicates an experience I have had several times is that a Full Spectrum analysis (Hoover, 2014b), based on Craig’s version of Burrows’s Zeta (Burrows, 2007; Craig and Kinney, 2010) can give strange results if uncorrected texts are inadvertently included. For example, in a test of Charlotte Brontë versus Anne and Emily Brontë, 11 of the 100 most distinctive words were words with inappropriate initial “apostrophes” because the novels of Anne and Emily in the analysis both used single quotation marks for dialogue.
            
            Far from being an insignificant tool that can be taken for granted, a tokenizer expresses its author’s theory of text and can significantly affect the results of many kinds of text analysis.
            Notes
            1. As a reviewer of this paper has pointed out, the problems of tokenization have been more widely recognized recently in the NLP community. For example, Dridan and Oepen (2012) and Chiarcos et al. (2012) address and suggest partial solutions for some of the problems discussed here. Even if the problems had all been solved within the NLP community (a fact not in evidence), however, this would not diminish the force of my argument for the DH community, where there has been much less attention paid to them.
            2. These programs represent a variety of those used in DH work (in order): a mature Java program with a database function, a venerable corpus linguistics program with lots of functions and user-options, a highly customizable and powerful commercial program from OUP, a widely used online tool, and a recently developed set of tools written in the currently popular R.
        
        
            
                
                    Bibliography
                    
                        Burrows, J. F. (2007). All the Way Through: Testing for Authorship in Different Frequency Strata. 
                        LLC,
                        22(1): 27–47.
                    
                    
                        Chesnutt, C. W. (1900). 
                        The House Behind the Cedars. Houghton Mifflin, Boston, http://babel.hathitrust.org/cgi/pt?view=plaintext;size=100;id=nc01.ark%3A%2F13960%2Ft7cr7221k;page=root;seq=25;num=13.
                    
                    
                        Chiarcos, C., Ritz, J. and Stede, M. (2012). By All These Lovely Tokens . . . : Merging Conflicting Tokenizations. 
                        Language Resources and Evaluation,
                        46(1): 53–74. 
                    
                    
                        Craig, H. and Kinney, A. F. (2010). 
                        Shakespeare, Computers, and the Mystery of Authorship. Cambridge University Press, Cambridge. 
                    
                    
                        Dridan, R., and Oepen, S. (2012). Tokenization: Returning to a Long Solved Problem: A Survey, Contrastive Experiment, Recommendations, and Toolkit. 
                        Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pp. 378–82.
                    
                    
                        Eder, M. (2013). Mind Your Corpus: Systematic Errors in Authorship Attribution. 
                        LLC,
                        28(4): 603–14.
                    
                    
                        Eder, M., Rybicki, J. and Kestemont, M. (2014). Stylo.
                    
                    
                        Gaskell, E. (1910 [1851]). 
                        Cranford. Houghton Mifflin, Boston, http://babel.hathitrust.org/cgi/pt?q1=twelve;id=hvd.32044097042071;view=plaintext;start=1;sz=10;page=root;size=100;seq=143;num=107.
                    
                    
                        Hoover, D. L. (2014a). Making Waves: Algorithmic Criticism Revisited. 
                        DH2014, Lausanne, Switzerland: EPFL-UNIL, pp. 202–4.
                    
                    
                        Hoover, D. L. (2014b). The Full-Spectrum Text-Analysis Spreadsheet. 
                        Digital Humanities 2013, Center for Digital Research in the Humanities, Lincoln, NE, University of Nebraska, pp. 226–29.
                    
                    
                        The Intelligent Archive. (2012). Centre for Literary and Linguistic Computing, University of Newcastle, Australia.
                    
                    
                        Jockers, M. L. (2013). 
                        Macroanalysis: Digital Methods and Literary History. University of Illinois Press, Urbana-Champaign.
                    
                    
                        Levitan, S. and Argamon, S. (2006). Fixing the Federalist: Correcting Results and Evaluating Editions for Automated Attribution. 
                        Digital Humanities 2006. Paris: Centre de Recherche Cultures Anglophones et Technologies de l’Information, pp. 323–26.
                    
                    
                        Plasek, A. and Hoover, D. L. (2014). Starting the Conversation: Literary Studies, Algorithmic Opacity, and Computer-Assisted Literary Insight. 
                        DH2014, Lausanne: EPFL-UNIL, pp. 305–6.
                    
                    
                        Ramsay, S. (2011). 
                        Reading Machines: Toward an Algorithmic Criticism. University of Illinois Press, Urbana.
                    
                    
                        Scott, M. (2012). WordSmith Tools version 6. Liverpool: Lexical Analysis Software.
                    
                    
                        Sinclair, S., Rockwell, G. and the Voyant Tools Team. (2012). Voyant Tools (web application).
                    
                    
                        Tsukamoto, S. (2004). KWIC Concordance for Windows version 4.7.
                    
                
            
        
    


        
            
                Introduction
                The development of applications in the field of Digital Humanities (DH) does not adequately take into account domain modelling, software design principles and software engineering methodologies (Bozzi, 2013; D'Iorio, 2015; McCarty, 2008; Terras and Crane, 2010). In fact, many systems developed in the context of DH-related projects have not been conceived to be modular, extensible, and scalable: they only tend to solve specific problems such as data-driven and project-oriented tools (Boschetti and Del Grosso, 2015). In addition, most projects focus on the requirements of humanists (as end users), but leave out the needs of software developers.
                This research was motivated by a number of issues emerged from the projects we worked on 
                     (Abrate et al., 2014a; Albanesi et al., 2015; Bellandi et al., 2014; Bozzi, 2015; Del Grosso, 2013; Ruimy et al., 2012) and it fits into an ongoing discussion about textual modelling and research infrastructures (Moulin et al., 2011; Pierazzo, 2015; Schmidt, 2014). In particular, this work aims at providing methodological guidelines for the definition of the core entities of a digital scholarly environment. We chose to adopt an object-oriented approach since it can bring benefits in the definition of efficient and effective digital tools (Boschetti et al., 2014; Del Grosso and Nahli, 2014). To give an analogy, the environment we propose can help developers and scholars as CMS (e.g. Wordpress) can help Web designers and publishers.
                
                The development of the environment follows three criteria: i) adopting an agile process (Ashmore and Runyan, 2014) to define the nature and behavior of the environment through both functional (e.g. user stories) and non-functional requirements (e.g. data model, system architecture) (Cohn, 2004; Collins-Cope et al., 2005); ii) providing well-defined Application Programming Interfaces (APIs) among components (Grill et al., 2012; Tulach, 2008); iii) applying analysis, architectural and design patterns for the sake of abstraction, generalization and flexibility (Ackerman and Gonzalez, 2011; Buschmann et al., 2007; Gamma et al., 1995).
                Following the agile methodology, we are developing a modular environment by starting from the design and implementation of a 
                    microkernel (Buschmann et al., 1996) as the manager of the different components. In addition, the microkernel provides all the operations needed to manipulate the domain basic entities which are described in the section “Domain entities and design patterns”.
                
                
                    Related works
                    Digital humanists have access to several tools for literary studies. TextGrid, for example, provides integrated tools for analyzing texts and gives computer support for digital editing purposes (Hedges et al., 2013). The NINES project offers an environment to support scholars in the creation of long-term digital research materials. It includes three main tools: Collex (Nowviskie, 2007), Juxta, and Ivanhoe. Annotation Studio is a collaborative system to annotate texts and add links to multimedia objects (Paradis et al., 2013). The CULTURA project aims at developing a “corpus agnostic research environment” providing customizable services for a wide range of users (Steiner et al., 2014). The development of an online workspace which helps scholars in the production of critical editions is the main objective of the Workspace for Collaborative Editing framework (Houghton et al., 2014). It uses existing standards and open-source solutions to create an architecture of reusable components. Other platforms worth mentioning are TUSTEP/TXSTEP (Ott, 2000; Ott and Ott, 2014), WebLicht (Hinrichs et al., 2010), Perseids (Almas and Beaulieu, 2013), Muruca/Pundit (Grassi et al., 2013), Textual Communities (Bordalejo and Robinson, 2015), SAWS (Jordanous et al., 2012), Voyant Tools (Sinclair and Rockwell, 2012), Transcribe Bentham (Causer and Terras, 2014) and Alcide (Moretti et al., 2014). However, the aforementioned initiatives allow digital scholars to meet specific needs, but none of them seems to provide, simultaneously, all the following characteristics: i) reusability and extensibility, ii) ease of use and configuration, iii) continuous availability of the services and development over time, iv) a well-grounded software data model.
                
            
            
                Domain entities and design patterns
                One of the main challenges of the DH community is to provide suitable software models and tools (Ciotti, 2014). To model the literary domain and the relative user requirements, we chose to follow the engineering principles of 
                    object-oriented analysis and design (Ackerman and Gonzalez, 2011). The digital representation of a textual resource is a challenge as it involves several theoretical and epistemological issues in semiotics, paleography, philology, linguistics, engineering, and computer science (McCarty, 2005; Meister, 2012; Moretti, 2013; Robinson, 2013; Sahle, 2013).
                
                In this work, we define each textual element by means of four properties: i) the 
                    version allows to select a specific textual element among those available in its history of changes; ii) the 
                    granularity represents a level of a hierarchical structure (e.g. a page composed of lines); iii) the 
                    position provides the location of a textual element within the hierarchical representation (e.g. the second page of a book); iv) the 
                    layer indicates the set of homogeneous information the textual element belongs to (e.g. morphological layer). As pointed outby (Buzzetti, 2002; McGann, 2004; Pierazzo, 2015), the information conveyed by a textual resource is logically organized through multiple layers (also called 
                    dimensions) of information.
                
                
                    
                        
                        Fig. 1: Class diagram of the domain entities
                    On these four properties we have designed and implemented a set of core entities as the fundamental data types shared among all the components of the environment (Fig. 1)
                     The ongoing implementation of the environment is available at: https://github.com/literarycomputinglab. The 
                    Source class is in charge of managing the low-level data: it is composed of a 
                    Payload representing the information conveyed by the textual resource and a
                    SourceType which indicates the nature of the Source (e.g. text, image, audio, etc.). Payload objects (as used in networking) have the only purpose of encapsulating the information. The 
                    Locus and the 
                    P
                    lace
                    OfInterest (POI) classes identify, through a 
                    composition pattern, specific data fragments of the source content, and they are used to establish the boundaries of an 
                    Annotation. A chunk of text, for example, can be addressed to by a locus having a POI (of type Sequence of Interest) representing its start and end coordinates. Similarly, a region of an image can be identified by a locus having a POI (of type Region of Interest) composed of a sequence of coordinates. The Locus and POI provide a stand-off text annotation technique able to tackle, for example, the overlapping hierarchies problem, which cannot be handled easily with inline markup techniques (Schmidt, 2010). As a matter of fact, it is possible, simultaneously, to manipulate a resource on the basis of its documental and textual structure (Renear et al., 1996; Robinson, 2013) (see the example in the following section). However, since stand-off models are affected by the issue of the indexing updating, a dedicated component must be in charge of automatically maintaining the coherence of the annotations each time the underlying text is edited.
                
                An Annotation represents an information associated to a locus and is defined by an 
                    AnnotationType (e.g. a token, a lemma, a named entity, etc.). Since the hierarchical structure of the source may evolve over time, the changes to the relative tree must be managed. For example, a tree structure having tokens as leaves could need to be updated with a finer-grained layer of characters (e.g. to assign annotations to specific letters). In this case, the tokens should become intermediate nodes and the characters would become the leaf nodes. Typically, this kind of editing is unpredictable and it often implies heavy adaptations if the software is not flexible enough to manage changes in the underlying text representation schema. Consequently, we decided to exploit the flexibility of the Object Oriented model by adopting the Role Design Pattern (Fowler, 1997) to switch between leaf and intermediate nodes dynamically. This pattern has been implemented by the 
                    AnnotationRole, 
                    AnnotationRoleElement and 
                    AnnotationRoleStructure classes. Moreover, an annotation is a source in itself (see the inheritance relationship between the Annotation and the Source classes in Fig. 1) and, thus, it can be annotated recursively.
                
            
            
                An Example
                We here introduce an example showing a representation of a snippet of text with annotations. The chosen text is an excerpt of a letter, written in Latin, belonging to the epistolary corpus of the Clavius on the Web project
                     Clavius on the Web is a project funded by Registro.it and partecipated by the Institute of Informatics and Telematics (IIT-CNR), the Institute of Computational Linguistics “A. Zampolli” (ILC-CNR), and the Historical Archives of the Pontifical Gregorian University (APUG). Website: http://claviusontheweb.it/ (Abrate et al., 2014b). Fig. 2 shows a typical way of encoding sentences and lines with a markup language as TEI (Burnard, 2014): the resulting XML hierarchical structure has been broken by the addition of the line anchors () mixing up the textual and documental structure of the text. Indeed, to preserve the integrity of the word “Dinostrati” (spanning across lines 4 and 5), it is necessary to encapsulate it with the element .
                
                
                    
                        
                        Fig. 2: A standard way of encoding a text with TEI-XML
                    
                    
                        
                        Fig. 3: Multi-layered stand-off annotation of text
                    The model we propose solves this problem with the stand-off annotations: as shown in Fig. 3 the document (made of lines) and the textual structure (made of sentences and words) are logically separated. Lines, sentences and words do not overlap and they are structured in separate hierarchies.
                
            
            
                Next Steps
                We plan, in future works, to release a first version of a web environment, called 
                    Omega, built around the core entities that we here described. The environment will allow to load, index, annotate, and query a textual collection. Furthermore, we’ll carry on the development of modules for text analysis and textual scholarship with the related APIs.
                
            
        
        
            
                
                    Bibliography
                    
                        Abrate, M., 
                        Del 
                        Grosso, A. M., Giovannetti, E., 
                        Lo 
                        Duca, A., Luzzi, D., Mancini, L., Marchetti, A., Pedretti, I. and Piccini, S. (2014a). Sharing Cultural Heritage: the Clavius on the Web Project. In Calzolari, N., Choukri, K., Declerck, T., Loftsson, H., Maegaard, B., Mariani, J., Moreno, A., Odijk, J. and Piperidis, S. (eds), 
                        Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC), Reykjavik. European Language Resources Association (ELRA), pp. 627–34.
                    
                    
                        Abrate, M., 
                        Del 
                        Grosso,
                         
                        A. M., Giovannetti, E., 
                        Lo 
                        Duca, A., Marchetti, A., Mancini, L., Pedretti, I. and Piccini, S. (2014b). Il Progetto Clavius on the Web: tecnologie linguistico-semantiche al servizio del patrimonio documentale e degli archivi storici. In Rossi, F. and Tomasi, F. (eds), 
                        Book of Abstracts of 3
                        o
                         AIUCD Conference, Bologna.
                    
                    
                        Ackerman, L. and Gonzalez, C. (2011). 
                        Patterns-Based Engineering: Successfully Delivering Solutions Via Patterns. Addison-Wesley.
                    
                    
                        Albanesi, D., Bellandi, A., Benotto, G., 
                        Di 
                        Segni, G. and Giovannetti, E. (2015). When Translation Requires Interpretation: Collaborative Computer–Assisted Translation of Ancient Texts. 
                        LaTeCH 2015: 84–88.
                    
                    
                        Almas, B. and Beaulieu, M.-C. (2013). Developing a New Integrated Editing Platform for Source Documents in Classics. 
                        Literary and Linguistic Computing, 
                        28(4): 493–503 doi:10.1093/llc/fqt046.
                    
                    
                        Ashmore, S. and Runyan, K. (2014). 
                        Introduction to Agile Methods. Upper Saddle River, NJ: Addison-Wesley Professional, Pearson Education.
                    
                    
                        Bellandi, A., Albanesi, D., Bellusci, A., Bozzi, A. and Giovannetti, E. (2014). The Talmud System: a Collaborative web Application for the Translation of the Babylonian Talmud Into Italian. 
                        The First Italian Conference on Computational Linguistics CLiC-It 2014, pp. 53–57.
                    
                    
                        Bordalejo, B. and Robinson, P. (2015). A new system for collaborative online creation of Scholarly Editions in digital form. 
                        1st Dixit Convension on Technology, Software, Standards for the Digital Scholarly Edition Workshop. The Hague.
                    
                    
                        Boschetti, F. and 
                        Del 
                        Grosso, A. M. (2015). TeiCoPhiLib: A Library of Components for the Domain of Collaborative Philology. 
                        Journal of the Text Encoding Initiative(8). doi:10.4000/jtei.1285. http://jtei.revues.org/1285 (accessed 3 March 2016).
                    
                    
                        Boschetti, F., 
                        Del 
                        Grosso, A. M., Khan, A. F., Lamé, M. and Nahli, O. (2014). A top-down approach to the design of components for the philological domain. 
                        Book of Abstract of Digital Humanities Conference (DH), Lausanne, Switzerland. Alliance of Digital Humanities Organisations, pp. 109–11.
                    
                    
                        Bozzi, A. (2013). G2A: A Web application to study, annotate and scholarly edit ancient texts and their aligned translations. (Ed.) ERC Ideas 249431 
                        Studia Graeco-Arabica, 
                        3: 159–71.
                    
                    
                        Bozzi, A. (2015). Greek into Arabic, a research Infrascructure based on computational modules to annotate and query historical and philosophical digital texts. Part I: Methodological aspects. In Bozzi, A. (ed), 
                        Digital Texts, Translations, Lexicons in a Multi-Modular Web Application: Methods and Samples. Firenze: Leo S. Olschki editore, pp. 27–42.
                    
                    
                        Burnard, L. (2014). TEI P5: Guidelines for Electronic Text Encoding and Interchange. Version 2.9.1. http://www.tei-c.org/Guidelines/P5/index.xml (accessed 3 March 2016).
                    
                    
                        Buschmann, F., Henney, K. and Schmidt, D. C. (2007). 
                        Pattern-Oriented Software Architecture, On Patterns and Pattern Languages. (Pattern-Oriented Software Architecture). Hoboken: John Wiley & Sons.
                    
                    
                        Buschmann, F., Meunier, R., Rohnert, H., Sommerlad, P. and Stal, M. (1996). Pattern-oriented Software Architecture - A System of Patterns. J. Wiley and Sons Ltd., pp. 171–92.
                    
                    
                        Buzzetti, D. (2002). Digital Representation and the Text Model. 
                        New Literary History, 
                        33(1): 61–88.
                    
                    
                        Causer, T. and Terras, M. (2014). “Many hands make light work. Many hands together make merry work”: Transcribe Bentham and crowdsourcing manuscript collections.
                    
                    
                        Ciotti, F. (2014). Digital Literary and Cultural Studies: State of the Art and Perspectives. 
                        Between, 
                        4(8). doi:10.13125/2039-6597/1392. http://dx.doi.org/10.13125/2039-6597/1392 (accessed 3 March 2016).
                    
                    
                        Cohn, M. (2004). 
                        User Stories Applied: For Agile Software Development. Redwood City, CA, USA: Addison Wesley Longman Publishing Co., Inc.
                    
                    
                        Collins-Cope, M., Rosenberg, D. and Stephens, M. (2005). 
                        Agile Development with ICONIX Process: People, Process, and Pragmatism. Berkely, CA, USA: Apress.
                    
                    
                        Fowler, M. (1997). Dealing with roles. 
                        Proceedings of the International Conference on Pattern Languages of Programs, vol. 97, pp. 13–37.
                    
                    
                        Gamma, E., Helm, R., Johnson, R. and Vlissides, J. (1995). 
                        Design Patterns: Elements of Reusable Object-Oriented Software. Boston, MA, USA: Addison-Wesley Longman Publishing Co., Inc.
                    
                    
                        Grassi, M., Morbidoni, C., Nucci, M., Fonda, S. and Piazza, F. (2013). Pundit: augmenting web contents with semantics. 
                        Literary and Linguistic Computing, 
                        28(4): 640–59.
                    
                    
                        Grill, T., Polacek, O. and Tscheligi, M. (October 29-312012). Methods Towards API Usability: A Structural Analysis of Usability Problem Categories. 
                        Proceedings of the 4th International Conference on Human-Centered Software Engineering, Toulouse, France. Berlin, Heidelberg: Springer-Verlag, pp. 164–80. doi:10.1007/978-3-642-34347-6_10.
                    
                    
                        Del 
                        Grosso, A. M. (2013). Indexing techniques and variant readings management. (Ed.) D'Ancona, C. 
                        Studia Graeco-Arabica, 
                        3: 209–30.
                    
                    
                        Del 
                        Grosso, A. M. and Nahli, O. (2014). Towards a flexible open-source software library for multi-layered scholarly textual studies: An Arabic case study dealing with semi-automatic language processing. 
                        Proceedings of 3rd IEEE International Colloquium, Information Science and Technology (CIST), Tetouan, Marocco. Washington, DC, USA: IEEE, pp. 285–90. doi:10.1109/CIST.2014.7016633.
                    
                    
                        Hedges, M., Neuroth, H., Smith, K. M., Blanke, T., Romary, L., Küster, M. and Illingworth, M. (2013). TextGrid, TEXTvre, and DARIAH: Sustainability of Infrastructures for Textual Scholarship. 
                        Journal of the Text Encoding Initiative(5). doi:10.4000/jtei.774 (accessed 3 March 2016).
                    
                    
                        Hinrichs, E., Hinrichs, M. and Zastrow, T. (2010). WebLicht: Web-based LRT services for German. 
                        Proceedings of the ACL 2010 System Demonstrations. Association for Computational Linguistics, pp. 25–29.
                    
                    
                        Houghton, H., Sievers, M. and Smith, Catherine (2014). The Workspace for Collaborative Editing. 
                        Digital Humanities 2014. Laussanne: Alliance of Digital Humanities Organisations, pp. 204–05.
                    
                    
                        D'
                        Iorio, P.  (2015). On the scholarly use of the Internet, a conceptual model. In Bozzi, A. (ed), 
                        Digital Texts, Translations, Lexicons in a Multi-Modular Web Application: Methods and Samples. Firenze: Leo S. Olschki editore, pp. 1–25.
                    
                    
                        Jordanous, A., Lawrence, K. F., Hedges, M. and Tupman, C. (June 13-152012). Exploring Manuscripts: Sharing Ancient Wisdoms Across the Semantic Web. 
                        Proceedings of the 2nd International Conference on Web Intelligence, Mining and Semantics (WIMS), Craiova, Romania. New York, NY, USA: ACM, pp. 44:1–44:12. doi:10.1145/2254129.2254184.
                    
                    
                        McCarty, W. (2005). 
                        Humanities Computing. Palgrave Macmillan.
                    
                    
                        McCarty, W. (2008). Signs of times present and future. 
                        Human Discussion Group, 
                        22(218).
                    
                    
                        McGann, J. (2004). Marking Texts of Many Dimensions. In Schreibman, S., Siemens, R. and Unsworth, J. (eds), 
                        A Companion to Digital Humanities. (Blackwell Companions to Literature and Culture). Blackwell Publishing Ltd, pp. 198–217.
                    
                    
                        Meister, J. C. (2012). DH is us or on the unbearable lightness of a shared methodology. 
                        Historical Social Research, 
                        37(3): 77–85.
                    
                    
                        Moretti, F. (2013). 
                        Distant Reading. Verso Books.
                    
                    
                        Moretti, G., Tonelli, S., Menini, S. and Sprugnoli, R. (2014). ALCIDE: An online platform for the Analysis of Language and Content In a Digital Environment. 
                        Proceedings of the First Italian Conference on Computational Linguistics (CLIC-2014). Pisa, Italy.
                    
                    
                        Moulin, C., Nyhan, J., Ciula, A., Kelleher, M., Mittler, E., Tadić, M., Ågren, M., Bozzi, A. and Kuutma, K. (2011). 
                        Research Infrastructures in the Digital Humanities. http://www.esf.org/hosting-experts/scientific-review-groups/humanities-hum/strategic-activities/research-infrastructures-in-the-humanities.html.
                    
                    
                        Nowviskie, B. (2007). Collex: Facets, Folksonomy, and Fashioning the Remixable web. 
                        Book of Abstract of Digital Humanities Conference (DH), University of Illinois at Urbana-Champaign. Alliance of Digital Humanities Organisations.
                    
                    
                        Ott, W. (2000). Strategies and tools for textual scholarship: the Tübingen system of text processing programs (TUSTEP). 
                        Literary and Linguistic Computing, 
                        15(1): 93–108. doi:10.1093/llc/15.1.93. http://llc.oxfordjournals.org/content/15/1/93.abstract.
                    
                    
                        Ott, W. and Ott, T. (2014). Critical Editing with TXSTEP. In Terras, M. (ed), 
                        Book of Abstracts of the Digital Humanities Conference, Lausanne, Switzerland. Alliance of Digital Humanities Organisations, pp. 509–13.
                    
                    
                        Paradis, J., Fendt, K., Kelley, W., Folsom, J., Pankow, J., Graham, E. and Subbaraj, L. (2013). Annotation Studio: Bringing a Time-Honored Learning Practice into the Digital Age. 
                        Whitepaper. http://cmsw.mit.edu/annotation-studio-whitepaper/ (accessed 3 March 2016).
                    
                    
                        Pierazzo, E. (2015). 
                        Digital Scholarly Editing : Theories, Models and Methods. Farnham Surrey: Ashgate.
                    
                    
                        Renear, A. H., Mylonas, E. and Durand, D. (1996). Refining our notion of what text really is: The problem of overlapping hierarchies. (Ed.) Hockey, S. M. 
                        Research in Humanities Computing, 
                        4: 263–80.
                    
                    
                        Robinson, P. (2013). Towards a theory of digital editions. (Ed.) Mierlo, W. V. and Fachard, A. 
                        Variants, (10): 105–31.
                    
                    
                        Ruimy, N., Piccini, S. and Giovannetti, E. (2012). Defining and Structuring Saussure’s Terminology. In Fjeld, R. V. and Torjusen, J. M. (eds), 
                        Proceedings of 15th EURALEX International Congress. Oslo, Norway, Department of Linguistics and Scandinavian Studies, University of Oslo, Reprosentralen: UiO press, pp. 828–33.
                    
                    
                        Sahle, P. (2013). 
                        Digitale Editionsformen: Teil 3: Textbegriffe Und Recodierung; Zum Umgang Mit Der Überlieferung Unter Den Bedingungen Des Medienwandels. Vol. 3. BoD–Books on Demand.
                    
                    
                        Schmidt, D. (2010). The inadequacy of embedded markup for cultural heritage texts. 
                        Literary and Linguistic Computing, 
                        25(3): 337–56. doi:10.1093/llc/fqq007.
                    
                    
                        Schmidt, D. (2014). Towards an Interoperable Digital Scholarly Edition. 
                        Journal of the Text Encoding Initiative(7). doi:10.4000/jtei.979.
                    
                    
                        Sinclair, S. and Rockwell, G. (2012). the Voyant Tools Team (web application) 
                        Voyant Tools. http://voyant-tools.org (accessed 3 March 2016).
                    
                    
                        Steiner, C., Agosti, M., Sweetnam, M., Hillemann, E.-C., Orio, N., Ponchia, C., Hampson, C., et al. (2014). Evaluating a digital humanities research environment: the CULTURA approach. 
                        International Journal on Digital Libraries, 
                        15(1): 53–70. doi:10.1007/s00799-014-0127-x.
                    
                    
                        Terras, M. and Crane, G. (eds). (2010). 
                        Changing the Center of Gravity: Transforming Classical Studies through Cyberinfrastructure. Piscataway: Gorgias Press.
                    
                    
                        Tulach, J. (2008). 
                        Practical API Design: Confessions of a Java Framework Architect. 1st ed. Berkely, CA, USA: Apress.
                    
                
            
        
    


        
            
                Motivation
                In last years, the advancements in computer science brought a global change in the way information is stored, retrieved and analyzed. The digital humanities also benefit from these developments, and now, a vast amount of texts is available in digital form. This information explosion generates interesting research questions for humanities scholars who are capable of deriving new insights from this knowledge bank. In order to support humanities scholars, many visualization techniques – summarized in a survey (Jänicke et al., 2015b) – were developed to aid exploring large texts collections. Most of these techniques are interactive and belong to the category of distant reading (Moretti, 2005). The authors of the mentioned survey observe that less work has been done to improve the close reading capabilities of humanities scholars even though they are often focused on close reading text passages.
                Close reading is the careful interpretation of the text, where the scholar iteratively reads the text in order to explore its meaning, inherent topics and occurring relationships (Boyles, 2013). Traditionally, close reading is done on paper. Several ideas and thoughts are made persistent by annotations written at the margins alongside the text (see Figure 1). But as the margin space is limited, not all observations can be put around the text. So, annotations may become cluttered and confusing for the reader, especially, when obsolete ideas are struck through. Despite its disadvantages, annotating on paper is still quite popular as it benefits the scholars to record observations about the hypothesis and all these changes reappear in front of the scholar’s eyes as soon as he re-reads the text passage. We observed that the way of annotating in close reading resembles the idea of mind maps (Buzan et al., 1993) that are based on a central concept and thoughts are represented around it using lines and text. In the close reading scenario, the text can be considered as the central concept and annotations represent thoughts.
                An important task of computer science is to enhance the original workflows of researchers with computational methods. As most humanities scholars are well trained in close reading and nowadays often work with digital texts, it is necessary to enhance their capabilities for digital close reading. We propose an enhanced close reading design inspired by mind-maps that not only mimics the traditional way of annotating a text on paper, but also helps humanities scholars to perform live visual analyses. Furthermore, we use extendible margins to provide enough space for all thoughts of the scholar.
                
                    
                    
                        
                            
                            Figure 1: Traditional close reading on paper
                        
                    
                    
                        
                            
                            Image reproduced with permission from Kehoe (Kehoe et al., 2013)
                        
                    
                
            
            
                Related Work
                Nancy Boyles (Boyles, 2013) defines close reading, which has become a fundamental method in literary criticism in the 20th century (Hawthorn, 2000), as follows: “Essentially, close reading means reading to uncover layers of meaning that lead to deep comprehension.” Annotating the text in close reading is a strong method for scholars to facilitate the understanding of a text passage. Figure 1 shows the result of a traditional close reading approach. In this example, various annotation methods were used by the scholar to annotate various features of a text passage in Charles Dickens' „David Copperfield“. 
                The availability of digital texts has further awaken the interest of humanities scholars in collaboratively close reading the same texts. There are several annotation tools for such a purpose, such as eMargin (Kehoe et al., 2013), Hypothes.is (Bonn et al., 2014) and NB (Zyto et al., 2012). These tools are beneficial for collaborative research and classroom environments as they provide an excellent paradigm to share thoughts, as well as find collective answers. To avoid clutter, these tools work with popup windows that are only shown on demand. In Figure 2, the eMargin system is shown where colors are used to highlight different text features, and a popup window on demand, lists the comments of collaborating scholars.
                
                    
                    
                        
                            
                            Figure 2: eMargin annotation tool
                        
                    
                    
                        
                            
                            Image reproduced with permission from Kehoe (Kehoe et al., 2013)
                        
                    
                
                Digital Ink Annotations systems (Schilit, 1998, Bargeron et al., 2003, Agrawala et al., 2005, Yoon et al., 2013) also support annotating text, but their use is only limited to pen-based computing devices such as tablets. The systems are designed to work well on smaller screens, and the adaption to larger screens is not appropriately implemented. 
                Close reading tasks can also be assisted via distant reading tools. For example, parallel coordinates, a heatmap and a dot plot are used to analyze the variance of a selected text passage from different German translations of Shakespeare’s Othello (Geng et al., 2013). Heat maps are appropriate visualizations to illustrate the distribution of specific phrases or annotations in a corpus (Muralidharan, 2011, Alex et al., 2015). Voyant Tools allow the user to perform basic text mining functions with selected word statistics shown in linked views (Sinclair et al., 2012). The Voyant Tools interface in Figure 3 shows statistics about Chapter 2 of Oscar Wilde's “David Copperfield”. Goffin's idea to enhance close reading is the integration of small visualizations (e.g., maps or bar charts) besides the words of a text (Goffin et al., 2014).
                
                    
                        
                        Figure 3: Screenshot of web-based Voyant Tools (Sinclair et al., 2012).
                    
                
            
            
                Enhanced Close Reading Design
                In contrast to the tools mentioned above, we combine traditional annotation tasks with distant reading analyses to enhance the close reading capabilities of the scholar. We suggest a design inspired by mind mapping (an example mind map is shown in Figure 4a), a methodology that allows a researcher to work on a central concept, and thoughts and features about that concept are placed around it using figures, lines etc. In a mind map, the associations spread out from a central concept in a free-flowing, yet organized and coherent manner (Budd, 2004) - thus forming a mental map of the central concept. We observe that like in the case of mind maps, fixed annotations around the central text in a traditional close reading process facilitate forming a mental map of the thoughts about the text of interest, and help the scholar to draw conclusions when seeing the whole picture.
                
                    
                        
                            
                                
                                Figure 4a: An example mind map
                            
                             Image reproduced with permission from Kanter (Kanter, 2015) (Figure under CC BY 2.0 license, see 
                                 for details).
                            
                        
                        
                            
                                
                                Figure 4b: Mind-map inspired close reading
                            
                        
                    
                
                Figure 4b illustrates the idea of a mind map inspired interface with multiple types of annotations supporting the scholar in the close reading process. Textual annotations known from the traditional close reading are also necessary in the digital process. In addition, images, videos and charts can facilitate text interpretation and the generation of valuable hypotheses about the text. To support dynamic, multifarious views on a certain text passage or a term of interest, we designed our interface the way that the literary scholar can apply a multitude of visual analyses and generate distant reading visualizations that are placed as annotations alongside the text. This combines the traditional close reading paradigm with elaborated text visualization techniques valuable for exploration purposes. An important feature of our proposed interface design is to support the scholar to „stay in the flow“ (Bederson, 2004), so that the central focus remains on the text, which can be analyzed without interrupting the scholar. The major advantage of our design over existing tools that assist close reading tasks is interface versatility. For example, Voyant Tools (see Figure 3) provide a predefined set of visualizations based on text statistics. On the other hand, our design allows the scholar to choose an appropriate text visualization as an annotation alongside the text, which is based on a user-defined query on the text.. Therefore, the scholar can apply different text visualizations for different passages of the text to support a variety of close reading tasks. 
                An example of the design discussed above is shown in Figure 5. The example from Figure 1 is annotated using different kinds of annotations. Like in other digital tools, certain topics of the text are annotated using colors. In addition, the character(s) Peggotty is marked and a panel shows thumbnail images based on a Google Images search. Also the relative word frequency chart of the term “Peggotty” in Chapter 2 is shown on the bottom left. Furthermore, on the left area, a TagPie (Jänicke et al., 2015a) showing the co-occurrences of both the terms memory and observation helps to investigate the hypothesis of the literary scholar about the similar meaning of both topics. The example depicts how the scholar can use different annotation tools as well as different distant reading tools to enrich the close reading experience.
                
                    
                        
                        Figure 5: Example of our design
                    
                
            
            
                Future Work and Conclusions
                We held discussion with the collaborating humanities scholars about the design as well as the usability of the proposed interface. The scholars remarked that such an interface will help removing fears of using digital humanities tools and that they intend to use the tool as it mimics their existing workflows. They also mentioned that such a tool could help users getting a better big picture of the text, and that it enhances the close reading capabilities of the scholar. Another important point is the capability in supporting teaching activities. They mentioned that various types of annotations (text, pictures, charts) are also used in teaching material, but it is not easy to share these with students. Such a tool could support this process as it generates persistent annotations to be analyzed and discussed collaboratively in courses. 
                We observe that the scholar’s initial reactions after seeing the prototype of the tool, which is still in development, are convincing and encouraging. We think that rigid modeling syntax is inappropriate for annotation. Our final interface will allow the scholar to make annotation styles versatile. At the digital humanities conference, we will demonstrate our prototype and discuss future prospects within the community. An additional user study will compare the viability of our proposed, mind map inspired annotation technique to existing approaches.
            
            
                Acknowledgements
                We thank our colleagues from the humanities department, Judith Blumenstein in particular, who provided insights and expertise that greatly assisted this research.
            
        
        
            
                
                    Bibliography
                    
                        Agrawala, M. and Shilman, M. (2005). DIZI: a digital ink zooming interface for document annotation. Human-Computer Interaction-INTERACT 2005, Springer Berlin Heidelberg, pp. 69-79.
                    
                    
                        Alex, B., Grover, C., Zhou, K., Hinrichs and Palimpsest, U. (2015). Improving Assisted Curation of Loco-specific Literature. Proceedings of the Digital Humanities 2015, pp. 5-7.
                    
                    
                        Bargeron, D. and Moscovich, T. (2003). Reflowing digital ink annotations. Proceedings of the SIGCHI conference on Human factors in computing systems, ACM, pp. 385-93.
                    
                    
                        Bederson, B. B. (2004). Interfaces for staying in the flow. Ubiquity, 1-1.
                    
                    
                        Bonn, M. and McGlone, J. (2014). New Feature: Article Annotation with Hypothesis. Journal of Electronic Publishing, 17(2).
                    
                    
                        Boyles, N. (2013). Closing in on Close Reading. Educational Leadership, 70(4): 36–41.
                    
                    
                        Budd, J. W. (2004). Mind Maps as Classroom Exercises. The Journal of Economic Education, 35(1): 35–46.
                    
                    
                        Buzan, T. and Buzan, B. (1993). The Mind Map Book How to Use Radiant Thinking to Maximise Your Brain's Untapped Potential. New York: Plume.
                    
                    
                        Geng, Z., Cheesman, T., Laramee, R. S., Flanagan, K. and Thiel, S. (2013). ShakerVis: Visual analysis of segment variation of German translations of Shakespeare’s Othello. Information Visualization, 15: 93-116.
                    
                    
                        Goffin, P., Willett, W., Fekete, J. D. and Isenberg, P. (2014). Exploring the placement and design of word-scale visualizations. Visualization and Computer Graphics, IEEE Transactions, 20(12): 2291-300.
                    
                    
                        Hawthorn, J. (2000). A glossary of contemporary literary theory. Oxford University Press.
                    
                    
                        Jänicke, S., Blumenstein, J., Rücker, M., Zeckzer, D. and Scheuermann, G. (2015a). Visualizing the Results of Search Queries on Ancient Text Corpora with Tag Pies. Digital Humanities Quarterly.
                    
                    
                        Jänicke, S., Franzini, G., Cheema, M. F. and Scheuermann, G. (2015b). On Close and Distant Reading in Digital Humanities: A Survey and Future Challenges. In Borgo, R., Ganovelli, F., and Viola, I. (eds.), Eurographics Conference on Visualization (EuroVis) - STARs (2015), The Eurographics Association.
                    
                    
                        Kanter, B. (2015). Cambodia4kids.org, https://www.flickr.com/photos/cambodia4kidsorg/6195211411 (Retrieved 2015-11-25).
                    
                    
                        Kehoe, A. and Gee, M. (2013). eMargin: A Collaborative Textual Annotation Tool. Ariadne, 71.
                    
                    
                        McCabe, M. M. (2015). Platonic Conversations. Oxford University Press.
                    
                    
                        Moretti, F. (2005). Graphs, Maps, Trees: Abstract Models for a Literary History. New York: Verso.
                    
                    
                        Muralidharan, A. (2011). A Visual Interface for Exploring Language Use in Slave Narratives. Proceedings of the Digital Humanities 2011.
                    
                    
                        Schilit, B. N., Golovchinsky, G. and Price, M. N. (1998). Beyond paper: supporting active reading with free form digital ink annotations. Proceedings of the SIGCHI conference on Human factors in computing systems, ACM Press/Addison-Wesley Publishing Co., pp. 249-56.
                    
                    
                        Sinclair, S. and Rockwell, G. (2012). Voyant Tools. Online: http://voyant-tools.org (Retrieved 2015-11-25).
                    
                    
                        Yoon, D., Chen, N. and Guimbretière, F. (2013). TextTearing: Opening white space for digital ink annotation. Proceedings of the 26th annual ACM symposium on User interface software and technology, ACM, pp. 107-12. 
                    
                    
                        Zyto, S., Karger, D., Ackerman, M. and Mahajan, S. (2012). Successful classroom deployment of a social document annotation system. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM, pp. 1883-92.
                    
                
            
        
    


        
            
                Introduction
                Can we find and track theory, especially literary theory, in very large collections of texts using computers? This panel discusses a pragmatic two-step approach to trying to track and then visually explore theory through its textual traces in large collections like those of the HathiTrust.
                
                    
                        Subsetting: The first problem we will discuss is how to extract thematic subsets of texts from very large collections like those of the HathiTrust. We experimented with two methods for identifying “theoretical” subsets of texts from large collections, using keyword lists and machine learning. The first two panel presentations will look at developing two different types of theoretical keyword lists. The third presentation will discuss a machine learning approach to extracting the same sorts of subsets.
                    
                    
                        Topic Modelling: The second problem we tackled was what to do with such subsets, especially since they are likely to still be too large for conventional text analysis tools like Voyant (voyant-tools.org) and users will want to explore the results to understand what they got. The fourth panel presentation will therefore discuss how the HathiTrust Research Center (HTRC) adapted Topic Modelling tools to work on large collections to help exploring subsets. The fifth panel talk will then show an adapted visualization tool, the Galaxy Viewer, that allows one to explore the results of Topic Modelling. 
                    
                
                The panel brings together a team of researchers who are part of the “Text Mining the Novel” (TMN) project that is funded by the Social Sciences and Humanities Research Council of Canada (SSHRC) and led by Andrew Piper at McGill University. Text Mining the Novel (novel-tm.ca) is a multi-year and multi-university cross-cultural study looking at the use of quantitative methods in the study of literature, with the HathiTrust Research Center is a project partner.
                The issue of how to extract thematic subsets from very large corpora such as the HathiTrust is a problem common to many projects that want to use diachronic collections to study the history of ideas or other phenomena. To conclude the panel, a summary reflective presentation will discuss the support the HTRC offers to DH researchers and how the HTRC notion of “worksets” can help with the challenges posed by creating useful subsets. It will further show how the techniques developed in this project can be used by the HTRC to help other future scholarly investigations.
            
            
                Using Word Lists to Subset
                Geoffrey Rockwell (Kevin Schenk, Zachary Palmer, Robert Budac and Boris Capitanu)
                How can one extract subsets from a corpus without appropriate metadata? Extracting subsets is a problem particular to very large corpora like those kept by the HathiTrust (www.hathitrust.org/). Such collections are too large to be manually curated and their metadata is of limited use in many cases. And yet, one needs ways to classify all the texts in a collection in order to extract subsets if one wants to study particular themes, genres or types of works. In our case we wanted to extract theoretical works which for the purpose of this project we defined as Philosophical works or Literary Critical works. In this first panel presentation we will discuss the use of keyword lists as a way of identifying a subset of “philosophical” texts.
                Why philosophical? We choose to experiment extracting philosophical texts first as philosophy is a discipline with a long history and a vocabulary that we hypothesized would lend itself to a keyword approach. Unlike more recent theoretical traditions, philosophical words might allow us to extract works from the HathiTrust going back thousands of years.
                
                    Keywords. For the first part of this project we adapted a list of philosophical keywords from the Indiana Philosophy Ontology Project (inpho.cogs.indiana.edu/). Our adapted list has 4,437 words and names starting with "abauzit", "abbagnano", "abdolkarim", "abduction", "abduh", "abel", and so on. There are a number of ways of generating such lists of keywords or features, in our case we were able to start with a very large curated list. The second paper in this panel discusses generating a list of literary critical keywords.
                
                
                    Process. We used this list with a process we wrote in Python that calculates the relative frequency of each word in a text and does this over a collection. The process also calculates the sum of the relative frequencies giving us a simple measurement of the use of philosophical keywords in a text. The word frequency process generates a CSV with the titles, author, frequency sum and individual keyword frequencies which can be checked and manipulated in Excel.
                
                
                    Testing. We iteratively tested this keyword approach on larger and larger collections. First we gathered a collection of 20 philosophical and 20 non-philosophical texts from Project Gutenberg (www.gutenberg.org/). We found the summed frequency accurately distinguished the philosophical from the non-philosophical texts. The process was then run by the HTRC on a larger collection of some 9,000 volumes and the results returned to us. We used the results to refine our list of keywords so that a summed relative frequency of .09 gave us mostly philosophical works with a few false positives. We did this by sorting the false positives by which words contributed to their summed relative frequency and then eliminating those words from the larger list that seemed to be ambiguous.
                
                The process was then run on the HathiTust Open Open collection of 254,000 volumes. This generated some 3230 volumes that had a summed relative frequency over .1, which seemed a safe cut-off point given how .09 had worked with a smaller collection. To assess the accuracy of this method we manually went through these 3,230 and categorized them using the titles producing a CSV that could be used with other classification methods.
                
                    
                
                The table below summarizes the categories of volumes that we found, though it should be noted that the categorization was based on the titles, which can be misleading. “Unsure” was for works which we weren’t sure about. “Not-Philosophical” were those works that we were reasonably sure were not philosophical from the title. The categories like Science and Education were for works about science and philosophy or education and philosophy.
                
                    
                        Tag (Type)
                        Number of Volumes
                        Example
                    
                    
                        Unsure
                        349
                        The coming revolution (1918)
                    
                    
                        Education
                        473
                        Education and national character (1904)
                    
                    
                        Philosophy
                        813
                        Outlines of metaphysics (1911)
                    
                    
                        Science
                        189
                        Relativity; a new view of the universe (1922)
                    
                    
                        Social
                        526
                        The study of history and sociology (1890)
                    
                    
                        Religion
                        722
                        Prolegomena to theism (1910)
                    
                    
                        Not Philosophical
                        158
                        Pennsylvania archives (1874)
                    
                
                One of the things that stands out is the overlap between religious titles and philosophical ones. This is not surprising given that the fields have been intertwined for centuries and often treat of the same issues. We also note how many educational works and works dealing with society can have a philosophical bent. It was gratifying to find only 4.9% of the volumes classified seemed clearly not philosophical. If one includes the Unsure category it is 15.7%, but the Unsure category is in many ways the most interesting as one reason for classifying by computer is to find unexpected texts that challenge assumptions about what is theory.
                
                    Conclusions. Using large keyword lists to classify texts is a conceptually simple method that can be understood and used by humanists. We have lists of words and names at hand in specialized dictionaries and existing classification systems. Lists can be managed to suit different purposes. Our list from InPhO had the advantage that is was large and inclusive, but also the disadvantage that included words like “being” and “affairs” that have philosophical uses but are also used in everyday prose. The same is true of the names gathered like Croce that can refer to the philosopher or the cross (in Italian). Further trimming and then weighting of words/names could improve the classification of strictly philosophical texts. We also need to look deeper into the results to find not just the false positives, but also the true negatives. In sum, this method has the virtue of simplicity and accessibility and in the case of philosophical texts can be used to extract useful, though not complete, subsets. 
                
            
            
                The Problem with Literary Theory
                Laura Mandell (Boris Capitanu, Stefan Sinclair, and Susan Brown)
                In this short paper, I describe adapting the word list approach developed by Geoffrey Rockwell for extracting a subset of philosophical texts from a large, undifferentiated corpus, to the task of identifying works of literary theory. The degree to which running the list of terms did in fact pull out and gather together works of literary criticism and theory is very high, despite potential problems with such an enterprise, which we discuss in this talk in detail.
                
                    
                        Developing the list of literary terms. Susan Brown and I decided to gather lists of literary terms. Susan initiated a discussion with the MLA about using terms from the 
                        MLA Bibliography but upon consideration these were in fact not at all what we needed: they classified subjects of texts as opposed to listing terms that would appear in those texts. I had recently spent some time learning about JSTOR’s new initiative in which sets of terms are created by what they call “SMEs”--Subject Matter Experts--and then used to locate articles all participating in an interdisciplinary subject. Their first foray is available in Beta: it gathers together all articles in no matter what field on the topic of Environmental Sustainabilty (labs.jstor.org/sustainability/). The terms collected are terms that would appear 
                        in the relevant texts, not in the metadata about them; the goal is to collect documents across multiple categories related to specialization, discipline, and field, since the desired result to gather together interdisciplinary texts concerning a common topic. 
                    
                    
                        Anachronism. JSTOR had started a “literary terms” list, and I finished the list of terms relying on encyclopedias of literary theory. Could a list of terms significant in the late-twentieth-century theories of literature as expressed in articles gathered in JSTOR be used to extract a set of texts published much earlier that analyze literature? What about the historical inaccuracy of using twentieth-century terms to find eighteenth- and nineteenth-century literary criticism?
                    
                
                Results:
                
                    
                
                In fact, results show solidly that this anachronistic list of terms developed by experts do work to gather materials that preceded and fed into, served to develop, the discipline of literary theory. One of two falsely identified texts among the top relevant documents has to do with water distribution systems which had, as part of its most frequent terms, “meter” and “collection,” two terms relevant to analyzing the medium and content of poetry. Other false positives are similarly explicable, and, most important, they are rare.
                In this paper, we report upon the effects of running these frequent words on very large datasets using both unsupervised to supervised learning.
            
            
                Machine Learning
                Stefan Sinclair (and Matthew Wilkens)
                The third panel presentation deals with machine learning techniques to extract subsets. Unsupervised learning techniques allow us to evaluate the relative coherence of theoretical clusters within large textual fields and to identify distinct theoretical subclasses in the absence of any firmly established anatomy of the discipline. For these reasons, we performed unsupervised classification on three corpora: (1) A large collection (c. 250,000 volumes) of mixed fiction and nonfiction published in the nineteenth and twentieth centuries. (2) A subset of that corpus identified by algorithmic and manual methods as highly philosophical. And (3) A subset similarly identified as literary-critical.
                In the case of the large corpus, the goal was to identify subsets containing high proportions of philosophy and criticism. For the smaller sets, we sought to produce coherent groupings of texts that would resemble subfields or concentrations within those areas. In each case, we extracted textual features including word frequency distributions, formal and stylistic measures, and basic metadata information, then performed both 
                    k-means and DBSCAN clustering on the derived Euclidean distances between volumes.
                
                As in past work on literary texts (Wilkens, 105), we found that we were able to identify highly distinct groups of texts, often those dealing with specialized and comparatively codified subdomains, and that we could subdivide larger fields with reasonable but lower accuracy. The model that emerges from this work, however, is one emphasizing continuity over clear distinction. Subfields and areas of intensely shared textual focus do exist, but a systematic view of large corpora in the philosophical and literary critical domains suggests a more fluid conception of knowledge space in the nineteenth and twentieth centuries.
                In parallel with the unsupervised classification performed – an attempt to allow distinctive features to emerge without, or with less, bias – we also performed supervised classification, starting with the training set of 40 texts labelled as Philosophical and Other (mentioned in "Using Word Lists to Subset" above). We experimented with several machine learning algorithms and several parameters to determine which ones seemed most suitable for our dataset. Indeed, part of this work was to recognize and and normalize the situation of the budding digital humanist confronting a dizzying array of choices: stoplists, keywords, relative frequencies, TF-IDF values, number of terms to use, Naïve Bayes Multinomial, 
                    Linear Support Vector Classification, penalty parameter, iterations, and so on ad infinitum. Some testing is desirable; some guesswork and some craftwork are essential. We reflect on these tensions more in the iPython notebook (Sinclair et al., 2016) and we will discuss them during the presentation as well.
                
                One of the surprises from these initial experiments in machine learning was that using an unbiased list of terms from the full corpus (with stopwords removed) was considerably more effective than attempting to classify using the constrained philosophical vocabulary. Again, this may be because the keywords list was overly greedy.
                
                    Just as we experimented with ever-larger corpora for the "Using Lists to Subset" sub-project, the supervised learning subproject broadened its scope gradually in an attempt to identify theoretical texts unknown to us while examining the efficacy of the methodologies along the way. Indeed, the overarching purpose of adopting all three approaches (keyword-based, unsupervised classification, machine learning) was to compare and contrast different ways of studying theory in a large-scale corpus.
                
            
            
                Working with HTRC datasets
                Boris Capitanu
                The fourth panel presentation focuses on working with the HathiTrust and the particular format of HathiTrust texts. Researchers may obtain datasets directly from HathiTrust [1] by making a special request, after having fulfilled appropriate security and licensing requirements. Datasets in HathiTrust and HTRC are available in two different ways:
                
                    via rsync in Pairtree format
                    via Data API
                
                According to “Pairtrees for Object Storage (V0.1)” [2], the Pairtree is "a filesystem hierarchy for holding objects that are located within that hierarchy by mapping identifier strings to object directory (or folder) paths, two characters at a time”. In the HathiTrust, the objects consist of the individual volume and associated metadata. Volumes are stored as ZIP files containing text files, one text file for each page, where the text file is named by the page number. A volume ZIP file may contain additional non-page text files, whose purpose can be identified from the file name. The metadata for the volume is encoded in METS XML [3] and lives in a file next to the volume ZIP file. For example, a volume with id “loc.ark:/13960/t8pc38p4b” is stored in Pairtree as:
                loc/pairtree_root/ar/k+/=1/39/60/=t/8p/c3/8p/4b/ark+=13960=t8pc38p4b/ark+=13960=t8pc38p4b.zip loc/pairtree_root/ar/k+/=1/39/60/=t/8p/c3/8p/4b/ark+=13960=t8pc38p4b/ark+=13960=t8pc38p4b.mets.xml
                where “loc” represents the 3-letter code of the library of origin (in this case Library of Congress). As mentioned, the volume ZIP files contain text files named for the page number. For example, here are the first few entries when listing the contents of the above ZIP file:
                 ark+=13960=t8pc38p4b/
                 ark+=13960=t8pc38p4b/00000001.txt
                 ark+=13960=t8pc38p4b/00000002.txt
                 ark+=13960=t8pc38p4b/00000003.txt
                 …
                Note that the strings that encode the volume id and the ZIP filename are different. Before a volume id can be encoded as a file name, it goes through a “cleaning” process that converts any character that is not a valid character to be used in a filename into one that is (for example “:” was converted to “+” and “/” to “=“), also dropping the 3-letter library code. The specific conversion rules are obscure, but library code already exists [4][5] for multiple languages that is able to perform this conversion both ways.
                The pairtree is an efficient structure for storing a large number of files. However, working with this structure can pose certain challenges. One of the issues is that this deeply nested folder hierarchy is slow to traverse. Applications needing to recursively process the volumes in a particular dataset stored in pairtree will have to traverse a large number of folders to “discover” every volume. A second inconvenience stems from the use of ZIP to store the content of a volume. While efficient in terms of disk space usage, it’s inconvenient when applications need to process the text data of the volume as they would need to uncompress the ZIP file and read its contents, in the proper order, concatenating all pages, in order to obtain the entire volume text content. A further complication is due to the fact that the exact ordering and naming of the page text files in the ZIP file is only provided as part of the METS XML metadata file. So, if the goal is to create a large blob of text containing all the pages of a volume (and only the pages, in the proper order, without any additional non-page data), the most correct way of doing so is to first parse the METS XML to determine the page sequence and file names, and then uncompress the ZIP file concatenating the pages in the exact sequence specified. This, of course, has a large performance penalty if it needs to be done on a large dataset every time this dataset is used to address some research question.
                An alternative way to obtain a particular dataset is to use the Data API [6]. Currently, access to Data API is limited, and is allowed only from the Data Capsule [7] while in Secure Mode. Using the Data API a researcher can retrieve multiple volumes, pages of volumes, token counts, and METS metadata documents. Authentication via the OAuth protocol is required when making requests to the Data API. The advantage of using the Data API in place of the pairtree (other than disk storage savings) is that one can request already-concatenated text blobs for volumes, and make more granular requests for token counts or page ranges without having to traverse deeply-nested folder structures or parse METS metadata.
                In this panel presentation we will show how the tools developed for the Trace of Theory project were adapted to work with the Pairtree format. The goal is to help others be able to work with the HathiTrust data format.
                Notes:
                [1] https://www.hathitrust.org/datasets
                [2] http://tools.ietf.org/html/draft-kunze-pairtree-01
                [3] http://www.loc.gov/standards/mets/
                [4] https://confluence.ucop.edu/display/Curation/PairTree
                [5] https://github.com/htrc/HTRC-Tools-PairtreeHelper
                [6] https://wiki.htrc.illinois.edu/display/COM/HTRC+Data+API+Users+Guide
                [7] https://wiki.htrc.illinois.edu/display/COM/HTRC+Data+Capsule
            
            
                Topic Modelling and Visualization for Exploration
                Susan Brown (Geoffrey Rockwell, Boris Capitanu, Ryan Chartier, and John Montague)
                When working with very large collections even subsets can be too large to manage with conventional text analysis tools. Further, one needs ways of exploring the results of extraction techniques to figure out if you got what you were expecting or something surprising in an interesting way. In the fifth panel presentation we will discuss the adaptation of a tool called the Galaxy Viewer for visualizing the results of Topic Modelling (Montague et al., 2015). Topic modeling is an automated text mining technique that has proven popular in the humanities that tries to identify groups of words with a tendency to occur together within the same documents in a corpus. Chaney and Blei explain that, “One of the main applications of topic models is for exploratory data analysis, that is, to help browse, understand, and summarize otherwise unstructured collections.” (Chaney et al., 2012)
                
                    
                
                The Galaxy Viewer prototype was developed to explore the results of topic modelling over large collections. It combines different views so that one can select topics, compare topics, explore the words in topics, follow topic tokens over time, and see the document titles associated with topics. In this presentation we will demonstrate the Galaxy Viewer and then discuss how it was scaled to handle much larger collections.
                The prototype Galaxy Viewer backend code uses Mallet (McCallum, 2002) to infer the set of topics, topic distributions per document, and word probabilities per topic. Unfortunately, Mallet is meant to be used on small- to medium-sized corpora as it requires that the entire dataset be loaded into RAM during training. An additional constraint with Mallet is the fact that although Mallet can fully utilize all the CPU cores on a single machine, it’s not designed to work in a distributed-computing fashion across a number of machines, to speed up execution. As such, processing very large datasets (if even possible) might take a very long time (as the algorithm makes multiple passes over the entire dataset). Many implementations of LDA exist, which primarily fall into one of two categories: Batch LDA, or Online LDA. The core difference between batch and online LDA stems from what happens during each iteration of the algorithm. In batch mode, as mentioned earlier, each iteration of the algorithm makes a full pass over all the documents in the dataset in order to re-estimate the parameters, checking each time for convergence. In contrast, online LDA only makes a single sweep over the dataset, analyzing a subset of the documents each iteration. The memory requirement for online LDA depends on the chosen batch size only, not on the size of the dataset - as is the case with batch LDA.
                We are currently in the process of researching/comparing the available implementations of LDA to establish which one would be best suited to use for the Galaxy Viewer. We are also considering the option of not fixing the LDA implementation, but instead make the backend flexible so that any LDA implementation can be used (as long as it provides the appropriate results that are needed). In the latter case we’d have to create specific result interpreters that can translate the output from the specific implementation of LDA to the appropriate format to be used to store in the database (to be served by the web service).
                Given that Topic Modeling results do not expose the textual content of the documents analyzed, and cannot be used to reconstruct the original text, they are safe to be publicly shared without fear of violating copyright law. This is great news for researchers working with collections like those of the HathiTrust as they should be able to gain insight into datasets which are still currently in-copyright and would, otherwise, not be available to be inspected freely.
                In the prototype Galaxy Viewer implementation, the output of the topic modeling step is processed through a set of R functions that reshape the data and augment it with additional calculated metrics that are used by the web frontend to construct the visualization. These post-processing results are saved to the filesystem as a set of five CSV files. One of these CSV files is quite large as it contains the topic modeling state data from Mallet (containing topic assignments for each document and word, and associated frequency count). The visual web frontend code loads this set of five files into memory when the interface is accessed the first time, which can take several minutes. For the prototype this approach was tolerated, but it has serious scalability and performance issue that needs to be addressed before the tool can be truly usable by other researchers.
                Scaling the Galaxy Viewer therefore consists of creating a web service backed with a (NoSQL) database which will service AJAX requests from the front-end for the data needed to construct the topic visualization and related graphs. We are developing the set of service calls that need to be implemented/exposed by the web service to fulfill the needs of the front-end web-app. The backend service will query the database to retrieve the necessary data to service the requests. The database will be created based on the output of the Topic Modeling process, after required post-processing of the results is completed (to calculate the topic trends, topic distances, and other metrics used in the display). Relevant metadata at the volume and dataset level will also be stored to be made available to the front-end upon request. This work will be completed by the end of December 2015 so that it can be demonstrated in the new year. The scaled Galaxy Viewer will then provide a non-consumptive way of allowing users of the HathiTrust to explore the copyrighted collections. Extraction of subsets and Topic Modelling can take place under the supervision of the HTRC and the results database can then be exposed to visualization tools like the Galaxy Viewer (and others) for exploration.
            
            
                Closing reflections: How “Trace of Theory” will improve the HTRC
                    . 
                
                J. Stephen Downie
                The HathiTrust Research Center exists to give the Digital Humanities community analytic access to the HathiTrust’s 13.7 million volumes. The HT volumes comprise over 4.8 billion pages each in turn represented by a high-resolution image file and two OCR files yielding some 14.4 billion data files! Thus, as the earlier papers have highlighted, the sheer size of the collection, along with the idiosyncratic nature of the HT data, together create several hurdles that impede meaningful analytic research.The HTRC is engaged in two ongoing endeavours designed to assist DH researchers in overcoming these obstacles: The Advance Collaborative Support (ACS) program [1]; and, the Workset Creation for Scholarly Analysis (WCSA) project [2]. 
                The ACS program at HTRC provides no-cost senior developer time, data wrangling assistance, computation time and analytic consultations to DH researchers who are prototyping new research ideas using the HT data resources. The ACS program is an integral part of the HTRC’s operation mission and was part of its value-added proposition when the HTRC launched its recent four-year operations plan (2014-2018). It is a fundamental component of the HTRC’s outreach activities and as such, has staff dedicated to its planning, management and day-to-delivery. The ACS team was responsible for creating, and then reviewing, the competitive ACS Request for Proposals (RFP) that ask interested DH researchers outline their intellectual goals, describe their data needs, and estimate their computational requirements. The ACS team is generally looking for new projects that could benefit from some kickstarting help from HTRC. HTRC welcomes proposals from researchers with a wide range of experience and skills. Projects run 6 to 12 months.
                Originally funded by the Andrew W. Mellon Foundation (2013-2015), the current WCSA program is building upon, extending and implementing the development made during the funding period. The HTRC project team, along with subaward collaborators at University of Oxford, University of Maryland, Texas Agriculture and Marine University and University of Waikato, developed a group of prototype techniques for empowering scholars who want to do computational analyses of the HT materials to more efficiently and effectively create user-specific analytic subsets (called “worksets”). A formal model has been designed to describe the items in a workset along with necessary bibliographic and provenance metadata that is now being incorporated into the HTRC infrastructure (Jett, 2015). 
                The Trace of Theory project was selected from the first round of ACS proposals. This concluding panel presentation will discuss in what ways the Trace of Theory project has been both a representative and a unique exemplar of the ACS program. It will present some emergent themes that evolved from the HTRC-Trace of Theory interactions that we believe will have an important influence on the delivery of future ACS projects. In the same manner, it will reflect upon the problems the team of researchers had in subsetting the data to build their necessary worksets along with the solutions that the HRTC-Trace of Theory collaboration developed to surmount those difficulties. The panel will finish with a summary of how HTRC intends to incorporate the lessons learned into its day-to-day operations as well as future ACS projects. 
                Notes:
                [1] The 2014 ACS RFP is available at: https://www.hathitrust.org/htrc/acs-rfp
                [2] https://www.lis.illinois.edu/research/projects/workset-creation-scholarly-analysis-prototyping-project
            
        
        
            
                
                    Bibliography
                    
                        Chaney, A. J. and Blei, D. M. (2012). Visualizing Topic Models, ICWSM. 
                        http://www.aaai.org/ocs/index.php/ICWSM/ICWSM12/paper/download/4645%26lt%3B/5021 (accessed Dec 2015).
                    
                    
                        Jett, J. (2015). Modeling worksets in the HathiTrust Research Center. CIRSS Technical Report WCSA0715. Champaign, IL: University of Illinois at Urbana-Champaign. Available via: http://hdl.handle.net/2142/78149 (accessed Dec 2015)
                    
                    
                        McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit, http://mallet.cs.umass.edu (accessed Dec 2015).
                    
                    
                        Montague, J., Simpson, J., Brown, S., Rockwell, G. and Ruecker, S. (2015). Exploring Large Datasets with Topic Model Visualization. Paper presented by Montague at DH 2015 at the University of Western Sydney, Australia.
                    
                    
                        Sinclair, S., G. Rockwell and the Trace of Theory Team. (2016). Classifying Philosophical Texts. Online at 
                        http://bit.ly/1kHBy56 (accessed Dec 2015). 
                    
                    
                        Wilkens, M. (2016). Genre, Computation, and the Weird Canonicity of Recently Dead White Men. NovelTM Working Paper.
                    
                
            
        
    


        
            Summary: This poster presents work on documenting user needs in the Humanities and Social Sciences as illustrated through Case Studies in the context of the Europeana Cloud “Unlocking Europe's Research via the Cloud” project. Conducted as part of a wider methodological effort including desk research, expert fora and a web survey, methodology and findings of actual use of innovative digital tools and services will be visually represented. This work will form the basis of the Europeana Research Case Studies which will seek to gather and process an evidence-based record of the information practices, needs and scholarly methods in the respective communities.
            This poster reports on collaborative, cross-European work conducted during 2013-2015 in the context of the Europeana Cloud “Unlocking Europe's Research via the Cloud” project, and touches upon planned activities in the context of Europeana Research in 2016. Europeana Research is an initiative which aims to create stronger links between the cultural heritage sector and academia. More particularly, it aims to ensure that open, high-quality data from the cultural sector is available for reuse by the digital humanities community.
            One of the main objectives of Europeana Cloud was the enhancement of the understanding of digital tools, research processes and scholarly content used in the Humanities and Social Sciences, thus informing the development of tools and aggregation of content in Europeana for research purposes. To this end, and in order to contribute towards the development of the new platform of Europeana Research, Case Studies were developed as part of a wider methodological effort which included desk research, expert fora and web survey for reaching user requirements.
            The purpose of this poster is to visually represent the methodology followed and results reached in documenting actual use of innovative digital tools and services in the Humanities and Social Sciences research communities illustrated in three main Case Studies in the disciplines of Education, Art History and Sociology, and further complemented by satellite cases. 
            The Case Studies were initially selected based on the disciplines and tools that might best make use of current Europeana content. By defining “innovative” as “either performing functions that were previously unavailable, or performing already available functions in a qualitatively different way”, three tools were identified as best fitting this criteria (Transana, HyperImage, NodeXL) enriched by two “satellite” tools more frequently used in the respective research disciplines (NVivo, Voyant).
            These were further approached following a threefold methodology of semi-structured interviews, empirical observation of the tools and background research. The results were then discussed both from the perspective of the discipline area, and through the lens of the scholarly primitives (Unsworth 2000, Palmer et al 2009), to determine their use with Europeana content. The poster will also highlight the importance of accessibility of data for research infrastructures and research groups and need to focus on high quality metadata and content both for Europeana Research and the wider GLAM sector, and will illustrate how digital tools are not themselves a guarantee of good research, as researchers do not necessarily use the same digital tool throughout the research process; rather they use one tool per step (one tool = one research primitive).
            This poster will also present future work planned to be undertaken in the context of Europeana Research in 2016. Based on Europeana Cloud, a series of new Case Studies will be developed and expanded towards different research communities. The Europeana Research Case Studies will be undertaken in collaboration with existing European research initiatives, and will seek to gather and process an evidence-based record of the information practices, needs and scholarly methods of arts and humanities and social sciences researchers within the broad Europeana ecosystem and particularly in relation to Europeana content. Τhe Case Studies will employ a mixed methods approach combining various ways of gathering empirical evidence on the information needs and scholarly methods employed in digitally-enabled arts and humanities and social sciences research across Europe and beyond.
        
        
            
                
                    Bibliography
                    
                        Hughes, L. (2011). Using ICT methods and tools in arts and humanities research, L. Hughes (Ed.), 
                        Evaluating and measuring the value, use and impact of digital collections. London: Facet Publishing.
                    
                    
                        Palmer, C. L., Teffeau, L. C. and Pirmann, C. M. (2009). 
                        Scholarly information practices in the online environment. Report commissioned by OCLC Research. Published online at: 
                        www.oclc.org/programs/publications/reports/2009-02.pdf. 
                    
                    University of Virginia (2005). 
                        Summit on Digital Tools for the Humanities - Report on Summit Accomplishments. Retrieved from http://www.iath.virginia.edu/dtsummit/SummitText.pdf.
                    
                    
                        Unsworth, J. (2000). Scholarly primitives: What methods do humanities researchers have in common, and how might our tools reflect this. In J. Unsworth (Ed.), 
                        Humanities Computing, Formal Methods, Experimental Practice Symposium, pp. 5-100.
                    
                    
                        Unsworth, J. (2003). Tool-Time, or Haven't We Been Here Already?. Presented at the 
                        Transforming Disciplines: The Humanities and Computer Science. Washington, DC. Retrieved from http://people.lis.illinois.edu/~unsworth/carnegie-ninch.03.html.
                    
                
            
        
    


        
            “Remapping Leigh Hunt’s Circles” is an ambitious project that explores Leigh Hunt’s central position in the London literary and critical scene of the first half of the nineteenth century, through the lens of digital humanities tools. Hunt is today considered one of the key figures of the Romantic period in England, known for his work as editor, journalist, poet, and facilitator. Numerous articles, essay collections, biographies, and monographs published in the last fifteen years have made this clear. Hunt's contribution to Romantic and Victorian literature was as extensive as it has proven durable, in matters as various as prosodic experimentation and the modernization of the magazine essay. Yet little work (beyond some biographical notes) has been done on the second half of his life, a period that was as productive as the first, and during which Hunt was intimate with many of the finest writers of the time, and continued to contribute to London’s literary circles through the ongoing publication of critical essays in periodicals and anthologies. This project aims to redress this imbalance/oversight and reassert Hunt’s place in the Romantic and Victorian eras, as well as his continuing significance for understanding the London literary scene between 1805 (publication of his first critical essay) and 1859 (date of his death, with his last article published only a few weeks before). 
            “Remapping Leigh Hunt’s Circles” makes a case for Hunt’s position as a key critical voice in London beyond his already established prominence during the 
                Examiner years. It does so through a careful analysis of his critical reviews and essays (with a specific focus on his drama criticism to underscore Hunt’s ongoing engagement with the public sphere) published during his entire career, which spanned the first half of the nineteenth century. Data mining and textual analysis offer exciting opportunities to bring together different sets of data which, when prepared to the highest standard of text encoding, can yield new and innovative results that encourage reconsideration of preconceived notions regarding the transfer of ideas from one author to another, or one literary genre to another. The results of the research undertaken in “Remapping Leigh Hunt’s Circles” will be presented in a collaborative, visual context that reimagines the digital scholarly edition as a transparent workspace in which established primary objects from existing databases can be gathered, organized, correlated, annotated, and augmented by multiple users in a dynamic environment. 
            
            All the texts prepared for inclusion in our project are encoded to the Text-Encoding-Initiative (TEI) standards. The mark-up language and quality controls for improving metadata in all the resources provide more accurate search and discovery, allow for the presentation of well-supported content on multiple devices and develop tools for assembling, archiving and indexing research objects and artifacts. Ongoing work on this platform will enable researchers to undertake world-class research by providing the means to link data-sets to published content, encouraging data reanalysis, replication studies, and data re-purposing, all of which improve research quality and efficiency.
            Our poster will report on the first year of this project, and the implementation of the latest version of the 
                Voyant Tools to examine the dramatic essays written by Hunt between 1805 and 1813 (when he was sentenced to two years in prison for libel against the Prince Regent). We will showcase in particular two aspects of the integration between the Hunt archives and Voyant Tools. First, the ability to identify and visualize named entity connections and their networks across multiple documents (this a refinement of the previous RezoViz tool in Voyant). The Hunt collection presents an ideal corpus for network exploration given the interconnectedness of the people, locations and events that animate the documents. Second, Voyant provides a generic and customizable way of presenting a web-based corpus catalogue with the same kinds of faceted browsing and advanced querying capabilities we have come to expect from library databases and online stores. A further benefit of this functionality is the ability to create dynamic subsets of a corpus to examine more closely (in other words, using a catalogue skin in Voyant to create worksets destined for Voyant’s more conventional analytic skin).
            
            The “Remapping Leigh Hunt’s Circles” is essentially a project of digital text editing and literary criticism whereas Voyant Tools is essentially a software platform for reading, analyzing and visualizing digital texts. These are separate traditions and separate concerns, but this poster will demonstrate the value of symbiotic development: both projects benefit from the collaboration.
        
        
            
                
                    Bibliography
                    McGann, J. (2014). 
                        A New Republic of Letters. Cambridge, MA: Harvard UP. 
                    
                    Sinatra, M., and Sinclair, S. (2015). Special issue “Repenser le numérique au 21
                        ème siècle”. 
                        Sens public (hiver 2015). 
                    
                    Sinatra, M. (2015). “Representing Leigh Hunt’s Autobiography”.
                        Virtual Victorians: Networks, Connections, Technologies. Eds. Stauffer, A. and Alfano, V. R., Palgrave.
                    
                    Sinclair, S., Rucker, S. and Radzikowska, M. (2011). 
                        Visual Interface Design for Digital Cultural Heritage. Ashgate.
                    
                
            
        
    


        
            
                Introduction
                The HathiTrust Research Center (HTRC) aims to facilitate large-scale computational text analysis of the contents of the HathiTrust Digital Library (HTDL) through data services and analytical tools. We conducted a study of current and potential users of the HTRC to investigate how scholars integrate text analysis into their research. Our study aims to inform the development of HTRC services and also to generate deeper insights into scholarly research practices with large-scale digitized text corpora.
            
            
                Background
                Studies on the use of digital content by humanities scholars, ranging from humanities cyberinfrastructure (ACLS, 2006) and patterns in scholarly practices (Brockman et al., 2001; Palmer and Neumann, 2002; Green and Courtney, 2015), to discipline-specific studies (Zorich, 2012; Babeu, 2011; Rutner and Schonfeld, 2011), reveal that scholars acquire and analyze digital content in multi-faceted ways. Several investigations particularly examine scholarly uses of digital tools (Frischer et al., 2006; Toms and O’Brien, 2008; Gibbs and Owens, 2012). Computational text analysis dates from the beginnings of humanities computing (Hindley, 2013), and the resources of the ARTFL Project (Argamon et al., 2009; Horton et al., 2009), MONK (Unsworth, 2011), Wordseer (Muralidharan and Hearst, 2013), Voyant and TaPOR (Rockwell et al., 2010), and Lexos (LeBlanc et al., 2013), among others, inform the current work of the HTRC to provide a secure computational and data environment for researchers to conduct analyses of content from the HathiTrust Digital Library.
                Our study builds on an earlier user needs assessment conducted for the HTRC and its Mellon Foundation-funded Workset Creation for Scholarly Analysis project. That earlier study analyzed interviews and focus groups in order to identify capabilities needed in large text corpora to facilitate scholarly research use (Fenlon et al., 2014). These desired capabilities included the ability to create and manipulate collections as reusable datasets and research products, the ability to work at different units of analysis, and access to highly enriched metadata (Green et al., 2014; Fenlon et al., 2014). 
                Our present study especially builds upon that previous investigation by examining the text analysis research practices of current and potential users of the HTRC.
            
            
                Research Design
                
                    Goals
                    Our study’s primary goals are:
                    
                        To analyze current scholarly research practices with textual corpora to identify user requirements for HTRC services;
                        to develop illustrative use cases of text analysis research for shaping training curricula; and
                        to obtain information for guiding the development of HTRC research services in the University of Illinois Library’s Scholarly Commons and similar digital scholarship centers.
                    
                    While the findings of this study specifically will inform the development of services to meet the needs of HTRC users, it also contributes broader insights into how to develop similar digital resources and research services for computational text analysis.
                
                
                    Methods
                    We conducted fifteen semi-structured interviews with students, faculty, researchers, administrators, and librarians who pursue work that includes text analysis, or have familiarity with text analysis methods. Some participants were recruited at professional conferences for digital humanities and libraries, while others were active in HTRC user group forums. Several of the interviewees had previously interacted with the HTRC, and most had experience with the HTDL. The participants were from various disciplines — including English, Anthropology, History, and Computer Science —and ranged from newcomers to digital humanities to long-time researchers. 
                    We performed an initial analysis of the interview data through open coding and will continue detailed qualitative analysis using ATLAS.ti. Data was independently coded by the authors to ensure inter-coder reliability. While we are still actively analyzing interview data, we identified several preliminary themes discussed here. These themes include strategies for obtaining and managing data, research workflows and results, collaborations, and teaching. 
                
            
            
                Analysis and Discussion
                
                    Data Acquisition and Management
                    Several respondents characterized text analysis research as being time-intensive in spite of the speed of computational tools. One interviewee noted, ‘It’s funny, often people think, “Oh we have it digitized, now it’s useful.” Scholars realize that you have a lot more work to do after that. And that can often slow projects down terribly.’
                    The interviewees indicated that gathering, managing, and manipulating text data comprised a considerable portion of their work. An interviewee explained, ‘I think the biggest challenge is data, getting good data to work with. I think people underestimate the problems and difficulties in doing that.’
                    Interviewees also expressed a desire for improved ways to identify and extract the content they needed, especially when navigating large-scale collections to find the volumes, pages, or passages relevant to a research project. As one interviewee remarked, ‘Even if you had somehow structured your texts, I would be saying, “What was left out? How do I bring it back in?”’ 
                
                
                    Research Workflows and Results
                    Several interviewees described the potential of text analysis to challenge previously held understandings of text, as differences between human and computational readings emerged. One respondent noted, ‘There are many cases in which the computer is at least as good—if not better—a reader than humans are. That’s very difficult for people to accept... sometimes the computer gets it right and it bears looking at that difference. So we kind of want to get that new ground truth on this kind of work.’
                    Many researchers highlighted the importance of interpretive work in understanding how the tools interact with the text, and characterized the interactions as dynamic. One respondent observed, ‘I yearn for workflows where the scholar could actually set their own tokenization rules.... It would be a way that we could create less language-specific [rules] or control the language specificity of the algorithm. I think that is the real need.’ Several respondents highlighted the importance of tools that flexibly fit into various stages of the research process, and also are accessible to users of different skill levels. Interviewees also suggested enhancements specific to the HTRC, which included expanded visualization capabilities, improved generation of statistics about text corpora, and better ability to handle languages other than English. 
                
                
                    Research Collaborations
                    Interviewees repeatedly cited collaboration and research support, both virtual and in-person, as important. Many interviewees worked with digital humanities initiatives, and reported that their local resources ranged from limited technical support to well-resourced research centers. For some interviewees, online support communities— such as Digital Humanities Questions and Answers or Stack Overflow — also were significant.
                    Interdisciplinary collaborations between departments and across institutions emerged as the most prominent kind of partnership, but interviewees also noted the challenges that such collaborations pose. As one interviewee explained, ‘Collaborations between institutions: much more difficult. There’s money, there’s institutional blockages, and then anything over half a dozen people, it gets complicated very quickly. And so the people dynamics get very complicated.’ Some respondents noted that these collaborations affected their research practices and acquisition of research resources. 
                     Interviewees reported that their collaborations with libraries ranged from non-existent to critical partnerships. Many saw the library as a key space because ‘the library is actually the one functioning interdisciplinary space on a university campus.’ Collaborations with the HTRC and digital repositories for working with data also were important to respondents.
                
                
                    Teaching and Training
                    Interviewees mentioned their active efforts and intentions to incorporate computational text analysis into their teaching. Some remarked on institutional constraints that make it difficult to incorporate computational tools into curricula. As one respondent explained: ‘I once imagined teaching a class in which students learn to script and actually run analyses against data, but I was told, basically, that that class isn’t a humanities class anymore—that belongs in computer science.’
                    Some stated that the courses that they currently teach may not require or allow for the incorporation of computational analysis. Yet others noted that there is only a limited amount of technical or scientific skills that a humanities student could realistically master within a short period of time, with one interviewee noting that ‘you can only get people to learn so much about the math; as much as they can learn, they should — at the same time, it’s hard.’
                     Although the demand from students for learning about computational text analysis was, overall, reported to be increasing, some interviewees noted that they are constrained by not only limited resources, but also uncertainty as to how to carry out such activities. One interviewee reported prevailing sentiments that the digital humanities ‘doesn’t even fit anywhere,’ leading to the question of whether ‘there should be a whole separate department that’s digital humanities,’ or to offer training within existing curricula.
                
            
            
                Conclusion
                 The immediate aims of this study are to generate an updated framework of user requirements that will guide the development of the HTRC’s educational programming and research support services and also to inform forthcoming Mellon Foundation-funded development of the HTRC Data Capsule. But our preliminary findings also provide insights into scholars’ needs as they increasingly incorporate text analysis in research and teaching. These findings also reveal how digital scholarship centers, information professionals, and providers of digitized content can best support scholarship as digital humanities resources evolve.
            
            
                Acknowledgements
                We thank Megan Senseney, Angela Courtney, Nicholae Cline, and Leanne Mobley for their collaboration in this study. 
            
        
        
            
                
                    Bibliography
                    
                        American Council of Learned Societies. (2006). 
                        Our Cultural Commonwealth: The report of the ACLS Commission on Cyberinfrastructure for the Humanities and Social Sciences. New York: American Council of Learned Societies. 
                        http://www.acls.org/uploadedFiles/Publications/Programs/Our_Cultural_Commonwealth.pdf (accessed 4 March 2016).
                    
                    
                        Argamon, S., et al. (2009). Gender, Race, and Nationality in Black Drama, 1850-2000: Mining Differences in Language Use in Authors and their Characters. 
                        Digital Humanities Quarterly
                        3(2): http://www.digitalhumanities.org/dhq/vol/3/2/000043/000043.html 
                    
                    
                        Babeu, A. (2011). 
                        Rome wasn't digitized in a day: Building a cyberinfrastructure for digital classics. CLIR Publication, 150, Washington, DC: Council of Library and Information Resources. 
                        http://www.clir.org/pubs/reports/pub150/reports/pub150/pub150.pdf (accessed 4 March 2016).
                    
                    
                        Brockman, W. S., et al. (2001). 
                        Scholarly work in the humanities and the evolving information environment CLIR Publication, 104, Washington, D.C.: Digital Library Federation, Council on Library and Information Resources. 
                        http://www.clir.org/pubs/reports/pub104/pub104.pdf (accessed 4 March 2016).
                    
                    
                        Fenlon, K., et al. (2014). Scholar-built collections: A study of user requirements for research in large-scale digital libraries. 
                        Proceedings of the American Society for Information Science and Technology,
                        51(1): 1–10.
                    
                    
                        Frischer, B., et al. (2006). 
                        Summit on digital tools for the humanities: Report on summit accomplishments. Institute for Advanced Technology in the Humanities, University of Virginia. 
                        http://www.iath.virginia.edu/dtsummit/SummitText.pdf (accessed 4 March 2016).
                    
                    
                        Gibbs, F. and Owens, T. (2012). Building Better Digital Humanities Tools: Toward broader audiences and user-centered designs. 
                        Digital Humanities Quarterly,
                        6(2).
                        http://www.digitalhumanities.org/dhq/vol/6/2/000136/000136.html (accessed 4 March 2016).
                    
                    
                        Green, H. and Courtney, A. (2015). Beyond the Scanned Image: A Needs Assessment of Scholarly Users of Digital Collections. 
                        College and Research Libraries,
                        76(5): 690-707.
                    
                    
                        Green, H. E., et al., (2014). Using Collections and Worksets in Large-Scale Corpora: Preliminary Findings from the Workset Creation for Scholarly Analysis Prototyping Project. Poster presented at iConference 2014, Berlin, Germany.
                    
                    
                        Hindley, M. (2013). The Rise of the Machines: NEH and the Digital Humanities: the early years. 
                        Humanities,
                        34(4).
                        http://www.neh.gov/humanities/2013/julyaugust/feature/the-rise-the-machines (accessed 4 March 2016).
                    
                    
                        Horton, R., et al. (2009). Mining Eighteenth Century Ontologies: Machine Learning and Knowledge Classification in the Encyclopédie. 
                        Digital Humanities Quarterly,
                        3(2): http://www.digitalhumanities.org/dhq/vol/3/2/000044/000044.html 
                        
                    
                    
                        LeBlanc, M. D., et al. (2013). Lexomics: Integrating the Research and Teaching Spaces. 
                        Digital Humanities 2013 Conference Abstracts, University of Nebraska–Lincoln, 16-19 July 2013. Lincoln, NE: Association of Digital Humanities Organizations, pp. 274-76. http://dh2013.unl.edu/abstracts/ab-293.html (accessed 4 March 2016).
                    
                    
                        Muralidharan, A. and Hearst, M. A. 
                        (2013). Supporting Exploratory Text Analysis in Literature Study.
                        Literary and Linguistic Computing,
                        28(2): 283-95. 10.1093/llc/fqs044.
                    
                    
                        Palmer, C. L. and Neumann, L. J. (2002). The Information Work of Interdisciplinary Humanities Scholars: Exploration and Translation.
                         Library Quarterly
                        7(1): 85-117.
                    
                    
                        Rockwell, G., et al. (2010). Ubiquitous Text Analysis. 
                        Poetess Archive Journal, 1(2).
                        https://journals.tdl.org/paj/index.php/paj/article/view/13 (accessed 4 March 2016).
                    
                    
                        Rutner, J. and Schonfeld, R. (2012). 
                        Supporting the Changing Research Practices of Historians. New York: Ithaka S+R. http://sr.ithaka.org/?p=22532
                    
                    
                        Sukovic, S. (2011). E-Texts in Research Projects in the Humanities, A. Woodsworth and W. D. Penniman (eds.), 
                        Advances in Librarianship. Bingley, UK: Emerald Group Publishing, pp. 131-202.
                    
                    
                        Toms, E. G. and O’Brien, H. (2008). Understanding the Information and Communication Technology Needs of the E-Humanist. 
                        Journal of Documentation,
                        64(1): 102-30.
                    
                    
                        Unsworth, J. (2011). Computational Work with Very Large Text Collections: Interoperability, Sustainability, and the TEI. 
                        Journal of the Text Encoding Initiative
                        1, 10.4000/jtei.215 (accessed 4 March 2016). 
                    
                    
                        Zorich, D. (2012). 
                        Transitioning to a Digital World: Art History, Its Research Centers and Digital Scholarship: A Report to the Samuel H. Kress Foundation and the Roy Rosenzweig Center for History and New Media. New York: Samuel H. Kress Foundation. http://www.kressfoundation.org/research/Default.aspx?id=35379 (accessed 4 March 2016).
                    
                
            
        
    


        
            Writing in 
                Literary and Linguistic Computing, Julianne Nyhan et al argue that “without a better understanding—a more appropriate term might be “body of interpretations”—of the near and distant history of computing in the humanities, we are condemned to repeat the revolutionary trope 
                ad infinitum.” (Nyhan, Flinn, and Welsh, 2013). Willard McCarty, amplifying this, writes that “rather than hypnotizing ourselves with supposedly unprecedented marvels, we must learn to see computing in its historical and social contexts, and so be able to ground modelling in something larger than itself. For computing to be 
                of the humanities as well as 
                in them, we must get beyond catalogues, chronologies, and heroic firsts to a genuine history. There are none yet.” (McCarty, 2008). 
            
            Susan Hockey wrote in 
                Companion to Digital Humanities that “humanities computing has a very well-known beginning,” by which she means the decades-long collaboration between Father Roberto Busa and IBM to create a concordance of the work of Thomas Aquinas (Hockey, 2004). This is the heroic first, which Busa, writing in the forward of that same volume, summed up with admirable brevity: “During the World War II, between 1941 and 1946, I began to look for machines for the automation of the linguistic analysis of written texts. I found them, in 1949, at IBM in New York City.”
                _ This narrative is familiar to many of those working in digital humanities today, and has become openly accepted as the standard historical background for, first, humanities computing and, subsequently, the digital humanities writ large. 
            
            This presentation aims to upset this easy narrative by re-situating the history of one type of digital humanities project—Early English Books Online—as one chapter in an overall history of a technological humanities. That history—the history of a technological humanities—the story of how academics have deployed technology to better understand human creations, especially in textual form—or to understand and explore texts—did not begin in 1949. The creation of digital humanities, radiating outward from those early years, is surely part of the larger story of how technology and text have come together and drifted apart over many centuries. I claim that there is a great deal more continuity in the apparatuses, in the knowledge infrastructure of the humanities than we put forward in our “official” histories. In our search for a neat disciplinary history, we elide technology as a whole with the digital electronic computer. Busa’s project likely does represent the beginning of one type of computational textual processing. It bears remembering, however, that his goal was to create a concordance, a type of reference tool and interface in existence in Western Europe since at least the 13th century. What is the history of 
                this type of textual processing in the intervening six hundred years? 
            
            Instead of a history of tools for textual work beginning with the rise of humanities computing and moving forward to the present day, I hope to juxtapose a different narrative, one that troubles the rhetoric of a textual digital humanities that arises from a clear break with what came before. I hope to, perhaps polemically, test the boundaries of histories of digital humanities by considering an equally technologically sophisticated pre-digital humanities. Such a reframing opens many avenues of inquiry, including a consideration of Linked Open Data in the context of cooperative cataloguing practices from the 19th & 20th centuries, or contemporary textual analysis tools such as Voyant alongside the imposing machinery of an electromechanical Hinman Collator. This presentation, however, will highlight particularly those technologies of textual reproduction developed prior to the oft-quoted originary moment of 1949. Drawing on the history of Early English Books Online (EEBO), I argue that while a 
                computational humanities may indeed be limited to the last half-century, the 
                technological humanities—in both materialist and cultural senses—have a much longer history. 
            
            ProQuest introduces the resource on their front page: 
            From the first book printed in English by William Caxton, through the age of Spenser and Shakespeare and the tumult of the English Civil War, Early English Books Online (EEBO) will contain over 125,000 titles listed in Pollard and Redgrave's Short-Title Catalogue (1475-1640), Wing's Short-Title Catalogue (1641-1700), the Thomason Tracts (1640-1661), and the Early English Tract Supplement - all in full digital facsimile from the Early English Books microfilm collection.
                _
                
                     This text was current as of summer 2015 and is available in cached form. Since that time, Proquest has altered the front page description of EEBO to the following: 
                    Early English Books Online (EEBO) contains digital facsimile page images of virtually every work printed in England, Ireland, Scotland, Wales and British North America and works in English printed elsewhere from 1473-1700 - from the first book printed in English by William Caxton, through the age of Spenser and Shakespeare and the tumult of the English Civil War. 
                    Strangely, this newer version eliminates reference to the Early English Books microfilm Collection, as well as collapsing a number of distinct early modern collections of content into what might be called the EEBO brand. See the current version of  and a cached version  from September 2015.
                
            
            In practice, this means that users are able to view the metadata for a given text; view page images of the original, early modern books in TIFF or PDF format; and, where available, view a full text transcription of the volume that are derived from the EEBO - Text Creation Partnership. Efforts to microfilm early English books began in 1931, intensified as World War II loomed, and continue today. Digital images of these microfilmed documents were made (and are still being made) available online first in 1998. The printed 
                Short Title Catalogue (itself published in 1926) has determined what objects were photographed and, subsequently, scanned and put online
                _(EEBO).
            
            The history of EEBO crosses multiple media, was directly impacted by global war, involves private companies and public universities, and is both analog and digital. To bracket EEBO (or EEBO-TCP) as a only a digital project impoverishes our understanding of how digital technologies have impacted the reproduction, preservation, and use of texts in humanistic scholarship. To write the full history of the early English books project, Early English Books Online, the Early English Books Online Text Creation Partnership is to engage in an act of disciplinary archaeology, one that forces digital humanists to grapple with the pre-digital origins and ideologies that inflect contemporary digital resources undergirding scholarship. EEBO is a microcosm through which one body of interpretations of digital humanities might be seen. 
            As much as it is a history, this presentation is also engaged in answering claims by Alan Liu and Tara McPherson, amongst others, that digital humanities has chosen disciplinarily to disengage from socio-critical questions.
                _
                
                     For work by Liu on this topic, see “Where is the Cultural Criticism in the Digital Humanites?,” published in 
                        Debates in the Digital Humaniteies, ed Matthew K. Gold . For McPherson work on UNIX and ideologies of race, see “Why are the Digital Humanities So White? or Thinking the Histories of Race and Computation” in the same volume .
                    
                 Thinking through the history of EEBO is one way to approach the digital humanities as a discipline tied to war-driven technological development; the uneasy relationships between private-sector providers and our shared cultural heritage; or the many varieties of labour that are imbricated within the knowledge infrastructures humanists use day in and day out.
                _
                
                     It is worth noting that one of the very few publications to deal with the EEBO set of projects in this way was published in 
                        Literary and Linguistic Computing (now 
                        Digital Scholarship in the Humanities). See Diana Kichuk, “Metamorphosis: Remediation in 
                        Early English Books Online (EEBO)” (2007) 22 (3): 291-303. DOI: http://dx.doi.org/10.1093/llc/fqm018. Kichuk’s efforts have helped establish a historical framework for this discussion; this presentation seeks to contextualise the facts she has brought together and extend their relevance into discourses about DH as a whole.
                    
                 Blending media analysis, historical perspectives, and in-depth knowledge of humanities research tools, I hope to question the boundaries of what we consider digital humanities to be, how we write our histories, and how we move forward.
            
        
        
            
                
                    Bibliography
                    About EEBO. 
                        Early English Books Online. 
                    
                    Hockey, S. (2004). The History of Humanities Computing. 
                        A Companion to Digital Humanities. (Eds.) Susan Schreibman, Ray Siemens, and John Unsworth. Oxford: Blackwell. 
                    
                    McCarty, W. (2008). What’s going on? 
                        Literary and Linguistic Computing, 23(3): 253-61, doi: 10.1093/llc/fqn014.
                    
                    Nyhan, J., Flinn, A. and Welsh, A. (2013). Oral History and the Hidden: Histories project: towards histories of computing in the humanities. 
                        Literary and Linguistic Computing, doi: 10.1093/llc/fqt044. 
                    
                    Svensson, P. (2009). Humanities Computing as Digital Humanities. 
                        Digital Humanities Quarterly, 3(3). http://digitalhumanities.org/dhq/vol/3/3/000065/000065.html.
                    
                
            
        
    


        
            Digital Humanities has seen slow adoption in the Slavic language and literature fields in North American academia. This issue frames our project, the Digital Émigré, a digital resource for exploring Russian émigré periodical literature. Our project has a threefold aim. As periodical studies scholars, we want to enable access to Russian émigré journals for new audiences. As digital humanists, we believe that DH tools and methodologies can facilitate new forms of knowledge about twentieth-century Russian, and more broadly diaspora, literary and cultural history. Finally, as Slavists, we hope our project will be a hub for discussion about the applicability of DH theory and practice for scholars working with Russian-language material.
            At this pilot stage, Digital Émigré is a web-based searchable database of article-level metadata of Russian-language journals published outside of Russia in the twentieth century. Our pilot contains four titles (approximately 100 issues and 1,500 articles): 
                Novoselye and 
                Novyi zhurnal were published in the 1940s in New York, and 
                Sintaksis and 
                Kontinent, in the late 1970s and 1980s in Paris. Our pilot site provides insight into literary culture at both the beginning and end of the Cold War, bookending the twentieth-century Russian diaspora experience. Digital Émigré is intended to scale, and will eventually contain additional titles and new functionality. 
            
            We will highlight the main scholarly avenues that DH methods allow us investigate, such as mapping networks of co-publication, tracking evolving political, social and cultural concerns of émigrés over the course of the Cold War, demonstrating the increased opportunities for émigré women as editors and contributors, and highlighting the proportion of original vs. re-printed work in émigré publications. This way, our project encourages experimentation that will enrich the study of Slavic periodical culture: accessing journals through their data can challenge narratives that are often framed by retroactive canonization, close reading and focus on individual authors. Digital Émigré thereby bridges philological approaches and sociological questions about intellectual networks and communities of artistic production. 
            The poster address the project’s core technical design: our strategy for data modeling and management and database design.  We will also present our plans for next steps, which is to provide full-text access and to federate our titles with other digital periodical collections. For this, we are designing a TEI schema modeled on major periodical studies digital collections -  specifically the Blue Mountain Project at Princeton University (
                http://bluemountain.princeton.edu and the Yellow 90’s Online at Ryerson University (
                http://www.1890s.ca)  
            
            We will also discuss the specific challenges of working with Russian language material and Cyrillic script, such as character encoding, transliteration, translation, and  tokenizing and stemming. These issues can be barriers to success when working with popular DH tools that are developed primarily for Western scripts and languages, and we will show our solutions for using some well-known tools for: data normalization (OpenRefine), text analysis (Voyant), network analysis (Gephi), visualization (Raw, Palladio), and topic modeling (MALLET).
            Digital Émigré is committed not only to the exploration of the intellectual experience of diaspora cultural life. As a digital humanities project, it is itself invested in building intellectual communities around the engagement with this material and its afterlife. It aims to foster contact between scholars working with Russian and other Slavic languages internationally, especially through the discussion of issues of interoperability and creating multilingual digital research environments.
        
    

Introduction
Scholarly discourse evaluating the digital turn in biblical and religious studies is at an early stage in its development, as attested to by the creation of two new

book series in 2016: Introduction to Digital Humanities: Religion (IDH, de Gruyter), and Digital Biblical Studies (DBS, Brill). Previously, Heidi Campbell published an overview of the topic (Campbell 2013), developed in further publications (Campbell-Althenho-fen 2015, Campbell-Garner 2016). In a recent overview, Carrie Schroeder develops two central questions on the topic: “what does it mean for Biblical Studies to be marginal to the Digital Humanities when DH is a field positioning itself as transformative for the humanities? How can our expertise in Biblical Studies influence and shape Digital Humanities for the better?” (Schroeder 2016). Using her field, Coptic studies, as an example she shows that the particular skills and needs of a marginal field within a marginal field can be a strong driver in DH.

Consequently, and for the first time at a DH meeting, this ninety-minute panel session asks what is the impact of the digital turn on religious studies and theology, and to what extent these somewhat marginal fields can bring something specific to the big DH tent. They particularly focus on textuality and on the symbolic impact of the “book” as attested to in the expression, “religions of the book,” coined in a programmatic lecture given in 1870 by F. Max Müller (2010). The symbolic, Western impact of books and writing was amplified by this notion, born at the time when the legal status of printed texts and authorship was completely secured in Western culture (Clivaz 2012).

For centuries, “books were perceived as a ‘wide angle' from which it was possible for everything to be observed, related to, and perhaps even decided” (Carrière-Eco 2009). The panel will consequently consider the hypothesis that the DH have been deeply influenced by this fascination with textuality and books during the first decades of their development; while keeping “the discourse of written texts” as a central pillar to the discussion according to the words of Roberto Busa, a foundational DH figure (Busa 2004). Busa's relationship to Biblical and religious materials has played a role in his approach to the computing field, as Jones point out (Jones 2016). The double impact of the book and the notion of “religions of the book”, successful in Western culture since the 19th century, provides an opening to understanding why DH in religious fields is still so focused on textuality. Indeed, when we collect examples of DH studies in diverse religious fields, we are unsurprisingly faced with very textual DH (Clivaz et al. 2016e). This observation

strengthens the necessity for religions in DH to consider the multimodal and multicultural turn provoked by digital culture.

With these different questions in mind, five panelists will participate in the presentations (sixty minutes in total) and a thirty-minute panel discussion that will be moderated by Claire Clivaz representing the Swiss Institute of Bioinformatics, Vital-IT (Lausanne, CH). The following five speakers have agreed to participate and to discuss the general topic from the perspectives of their own research projects. In alphabetical order:

A Neophyte Proselytizes for Digital Humanities Pedagogy
Emily S. Clark
This presentation explores the ways in which Digital Humanities can enhance a Religious Studies classroom by focusing on two assignments that ask new questions of traditional course materials. The first is a project that was the culmination of a month's work collaboratively amongst a class of 25 students with a database platform (Omeka). This project entailed the digitization of archival photographs of a Native American community from 1916, along with the reading of Jesuit mission material (Clark et al. 2016). The second is an assignment that took two class periods and introduced students to data visualization (Voyant). This assignment introduced students to the differences between close reading and distant reading, along with

practicing both on excerpts from Jesuit mission documents (Mentrak - Bucko, 2016).

Topic Modeling the Bible
Paul Dilley
The talk will present the first full-scale topic model of the Bible and related literature in four different languages: Greek, Latin, Syriac and English. It will discuss both technical aspects of the process (e.g., the use or not of lemmatization; retention or removal of function words; optimal number of topics), as well as what we gain from comparing topic models of the same corpus translated into different languages. The presentation will focus on the interpretive gains and losses involved in topic modeling, one of the richest strategies of distant reading to the Bible which has been the subject of centuries of minute examination of the close reading tradition which Moretti has pointedly labeled a “theological exercise” (Moretti 2013).

Digital Lives: Reading Moravian Memoirs in the Age of the Internet
Katherine M. Faull
An international collaborative research project (USA, Sweden, Germany) is developing a digital platform for the investigation of the metadata and text of Moravian memoirs, composed since the mid-18th century by members of the Moravian Church to be read at their funeral (over 65,000 memoirs, housed in Germany and the US, Faull 1997). Less than 10% of the earliest manuscripts have been published. The developing digital interface (moravianlives.org) allows for geospatial and chronological visualization of author's birth and death place (Haskins 2007). This paper will investigate the intersection of the digital, the autobiographical, and the sacred in the age of the internet. How can the act of reading the lives of thousands of Moravians also be understood as an act of reconstituting the “invisible church” ? (van Dijk, 2007; Eakin 2014).

Material Religions in a Digital World
Rachel McBride-Lindsey
For much of the modern era, religion and theology have been intertwined in a decidedly material world. Over the last several decades, students of religion have begun to carve out intellectual headroom for an approach to material culture that recognizes objects and images as generative sources of theological inquiry and religious practice. Cultural institutions can be an effective tool for inviting researchers and the public into physical spaces and into contact with deeper dimensions of the material world. At the same time, these very contributions work against methodological gains in the study of material culture. Rachel McBride-Lindsay's presentation starts with this tension and draws from pedagogical attempts to incorporate digital platforms into projects anchored in the study of objects.

Exploring developmental patterns within Digital Theology Research within the Digital Humanities
Peter Phillips
Campbell and Altenhofen (2015) explore four waves in digital research development in theology and religion back into the late twentieth century. Their wave pattern picks up both historical and technological trends and patterns in research. However, a three wave theory dominates discussion within introductions to the Digital Humanities, discussed by David Berry (2011) and in the Digital Humanities Manifesto 2.0. It tends to reflect modes of research, or groups of methodologies used in research rather than time periods. Reflecting on CODEC's own experience of Digital Theology in association with a range of other scholars, this paper will assess whether too many waves are a problem in our methodological theorizing.

Bibliography
Berry, D (2011), “The computational turn: thinking about

the Digital Humanities”, Culture Machine 12, 1-22.

Busa, R. (2004), “Foreword: Perspectives on the Digital Humanities”, in S. Schreibman, R. Siemens, J. Unsworth (ed.), A Companion to Digital Humanities, Oxford: Blackwell, http://www.digitalhumanities.org/companion/ (Accessed 27 March 2017).

Campbell, H.A (2013) (ed.), Digital Religion. Understanding religious practice in new media worlds, London/ New

York : Routledge.

Campbell, H.A. and Altenhofen, B (2015), “Methodological Challenges, Innovations and Growing Pains in Digital Religion Research”, in Digital Methodologies in the Sociology of Religion, S. Cheruvallil-Contractor - S. Shakkour (eds.), Blumsburry Publishing, Kindle edition.

Campbell, H.A. and Garner, S. (2016), Networked theology.

Negotiating faith in digital culture, Grand Rapids, MA : Baker Academy.

Carrière, J-.C. and Eco, U. (2009), N'espérez pas vous débarrasser des livres, Paris, Seuil.

Clark, E.S. et al. (2016), Digital Jesuits and Ignatian Pedagogy, King Island Collection, Jesuit Oregon Province Archives, Gonzaga University,    http://as-dh.gon-

zaga.edu/omeka/ (Accessed 27 March 2017)

Clivaz, C. (2012a), “Homer and the New Testament as ‘Multitexts’ in the Digital Age ?”, SRC 3/3, 1-15 ; http://src-online.ca/index.php/src/article/view/97 (Accessed 27 March 2017).

Clivaz et al. (eds.) (2016), Digital Humanities in Jewish, Christian and Arabic traditions, special issue JRMDC 5 (2016/1),    https://www.jrmdc.com/journal/is-

sue/view/9 (Accessed 27 March 2017).

Von Dijk, J. (2007), Mediated Memories in the Digital Age. Stanford: Stanford UP.

Moretti, F. (2013), Distant Reading, Verso, London, New York.

Schroeder, C.T. (2016), “The Digital Humanities as Cultural Capital: Implications for Biblical and Religious Studies”, Journal of Religion, Media and Digital Culture 5(1), 21-49, <http://www.jrmdc.com/journal/issue/view/9>    (Ac

cessed 27 March 2017).

Eakin, P. J. (2014), “Autobiography as Cosmogram”, Story-worlds: A Journal of Narrative Studies 6/1: 21-43.

Faull, K. M. (1997), ed. and trans., Moravian Women's Memoirs: Their Related Lives, 1750-1820, Syracuse, NY: Syracuse University Press.

Haskins, E. (2007), “Between Archive and Participation: Public Memory in a Digital Age”, Rhetoric Society Quarterly 37/4, 401-422.

Jones, S. (2016), Roberto Busa, S. J., and the Emergence of Humanities Computing: The Priest and the Punched Cards, Routledge Press, London.

Mentrak, T. and Bucko, R.A. (2016), Jesuit Relations and Allied Documents 1610 to 1791, http://mo-ses.creighton.edu/kripke/jesuitrelations/ (Accessed 27 March 2017).

Müller, F.M. (2010), “Second Lecture Delivered at the Royal Institution, February 26, 1870”, in F. M. Müller, Introduction to the science of religion. Four lectures delivered at the Royal Institution, Seneca Falls, NY: Wilson Press, 5282 (Original lectures February & May 1870).
Summary
Reproducing experimental results is a hallmark of empirical investigation and serves both to verify and inspire. This paper is a call for more systematic documentation of computational stylistic experiments. Publishing only summaries of the methods and results of empirical work is an artifact of traditional print media. To facilitate experimental reproducibility and to help the growing community who wish to learn how to apply computational methods and subsequently teach the next generation of scholars, the publication of results must include (i) access to the digitized texts, (ii) a clear workflow and most essentially (iii) the source code that led to each and all of the experimental results. By way of example, we present the steps and process in a GitHub repository for computationally probing the unknown and contested authorship of an 1831 short story entitled “A Dream” as we seek evidence if this work is similar to other attributed works by Edgar Allan Poe. The entire framework is intended as a pedagogical jumpstart for others, especially those new to computational stylometry. If Poe did write the story, it would be his first published work.

Introduction
As the Digital Humanities gains access to a wide array of digitized corpora and matures to a discipline that creatively defines new methods for computationally close and distant readings, a growing gap has emerged between those who apply sophisticated programming, e.g., Stylo In R (Eder et al., 2016) and those who are new to the game and need an introduction to the field. Typical of the community spirit in DH, significant efforts are underway to bridge this gap, including web-based tools for entry-level exploration including

Voyant Tools (Sinclair and Rockwell, 2016) and Lexos (Kleinman et al., 2016) and domain-specific introductions to programming, including Jockers' text (2014) and the Programming Historian (Crymble et al., 2016). This paper attempts to narrow the gap by encouraging both sides to document their experimental methods more fully to embrace previous calls for the replication of experimental methods (Rudman, 2012 et al.) and thereby teach effective practices by “leaving a trail” of experimental methods that enable others to execute and extend.

A Good Mystery: Towards Reproducibility
A GitHub repository or “repo” offers a workflow

that explores whether an 1831 story published under

the attribution of only ‘P' might have been written by Edgar Allan Poe. If so, it would be Poe's first published work. In addition to sharing a set of analytical methods applied in this experiment, the broader methodological-pedagogical goals are two-fold: (i) the dissemination of data and code should be championed as a cornerstone of DH research, thereby facilitating the replication of results and (ii) to share a workflow so that others may apply similar analyses to their texts of interest.

The workflow is stored as a set of numbered folders containing the texts and scripts (code) needed to complete each step. The workflow includes: collecting texts, the preprocessing, tokenization, and culling decisions made, unsupervised cluster analyses (k-means, hierarchical-agglomerative, bootstrap consensus tree), and supervised classification methods using Stylo in R's Delta, SVM, and NSC models. Each step represents scaffolding for a “teachable moment” with materials provided so faculty can more easily use them with students.

Scrubbing, Tokenization, Cutting, and
Culling
Lexos, a web-based, open-source workflow of tools (Kleinman, et al., 2016) was used to upload texts and “scrub” them by applying the following options: (i) convert words to lowercase, (ii) all punctuation was removed, (iii) however, a single word-internal hyphen and word-internal apostrophes were kept, and (iv) all digits were removed. Each individual word is considered as its own token. Larger stories were segmented (“cut”) into pieces. We experimented with various culling options, e.g., keeping only the most frequent words that appear in each text at least once.

Cluster Analysis
As a set of initial probes, we compared the contested story “A Dream” to (i) other stories attributed to Poe and (ii) mixed in with stories by other contemporaries. In the repo, we share four variations using cluster analysis:

1. K-means clustering on only Poe's stories (using Lexos)

2. Hierarchical agglomerative clustering on only Poe's stories (uses a Python sklearn module and a script to convert the cluster to ETE and Newick formats)

3. K-means clustering when all stories by each author are concatenated together (Lexos)

4. Bootstrap Consensus Tree (using Stylo in R).

The result from the Bootstrap Consensus Tree is shown in Figure 1. Of interest is that each author's stories cluster consistently together (with the exception that Bird's initial section of “Sheppard Lee” and his “Calavar” are found in different clades, at six and eight o'clock). “A Dream” clusters with the smaller Poe texts. As you'll see, we couldn't resist tossing in the four stories sometimes attributed to Edgar's brother Henry (“Monte Video”, “A Fragment”, “The Pirate”, and “Recollections”). These four stories are found within the cluster of Poe's known works (c.f. Collins, 2013).

A series of cluster analyses often serves well as a preliminary exploration, especially for scholars who are new to this game. Some of the file sizes are very small (e.g., one-half of the Poe stories in this corpus have fewer than 2000 words) and when strict culling is enforced (top-N words that appear at least once in each segment), the available set of words is reduced to only 38 when dealing with “A Dream” and the other eighteen Poe stories. That noted, these exploratory investigations shed some light on why some scholars consider that Poe's “first published tale may have been ‘A Dream'” (Silverman, 1991, p87).


Figure 1. Using Stylo in R Bootstrap Consensus Tree (BCT) showing “A Dream” consistently clustering with other Poe stories. The BCT aggregates results over multiple cluster analyses and shows those texts that satisfy a consensus number of the individual trials. Using 12 different authors

and at least two texts by each author for a total of 46 stories, Stylo formed clusters of the texts for the following frequency bands when using the most-frequent words: 100 to 1000 MFW.

Classification
Three classification models differentiated authorial writing style as implemented in Stylo in R. We scripted in R alongside Stylo to test “A Dream” over N-trials (N=10, 100) using a random selection of files for training sets in each trial. At least one text from each author is also included in the test set for each trial. A followup Python script parses the collected results to build confusion matrices for each author to provide metrics on how well the models predict each author's works. The most-frequently occurring, top-40 words (MFW, 1-grams) that appear in all the texts at least once were used.

Confusion Matrix values for all Poe Stories

Model

Attributions of

“A Dream” to Poe

True+

True-

False+

False-

Delta

9

13

200

0

7

NSC

10

16

170

30

4

SVM

7

14

198

2

6

Table 1: Attributions of the contested story “A Dream” over ten (10) trials with “A Dream” and another randomly selected Poe story in the test set in every trial. Confusion matrix values for results of testing Poe texts over all trials provide overall measures of model effectiveness. In the three cases

where “A Dream” was attributed to a different author, Poe was ranked second.

Summary
We offer a start to an exploration to collect evidence

as to whether Poe may have written the 1831 story “A

Dream” (c.f., Schoberlein (2016) who used the most frequent character 3-grams and attributed the story to Poe using Delta, but not so when using NSC nor SVM models). Evidence and methods aside, a GitHub repo provides a framework to share experimental workflows in a spirit similar to Jupyter notebooks, as well as one that facilitates both reproducible results and opportunities for subsequent contributions.

Notes
Forming an appropriate corpus is hard: thanks to Sam Coale, Ryan Cordell, Cary Gouldin, David Hoover, Shirrel Rhoades, and Ted Underwood. Four undergraduates: Weiqi Feng, Alec Horwitz, Jingxian Liu, and Khaled Sharafaddin worked with us on this problem. Thanks to Maciej Eder for his help with Stylo in R.

Sinclair, S. and Rockwell, G., (2016). Voyant Tools. Web: http://voyant-tools.org/.

Bibliography
Crymble, A., Gibbs, F., Hegel, A., McDaniel, C., Milligan, I., Taparata, E., Visconti, A., and Wieringa, J.,

eds. (2016). The Programming Historian. 2nd ed.. Web: http://programminghistorian.org/.

Eder, M., Kestemont, M. and Rybicki, J. (2016). Stylometry with R: A package for computational text analysis. R Journal, 16(1): 107-121.

GitHub repository:    A Good    Mystery.    .

https://github.com/WheatonCS/aGoodMystery

Jockers, M. (2014). Text Analysis with R for Students of Literature. Springer, New York.

Kleinman, S., LeBlanc, M.D., Drout, M., and Zhang, C.

(2016). Lexos v3.0. Web: http://lexos.wheatoncol-lege.edu.

Rudman, J. (2012). The State of Non-Traditional Authorship Attribution Studies -- 2012: Some Problems and Solutions. English Studies, v93(3), 259-274.

Schoberlein, S. (2016). Poe or Not Poe? A Stylometric Analysis of Edgar Allan Poe's Disputed Writings. Digital Scholarship in the Humanities, July 24, 2016.

Silverman, K. (1991). Edgar A. Poe: Mournful and Never-Ending Remembrance. HarperCollins, New York.
Lexos is a browser-based suite of tools that helps lower barriers of entry to computational text analysis for humanities scholars and students. Situated within a clean and simple interface, Lexos consolidates the common pre-processing operations needed for subsequent analysis, either with Lexos or with external tools. It is especially useful for scholars who wish to engage in research involving computational text analysis and/or wish to teach their students how to do so but lack the time for a manual preparation of texts, the skill sets needed to prepare their texts analysis, or the intellectual contexts for situating computational methods within their work. Lexos is also targeted at researchers studying early texts and texts in non-Western languages, which may involve specialized processing rules. It is thus designed to facilitate advanced research in these fields even for users more familiar with computational techniques. Lexos is developed by the Lexomics research group led by Michael Drout (Wheaton College), Mark LeBlanc (Wheaton College), and Scott Kleinman (California State University, Northridge). It is built on Python 2.7-Flask microframework, with jQuery-Bootstrap UI, and visualizations in d3.js. The Lexomics research group provides access to an public installation of Lexos which does not retain data after a session has expired. Users may also install Lexos locally by cloning the GitHub repository.

Lexos guides users through a workflow of steps that reflects effective practices when working with digitized texts. The workflow includes: (i) uploading Unicode-encoded texts in plain text, HTML, or XML formats; (ii) “scrubbing” functions for consolidating preprocessing decisions such as the handling of punctuation, white-space, and stop words, the use of lemmati-zation rules, and the handling of embedded markup tags and special character entities; (iii) “cutting” texts into segments based on the number of characters, tokens, or lines, or by embedded milestones such as chapter breaks; (iv) tokenization into a Document Term Matrix of raw or proportional counts using character or word n-grams; (v) visualizations such as comparative word clouds per segment (including the ability to visualize topic models generated by MALLET); Rolling Window Analysis that plots the frequency of string, phrase, or regular expression patterns or pattern-pair ratios over the course of a document or collection; and (vi) analysis tools including statistical summaries, hierarchical and k-means clustering, cosine similarity rankings, and Z-tests to identify the relative prominence of terms in documents, document classes, and the collection as whole. At each stage in the workflow the user may download data, visualizations, or the results of the analytical tools, along with metadata about their preprocessing decisions or the parameters selected for their experiments. Lexos thus enables the export of data for use with other tools and facilitates experimental reproducibility.

Lexos {scrubber} An Integrated Lexomics Workflow

Scrubbing Options

Q Remove All Punctuation

B Keep Hyphens ©

Q Make Lowercase

B Keep Word-Internal Apostrophes©

Q Remove Digits

■ Remove Whitespace 0

B Scrub Tags 0

Additional Options

Stop Words/Keep Words O >


Previews of Documents


A1.3_Dan_T00030.txt




Gefr&ae;gn ic Hebreos eadge lifgean in Hierusalem goldhord d&ae;lan cyningdom hab ban swa him gecynde w&ae;s si&d;&d;an &t;urh metodes m&ae;gen on Moyses hand w

ealra gesceafta drihten and waldend se him dom forgeaf unscyndne bl&ae;d eor&d;an rices and &t;u lignest nu &t;&ae;t sie lifgende se ofer deoflum duge&t;um wealde&d;. A3.3_Az_T00130.txt


orn dryhten herede wis in weorcum ond &t;as Word acw&ae;&d;: Meotud allwihta &t;u eart meahtum swi&d; ni&t;as to nerganne. Is &t;in noma m&ae;re wütig ond wul h




Special Characters 0 v


Lemmas 0 v


Consolidations 0 v


Figure 1: The Lexos Scrubber Tool

Lexos addresses three significant challenges for our intended users. The first challenge involves the adoption of computational text analysis methods. Many approaches require proficiency with command line scripting or the use of complex user interfaces that require time to master. Lexos addresses this problem through a simple, browser-based interface that manages workflow through the three major steps of text analysis: pre-processing, generation of statistical data, and visualization. In this, Lexos resembles Voyant Tools (Sinclair and Rockwell, 2016), although Lexos places more emphasis on and providing more tools for preprocessing and segmenting texts. Lexos also shares with tools like Stylometry with R (Eder, et al., 2013; Eder, 2013) and emphasis on cluster analysis, providing both hierarchical and K-Means clustering with silhouette scores as limited form of statistical validation. While Lexos is not a topic modeling tool, it provides a useful “topic cloud” feature for MALLET data that will be useful for beginners since there are few accessible ways to visualize MALLET output that work well out of the box.


Figure 2: The Lexos Multicloud tool showing Chinese "topic clouds"

The second challenge is the opacity of the procedures required to move between computational and traditional forms of text analysis. In order to reduce the “black boxiness” of algorithmic methods, Lexos contains an embedded component called “In the Margins” which provides non-technical explanations of the statistical methods used and effective practices for

handling situations typical of humanities data. “In the

Margins” is a Scalar “book” which can be read separately; however, its individual pages are embedded in Lexos using Scalar's API, making them easily accessible for users of the tool. Lexos shares with tools like Voyant an engagement with the hermeneutics of text analysis and attempts to embed “In the Margins” discussion of these issues in the user interface close to the user's workflow. We hope “In the Margins” will host advice and commentary from contributors with the Digital Humanities community.

A third challenge is the tension between quantitative and computational approaches and the traditions of theoretical and cultural criticism that dominate the humanities in the academy. As Alan Liu (2013) has recently argued, the challenge is to give a better theoretical grounding to the hybrid quantitative-qualitative method of the Digital Humanities by exploring the ways in which we negotiate the difficulties imposed by “the aporia between tabula rasa quantitative interpretation and humanly meaningful qualitative interpretation” (414). The design of Lexos and the discussions in “In the Margins” are intended to open a space for discussion of issues related to the opacity of algorithmic approaches and the limitations and epistemological challenges of computational stylistic analysis and visual representation of humanities data.

This poster presentation provides demonstrations of Lexos using some literature from Old, Middle, and Modern English, as well Chinese, which are in our current test suite. We also discuss use cases and best practices, how to install Lexos locally, and how scholars may contribute to the still growing content of “In the Margins”.

Bibliography

Drout, M., Kleinman, S., and LeBlanc, M. 2016-. “In the Margins.” http://scalar.usc.edu/works/lexos./

Eder, M. (2013). “Mind Your Corpus: Systematic Errors in Authorship Attribution.” Literary and Linguistic Computing 28 (4): 603-14.

Eder, M., Kestemont, M., and Rybiki, J. 2013. “Stylometry with R: A Suite of Tools (Abstract of Poster Session)”. Presented at Digital Humanities 2013, Lincoln, Nebraska. http://dh2013.unl.edu/abstracts/ab-136.html, https://sites.google.com/site/computationalstylistics/

Kleinman, S., LeBlanc, M.D., Drout, M. and Zhang, C. 2016. Lexos v3.0. https://github.com/WheatonCS/Lexos/.

Liu, A. (2013). “The Meaning of the Digital Humanities.” PMLA 128 (2): 409-23.

McCallum, A.K. (2002). MALLET: A Machine Learning for

Language Toolkit. http://mallet.cs.umass.edu.

Sinclair, S., and Rockwell, G. (2016). Voyant Tools. Web.

http://voyant-tools.org/.
Introduction
Visualization techniques developed in the sciences normally focus on the (re)presentation of empirical data. But how can we graphically express interpretations? This paper presents the intellectual framework

underpinning the 3DH project (Three-dimensional Visualizations for the Digital Humanities), a collaborative project conducted at the University of Hamburg from 2016 to 2019. The project foregrounds data interpretation and develops a visualization paradigm from the epistemological perspective of the humanities. The “third dimension” required in DH visualization techniques is therefore not merely that of an additional quantitative z-axis. Rather, it is an axis that can ‘unflatten' (Sousanis 2015) the objectivist notion of visualized data. In our presentation, we will do three things:

•    Digital and visual turn: Review existing visualization paradigms that emphasize the representational approach. We start with the epistemological issues raised by the digital and visual turn.

•    Visual modelling: Outline and discuss an interpretative modelling alternative through two case studies of existing tools, CATMA and Voyant, and Temporal Modelling, a platform for creating data through graphical means.

• “Hermeneuticizing” visualization: Discuss the design of a full visual framework. We will present possible conventions and prototypes that use them. These inform our case studies and the envisaged infrastructure.

Case studies in our presentation will be drawn from CATMA (a collaborative mark-up & text analysis environment), Voyant (a text analysis platform), and humanities research projects using base images (historical maps) and original models (for non-standard chronologies).

The digital and the visual turn: a hermeneutic ceterum censeo

For centuries, academic discourse in humanities disciplines has relied predominantly on text. In DH, however, visualizations increasingly claim the status of arguments and proofs that play a decisive role in the development and presentation of ideas, findings, and conclusions.

The visual and the digital turn have thus gone hand in hand - but the way in which this synergy manifests itself remains constrained in a symptomatic way. We can print a chart or render it on screen just as we can print or display a text in various media, but we normally cannot subject the chart to in-depth critique in the way we can question and respond to the text. Inadvertently, once generated and communicated as ‘output', visualizations seem to take on a quasi-dogmatic quality - they are hard to deconstruct, let alone reconfigure; they state their case but seem removed from critical reflection.

Most current DH visualizations are thus epistemological one-way avenues toward knowledge, from data via rendering algorithm to visual display. Charts, graphs, interactive maps, timelines, and similar representations are by and large imports from the natural and social sciences (Friendly 2008). Many of them emanate from domains of empirical research that conceptualize knowledge production as a function of empirical observation and objective measurement followed by analysis, inference, and conclusion. These approaches to visualization, however, hide two critical aspects, namely

(a) the underlying human modeling of the represented phenomena as data, which is already an interpretive and meaning-creating act that often oscillates repeatedly between observation and interpretation (Kitchin 2014), and

(b) the meaning-lessness of certain visual effects that are owed to contingent technological constraints (screen size, rendering, scaling, choice of color, etc.).

DH is in a unique position to investigate the domains of human experience and of its expression in symbolic practices and artefacts from two complementary methodological vantage points: the numeric, which models them as statistical phenomena, and the hermeneutic, which explores them as phenomena of meaning and thus by definition as a function of interpretation (Rockwell & Sinclair 2016). Where meaning comes into focus, our theories, object models, and practices

must therefore be conceptually aligned and ‘herme-

neuticized' - just as numeric approaches come with the pre-requisite of quantification. Against this backdrop, we propose to reintroduce the dimension of interpretation into visualization: Methodological principles of hermeneutic approaches, such as multi-per-spectivity, subjectivity, and context-boundedness present a challenge which representational visualization cannot and which interpretational visualization must meet.

Two questions arise: What are the defining principles of a genuinely humanistic and hermeneutically oriented approach to visualization? And how can we graphically express and support interpretation in DH visualizations - both as an activity and as a product of humanistic enquiry?

Visual modeling of interpretation vs. visualization of data

In the 3DH project, we address the former question by conceptual analysis and critique of existing approaches to visualization in DH, and then by systematically specifying and developing a visualization environment that can support higher level data interpretation rather than base-level data representation. In the presentation, we will share our survey of existing tools and their affordances but focus on two tools that we have developed, CATMA and Voyant.


Figure 1: Visualization of interpretive text annotation in CATMA

Our premise is that interpretation happens through the deliberate activity of an individual engaging with an image, text, display, or other artifact to create an argument about its meaning and a way it should be read.

For example, in CATMA (Figure 1) such an activity - in this instance the interpretive act of text annotation - is executed and represented by (a) highlighting a string on screen, (b) assigning it a tag, and (c) storing the annotation in a stand-off markup file. However, the annotation is at the same time (d) visually expressed as colored underlining. Moreover, via its visual representation on screen - the colored underlining - the markup data can also be (e) inspected, analyzed, manipulated directly, and even (f) enriched with meta-annotation. This is but one example of interpretative modeling.

Current representational ‘one-way' techniques like topic modeling (see Figure 2) are seen as a way to deal with scale, they process large amounts of data into summary abstractions called topics that can be displayed as lists or in other ways (Montague et. al 2015). In our second case study, we will therefore show how we are adapting scale tools to create a prototypical bidirectional 3DH visual modeling environment for big



Figure 2: Galaxy Viewer

data. We believe visual modeling can support not only interpretative close reading of primary data but also the reading of large collections like the collections of the Hathi Trust.

‘Hermeneuticizing' base-level visualization through activators: the 3DH framework of interpretive parameters and dimensions

A key goal of the 3DH project is to develop a set of generic graphic features that can be used to create interpretative attributes and/or inflections of visual representations of data, alter underlying data structures, and activate three-dimensional space in the service of interpretative activity. These features which aim to ‘hermeneuticize' visualizations are termed activators. In the presentation we will show the framework of the

activator set that was developed during a series of cha-

rettes (design workshops) in 2016.


Figure 3: Framework of Concept Modeling workspace: Shows features, activators, and dimensions from various pictorial conventions.

The visual activators in our feature set are not simply graphical marks or animations on a screen display: They perform data structuring functions and as such provide a conceptual framework for ‘hermeneu-ticizing' existing base-level data visualization techniques (see Fig.3). The individual features of this framework indicate and facilitate interpretative moves made by the user, such as a qualification of visualized data structures in terms of salience, irrelevance, uncertainty, degree of completeness, and other attributes or inflections. For example, uncertainty can be expressed by overlaying a standard graph with visual effects such as blur or shading, whereas the introduction of additional interpretative dimensions, such as point of view systems, parallax, relative scales, and other conventions from the visual arts, will support higher levels of interpretative critique and reflection, such as explicitly marking the historicity and context-dependency of underlying data.

Conclusion

As Pinker (1990) argues, the ease with which a particular graph can be understood is a function of the processing effort that goes into the exercise: The more we can rely on ‘hard-wired' encoding connections between the visual and the conceptual and the more we are guided by established graph and comprehension schemata (such as Gestalt phenomena), the less ‘intelligent' effort we have to put into reading a graph. Yet in a humanities perspective such conventionalized ‘ease of comprehension' is a double-edged sword: It may optimize the process of (re)cognition - but it also progressively obscures the constructedness of a visualization, turning it into an apparently self-evident object of perception. The 3DH project counters this anti-hermeneutic tendency toward reification by moving from a conceptualization of the principles of visualization as interpretative modeling to the development of a visual language framework, and finally the instantiation of the principles and language in two case studies. In terms of implementation, this approach is supported by drawing on Bertin's Semiology of Graphics and the high-level object-oriented Grammar of Graphics approach outlined by Wilkinson (2005), and features from game engines, three-dimensional modelling, and other pictorial conventions (Panofsky (1991) and Burgin (1991)).

To conclude, we will discuss next steps toward developing a 3DH environment that can act as a generic, project independent infrastructure for introducing user parameterized enunciative functionality into graphical displays. This infrastructure will make it possible to inscribe into visualizations the critical features of authorship, speaking/spoken subject, and an epistemological perspective grounded in situated and constructed approaches to knowledge. These interpretative principles are well mapped in, e.g., critical theory, narratology, visual studies, and cultural studies, but they have not been integrated into a graphical environment for hermeneutic practice yet: the methodological lacuna which the 3DH project tries to address.

Bibliography

Bertin, J. (1983). Semiology of Graphics: Diagrams, Networks, Maps. Madison, WI, University of Wisconsin Press.

Burgin, V. (1991). “Geometry and Abjection”. In: J. Donald

(ed.), Psychoanalysis and Cultural Theory: Thresholds.

London, Macmillan Education, pp. 11-26 .

Chandrasekaran, B. & Lele, O. (2010). “Mapping Descriptive Models of Graph Comprehension into Requirements for a Computational Architecture: Need for Supporting Imagery Operations”. In: A. K. Goel, M. Jamnik & N. H. Narayanan (eds.), Diagrammatic Representation and Inference. 6th International Conference, Diagrams 2010, Portland, OR, USA, August 9-11, 2010. Proceedings. Berlin & Heidelberg, Springer Verlag, pp. 235-242.

Drucker, J. (2011). “Humanities approaches to Graphical

Display”. In: DHQ, Digital Humanities Quarterly, 5 (1). http://digitalhumani-

ties.org/dhq/vol/5/1/000091/000091.html [March 17 2017].

(2014). Graphesis, Cambridge, Harvard University Press.

Friendly, M. (2008). “A Brief History of Data Visualization”. In: C.-H. Chen, W. K. Hardle & A. Unwin (eds.), Handbook of Data Visualization. Heidelberg, Springer-Verlag, pp. 134.

Kath, R., Schaal, G. S. & Dumm, S. (2016). „New Visual Hermeneutics“. In: Cybernetics and Human Knowing, 23 (2), pp. 51-75.

Kitchin, R. (2014). The Data Revolution: Big Data, Open Data, Data Infrastructures & Their Consequences. Los Angeles, SAGE.

Montague, J., Simpson, J., Brown, S., Rockwell, G. & Ruecker, S. (2015). “Exploring Large Datasets with Topic Model Visualization”. Conference paper at DH 2015, University of Western Sydney, Australia.

Panofsky, E. (1991). Perspective as Symbolic Form; C. Wood, trans.; New York, Zone Books.

Pinker, S. (1990). “A Theory of Graph Comprehension”. In: R. Feedle (ed.), Artificial Intelligence and the future of testing. Marwah, NJ, Erlbaum Hillsdale, pp. 73-126.

Rockwell, G. & Sinclair, S. (2016). Hermeneutica. Cambridge, MS & London, MIT Press.

Sousanis, N. (2015). Unflattening. Cambridge, MS & London, Harvard University Press.

Wilkinson, L. (2005). The Grammar of Graphics. 2nd ed.; New York, Springer.
Introduction
Voces (from Lat. vox 'voice', 'word') is an analysis

and visualisation dashboard for corpus-based research in lexical semantics. Currently developed as a Shiny application communicating with R session running in the background, Voces provides users with possibly exhaustive account of how selected Latin word is distributed across the corpus and what can be told about its meaning. The application is built around a corpus which currently consists of ca. 200M words from texts dating from the Classical era (1 BCE) to the Middle Ages (14th CE). Although Voces was originally conceived as a tool of historical semantics research, the application - due to its modular design - may be modified and the code basis can be re-used in new research contexts.

Lexical Semantics with R


Information computed on a basis of a CWB-indexed corpus is presented to a user through a single-page interface composed of separate widgets arranged in a clear grid layout. Each widget is responsible for displaying in textual or graphical form a clear-cut property of word's distribution or meaning. A heavy use of data visualisation techniques renders Voces a convenient tool for exploratory analysis of textual corpora, but the grid layout is also reflection of modular architecture of the application. Each widget is implemented as a separate function which can be extended and adopted by researchers with even limited R programming skills.

Use scenarios
A typical use scenario is triggered when the user specifies a lemma to be looked up. If the search fails, a list of lemmas to choose from is provided. In case of success, neatly separated sections of the dashboard are populated with widgets, each of which corresponds to one sense or distributional property of the word under scrutiny.


Fig. 2: Voces. User Interface: Frequency Spectrum Plot (Voces. User Interface (tempus 'time')

Word's frequency is summarised as a number of occurrences in the corpus (both raw and p.m.w. counts) and displayed as a highlighted point on a frequency spectrum plot (Baayen 2001). A barplot is provided for investigating change of frequency in subsequent corpus sections. Study of language variation is enabled through widgets presenting word's frequency as a function of such variables as author, work, genre, and - most importantly - time. Users are, therefore, provided with a list of authors who use the word most frequently or a word cloud summarising terms to be found in the titles of works with a particularly frequent use of the word under scrutiny. Genre variation is presented in form of a pie chart, while diachronic dimension - through a bar plot of frequency counts in partitions of the corpus. Diatopic variation study is still to be implemented.

A word's meaning potential can be investigated by means of a set of widgets presenting its contextual properties. The most frequent co-occurrences are enumerated on a simple count list which may be further analysed according to period and genre criteria. A Distributional Semantics Model (Baroni and Lenci 2010) is built from the corpus in order to enable simple meaning computation. Evert's (2014) wordspace package and a set of Alain Guerreau's scripts is employed in order to cluster co-occurrences. Similar terms of a looked up word are also computed and then presented in both textual and graphical form.

Users are supported in data and visualisation interpretation through hints which accompany every widget. Their role is to explain not only what the data can mean, but also how the figures were computed, how one can interpret the geometrical properties of a plot, and so on. This, along with the availability of data sets, code snippets, and reports generated on the fly, is what makes Voces a tool of reproductive research.

Architecture
Voces was built as a Shiny application (Chang et al. 2016). Its development was greatly facilitated by the availability of a decent documentation and community support (both particularly useful when dealing with framework's complex reactivity model). It turned out soon, however, that it may not be the best choice for web application which has to combine heterogeneous data and non-R code as well. Hence, other solutions are being tested at the moment, those in particular which would provide, for example, more flexible integration of external APIs. The most promising seems to be OpenCPU (Ooms 2014), an application which exposes R session through a RESTful API. This approach allows any application written in some of the less or more popular web development frameworks to easily communicate with an R server instance.

As for the architecture, Voces depends on a CQP server instance running in the background which requires corpora to be indexed with the CWB. Communication of the R server with the CWB is assured through the rcqp package (Desgraupes and Loiseau 2012) which offers a set of useful functions providing access to both positional (token-level) and structural (document-level) attributes. Unfortunately, development of this very helpful tool seems to be less active recently and thus Voces will soon accept also tabular data as input.

Previous research
Nowadays, corpus linguists may chose from a vast array of free, open source and stable corpus query systems (CQS) which not only allow for efficient indexing of large corpora, but also provide a user-friendly concordance interface and offer out-of-the-box a set of such essential functionalities as collocation lists, simple corpus statistics etc. Both web (CQPweb, NoSketchEngine etc.) and desktop applications (TXM etc.) are also usually equipped with a less or more intuitive corpus management interface. Voces, a dashboard for vocabulary research, is not yet another CQS and has no intention to supersede well-established tools which cannot be easily combated in terms of either robustness or speed. Quite the contrary, the application communicates with the CWB engine and adapts some of the design choices and features of the popular CQS, while hopefully does not inherit their drawbacks.

Unlike the case of the well-known CQS, more emphasis has been put on quick access to multifaceted information rather than on close analysis of occurrences. Voces does not attempt, then, to implement some of the features which are traditionally considered an important part of the corpus analytical toolbox, such as concordance sampling, sorting etc. Undoubtedly, the strength of popular CQS lies in their wide applicability: by default, they do not preclude any research scenario. Although agnostic of linguistic theory, Voces was originally built for more specific purposes and focuses on semantic properties of the word and its distribution.

What is believed to be one of the main advantages of the present application is that - thanks to its modular architecture - it can be easily extended or adopted by a researcher with even moderate programming skills. In that Voces attempts to fill the gap that exists between, on the one hand, fully-blown CQS, which are normally quite conservative when it comes to adding

new features, and, on the other hand, single-purpose

research workflows built ad hoc by researchers. What also distinguishes Voces from other CQS is its emphasis on helping users to interpret data. A system of visual and textual hints keeps a researcher informed about where does the data come from, how have they been computed etc.

The grid layout is well-known from analytical environment and is especially popular in finances or engineering (Few 2013); in humanities it was adopted, among others, in the Voyant Tools project. It offers a quick insight into otherwise dispersed data and a coherent account of word's properties.

Further research
Voces is currently in an early stage of development. The work focuses on adding new functionalities and plotting types which may sometimes affect application's efficiency. Future work will focus on: (1) optimising user experience; (2) implementing tools for (a)

comparative (ie. two-lemma) research and (b) tracking language change; (3) better processing user input (multi-word search).

Bibliography
Baayen, R. H. (2001). Word Frequency Distributions. Dordrecht: Kluwer.

Baroni, M., and Lenci, A. (2010). Distributional Memory: A

General Framework for Corpus-Based Semantics. Com-

putationalLinguistics 36 (4): 673-721.

Chang, W., Cheng, J., Allaire, J. J., Xie, Y., and McPherson,

J. (2016). Shiny: Web Application Framework for R.

https://CRAN.R-project.org/package=shiny.

Desgraupes, B., and Loiseau, S. (2012). Rcqp: Interface to the Corpus Query Protocol. http://CRAN.R-pro-ject.org/package=rcqp.

Evert, S. (2014). Distributional Semantics in R with the Wordspace Package. In Proceedings of COLING 2014, the

25th International Conference on Computational Linguistics: System Demonstrations, 110-114. Dublin, Ireland:

Dublin City University and Association for Computational Linguistics.

Few, S. (2013). Information Dashboard Design: Displaying Data for at-a-Glance Monitoring. Burlingame, CA: Analytics Press.

Nowak, K., and Bon, B. (2015). Medialatinitas.eu. Towards

Shallow Integration of Lexical, Textual and Encyclopaedic Resources for Latin. In Electronic Lexicography in the

21st Century: Linking Lexical Data in the Digital Age. Proceedings of the eLex 2015 Conference, edited by Iztok

Kosem, Milos Jakubi'cek, Jelena Kallas, and Simon Krek, 152-69. Ljubljana-Brighton: Trojina, Institute for Applied Slovene Studies - Lexical Computing Ltd.

Ooms, J. (2014). The OpenCPU System: Towards a Universal Interface for Scientific Computing through Separation of Concerns. ArXiv E-Prints, June.
Introduction

Pedagogy in the Digital Humanities is now leaving its “bracketed” state - a term used by HIRSCH 2012 to emphasise the fact that this dimension was not given the consideration its practical importance deserves. As programmes and courses are created on a larger scale and increasingly drive institutional strategies, also in Europe (see Sahle, 2013 and the DARIAH Digital Humanities Course Registry), it becomes essential to make comparisons and shared reflections possible.

Since 2014 all students of Greek and Latin languages and literatures at the Université Paris-Ouest Nanterre (France) have been enrolling in a Master programme entitled “Humanités classiques et humani-tés numériques.” Each semester features a fully fledged course of Digital Humanities: it is therefore an experiment in embedding Digital Humanities into an existing discipline, or rather into the array of disciplines which constitute the field of Classical studies around its philological backbone.

The aim of this poster is to share the approach I take in designing and teaching these courses, and to reflect on what this experience suggests about digital educational models, in Classics and beyond.

The poster will have three components, devoted to situating, describing and comparing the courses. Context and History

I will set out the conditions in which the curriculum was reformed (which involves both national and local contexts), the specific problems encountered (as the heterogeneous levels and motivations of the students, the relationships with the other courses, the available technical options, or the recent introduction of podcasting and distance learning), as well as the rationale and methods which shape the courses, including its main sources of inspiration in the Digital

Humanities community, whether online syllabi or publications like Jockers (2014) and Rockwell and Sinclair (2016).

Overview of the Courses

The courses alternately take the form of more traditional classes and collaborative or personal projects. Across the two years, their contents include theoretical and historical insights, while concentrating on hands-on experience: digital literacy elements are gradually integrated as students go from traditional scholarly editing recreated in Markdown and HTML to critical editing in TEI XML (the focus of year 1) and, beyond text and editing, discover computer-assisted analytical and visualisation methods with the Voyant Tools software environment and then work in a literate programming framework (For which the canonical reference is Knuth, 1984) implemented in R Markdown (the focus of year 2, see Figure 1).


Figure 1: Text analysis in RStudio

The principles of the courses will be expounded: favouring active participation, learning-by-doing and flipped classroom teaching; insisting on the critical, reflexive dimension of digital procedures; promoting free resources like TEI by Example (Van den Branden, Terras, and Vanhoutte) and The Programming Historian (Crymble et al), as well as data reuse; developing an open publication culture through the Classiques et numériques blog maintained by the students (see Figure 2) or a shared Zotero group library; creating an awareness of the surrounding Digital Humanities communities; fostering actual collaboration, both between the students and with other projects or programmes - to date, with another MA specialised in Web design on an online edition prototype, with the Pelagios Commons project on the annotation of place names and with the Sunoikisis

Digital Classics network in its effort to collectively define a core syllabus.


Figure 2: Classiques et numériques, the blog of the MA

Sahle, P. (2013). “DH Studieren! Auf Dem Weg Zu Einem Kern- Und Referenzcurriculum Der Digital Humanities.” DARIAH-DE Working Papers, 1. http://web-

doc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf.

Van den Branden, R., Terras, M. and Vanhoutte, E. (n.d.)

“TEI by Example.” accessed 1 November 2016.

http://teibyexample.org/.

Comparing Models

Finally, drawing on this experience I will address several aspects of the current development of Di-gital Humanities pedagogy: as a separate entreprise or within established disciplines, with or without infrastructural, collegial or cross-departmental support, in various time formats, with different modes of external collaboration, etc. To sketch this broader typology, I will compare this French series of courses with other models, using in particular the data contributed to the aforementioned Digital Humanities Course Registry.

The poster will be in English, but I will naturally interact with the audience of the poster sessions both in English and French.

Bibliography

Crymble, A., Gibbs, F., Hegel, A., McDaniel, C., Milligan, I.,

Taparata, E., Visconti, A. and Wieringa, J. (eds.) (n.d.)

. The Programming Historian. http://programmin-ghistorian.org/.

Hirsch, B. (ed.) (2012). Digital Humanities Pedagogy: Practices, Principles and Politics. Open Book Publishers. http://www.openbookpublishers.com/reader/161.

Jockers, M. (2014). Text Analysis with R for Students of Literature. New York: Springer.

Knuth, D. (1984). “Literate Programming.” The Computer

Journal, 27(2): 97-111. http://comjnl.oxfordjour-

nals.org/content/27/2/97.short.

Rockwell, G. and Sinclair, S. (1984). Hermeneutica. Computer-Assisted Interpretation in the Humanities. Cambridge, Massachusetts: MIT Press.
Following the publication of Franco Moretti’s GrapHs, Maps, Trees, scholars looking to apply digital humanities methods to literature have increasingly been drawn to “distant reading.” The influence of distant reading in digital humanities is apparent not only in the work it has inspired (see, among others, Cordell and Smith; Elson, Dames, and McKeown; Jockers; Long and So; Rhody; and Underwood) but also for its regular inclusion as a method in courses introducing DH. “Teaching digital humanities,” it turns out, often means “teaching distant reading.”

Teaching students the techniques of distant reading can be challenging as it depends on re-framing the familiar object of study. But another difficulty altogether is that this approach depends on a digitized corpus; and such a corpus, in turn, depends on someone, somewhere doing the difficult labor of digitization. One might ask, then: if “teaching digital humanities” means “teaching distant reading,” shouldn’t it also mean “teaching digitization”?

In this paper, I will discuss a collaborative, multiyear assignment that I conducted in two of my “Introduction to Digital Humanities” courses at Emory University: the digitization and analysis of the complete works of Ernest Hemingway (Croxall). With the goal of teaching my students not only how to do distant reading but also about the intense labor that goes into corpus preparation, we digitized the whole of Hemingway’s work in just two weeks. Working from newly purchased copies of the texts, the students and I rapidly scanned hundreds of pages, performed and corrected optical character recognition, and assembled a corpus—with each of us spending no more than 4 hours on the task. Our from-scratch corpus was composed expressly so we could draw important distinctions among Hemingway’s works: individual works vs the whole collection; fiction vs non-fiction; and works published before while Hemingway was alive vs those that appeared after his death in 1961. I will detail what we learned from rapid digitization and how those lessons affected the second iteration of the assignment.

After preparing the corpus, students worked in groups to analyze the many works of Hemingway that they had not had time to read. Making use of Voyant Tools, they identified themes in the corpus and charted patterns that could never have been observed through regular, close reading methods. For example, the class confirmed that while Hemingway insists on writing about “men,” the women to whom they are attached are inevitably just “girls.” In an attempt to chart the patterns of Hemingway’s diction, another group of students investigated the terms he uses to introduce dialogue. Unsurprisingly, the students discovered that “said” is by far the most frequent such term across the entire corpus. What was more surprising, however, was to observe that in late and posthumous writings, the frequency of “said” suddenly drops by 50%. In short, by building our own corpus from scratch, the students were able to conduct original research, something that is relatively rare for many undergraduates in humanities programs.

Building our collection of texts from scratch had two critical advantages. First, we were able to create a small, relatively clean corpus whose provenance we knew. This provided a sense of confidence in the data as we began to distant read. Furthermore, while our analysis of Hemingway’s works was “distant” compared to traditional close reading of a single novel or story, it was not nearly as distant as projects that deal with several thousand texts. We became engaged, in short, in close-distant reading. Second, digitizing the texts ourselves allowed us to skirt a problem that frequently plagues distant reading texts from the twentieth century: copyright. As an educational endeavor focused on teaching the students how to prepare their research materials, this guerilla digitization project fell under the regime of fair use in the United States.

To close, I will discuss how students at Brown University and I have taken further steps with the Hemingway corpus and with their digital humanities education as we have used it as a means to explore the methods and utility of topic modeling. Topic modeling is frequently deployed to come to terms with large and unwieldy corpora (see Jockers; Nelson; Nelson, Mimno, and Brown; Underwood and Goldstone). But working with a small, relatively clean corpus that is created from scratch allows students to better understand what takes place via unsupervised machine learning. At the same time, topic modeling allows us to ask in a new way some of the same questions that my former students had already uncovered: how does Hemingway’s dialog differ from his prose? how different are the topics in Hemingway’s fiction from those of his non-fiction? to what degree does his late—or even posthumous—work differ from what he wrote three decades earlier?

In the end, the process of modeling Hemingway becomes a means by which we can model all of digital humanities—both analysis and corpus creation—in a student-focused environment (see also Brier; Croxall and Singer; Harris; Hirsch; Jewell and Lorang; and Swafford). By doing digital humanities from scratch, students can be engaged in original research and see for themselves, from start to finish, how digital humanities gets done.

Bibliography

Brier, S. (2012). “Where’s the Pedagogy? The Role of

Teaching and Learning in the Digital Humanities.” In

Gold, M. K. (ed), Debates in the Digital Humanities.

Minnesota University Press, pp. 350-367.

Cordell, R. and Smith, D. A. (2017). Viral Texts: Mapping

Networks of Reprinting in 19th-Century Newspapers and

Magazines. http: //viraltexts.org/ (accessed 7 April

2017).

Croxall, B. (2015). “How to NOT Read Hemingway.” Intro to DH.

http://www.briancroxall.net/s15dh/assignments/how

-to-not-read-hemingway/ (accessed 7 April 2017).

Croxall, B. and Singer, K. (2013). “The Future of

Undergraduate Digital Humanities.” Digital Humanities

2013, Lincoln, NE, July 2013.

Elson, D. K., Dames, N. and McKeown, K. R. (2010).

“Extracting Social Networks from Literary Fiction.”

Proceedings of the 48th Annual Meeting of the

Association for Computational Linguisticsi, Uppsala,

Sweden.

http://www.cs.columbia.edu/~delson/pubs/ACL2010-

ElsonDamesMcKeown.pdf (accessed 7 April 2017).

Goldstone, A. and Underwood, T. (2014). “The Quiet

Transformations of Literary Studies: What Thirteen

Thousand Scholars Could Tell Us.” New Literary History

45.3: 359-384.

Harris, K. D. (2011). “Pedagogy & Play: Revising Learning through Digital Humanities.” Digital Humanities 2011, Stanford, CA, June 2011.

Hirsch, B. D. (2012). Digital Humanities Pedagogy:

Practices, Principles and Politics. Open Book Publishers.

Jewell, A. and Lorang, E. (2016). “Teaching Digital Humanities Through a Community-Engaged, Team-Based Pedagogy.” Digital Humanities 2016, Krakow, Poland, July 2016.

Jockers, M. L. (2013). Macroanalysis: Digital Methods and Literary History. Urbana Champaign, IL: University of Illinois Press.

Long, H. and So, R. J. (2016). “Literary Pattern

Recognition: Modernism between Close Reading and Machine Learning.” Critical Inquiry 42.2: 235-267.

Moretti, F. (2013). Distant Reading. London: Verso. Moretti, F. (2007). Graphs, Maps, Trees. London: Verso.

Nelson, R. K. (2011). Mining the Dispatch.

http://dsl.richmond.edu/dispatch/ (accessed 7 April 2017).

Nelson, R. K., Mimno, D. and Brown, T. (2012) “Topic Modeling the Past.” Digital Humanities 2012, Hamburg, Germany, July 2012.

Rhody, L. M. (2013). “Revising Ekphrasis: Methods and Models.” The Association for Computers and the Humanities. http://ach.org/2013/12/30/revising-ekphrasis-methods-and-models/ (accessed 7 April 2017).

Sinclair, S. and Rockwell, G. (2017). Voyant Tools. http://voyant-tools.org/ (accessed 7 April 2017).

Swafford, J. E. (2016). “Read, Play, Build: Teaching Sherlock Holmes through Digital Humanities.” Digital Humanities 2016, Krakow, Poland, July 2016.

Underwood, T. (2013). Why Literary Periods Mattered: Historical Contrast and the Prestige of English Studies. Stanford: Stanford University Press.
Over the past year and a half, the Cyberinfrastructure for Digital Humanities (CyberDH) Group at Indiana University has been developing an open instructional workflow for text analysis that aims to build algorithmic understanding and basic coding skills before scaling up analyses (Gniady et al., 2017). We have chosen to bootstrap in R, a high level and high productivity language, with methods that are open, repeatable, and sustainable. The aim is to provide code templates that can be adapted, remixed, and scaled to fit a wide range of text analysis tasks. This poster presents our approach to teaching computational text analysis and a representative hypothetical case study in which two different users are able to start with the same corpus and adapt code to achieve very different end results in a way not currently possible with black box tools.

This paradigm is fundamentally different from that currently practiced by many in the digital humanities. Black-boxed tools with GUIs that hide computation are very popular for introducing new practitioners of text analysis in the digital humanities to basic algorithms and outputs. In 2012, AntConc was downloaded 120,000 times by users in 80 different countries (Anthony, 2014). Voyant 1.0 had 113 sites linking to it actively in 2012 (Sinclair and Rockwell, 2013) and the week Voyant 2.0 was released the server went down multiple times from excess traffic (@VoyantTools, 2016). However, one of its default corpora is the Shakespearean dramas, with speaker names and stage directions. ((Sinclair and Rockwell, 2016). The inclusion of speaker names skews all algorithms related to frequency counts of characters (e.g. word clouds), which a new user may not even think to take into account. Using AntConc's concordance tool with a Shakespearean corpora including speaker names gives an idea of when a character speaks and when a character is mentioned, but this conflation might not jump out at a new user. If anything, we suggest learning about algorithms first and then moving up to black-box tools when one has the means to critique them.

Having looked at popular “plug-and-play” tools for corpora visualization, it becomes evident that even simple visualizations can lead to inaccurate results if the user is not thinking through how a corpus is being processed to produce a result. We believe that if the user understands how the algorithm is generating visualizations, they can contribute more meaningfully to critiques of sophisticated algorithms when partnered with programmers or even go on to bootstrap themselves with awareness of their domain's particular caveats. Thus, we advocate teaching humanists the basics of coding to create conversant programmers similar to the methodology behind Matthew Jockers' Text Analysis with R for Students of Literature, but with a slightly slower ramp up. To this end we have a three-step process of introducing R: web-deployed Shiny apps, highly marked up RNotebooks, and lightly commented RScripts, both in “regular” and higher performance versions. All are available for download on Github (with associated sample data from Shakespeare and Twitter) (CyberDH Team, 2017). We hope that this simpler bootstrapping method that mixes code and explanation, pedagogy and self-driven inquiry, will be of use to those looking to onramp new practitioners who may go on to partner with programmers if needed or to remix available code to look at their own knowledge domain.

Bibliography

Anthony, L. (2016). Antconc 3.4.4. Software.

http://www.laurenceanthony.net/soft-

ware/antconc/.

Gniady, T. Thomas, G. and Kloster, D. (2017). Text Analysis Github Repository. https://github.com/cyberdh/Text-Analysis.

Jockers, M. (2014). Text Analysis with R for Students of Literature. New York: Springer International Publishing.

Sinclair, S. and Rockwell, G. (2013). “Voyant Notebooks: Literate Programming, Programming Literacy.” Digital

Humanities 2014: Conference Abstracts. Nebraska-Lin-

coin: http://dh2013.unl.edu/abstracts/ab-295.html.

Sinclair, S. and Rockwell, G. (2016). Voyant Tools. http://voyant-tools.org/.

@VoyantTools. Twitter. 8 April 2016.
Hermeneutica
In Hermeneutica, Geoffrey Rockwell and Stéfan Sinclair (2016) argue for an approach to the digital humanities that deemphasizes the tool and positivist notions of proof. Their proposed approach, also called Hermeneutica, champions tool accessibility over tool sophistication. Similarly, scholarly play is legitimated as a useful step in developing research questions and as a means to reconsider established notions within literary disciplines. The aim of Hermeneutica as a methodology seems to be the generation of interesting humanistic questions as much as the resolution of open questions.

Rockwell and Sinclair demonstrate the difference between Hermeneutica and typical DH approaches by quoting from Gary Wong's 2009 blog post:

[Typical DH] takes the worst part of the scientific papers (really really long sets of tabular data in the body of the text) and the worst part of papers from the humanities (really really complicated language where simple language would have done) and puts it in one. If this is what the cooperation of computational text analysis and traditional literary analysis yield, I

am scared.

Because Hermeneutica attempts to join the best parts of these fields, it has the potential to turn DH into a discipline that is more useful for the vast majority of non-DH humanists. It could be the means of accelerating the mainstreaming of DH methods and bringing us to the eventual point where all humanities are digital—a destination Claire Clivaz described succinctly (DARIAH, 2016).

Voyant
One feature that distinguishes Hermeneutica from many other DH approaches is its companion set of tools meant to demonstrate its application. Voyant Tools, now referred to simply as Voyant, is a web-based, modular suite of tools meant to be “worth thinking with” (Rockwell and Sinclair, 2016: 10, original emphasis). The goal is to accommodate playful exploration of text and sharing of corpora across the web. It is not designed as an industrial-grade text analysis tool, but as a “toy” that allows scholars to uncover new questions and gain new appreciation of texts.

Current limitations of Hermeneutica
A fundamental component of Hermeneutica is that the scholar views text through the lens of Voyant (or other computational text analysis tools), and then synthesizes that experience with their prior knowledge of the text and its milieu. A problem that Voyant addresses, but does not solve, is that many scholars who know the most about specific texts lack the technological skills that would be considered pre-novice in DH circles. Voyant allows everyone with a text and a browser to explore word frequencies, collocations, etc., but it presupposes that the text is available and clean enough for use. In order for Hermeneutica to appeal to non-DH humanities scholars, these issues of text availability and the lack of user skill must first be addressed.

On the issue of text availability, it is not often that scholars wish to analyze text that is rare or missing. More often they are interested in text that is protected by various copyright laws, which prohibit posting the text to public websites such as Voyant. Thankfully, in the Unites States at least, Google Books' recent court victory (Stohr, 2016) now permits scholars to publish online the analysis results derived from copyrighted texts, so long as the original text is not recoverable by the user. To this end Rockwell and Sinclair developed Voyant 2's “non-consumptive” mode which restricts access to tools that allow full-text views.

While such developments represent Rockwell and Sinclair's amenability to meet the ever-evolving needs of Hermeneuticans, accommodating users' lack of technology skill is beyond the scope of their involvement. For example, it is not reasonable to expect the Voyant developers to be concerned over issues of text acquisition or text preparation. Rather, those con-cerns—while critical to expanding the pool of potential Hermeneuticans—are issues of local implementation. Similarly, it makes sense that Voyant would offer the ability to link to a corpus after uploading the text, but uploading the text and keeping track of various versions of corpora is beyond the scope of Voyant. A local practice of adding some structure around the

Voyant suite ought to make Hermeneutica useful to a far greater audience than it is now.

Scaffolding
In the field of instructional design, such structure is called scaffolding. Specifically, scaffolding refers to the process of providing learners adequate introduction and examples before allowing them to attempt a task on their own (Bruner, 1978). For scaffolded Her-meneutica, DH-savvy professionals can work to acquire, clean, and upload text to Voyant (and other tools), and then provide public listings of the resulting corpora.

Examples of scaffolded Hermeneutica
We have implemented this scaffolded Hermeneu-tica approach in our Office of Digital Humanities beginning with the Cormac McCarthy Corpus Project (CMCP). The CMCP includes 13 Voyant corpora of McCarthy's 10 novels: one for the complete works, one for each novel, and two for novels (The Orchard Keeper and The Road) where the narration has been segregated from the dialogue. But the linchpin of scaffolded Hermeneutica is the CMCP's publicly-accessible website that organizes these Voyant corpora. The website is built on WordPress with the Pods content management plugin, and contains information on McCarthy's work, descriptions of Voyant (and other tools), and listings of links to the Voyant corpora. An essential feature of the website's structure is the ability to accommodate revisions to the current corpora as well as the addition of other tools in the future. Already, there is a non-Voyant sentence structure search tool attached as a beta-testing option.

A rough version of the Cormac McCarthy Corpus Project was presented at the 2015 conference of the Cormac McCarthy Society. The reaction to these tools being available for public use was strongly positive. One attendee referred to the website as “a game-changer.”

The same scaffolded Hermeneutica is being implemented on two other projects: Machado a longa distancia and The Modernist Short Fiction Project. Preliminary demonstrations of the approach have yielded similar reactions to what we observed with the CMCP. Non-DH scholars become excited rather than anxious when the digital analysis tools are scaffolded to provide them ready access. In fact, these demonstrations turn into play sessions where non-DH scholars repeatedly request for certain words to be added to the frequency charts and other Voyant panels.

Hermeneutica and Voyant represent the greatest potential for growth in DH not because they are the most technologically or theoretically advanced developments, but because they are the most accessible to non-DH scholars. Still, they don't quite reach the ground level of technology skills possessed by most researchers in the humanities. The scaffolded Herme-neutica approach proposed in this paper seems to span that gap to make Hermeneutica more accessible.

Bibliography
Bruner, J. S. (1978). “The role of dialogue in language acquisition.” In Sinclair, A., Jarvelle, R., J., and W. J.M. Levelt (eds), The Child's Concept of Language. New York: Springer-Verlag.

DARIAH (2016). My Digital Humanities - Part 1. YouTube.

https://www.youtube.com/watch?v=I8aRtHW3b6g

(accessed 1 November 2016).

Rockwell, G. and Sinclair, S. (2016). Hermeneutica. Cambridge: MIT Press.

Stohr, G. (2016). Google Book Project Can Proceed as Supreme Court Spurns Appeal. Bloomberg Politics. http://www.bloomberg.com/politics/articles/2016-04-18/google-book-project-can-proceed-as-top-u-s-court-spurns-appeal (accessed 1 November 2016).

Conclusion

        
            Abstract 
            In this workshop for non - coders, participants will be guided through two tasks: the first task will guide participants in creating an application to tap into Twitter’s API, in our case to get Twitter data. The second task will guide participants in the use of a Google spreadsheet to capture streaming (live) data from Twitter in order to archive it, download it and perform text analysis, data visualization and other studies. This workshop will include a brief introduction contextualizing social media data collection good practices including user data privacy issues. 
            
                Keywords: Archiving, Data Collection, Social Media, Twitter, Text Analysis 
            
            Rationale 
            Twitter data can be very valuable for researchers of perhaps all disciplines, not just DH. Given the difficulties to properly collect and analyse Twitter data as viewable from most Twitter Web and mobile clients (as most people use Twitter) and the very limited short - span of search results, there is the danger of losing huge amounts of valuable historical material. 
            Tweets are like butterflies – one can only really look at them for long if one pins them down out of their natural environment. The reason why we have access to Twitter in any form is because of Twitter’s API, which stands for Application Programming Interface. Free access to historic Twitter search results is limited to the last 7 days. This is due to several reasons, including the incredible amount of data that is requested from Twitter’s API, and – this is an educated guess – not disconnected from the fact that Twitter’s business model relies on its data being a commodity that can be resold for research. Twitter’s data is stored and managed by Twitter’s enterprise API platform. 
            For the researcher interested in researching Twitter data, this means that harvesting needs to be do ne not only through automated means but in real time. It also puts scholars without the required coding and data mining skills at a disadvantage. As a researcher, this basically means that there is no way to do proper research of Twitter data without understanding how it works at API level, and this means understanding the limitations and possibilities this imposes on researchers. 
            What’s a n individual researcher without access to pay corporate access to do? The whole butterfly colony cannot be captured with the nets most of us have available. At small scale, however, and collecting in a timely fashion, it is still possible to capture interesting and more - or - less complete specimens using fairly simply, non - coding required methods. (The Library of Congress h s now 12 years’ worth of text - only Tweets. However, as before, the Library of Congress Twitter collection will remain embargoed and there was no projected timetable for providing public access as of 26 December 2017). 
            Most researchers out there are likely not to benefit from access to huge Twitter data dumps. For researchers without much resources that are trying to do the talk whilst doing the walk, and conduct research 
                on Twitter and 
                about Twitter, this workshop and tutorial will guide participants into creating a Twitter application in order to tap into the Twitter API, followed 
            
            by the setting up of a Twitter Google Archiving Spreadsheet. Once a trial archive or dataset has been collected, we will attempt text analysis and basic visualisations using Excel and Voyant Tools. This workshop will include a brief introduction contextualizing social media data collection good practices including user data privacy and research ethics issues. 
            Workshop Requirements 
            • Room with projector and screen 
            • Wifi access 
            • Power plugs for participants to charge devices if required 
            Participants Requirements 
            • Interest in collecting small Twitter datasets and basic Text Analysis 
            • Wifi - enabled Laptop with Excel or similar spreadsheet software 
            • Twitter account, and the login credentials to access it (username and password) 
            
                Tools We’ll Use 
            
            • TAGS 
            https://tags.hawksey.info/ 
            • Voyant Tools 
            https://voyant - tools.org/ 
            El taller se puede dar también en español o bilingüe inglés - español. 
        
        
            
                
                    Bibliography
                    For complete references please follow links in the referenced outputs below and in the body of the text above. 
                    Priego, E. 2018. #rfringe17: Top 230 Terms in Tweetage. 
                    
                        https://epriego.blog/2017/08/05/rfringe17-top 230-terms-in-tweetage/
                    
                    [Accessed 30 January 2018] 
                    Priego, E., 2016. Bar Chart: Number of #DH2016 Tweets in Archive per Conference Day (Sunday 10 to Friday 15 July 2016 GMT). Available from: 
                    
                        https://figshare.com/articles/Bar_Chart_Number_of_DH2016_Tweets_in_Archive_per_Conf erence_Day_Sunday_10_to_Friday_15_July_2016_GMT_/3490001/1 [Accessed 31 Jan 2018]. 
                    
                    Priego, E. 2016. “Stronger In”: Looking Into a Sample Archive of 1,005 StrongerIn Tweets. 
                    
                        https://epriego.blog/2016/06/21/stronger-in-looking-into-a-sample-archive-of-1005- strongerin-tweets/ [Accessed 30 January 2018] 
                    
                    Priego, E. and Zarate, C., 2014. #MLA14 Twitter Archive, 9 - 12 January 2014. Available from: 
                    
                        https://figshare.com/aticles_MLA14_Twitter_Archive_9_12_January_2014/924801/1
                    
                    [Accessed 31 Jan 2018]. 
                    Priego, E. 2014. Some Thoughts on Why You Would Like to Archive and Share [Small] Twitter Data Sets. Available from:
                    
                        https://epriego.blog/2014/05/28/some-thoughts-why-you-would-like-to-archive-and-share-twitter-small-data / [Accessed 30 January 2018] 
                    
                    Priego, E. 2014. Publicly available data from Twitter is public evidence and does not necessarily constitute an “ethical dilemma”. London School of Economics Impact Blog. Available from:
                    
                        http://blogs.lse.ac.uk/impactofsoc ialsciences/2014/05/28/twitter-as-public-evidence/ [Accessed 30 January 2018] 
                    
                
            
        
    

        
            Abstract
            We present LitViz, a webbased tool for visualizing literary data which utilizes the text2voronoi algorithm to map natural language texts onto voronoi diagrams. These diagrams can be used, for example, to visually differentiate between (groups of) authors. Text2voronoi utilizes the paradigm of text visualization to reconstruct text classification (e.g., authorship attribution) as a task of image classification. This means that, in contrast to conventional approaches to text classifiction, we do not directly use linguistic features, but explore visual features derived from the texts' visualizations to perform operations on texts. We illustrate LitViz by means of 18 authors, each of whom is represented by 5 literary works.
            Introduction
            In this paper we present a new tool, called LitViz, for the visual depiction of literary works. To this end, we utilize the text2voronoi algorithm (see Mehler et al. (2016b)) which maps natural language texts to image representations. The idea is to generate images of texts which can be used instead of these texts’ symbolic information to characterize them, for example, in terms of authorship, topic or genre. Text2voronoi is in line with the paradigm of text visualization to reconstruct text classification (e.g., authorship attribution) as a task of image classification. In contrast to conventional approaches to text classification, we therefore do not directly use linguistic features, but explore visual features derived from the texts’ visualizations in order to identify, for example, their authors. We exemplify LitViz by means of 18 authors each of whom is represent by 5 literary works. LitViz allows for interacting with the visualizations of these works in two modes: two- and three-dimensionally (see Figure 1 and 2).
            
                
                Figure 1: Visual depiction of E.T.A. Hoffmann’s Das steinerne Herz
                
            
            Related Work
            The idea of visualizing literature was inspired by Martin Wattenberg’s The Shape of Song1 (Wattenberg, 2001; Wattenberg, 2002). Wattenberg explores identical or otherwise repetitive passages of a composition to visually depict them. This is done by means of semicircles, which combine repeated and repetitive positions in such a way that the micro- and macro-structure of a composition becomes visible. Our idea is to transpose this idea to the visualization of literary data. 
            Kucher and Kerren (2015) give an overview of state-of-theart techniques of text visualization and present a website that allows for differentiating between these techniques. Cao and Cui (2016) provide a systematic review of many advanced visualization techniques and discuss the fundamental notion of information visualization. 
            Mehler et al. (2016a) present a web tool called Wikidition which allows for automatically generating large-scale editions of text corpora. This is done by using multiple text mining tools for automatically linking lexical, sentential and textual data. The output is stored and visualized using a MediaWiki. Thus, any Wikidition is extensible by its readers based on the wiki principle. 
            Rockwell and Sinclair (2016) present a detailed web tool, called Voyant tools, for visualizing texts. Unlike Voyant, our focus is on non-standard techniques of visualizing textual data that go beyond histograms, scatterplots, line charts and related tools. 
            Generally speaking, text visualization supports distant reading as introduced and exemplified by Moretti (2013), Rule et al. (2015) and Michel et al. (2011). These approaches show how visualizations that support distant reading may look like to get overviews of documents by just looking at the final visualizations. LitViz is a tool following this tradition: it utilizes text2voronoi to extend the set of techniques mapping textual data. In this way, it combines Wattenberg’s approach with distant reading techniques from the point of view of text visualization.
                
            
            
                
                Figure 2: 3D visualization of Franz Kafka’s Der Kübelreiter.
            
            Model
            Our goal is to generate images from literary works in a way that text classifiers can be fed by the features of these iconic representations in order to perform classification experiments, for which usually linguistic features are explored. This is the task of the text2voronoi algorithm, which calculates image representations of texts in four steps Mehler et al. (2016b): In the first step, the input text is analyzed by means of TextImager Hemati et al. (2016) to extract linguistic features in the usual way, that is, features, spanning a vector space of linguistic data. In the second step, the resulting vector space is used to compute embeddings for each of the extracted linguistic features. Embeddings are produced by means of word2vec (Mikolov et al., 2013). In the third step, a voronoi tessellation of the embedded features is computed. As a result, each lexical feature is mapped onto a separate voronoi cell whose neighborhood reflects the feature’s syntagmatic and paradigmatic associations with other features of the same space. The topology of the voronoi cells spans a voronoi diagram that visually represents the input text. Each of these cells is characterized by its filling level, transparency and height (third dimension) thereby reflecting its co-occurrence statistics within the input text, while the position and size of a cell is determined by the embedding of the corresponding feature – for the mathematical details of this algorithm see Mehler et al. (2016b). Finally, the text2voronoi algorithm extracts visual features from the voronoi diagrams to feed classifiers performing classifications of the input texts. 
            LitViz utilizes the first three steps of this algorithm. Unlike the classical text2voronoi procedure, it does not address the final step of classification. Rather, it gives access to voronoi diagrams of input texts via a two-dimensional graphical interface, which can be transformed into a three-dimensional one by means of user interaction. These two- and threedimensional text representations can be used by the user of LitViz to interact with the underlying input texts in order to highlight single voronoi cells, to change her or his reading perspective or to visually compare voronoi diagrams of different texts. In this way, LitViz paves the way to a kind of a comparative distant reading by making accessible the visual depictions of different texts in an interactive manner.
            The LitViz Tool
            We have selected 18 authors of German literature each of whom is represent by 5 literary works. The works are taken from the Project Gutenberg (https://www.gutenberg.org/) and visualized by means of the text2voronoi algorithm. Any of these examples is made accessible by the front page of LitViz (see Figure 3). When hovering over a voronoi cell of the voronoi diagram of a sample work, information about the underlying linguistic feature represented by this cell is displayed. According to Mehler et al. (2016b), we call these images VoTes: Voronoi diagram of a Text. LitViz presents VoTes via a graphical user interface for two- and three-dimensional interactive graphics. In this way, we go beyond Wattenberg’s 2D depictions of musical pieces. 
                
            
            
                
                Figure 3: Front page of LitViz.
            
            
                The second page (tab) of LitViz gives access to the comparison tool. Here the user first selects the number of VoTes to be compared. Then the user selects a subset of works of the authors to be compared. In the example in Figure 4, we compare four VoTes of two authors: two VoTes of two works of Heinrich Heine (top) and two VoTes of Heinrich Mann (bottom). It is easy to see that these VoTes fall into two classes, depending on the underlying authorship. Heinrich Mann’s two VoTes are organized around a center that is composed of many small cells, while there is a small subgroup of peripheral cells that are large. In contrast to this, the two VoTes of Heinrich Heine do not display such a center and are more evenly distributed in terms of their size. It is a main task of LitViz to allow for such comparisons. In this way, that is, by interacting with the texts’ image representations and by using the mouse-over technique, the user can study single features and how they are related to other features of the same representational space. 
                
            
            
                
                Figure 4: Comparison tool: Heinrich Heine (top) in comparison to Heinrich Mann (bottom).
            
            
                Last but not least, LitViz provides a so-called custom tab. Here, the user can upload and visualize its own texts. It is then possible to set filter options using an option tool (see Figure 5) in order to further restrict the visualization.
            
            
                
                Figure 5: Custom VoTe with filter options.
            
            Conclusion
            We introduced a novel web tool, called LitViz, for visually depicting natural language texts based on the text2voronoi algorithm. LitViz enables the comparison of the visualizations of different texts. This allows, for example, for comparing the styles of the underlying authors visually. In this way, we extend the existing tool palette of distant reading. LitViz can be accessed via: 
                http://alba.hucompute.org/text2voronoi
            
        
        
            
                
                    Bibliography
                    Cao, N. and Cui, W. (2016). Introduction to Text Visualization. Atlantis Briefs in Artificial Intelligence. Atlantis Press. 
                    Hemati, W., Uslu, T., and Mehler, A. (2016). TextImager: a distributed uima-based system for NLP. In Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: System Demonstrations, pages 59–63.
                    Kucher, K. and Kerren, A. (2015). Text visualization techniques: Taxonomy, visual survey, and community in sights. In Visualization Symposium (PacificVis), 2015 IEEE Pacific, pages 117–121. IEEE.
                    Mehler, A., Gleim, R., vor der Bruck, T., Hemati, W., Uslu, ¨ T., and Eger, S. (2016a). Wikidition: Automatic lexiconization and linkification of text corpora. Information Technology, pages 70–79. 
                    Mehler, A., Uslu, T., and Hemati, W. (2016b). Text2Voronoi: An image-driven approach to differential diagnosis. In Proceedings of the 5th Workshop on Vision and Language (VL’16) hosted by the 54th Annual Meeting of the Association for Computational Linguistics (ACL), Berlin. 
                    Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Pickett, J. P., Hoiberg, D., Clancy, D., Norvig, P., Orwant, J., et al. (2011). Quantitative analysis of culture using millions of digitized books. Science, 331(6014):176–182. 
                    Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. 
                    Moretti, F. (2013). Distant reading. Verso Books. 
                    Rockwell, G. and Sinclair, S. (2016). Hermeneutica: Computer-Assisted Interpretation in the Humanities. MIT Press.
                    Rule, A., Cointet, J.-P., and Bearman, P. S. (2015). Lexical shifts, substantive changes, and continuity in state of the union discourse, 1790–2014. Proceedings of the National Academy of Sciences, 112(35):10837–10844. 
                    Wattenberg, M. (2001). The shape of song. Website 
                        http://www.turbulence.org/Works/song/mono.html.
                    
                    Wattenberg, M. (2002). Arc diagrams: Visualizing structure in strings. In Information Visualization, 2002. INFOVIS 2002. IEEE Symposium on, pages 110–116. IEEE.
                
            
        
    

        
            The poster introduces a project to develop a visualization application for a unique data source on Czech sciences. Information Register of R&amp;D Results (RIV) is the Czech Republic’s inventory of the outputs of basic and applied research since 1992. Although it is potentially an important source of data for analyses of various aspects of the intellectual organization and publication culture in Czech sciences, this particular data source has earned itself a pejorative nickname – “a coffee grinder” – for its central role in purely mechanistic science evaluation in the country.
            By employing text-mining technique that are standard in the digital humanities and by getting inspiration from visualization platforms such as 
                
                    Voyant Tools
                 (Sinclair and Rockwell 2012), the project aims to contribute to the shift in the Czech narrative of science evaluation from the exclusively bibliometric perspective to a more diverse one. For example, the hope is that the visual display of the plethora of topics that are discussed in the research outputs registered in RIV will implicitly criticize the myopic vision in which all disciplines are leveled to the singular measure of the number of publications. The latter system is not only intellectually dubious, but it has had documented adverse effects on the quality of research results. Crucially, it stimulates institutions as well as individuals to prioritize quantity over quality (Good et al. 2015; Grančay, Vveinhardt, and Šumilo 2017).
            
            The ill-fated usage of the RIV data to mold nationwide fiscal policies for scientific research reminds us that data analytics is not necessarily a neutral enterprise. A proper treatment of the data is a matter that confronts a data analyst with questions on the borderline of ethics. Although it is perfectly feasible in technical terms, we wish to discourage users from attempts to track individuals researchers; instead we offer features that display institutional or disciplinary dimensions of the data (see Figure 1). Furthermore, the web application will provide a module to visualize textual information from the register. Textual strings, such as abstracts and keywords, have been part and parcel of the recorded entries, but have only served thus far as mere search terms. Meanwhile, the utility of textual data has been demonstrated in studies that strive to map the intellectual organization and relationships within and between disciplines (Leydesdroff 1989; Moody 2004).
             
            
                
                    
                 
            
            Figure 1. Using RIVVIZ to visualize a trend in the publication frequency of research outputs in the “J” (journal) category of the Information Register of R&amp;D Results for the discipline “Philosophy and Religion” [note: the data are only a sample used in the development version] 
             
            The target group of the application are the researchers themselves. Namely, the textual module is intended to serve their needs by providing an overview of the trending topics in research or to identify institutions working on similar problems. The specialist user sub-group is envisaged to come from the fields focusing on social and other studies of science. The accessibility of visualized data and the simplicity of the interface can also attract journalists or other members of the public. The prospective users are also likely to be recruit from among the stakeholders in scientific policy-making and management who may wish to gain quick insights into the quantitatively assessed rates of output per research institutions or funding bodies. 
            The RIVVIZ application is developed in the R language and deployed on the R Server platform using the standard Shiny library. The data are imported from the publicly available repository of the Czech Research, Development and Innovation Information System. The internal setup is also fairly straightforward, relying predominately on the Tidyverse collection of packages, with ggplot2 library being the primary engine for visualization tasks. The underlying principles of the “grammar of graphics”(Wickham 2009) are particularly suitable for programming a user-oriented environment that allows for a control over a wide range of visualization parameters.
            Giving the users more choices should help to make them more engaged with the application, although there is a trade-off between user-friendliness and complexity. Reasonable defaults can partially alleviate this dilemma. The user engagement will be important for the future application development (Galey and Ruecker 2010). In the case of visualization schemes, locking users in a single – no matter how aesthetically pleasing – perspective is problematic. The apparent self-explanatory style and transparent communication of images may draw attention away from the complex and multifaceted nature of the data by making some of their aspects more easily accessible than others (Drucker 2011).
        
        
            
                
                    Bibliography
                    
                        Drucker, J. (2011). Humanities Approaches to Graphical Display. 
                        Digital Humanities Quarterly (DHQ), 5(1).
                    
                    
                        Galey A. and Ruecker, S. (2010). How a Prototype Argues. 
                        Literary and Linguistic Computing, 25 (4): 405-424.
                    
                    
                        Good, B., Vermeulen, N., Tiefenthaler, B. and Arnold, E. (2015). Counting Quality? The Czech Performance-Based Research Funding System. 
                        Research Evaluation 24 (2): 91–105.
                    
                    
                        Grančay, M., Vveinhardt, J. and Šumilo, Ē. (2017). Publish or Perish: How Central and Eastern European Economists Have Dealt with the Ever-Increasing Academic Publishing Requirements 2000–2015. 
                        Scientometrics 111 (3): 1813– 37.
                    
                    
                        Leydesdroff, L. (1989). Words and Co-Words as Indicators of Intellectual Organization. 
                        Research Policy 18 (4): 209–223.
                    
                    
                        Moody, J. (2004). The Structure of a Social Science Collaboration Network: Disciplinary Cohesion from 1963 to 1999. 
                        American Sociological Review 69 (2): 213–238.
                    
                    
                        Sinclair, S., Rockwell, G. and the Voyant Tools Team. (2012). 
                        Voyant Tools (web application).
                    
                    
                        Wickham, H. (2009). 
                        Ggplot2: Elegant Graphics for Data Analysis. Dordrecht: Springer.
                    
                
            
        
    

        
            
                Overview
                Pedagogical exercises in the digital humanities rely on student access to humanities data. While strategies range from instructor-prepared datasets (Sinclair and Rockwell, 2012) to having students digitize texts directly from print materials (Croxall, 2017), data repositories and web-based DH projects are two of the most attractive sources for identifying, appraising, and accessing data for classroom use.
                Yet data for teaching is rarely cited as a prime motivation or rationale for sharing research data. In “The Conundrum of Sharing Research Data,” Christine Borgman examines four rationales for sharing research data: (1) to reproduce or to verify research, (2) to make results of publicly funded research available to the public, (3) to enable others to ask new questions of extant data, and (4) to advance the state of research and innovation (2012). Pedagogy may be included implicitly in the third rationale, but by foregrounding pedagogical intentions, we can more readily operationalize a process for how we enable others to ask new questions of our data, which, in turn, will inform our motivations for sharing as well as the manner in which we do so.
                Web-based DH projects are often conceived and developed for public consumption with short-term support through grant funding. While initiatives such as these have proliferated since the 1990s, they often languish as legacy projects on institutional servers without clear plans for sustainability or sunsetting (Rockwell et al., 2014). Rather than construe long dormant projects as an institutional burden, these artifacts may continue to function as object lessons and raw materials for use in the DH classroom. Evaluating early digital projects based on their fitness for use as pedagogical datasets distinguishes the project from its component parts and allows aspects of the project to live on in new contexts.
                This panel will include representatives from five public research universities across the United States. We will begin with a brief overview, followed by four case studies. Each panelist will speak for fifteen to twenty minutes, leaving time for questions from—and conversations with—the audience. Cases are drawn from the DH 101 course at UCLA, the DH Librarianship course at the University of Washington, the University of Miami Libraries’ Legacy Site Adoption Project, and the Humanities Data workshop at DHOxSS. Our goal is to explore the intersection of data sharing and digital pedagogy to interrogate how past projects (whether formally archived or otherwise) are adopted as data sets for teaching and training; propose evaluation criteria for selecting these data sets; discuss what these classroom efforts indicate about the sustainability of DH projects (and their data); and examine how our knowledge of these classroom cases might inform curatorial decisions in active DH projects.
            
            
                Learning from our mistakes: Using old projects to create better library/faculty collaborations
                The Legacy Sites Adoption Project (LSAP) developed in response to what the library administration saw as a significant problem: the library website hosted nearly 40 digital projects built 5-20 years earlier by a former library faculty member, now malingering in various states of brokenness, but still placed prominently on the website. Retiring and removing the sites would erase the memory of the library’s institutional history, but repairing them would create an impossible burden for the web &amp; application development team; and would reinforce the idea of the library playing a service-and-support role in DH, rather than an active partnership.
                The solution that we are currently implementing is to experiment with making the legacy sites “adoptable”: the content and metadata of each site are made available as a zip file containing CSVs of data and metadata and accompanying images/audio/visual files, along with a readme pointing both to the current site on the library servers and an archived (and often more functional) version of the site in the Internet Archive’s Wayback Machine. Faculty and students are able to use the zip files as base material for creating their own version(s) of the original sites, either carrying on the original concept as stated or taking it in a new direction. The original versions of the sites present opportunities for classes to think about developing DH projects with a direct focus on revision -- potentially reading and critiquing the original sites through the lenses of recent scholarly essays, or considering the choices made by the original creators in the light of how DH practices and tools have changed since the sites were built.
                LSAP engages with ongoing questions about what makes a good entry point into digital humanities work. Instead of building entry points around particular tools (Omeka, Voyant, etc.); or around a particular research question or collection of material that is not a project yet, adopting legacy sites centers and foregrounds the iterative nature and inevitable fragility of project webpages, while making explicit the relationship between the websites and the flat files of their content. 
                With LSAP, we are also attempting a positive intervention into collaborative relationships between departmental faculty and librarians. Frequently, faculty come to librarians to ask for support for a particular idea for a digital project; or to incorporate digital methodologies into a classroom setting. In such instances, the faculty member may have little experience or knowledge with various key factors, including scoping and scaling project milestones, the availability of digitized objects, copyright/permissions restrictions, and the affordances of out-of-the-box tools. Our hope is that by offering projects that are ripe for revision, and focusing on areas that are frequently taught and studied at the university, we can provide an entry-point for collaboration that is more appropriately bounded, resulting in less uncertainty and less labor-intensive experiences for faculty, students, and librarians. 
            
            
                Awakening sleeping data for the DH classroom
                As anyone who teaches digital humanities knows, humanities-related datasets are as hard to find as they are desirable. Since the closure of the Arts and Humanities Data Service in 2008, no centralized repository for humanities data has emerged. The DH instructor is faced with the necessity of scouring the web for data to share with students so that they can practice data-cleaning, -manipulation, and -visualization. Sometimes this data comes from libraries, archives, and museums, but it comes just as often from scholars’ long-hibernating research projects. Indeed, scholars are often surprised to learn that their data has taken on a new life as the basis for student projects.
                The last several decades have seen explosive growth in flexible, accessible tools for working with data. These new platforms offer possibilities for visualization and analysis that would only have been possible with custom programming just 10 or 15 years ago. Because of this palette of tools, even relatively inexperienced students can breathe new life into data left mostly untouched for years.
                This presentation offers some case studies of student projects built on “dormant” data, explaining how students are trained to analyze, contextualize, visualize, and make sense of data they had no involvement in collecting. It discusses best practices for providing this data, as well as a scaffolded approach to helping students become conversant in techniques for understanding and working with data. It suggests a “toolkit” of off-the-shelf platforms that are affordable and easy for students to grasp and shows how one can build on the other until even novice students are able to create full-fledged, sophisticated digital humanities projects in the space of a semester.
                For those who have collected data they wish to share with students, this presentation offers some suggestions for documenting, packaging, and contextualizing research data so that it is not only technically sound, but in a format that students can understand. It also offers a set of best practices for collaborating with students on a data-based research project, including methods for sharing, documenting, citing, and reusing data.
            
            
                Fit for use: Repurposing research data, reconstructing provenance, and refining “clean” data
                When it comes to teaching materials, data curation education may have become a victim of its own success: finding “dirty” data for classroom use is persistently difficult, in part because most published datasets have already been cleaned and curated! However, there are teachable moments to be found even when working with relatively “clean” data. Published data can be mined, re-structured, re-formatted and otherwise curated for new uses. Additionally, the process of tracking down and contextualizing already published datasets can prove instructive in and of itself. The detective work needed to understand someone else’s project, and to reconstruct its provenance, can reveal unexpected idiosyncrasies about the dataset, and thereby reveal useful data wrangling skills to be taught.
                In this talk, we describe our work finding, curating and reconstructing the provenance of “The Pettigrew Papers,” a published (and relatively clean) dataset we have used over two years of teaching week-long workshops on digital humanities data curation at the Digital Humanities at Oxford Summer School (DHOxSS). Thomas J. Pettigrew (also known as Thomas “Mummy” Pettigrew) was a Victorian surgeon, antiquarian, and Egypotologist. Pettigrew wrote several early texts on Egyptian mummies and was the founding treasurer of the British Archaeological Association. Though his correspondence is archived at Yale University’s Beinecke Rare Book and Manuscript Library, it came to our attention via a “data paper” published in the Journal of Open Archaeology Data (Moshenska, 2012), containing transcriptions of select letters. 
                In our first year teaching with the Pettigrew dataset, we wrote simple Python scripts to mine named entities from the letters, and to pull out header information about the letters as a spreadsheet for cleaning in OpenRefine. In hands-on sessions, we asked students to consider how they would clean and curate the dataset for new uses: what steps would need to be taken to create a network diagram of the entities named in his letters? To create a map of his correspondents? To create a timeline? 
                In our second year teaching with this dataset, we spent more time reconstructing the original provenance of the Pettigrew letters themselves. In addition to the hands-on sessions from the first year, we asked students to consider how they might improve the metadata for the original data paper, and how they might resolve discrepancies between the data paper and the original finding aids created by the Beinecke (Ducharme, 2010). We additionally discussed how they might incorporate copies of Pettigrew’s publications available in the HathiTrust Digital Library in their work.
                Overall, we found that asking students to clean and re-curate this already published dataset was only the starting point in our teaching; as we found further connections in digital libraries and archives beyond the original data paper, we identified subtle and important issues in the digital humanities and digital curation that guided our workshop design. In addition to teaching hands-on data cleaning and manipulation skills, we found it important to teach students a nuanced understanding of provenance: both in the sense of the archival “chain of custody” that contextualizes and validates a fonds, and in the sense of the processes that led to a dataset’s current form.
            
            
                Training DH librarians: Using old DH projects to move forward
                The DH Librarianship course at the University of Washington Information School investigates the multiple roles librarians play in DH scholarship and prepares students for a wide range of career options in libraries, DH centers, and academic departments. DH librarian roles range from fully-credited collaborator with faculty to last-minute data cleaner, and everything in between. DH librarians also need to be prepared to support projects and research across the spectrum of disciplines, so we examine varying research methods across the humanities. The final project for the course asks that students locate an abandoned, or complete but aging DH project, and insert themselves as a librarian; they provide an evaluation of the content as well as the technology of the project and suggest ways to improve or update both.
                The data sets in these projects varies and examples include: hand-collated quotations by a famous author on a fan site; census numbers provided in a project about London families in the 17th century; a list of shooting locations for a television show; metadata for photos of logging camps in the Pacific Northwest; multimedia elements in a documentary film; boxes of music programs from a summer camp; and quilting patterns. 
                Some projects also include the more typical (and larger) type of data set, such as those from HathiTrust or Google-generated Ngrams, but they have proven to be the exception. Working with small data sets means that cleaning doesn’t occupy much time during a 10-week quarter, and they can be rearranged quickly to utilize multiple visualization or data processing options.
                Students evaluate the data sets early in the process; in nearly all cases, data sets are either incomplete or inaccurate, and for some, updated data or other content is available. This is where the multi-disciplinary expertise of librarians comes in, as MLIS students are trained in searching out valid information sources from multiple perspectives, whether that’s using vendor-supplied databases, open web search engines, or (gasp) sources in print or microform. This is also where students begin to see the striation of roles between true collaborators, project leaders, subject specialists, technical consultants, or data-wranglers.
                In reviewing aging or abandoned projects, students learn how easily the data, other content, and the functionality of the site/project can be lost. This gives them the added perspective they need to start thinking about curation and preservation, rather than tackling those issues as add-ons if they have time.
                Through these immersive projects, students have a chance to see DH through multiple lenses: those of a potential user, a collaborator, and a disciplinary specialist. They learn how to re-create and improve on a project. In doing so, they gain experience in evaluating and collecting data as well as in multiple platforms and software that are prominent in DH (some current, some defunct). Some students also reach out to the original site or project owner, and in a few cases have worked with that person to update the project, putting preservation or stabilizing features in place for future users.
            
        
        
            
                
                    Bibliography
                    
                        Borgman, C. L. (2012). The conundrum of sharing research data, 
                        Journal of the Association for Information Science and Technology, 63(6): 1059-78.
                    
                    
                        Croxall, B. (2017). Digital humanities from scratch: A pedagogy-driven investigation of an in-copyright corpus
                        , Digital Humanities 2017: Conference Abstracts, Montreal: McGill University, pp. 206-7.
                    
                    
                        Ducharme, D. J. (2010). 
                        Guide to the Pettigrew Papers OSB MSS 113. New Haven: Beinecke Rare Book and Manuscript Library. 
                        
                            http://hdl.handle.net/10079/fa/beinecke.pettis1
                         (accessed 27 April 2018).
                    
                    
                        Moshenska, G. (2012). Selected correspondence from the papers of Thomas Pettigrew (1791-1865), surgeon and antiquary. 
                        Journal of Open Archaeology Data, 1(0). 
                        
                            https://doi.org/10.5334/4f913ca0cbb89
                         (accessed 27 April 2018).
                    
                    
                        Rockwell, G., Day, S., Yu., J., and Engel, M. (2014). Burying dead projects: depositing the Globalization Compendium. 
                        Digital Humanities Quarterly, 8(2). Retrieved from 
                        
                            http://www.digitalhumanities.org/dhq/vol/8/2/000179/000179.html
                         (accessed 27 April 2018).
                    
                    
                        Sinclair, S. and Rockwell, G. (2012). Teaching computer-assisted text analysis. In Hirsch, B. (ed) 
                        Digital Humanities Pedagogy: Practices, Principles, Politics. Open Book Publishers, pp. 241-54. Retrieved from 
                        
                            https://www.openbookpublishers.com/product.php/161
                         (accessed 27 April 2018).
                    
                
            
        
    

        
            Overview
            While scholarship on pedagogy in digital humanities has been growing, its focus has largely been on graduate and, to a lesser extent, undergraduate education. Yet, digital humanities pedagogy—namely its value for cultivating 21st century literacies tied to the production of knowledge and the ability to interpret digital media and computation—is as valuable, this panel argues, for middle- and high- school students as it is in higher education. Given that we are pursuing what Matthew Kirschenbaum describes as forms of "scholarship and pedagogy that are bound up with infrastructure in ways that are deeper and more explicit than we are generally accustomed to" (60), this panel examines the work of instructors who are beginning to plant the seeds of these new “customs” early on in humanities and social science training.
            Using digital humanities pedagogy in the middle- and high-school classroom, panelists argue, can redress gaps in these literacies. It enables students, as Mark Sample suggests, “[to think] through their engagement with seemingly incongruous materials, developing a critical thinking practice about process and product” (405). In this way, the approaches to digital humanities pedagogy in middle and high schools articulated by panelists are not an attempt to teach students particular technical skills, applications, or platforms. Rather, this pedagogical approach enables students to envision a relationship between themselves and knowledge production. 
            The approaches to digital humanities voiced in this panel are rooted in digital humanities pedagogies in higher education, particularly project-based approaches to humanities knowledge that foster collaboration. As Tanya Clement has argued:
            
                Like pedagogy intended to teach students to read more critically, project-based learning in digital humanities demonstrates that when students learn how to study digital media, they are learning how to study knowledge production as it is represented in symbolic constructs that circulate within information systems that are themselves a form of knowledge production. (366)
            
            In the case studies and pedagogical approaches discussed by panelists, the project form complements more traditional forms of knowledge production and evaluation in the classroom. As Brett D. Hirsch argues, this “introduces a new mode of work that emphasizes collectivity and collaboration in the pursuit and creation of new knowledge” (16). While these new modes can be linked to participatory forms of culture, made possible by low barriers for civic engagement and creative expression online (Jenkins et al. 9), panelists make the case for greater attention to pedagogies that offer instruction to middle- and high-school students in collaborative production. 
            However, as panelists argue, middle- and high-school pedagogies for digital humanities require attention to the unique needs of students in curricula, the developmental trajectories of the students, and the socio-economic dimensions of these students’ lives. In light of these concerns, what are the biggest challenges to doing digital humanities in middle and high schools? Which methods are most valuable and practically achievable? And how can we effectively prepare teachers to incorporate digital humanities into their teaching practices? In this panel we bring together an international team of researchers and faculty already engaged in answering these questions and implementing curricula in schools of education, digital humanities centers, and high schools. Our goal is both to present models and facilitate discussion with broader digital humanities communities about pedagogical infrastructures, methods, long-term goals, and the exciting possibility of cultivating digital humanities pipelines through intervention in middle and high schools.
            Panel moderator: Alex Gil, Columbia University Libraries
            Designing Digital Humanities Pedagogy Infrastructures for Teachers
            Roopika Risam, Salem State University
            While digital humanities pedagogy has increasingly received attention from practitioners who want to teach their own students more effectively, how do we prepare 
                teachers for the challenging task of engaging with digital humanities in their own classrooms? This talk offers an answer to this question by examining the digital humanities pedagogy infrastructure for middle- and high-school teachers designed at Salem State University. I first discuss findings from a study undertaken with teachers in Massachusetts to identify their attitudes towards digital humanities. The results indicate lack of knowledge about digital humanities but significant interest in incorporating computational approaches to humanities into teaching. Teachers also raised concerns including the time needed to learn technologies and teach them to students, cost of software and hardware, uneven access to computers or the internet in classrooms and for students at home, fear of implementing unsuccessful lessons, and a lack of professional development opportunities for digital humanities.
            
            This talk then considers the interdisciplinary graduate certificate in digital studies that Salem State University designed in response to the study. The program provides professional development while addressing teachers’ perceived obstacles to including digital humanities in their teaching. I discuss the relationship between study results and program design, focusing on development of core courses, selection of elective courses, differentiation of course delivery methods, integration into existing master’s programs, and creation of a directed study for curriculum design. To illustrate the impact of the program, I describe my work advising a team of teachers and administrators in the graduate certificate program who were planning technology needs for a new school building under construction and designing technology-infused curricula in English and History. While core and elective courses gave the teachers and administrators a solid background in digital humanities, a group directed study assisted the team with developing a scaffolded curriculum across middle-school humanities courses, designing classroom technology, and creating a professional learning community to provide in-school pedagogical support for teachers. 
            Finally, this talk discusses a follow-up study with graduates of the certificate programs that assessed program outcomes. These outcomes include assignments implemented by teachers in their classrooms, exemplar student work, and a marked difference in attitudes and perceptions of teachers who completed the certificate in comparison to those who participated in the initial study. Based on the outcomes and the success of the graduate certificate program, Salem State has begun integrating digital humanities pedagogy directly into its teacher training programs. Consequently, this talk argues, this digital humanities pedagogical infrastructure for teachers serves as an effective model for addressing the barriers to incorporating digital humanities into middle- and high-school curricula for teachers who are already in the classroom and those preparing for teaching careers in the humanities. 
            Digital Inquiry: The History of Youth
            Nina Rosenblatt, Trevor Day School
            David Thomas, Trevor Day School
            Stan Golanka, Trevor Day School
            On September 12th, 2017 Trevor Day School, an Independent School on the Upper East Side of New York City, launched two sections of an advanced history course entitled 
                Digital Inquiry: The History of Youth. This course was the culmination of seven years of curriculum development work that began with a November 16th, 2010 article in the 
                New York Times about Humanities 2.0 and the Stanford Republic of Letters Project. After an initial round of research we came to understand that digital projects had a role to play in our High School History curriculum. This realization coincided with our adoption of inquiry-based learning pedagogies. In a fundamental way, we argue, the techniques and disciplines involved in digital humanities allow high school students to conduct their own independent research in digital archives and become producers of history in their own right.
            
            In order to motivate students to collaborate and learn unfamiliar working methods, we developed our course around a subject that would engage all students. We wanted a subject that would not require a textbook, was accessible to juniors and seniors in high school, and would lend itself to seminar style classes. In addition, we wanted to be able to supplement the subject matter with texts illuminating the nature of historical narrative, archives, and the use of digital techniques in academic research such as the paper by Lauren Klein, “The Image of Absence: Archival Silence, Data Visualization, and James Hemings” in 
                American Literature Volume 85, Number 4, December 2013
                .
                The resulting course delved into the history of youth, looking at how being young is experienced and imagined differently in different times and places, and what we can learn about a society from its expectations for and attitude towards its youth, while teaching them production and analysis techniques for them to create new representations of that history.
            
            The final consideration was to craft a series of lesson plans to embed a digital humanities knowledge-production laboratory in the class. The course lab was divided into three modules: Digital editions and markup (an introduction to the fundamentals of plain text and markup), digital collections/exhibits (an introduction to the fundamentals of metadata and databases), and cultural analytics (an introduction to the fundamentals of algorithmic thinking and data mining). Through these modules the students were immersed in the process of selection, digitization, mark-up, the creation of a database/archive, data extraction and cleanup and data analysis, all driven by the imperative to create and interpret history. 
                Technologies taught included, but were not limited to, command line, git, GitHub, plain text editors, Markdown, YAML, Jekyll, Omeka, Python and Voyant Tools. These technologies were directly tied to the variety of ways in which historians collect “data” including using literary, psychological, sociological, statistical, and visual sources, working towards creating our own historical knowledge using the digital tools for collecting, visualizing, mapping, and analyzing the information.
            
            In this panel we will present the results of our two course prototypes, lessons learned, future improvements, and argue for a generalizable model of instruction for high schools in the United States based on our experiences.
            Digital Literary Studies in the High School Environment
            Eric Rettberg, Illinois Mathematics and Science Academy
            What are the challenges of adapting a course in Digital Humanities and Digital Culture from the pedagogical environment of the university to that of the high school classroom? What new challenges arise from asking minors to produce digital and public scholarship, and how can digitally inflected scholars and teachers foster innovative humanities work in school environments bound to pre-existing curricula? In this talk, I use my experience adapting a class in Digital Literary Studies to the high-school level to share unexpected challenges and opportunities and to suggest digital work as a strategy for promoting the humanities to administrators, peers, and students in STEM-oriented high school environments.
            In early 2016, I left higher education to teach in the English department of the Illinois Mathematics and Science Academy, a state-funded boarding school for students talented in math and science. Given the immediate appeal of classes combining humanities with computing for STEM-focused students, I naively expected that I might be able to simply bring a college elective for English majors to my high school students. The actual challenges of doing so, however have been instructive: administrators have been less familiar with the existence of the methods of the Digital Humanities, digital assignments have had to be reframed to accommodate shared practice among teachers in my department, my school’s technology environment has needed to be customized to accommodate the software installations that I took for granted before, oversight from administrators, colleagues, and parents has been more intensive, and without the support staff available to me at my higher education institutions, I’ve had to think creatively around constraints. By demonstrating small-scale digital humanities work in core classes, designing a week-long intersession class on a similar topic, and sharing my knowledge of University-level digital humanities, though, I’ve been able to design a class that has colleagues and students excited.
            Heeding Ryan Cordell’s call to embed digital humanities instruction in larger narratives beyond “recent scholarly revolution,” I treat digital humanities praxis as one of three major components of change in literary production and study in the digital era. In addition to digital humanities projects centered on historical texts of students’ choice, students read and discuss fictive works that represent cultures of technology in the digital era and computationally enabled works of electronic literature. Throughout the class, students sample digital humanities practice in lab sessions and build small-scale web resources and undertake digital-humanities experiments in group projects. By exploring electronic texts, they begin to more fully recognizes the affordances of digital technologies, and by reading print texts that represent digital culture, they think about their own roles as consumers of and creators of digital tools and cultures. While my school’s student population and focus are especially suited to the STEAM focus that a class like this one offers, my experiences suggest that students at a wide variety of high schools would be engaged by these materials and skills.
            Impact od Digital Humanities on High School History and Heritage Teaching and Learning in the Caribbean
            Schuyler K Esprit, Create Caribbean, Inc.
            The experience of Create Caribbean Research Institute, the first Digital Humanities center in the English Speaking Caribbean tells an interesting story of how digital humanities can covertly and explicitly reshape the curriculum in history and literature of the Caribbean without necessarily requiring a massive paradigm shift of the national and regional curriculum requirements.
            In Dominica (where Create Caribbean operates) and the Eastern Caribbean – among other islands – the secondary education curriculum responds to the mandates of the Caribbean Examinations Council (CXC) who sets the CSEC and CAPE syllabi for high school and post-secondary certification in the region. These examinations frame the education curriculum for the five to six years of high school in many islands and many educators in this system find themselves bound to deliver content in limiting and limited methods in order to ensure that students simply meet requirements to excel at subject exams at Caribbean History and English B (Literature), which has a heavy focus on Caribbean Literature.
            However, students leave with an abstract and formalized understand of Caribbean history and culture, without a nuanced understanding of its relevance to their own lived experiences and the implications for their future. Create Caribbean uses digital humanities projects to reframe the conversation and disrupt traditional methods for learning. One of these projects uniquely highlights the potential for technology to change the face of education in Dominica and to get students more invested in Dominica’s history and culture. This project, made by students for students, can be found at 
                www.dominicahistory.org. The college student change-makers of Create Caribbean’s internship program build digital humanities projects with a primary and secondary student audience in mind. The example of dominicahistory.org highlights one way that a collaboration with a national organization has allowed for a broader consideration of the methods of heritage and culture education for students while actually providing solid academic source material for their formal study requirements.
            
            This presentation will discuss the origin, process and impacts of the Dominica History and Imagined Homeland digital projects of Create Caribbean as examples of disruptive secondary education. The presentation will also address the ways in which the projects have attracted the attention of high school teachers and transformed their interests in using technology to revise classroom experiences when they face limitations in adjusting other curricular frameworks.
            Precarity and Practicality: DH, New Media, &amp; Secondary Education
            Matt Appegate, Molloy College
            Jamie Cohen, Molloy College
            In 2015, faculty at Molloy College in Long Island worked with faculty and administrators to found the Baldwin High School New Media Academy, a co-organized effort to bring the study of New Media and Digital Humanities to underserved high school populations in Baldwin, New York. Working collectively, faculty at both institutions have established a curriculum and internship path at Baldwin High School that exposes students to methods of DH praxis and principles of New Media in a variety of means and environments (high school, college, in-person, online).
            Our curriculum is based on five modules and two college-credit bearing courses. Our modules include Critical Making, Digital Storytelling, Multimodal Composition, Online Expression, and Social Media. Our college-credit bearing courses are Introduction to New Media and College Composition (the course is taught entirely on the methods of multimodal composition). Each module is integrated into existing high school courses, i.e., Social Studies, English, Wood Shop, etc., where students take college-credit bearing courses in their junior and senior years. Ultimately, the “academy” concept introduces students to DH methods and New Media in a gradated process--students choose their academy prior to entering their freshman year of high school and are enrolled in courses that employ our modules.
            Our curriculum is based on principles of social good; it emphasizes both civic engagement and social justice, and provides sample assignments with grading rubrics for each module (Ratto). The civic-minded focus of our curriculum was developed in consultation with Baldwin High School, and fleshed out over 18 months of training. Our curriculum attempts to account for the precarious position women and people of color already inhabited in online spaces and demonstrate how DH methods and New Media principles can be mobilized to empower students via digital tools and languages. 
            The focus of this paper is to report on our work with underserved high school populations and relay the challenges of bringing this kind of material to a secondary education setting. We focus on the practicalities of bringing DH methods and New Media principles to high school (i.e., funding, time, expertise, bureaucracy), as well as the necessary training that takes places between high school and college faculty (PT days, on campus conferences, and student events). Finally, we discuss the opportunities that working with underserved high school populations provides both politically and pedagogically. In this context, DH operates on a minimal scale, but addresses communal needs. 
            Bios
            Matt Applegate is an Assistant Professor of English and Digital Humanities at Molloy College. His work focuses on critical theory, digital humanities, digital literacy, and screen studies. His work as appeared in 
                Amodern, 
                Theory &amp; Event, 
                Cultural Politics, 
                Cultural Critique, 
                Telos, and more.
            
            
                Jamie Cohen is the director, co-founder and assistant professor of the New Media program at Molloy College in New York. Jamie is the author of 
                Producing New and Digital Media: Your Guide to Savvy Use of the Web
                 (Routledge 2015) and his published and presented research focuses on memes, YouTubers, populism, VR/AR/MR, and digital media literacy. He is a fellow of the Salzburg Academy on Media and Global Change and the Academy of Television Arts and Sciences.
            
            Schuyler K Esprit is the Director of Create Caribbean Research Institute at Dominica State College, the first Digital Humanities center in the Caribbean. Dr. Esprit holds a PhD in English literature from University of Maryland – College Park. She is a scholar of Caribbean literature and cultural studies, and postcolonial theory. She is now completing her book entitled 
                West Indian Readers: A Social History and its digital companion, both of which are historical explorations of reading culture in the Caribbean. She is currently Dean of Academic Affairs at Dominica State College. 
            
            Stan Golanka is Director of Academic Technology at Trevor Day School. He teaches computer programming and co-teaches Advanced History: Digital Inquiry. He holds a MA in Computing in Education from Teachers College at Columbia University. 
            Eric Rettberg teaches English at the Illinois Mathematics and Science Academy. He remains an active scholar of modernism, experimental poetry, sound studies, and the digital humanities. His work has appeared in 
                Comparative Literature Studies and 
                Jacket 2. 
            
            Roopika Risam is an assistant professor of English and English education at Salem State University. Her research considers the intersections of postcolonial cultures, African diaspora studies, and digital humanities. She is the author of 
                New Digital Worlds: Postcolonial Digital Humanities in Theory, Praxis, and Pedagogy (Northwestern UP 2018) and her work has recently appeared in 
                Debates in the Digital Humanities 2016, 
                Digital Scholarship in the Humanities, and 
                South Asian Review. 
            
            Nina Rosenblatt teaches US History, Art History, and Advanced History: Digital Inquiry at Trevor Day School. She holds a PhD in Art History from Columbia University. 
            David Thomas is Chair of the History Department at Trevor Day School, he teaches European History, Advanced European History, The History of China, and Advanced History: Digital Inquiry.
        
        
            
                
                    Bibliography
                    Clement, Tanya. “Multiliteracies in the Undergraduate Digital Humanities Curriculum: Skills, Principles, and Habits of Mind,” in 
                        Digital Humanities Pedagogy: Practices, Principles, and Politics, ed. Brett D. Hirsch (Cambridge: Open Book Publishers, 2012), 365-88.
                    
                    Cordell, Ryan. "How Not to Teach Digital Humanities." 
                        Ryancordell.org, 1 Feb. 2015, 
                        http://ryancordell.org/teaching/how-not-to-teach-digital-humanities/.
                    
                    Hirsch, Brett D. “&lt;/Parentheses&gt;: Digital Humanities and the Place of Pedagogy.” 
                        Digital Humanities Pedagogy: Practices, Principles, and Politics, ed. Brett D. Hirsch (Cambridge: Open Book Publishers, 2012), 3-30.
                    
                    Jenkins, Henry and Ravi Purushotma, Margaret Weigel, Katie Clinton, Alice J. Robison. 
                        Confronting the Challenges of Participatory Culture: Media Education for the 21
                        st
                         Century (Cambridge, MA: MIT Press, 2009).
                    
                    Kirschenbaum, Matthew. “What Is Digital Humanities and What’s It Doing in English Departments?” 
                        ADE Bulletin 150 (2010): 55-61.
                    
                    Ratto, Matt. “OPEN DESIGN NOW.” 
                        Open Design Now, Netherlands Institute for Design and Fashion and Waag Society, opendesignnow.org/index.html%3Fp=434.html.
                    
                    Sample, Mark. “What’s Wrong with Writing Essays?” 
                        Debates in the Digital Humanities, ed. Matthew K. Gold (Minneapolis: University of Minnesota Press, 2012), 404-5.
                    
                
            
        
    


    
        
            
                Voyant Tools 2.0: The New, The Neat &amp; the Gnarly
                
                    
                        Sinclair
                        Stéfan
                    
                    McGill University, Canada
                    sgsinclair@gmail.com
                
                
                    
                        Rockwell
                        Geoffrey
                    
                    University of Alberta, Canada
                    grockwel@ualberta.ca
                
            
            
                
                    2014-12-19T13:50:00Z
                
            
            
                Paul Arthur, University of Western Sidney
                
                    Locked Bag 1797
                    Penrith NSW 2751
                    Australia
                    Paul Arthur
                
            
            
                Converted from a Word document 
            
        
        
            
                
                    DHConvalidator
                
            
        
        
            
                
                    Paper
                
                
                    Pre-Conference Workshop and Tutorial (Round 2)
                
                
                    text analysis
                    Voyant
                
                
                    text analysis
                    data mining / text mining
                    English
                
            
        
    
    
        
            Voyant Tools (voyant-tools.org) is a web-based reading and analysis environment for digital texts. Users can create their own corpus of texts to study by pointing to URLs or uploading files in a variety of formats (plain text, XML, HTML, PDF, MS Word, RTF, etc.). Voyant allows users to navigate between macro views of the corpus (e.g., a word cloud visualization of the entire corpus) and micro views (e.g., a reading individual occurrences of a specific term in context). The default interface provides access to a basic set of tools for reading texts and studying word frequency and distribution. There are also more tools available in various pre-defined or user-defined ‘skins’ (a layout of tools that are coupled). 
            Voyant Tools is deliberately designed to be user-friendly and welcoming for text analysis. Voyant currently averages nearly 50,000 visits and about 750,000 tool invocations per month (not counting the downloadable instances of VoyantServer). This will be the seventh consecutive workshop of Voyant, with past sessions focusing on different aspects (pedagogy, multilingualism, customizability, standalone version, etc.). 
            The 2015 workshop will focus on the second major release of Voyant Tools (2.0), which represents an entire rewrite of the codebase to address several of the major shortcomings and irritants of the currently available version 1.0. Version 2.0 is currently available in a beta version online with a major release due in early spring 2015. In addition to performance improvements throughout, the search and filtering functionality have been vastly enhanced, and Voyant now supports proximity and n-gram operations. Voyant 2.0 also has improved corpus handling. Documents can be reordered or added to corpora on the fly, and there is a lightweight access management layer that differentiates between full access, full-text access, and expressive/consumptive access. 
            We have designed this workshop to be of interest both to new users of Voyant, who will get an introduction to the platform, and to existing users, who will discover all the new functionality 2.0 has to offer. As always, a crucial aspect of the workshop will be to get feedback from the community. 
            Workshop Outline 
            
                1. Introduction to Text Analysis with Voyant (1 hour) 
            
            We will begin with a general introduction to text analysis using Voyant aimed at those who haven’t used it before. We will provide a brief overview of Voyant’s user interface and discuss its strengths and weaknesses. We will provide initial text collections that users can use with Voyant, with a view to having participants experiment subsequently with their own text collections. 
            
                2. Voyant 2.0: What’s New? (1 hour) 
            
            This second part of the workshop will focus on what’s new in Voyant 2.0. Examples of changes include more powerful proximity and fuzzy searching of terms, infinite scrolling instead of paginated scrolling for tabular data, in-place modifications of corpora (adding documents or re-ordering them), and new tools (collocate networks, n-gram wordtrees, etc.). This part of the workshop will be useful both for users familiar with the old Voyant (to understand the changes and enhancements) and also to newcomers who will get a better sense of the variety of available tools. 
            
                3. Voyant: Text Repository or Analytic Platform? (1 hour) 
            
            One benefit of the enhanced scalability of Voyant Tools 2.0 is the ability to bridge the gap between existing text repositories (typically focused on searching for documents) and analytic platforms (for text mining). We are collaborating with several large-scale content providers (like TCP-EEBO and Érudit.org) to create custom Voyant skins that allow users to search and filter within very large text collections in order to create smaller worksets of relevant documents for analysis. Because everything is happening in Voyant, the jump from text repository to text analysis is smooth and efficient (very few text repositories allow mass downloading of worksets, but even when they do, additional steps are typically required for re-ingesting the workset into an analytic platform). This component of the workshop will demonstrate some of our existing collaborations and describe how other content providers and projects might be able to leverage this hybrid functionality. 
            Workshop Leaders 
            
                Stéfan Sinclair, sgsinclair@gmail.com, is an associate professor in digital humanities at McGill University. His research focuses primarily on the design, development, and theorization of tools for the digital humanities, especially for text analysis and visualization. He has led or contributed significantly to projects such as Voyant Tools, the Text Analysis Portal for Research (TAPoR), and BonPatron. Other professional activities include serving as associate editor of 
                Digital Humanities Quarterly, as well as serving on the executive boards of ACH, CSDH/SCHN, ADHO, and centerNET. 
            
            
                Geoffrey Rockwell, grockwel@ualberta.ca, is a professor of philosophy and humanities computing at the University of Alberta, Canada. He has published and presented papers in the area of philosophical dialogue, textual visualization and analysis, humanities computing, instructional technology, computer games, and multimedia. He was the project leader for the CFI (Canada Foundation for Innovation)–funded project TAPoR, a Text Analysis Portal for Research (tapor.ca), which has developed a text tool portal for researchers who work with electronic texts. He is the author of 
                Defining Dialogue: From Socrates to the Internet (Humanity Books).
            
            Target Audience 
            A wide range of DH practitioners interested in text analysis, particularly for research, teaching, or technical support. Voyant Tools workshops are typically fully subscribed; we prefer to limit registration to about 25 people to allow us to help participants as needed. 
            Format
            Half-day.
        
    

The Computational Thinking and Learning Initiative (CTLI) at Vanderbilt University formed out of an awareness that, across society and academe, computation is changing the nature of knowledge. As the practices and methods for producing, sharing, and contesting knowledge change, the enterprise of the university—its disciplinary scholarship, liberal arts mission, and charge to prepare professionals for the world of work—is being reshaped by algorithmic norms. The rise of computational thinking as a transdisciplinary category holds both promise and peril for the humanities. How can humanists, especially digital humanists, take advantage of the push for computational thinking across the curriculum while avoiding the dangers of appropriation? Alternatively, how can those developing and refining computational methods tap into the critical perspectives of the humanities? This paper discusses how an interdisciplinary group of colleagues is drawing on the theory of human-centered computing to develop environments and curricula for students of the humanities to explore the basics of text mining while also providing them with space to critique and resist the imposition of algorithmic rationality.The CTLI foregrounds a particular image of computational thinking across the curriculum, working to identify, stabilize, and study new forms of human-computer partnership that are responsive to disciplinary ways of knowing. Our aim is to study new ways in which individuals or groups of humans, along with computers or groups of computational entities, can come together productively and critically as collective computational-thinking units and build on complementary strengths to investigate problems while rejecting facile technical solutions. The perspective of human-centered computing, that is, the “design of computing systems with a human focus from beginning to end,”1 functions as an Archimedean point of our collaboration.A trans-institutional group of researchers and scholars have assembled at the CTLI, including faculty from the schools of arts and science, engineering, and education as well as the library and the data sciences institute. Together, we are exploring how human-centered computing collaborations transform epistemologies, practices, and pedagogies across disciplines.Following the programmatic overview that Anderson and Ramey presented at DH2019,2 these collaborators selected textual analysis as one of two focus areas (the other being climate change) for the first year of the initiative. Using NetsBlox,3 a block-based programming environment developed at Vanderbilt University, we constructed components and curricula to teach students in the humanities the fundamentals of computational thinking (variables, looping, functions, recursion, etc.) by manipulating textual corpora rather than matrices of numbers. We wanted this environment to be "low threshold" enough for middle and high school students to use it as an entry point to disciplinary inquiry, while having a sufficiently "high ceiling" to allow scholars of literature and history to use it meaningfully as well.Given our commitment to discipline-specific visions of computational thinking, our pilot project attempted to understand how the professional vision4 of the humanist might resonate with the technological capacity of new computational tools and methods. We viewed human-computer collaboration as an integration of productively-different ways of interacting with the objects of analysis (e.g., texts). Computers and humans “read” differently, and the trick was to find a way to put these ways-of-reading into conversation. We used design-based research5 to investigate possibilities, supporting and studying scholars' creative efforts to engage with technological tools and achieve what they regarded as progress on humanistic projects. We then developed a learning environment that enabled younger students to take on similar relations to computational tools, but in simpler, playful settings. Working with the same computational approaches at different levels of complexity and sophistication, we aimed to gain new perspectives on their power and limitations. We explored a design space with scholars and students centered on the analysis of style and affect in poetry, iteratively identifying and testing functionality with the ultimate objective of creating activities for both secondary-school and undergraduate courses.In the course of our explorations, we narrowed the team’s focus to three different technologies for textual analysis: (i) fundamental natural language processing concepts such as named entity recognition; (ii) word embeddings, such as Word2Vec;6 and (iii) a “query runner” to TEI-encoded documents in BaseX,7 a native XML database. During our collaboration, the team applied each of these technologies, first playfully, to explore what insights they could yield with familiar and constructed texts; then more deliberately, in settings in which it was likely that the computer’s “readings” might be put in productive conversation with humans’ readings. Finally, the team proposed questions, on the one hand, that revealed new patterns in larger corpora; and activities, on the other, that engaged younger learners in reflecting on style as a feature of writing under the writer’s control.In this paper, we report on this first year’s effort and our progress in building a block-based computing environment and curriculum that supports textual analysis by both high school students and professors of literature. We discuss how these tools and perspectives scaffold activities as diverse as teaching secondary students about the linguistic differences between poetry and prose and detecting stylistic patterns among Victorian writers. We also discuss the limitations of taking a human-centered computing approach when conducting digital humanities research; in particular, we examine the drawbacks of block-based languages for text mining in comparison with tools like Lexos8 and Voyant.9
On March 8 2019, Spanish-speaking Twitter communities erupted with a polyvocal outcry against gender violence, while simultaneous offline protests took place in cities across the globe. This paper presents the results of a multi-modal spatial and thematic analysis of regional appropriations of the hashtag #8M (8 March) to illustrate the carrefours/intersections of trans-Hispanic networks of feminist solidarity online. Through spatial, network, and word frequency analyses of #8M, we examine the hashtag’s intersections with existing transnational and regionally-specific feminist Twitter dialogues, including the #NiUnaMenos movement launched to combat femicide in Argentina and the hashtag #Cuéntalo used to protest the lenient sentencing of a group of men who raped a woman in Pamplona, Spain. By combining quantitative and geospatial analyses using Twitter Archiving Google Sheets, Voyant-Tools, and Carto with targeted close readings of tweets containing #8M, this paper traces the regional variations in a large, multinational online Spanish-language conversation about gender violence.
The increasing value of information visualization techniques to support investigating quantitative research questions in humanities applications is well-documented (Jänicke et al., 2017; Windhager et al., 2019). At the same time, it is important to make (digital) humanities scholars literate in dealing with visualizations to ensure that accurate conclusions can be drawn. In order to make younger generations ready for interdisciplinary work in a digital humanities context, I taught a module that attracted both computer science and humanities students. This article reflects on the most important aspects in teaching the course throughout three years.Cohort of studentsTeaching needs to be flexibly organized in dependency on the backgrounds of students joining a course. A course might be offered exclusively for either humanities or computer science students, or it can attract students with diverse study subjects. I faced different constellations, and emphasized different aspects according to the demands of the cohort. Whereas humanities students profit from more intense discussions on computational thinking and data modeling, it is especially helpful for computer science students to get to know typical research interests and traditional workflows of humanities scholars. The major focus should be to ensure that students, independent of their backgrounds, learn to “speak the same language” using the same terminology.Teaching visualization theoryTo serve students with an easy-to-digest overview of visualization design, I recommend Tamara Munzner’s book “Visualization Analysis and Design” (Munzner, 2014). It provides an introduction to data and task abstraction that are necessary to comprehend and to develop new or adapt existing visualizations. It further discusses how data features can be appropriately mapped to visual features, and how users can interact with visualizations. Also, emphasis should be devoted to Shneiderman’s Information Seeking Mantra “Overview first, zoom and filter, then details-on-demand” (Shneiderman, 1996) as it encapsulates the general idea of quantitative data analysis without losing the materials a visualization is composed of. This rather theoretical frame should be accomplished with discussing visualization techniques that are of particular importance for digital humanities research: geographical maps, timelines, tag clouds, heat maps and graphs.Ready-to-use-based vs. development-driven visualizationsNext to theoretical contents, the course should have a strong focus on practical work. It is known that different approaches to make use of visualization for knowledge discovery in digital humanities applications exist (Jänicke, 2016). The first approach is to apply ready-to-use tools like Voyant (Sinclair and Rockwell, 2020) for quantitative visual text analysis or Gephi (Bastian et al., 2009) for graph visualization. While the advantage of using such frameworks is generating arguable visual output in a short amount of time, research interests might deviate from what the tool can provide. Further, scholars need to learn how to use potentially complex tools and how to interpret upcoming results. In contrast to applying existing tools to generate visualizations, (digital) humanities and visualization scholars might also engage with each other aiming to generate a new visual vocabulary to expedite knowledge discovery, thereby facing the problems of interdisciplinary collaborations. While the first approach should be carried out simultaneously to the theoretical sessions, I recommend to conduct interdisciplinary student projects in the second part of the course.Supervisory roles during project workTraining to apply the learned visualization-related terminology should be the main focus of a project, while the visualization result itself plays a secondary role. Especially in a setting with mixed backgrounds, students should be engaged to think about potentially interesting project ideas. Each conducted project should include at least one participant with a (digital) humanities and one with a computer science background to expedite interdisciplinary exchange. This constraint generates different supervisory roles:The Mediator: Projects involve students having a computer science and a humanities background alike. The entire project is managed by the students, whereas the teacher might supervise in the form of a mediator during meetings. While students face typical pitfalls of interdisciplinary projects, such projects still brought forth considerable results, one of which is shown in Figure 1.The Real & the Fake Humanities Scholar: When the number of students with a humanities background is too low, project groups that only include computer science students need to be complemented with domain experts. For some of the projects, I was able to involve partners from the humanities, the real humanities scholars, with research interests targeted towards available data sets. This setting generated very good results (see Figure 2) and guaranteed the steepest learning curve for computer science students as they cooperated with domain experts experienced in digital humanities. However, for some projects I, educated in computer science, needed to act as a humanities scholar, being the least favorable setting as only fake interdisciplinary discussions are possible.The Helper: On the other hand, the number of computer science students joining a course might be limited. In that case, I could advise students in the role of a computer scientist, better suitable considering my own background. The focus in such projects was rather on data modeling and acquisition as well as applying existing tools and libraries than developing new solutions. An example is shown in Figure 3.Figure 1: The mediator project developed a method to semi-automatically extract biographical information about members of the German Bundestag in 2019, and the adapted stream visualization allows multifaceted exploration of biographical features.Figure 2: The real humanities scholar project focused on the development of an interactive tag cloud that supports composing engineering branches based on the study subjects of engineering professors. More information can be found in a related publication on the project (Meinecke and Jänicke, 2018).Figure 3: The helper project focused on the contents published at three German websites known for publishing fake news articles. For comparatively analyzing the results, the TagPies visualization (Jänicke et al., 2018) was adapted.I recommend making such courses accessible to all students as it prepares them best for potential future collaborations in a digital humanities context. More detailed information on theoretical contents, conducted student projects and course reflections can be found in my related IVAPP article (Jänicke, 2020).

        
            
                What is computational thinking in the digital humanities?
                The question whether digital humanists should learn to code has been highly-contested (Ramsay, 2011: 243–45). The ‘hack’ versus ‘yack’ debate has lost its edge as scholars concede that theory and programming praxis can be brought together productively through what Davidson terms “collaboration by difference” (Davidson, 2015: 134). Given that the majority of digital humanists will not need to program professionally, to what extent ought computation be taught in the digital humanities? Jeannette M. Wing coined the term “computational thinking” to address the teaching of digital literacy beyond computer science. She contended that “computational thinking is a fundamental skill for everyone, not just for computer scientists” (Wing, 2006: 33). In 
                    Digital Humanities, Berry and Fagerjord comment at length on Wing’s definition, concluding that “a critical understanding of computing at its different levels is a prerequisite for a digital humanist...” (Berry and Fagerjord, 2017: 59). There is little agreement, however, about the best way to teach computational thinking to humanists. We review the potential of visual or “block-based” programming languages for teaching computational literacy in the digital humanities. We argue that digital humanists should learn from these tools’ emphasis on the ludic over the pragmatic. We also offer suggestions about how digital humanists might adapt and critically adopt block-based programming as they seek to expand their understanding of fundamental concepts of computer science.
                
            
            
                Review of educational programming environments
                The use of visual or “block-based” programming has become a mainstay for computer science education in the K-12 arena. Block-based programming involves the manipulation of graphical elements to create units of computation. The authors of 
                    Learnable Programming: Blocks and Beyond argue that block-based programming makes it easier to learn to program for three primary reasons: emphasizing recognition over recall, “chunking code,” and constraining options (Bau et al., 2017: 72–80). These pedagogical advantages would also seem to apply in the digital humanities though, as a quick review of the evolution of these languages demonstrates, they were not created to teach computational thinking to adults.
                
                
                    
                        Logo. The origins of block-based programming stretch back to Logo, a programming language and graphical environment for computer science education. While not a visual programming language, when paired with Turtle Graphics Logo provides students with the ability to visualize their computations (Papert, 1980: 16–20). Logo has gained renewed popularity among the elementary age set due to Gene Luen Yang and Mike Holmes’ 
                        Secret Coders, a series of graphic novels that employs Logo to teach basic computational literacy (Yang and Holmes, 2015).
                        
                    
                    
                        Scratch. The designers of Scratch sought to create a computational environment for kids and teens to become active manipulators rather than passive consumers of digital media. The designers stripped away many of the complexities of software development (e.g., linking libraries and compiling binaries) to create what they term a “tinkerable” environment, pioneering the use of blocks for syntax (Maloney et al., 2010: 16:4).
                    
                    
                        Snap! While Scratch succeeded in developing an extensive community of users in the K-12 arena, its emphasis on semantic simplicity inhibits its usefulness for teaching students computer science at the postsecondary level. The Snap! programming environment emerged from a collaboration between Brian Harvey at Berkeley and Jens Mönig, a software developer currently at SAP. Drawing on long experience teaching functional programming in Scheme, the authors created a semantics with lambda expressions, recursion, and high-order functions using a Scratch-like syntax (Harvey and Mönig, 2015: 35–38).
                    
                    
                        NetsBlox. NetsBlox is an adaptation of Snap! that makes it straightforward for users to communicate with internet services and to communicate peer-to-peer (Broll et al., 2017: 81–86). Students can draw on these features of NetsBlox, for example, to place markers representing art museums in the vicinity on a Google Map or to create a shared digital whiteboard. By fostering the ability to communicate beyond the boundaries of the programmer’s laptop, NetsBlox paves the way for creating data-driven digital humanities projects. Data may also be persisted in the cloud, making it possible to preserve state. Currently, NetsBlox comes with the ability to call out to a select number of services. However, the developers envision “adding a lot of new services and data sources to NetsBlox...”(Broll et al., 2017: 86). This raises the question whether a version of NetsBlox could be developed specifically for digital humanists, integrating web-based application programming interfaces (or APIs) for platforms like the DPLA, the HathiTrust, and Europeana, among others.
                    
                
                
                    
                
                NetsBlox with prototype RPC block for Wikidata
                The digital humanities community also embraces visual programming models. Voyant Tools, for instance, provides a graphical interface for scholars seeking to study textual corpora (Sinclair et al., 2016). To date, there appears to be little to no scholarship about the pedagogical effectiveness of using visual programming environments in the digital humanities.
            
            
                Programming as ludic rather than pragmatic
                Is learning block-based programming a means to an end or an end in itself? While computer science students will inevitably move from block-based to text-based programming (Kölling et al., 2015: 29–38), the designers of Scratch claim that many students will fruitfully remain within its environment (Resnick et al., 2009: 66f). At the secondary and post-secondary level, “The Beauty and Joy of Computing” curriculum likewise promotes the enjoyment of programming within the Snap! environment: “having fun is an explicit course goal” (Garcia et al., 2015: 71). Leading digital humanists also acknowledge the playful aspects of programing. In “On Building,” Stephen Ramsay remarks, “Learn to code because it’s fun and because it will change the way you look at the world” (Ramsay, 2011: 245). Nick Montfort argues that motivations for learning programming go beyond the “merely instrumental” (Montfort, 2016: 268), remarking “it is enjoyable to write computer programs and to use them to create and discover” (Montfort, 2016: 277). By customizing the visual representations and selecting domain-specific exercises, block-based programming could find wide application in the digital humanities, promoting the joy of learning computation for its own sake while providing humanists with a better conceptual grounding for the evaluation and application of algorithms and software in their digital research.
            
            
                Prolegomena to any future visual programming environment for the digital humanities
                What would a digital humanities version of a block-based programing environment look like? By way of conclusion, we suggest how NetsBlox might evolve past its origins in Scratch to provide a shared platform for teaching computational thinking in the digital humanities. We propose three developments: 1. creating default sprites that represent the domains of digital humanities research (i.e. representing books rather than basketballs); 2. establishing libraries of blocks to call commonly-used web-based APIs in the digital humanities; 3. providing a curriculum focusing on major research areas in the digital humanities, including distant reading, educational gaming, geospatial analysis, and steganography, among other topics. By developing a block-based environment for the digital humanities, we hope not only to advance computational thinking in our field, but also to provide resources for introducing the digital humanities into secondary and postsecondary courses on computational thinking.
            
        
        
            
                
                    Bibliography
                    
                        Bau, D., Gray, J., Kelleher, C., Sheldon, J. and Turbak, F. (2017). Learnable programming: Blocks and beyond. 
                        Commun. ACM, 
                        60(6). New York, NY, USA: ACM: 72–80.
                    
                    
                        Berry, D. M. and Fagerjord, A. (2017). 
                        Digital Humanities: Knowledge and Critique in a Digital Age. Cambridge: Polity.
                    
                    
                        Broll, B., Lédeczi, A., Volgyesi, P., Sallai, J., Maroti, M., Carrillo, A., Weeden-Wright, S. L., Vanags, C., Swartz, J. D. and Lu, M. (2017). A visual programming environment for learning distributed programming. In, 
                        Proceedings of the 2017 ACM SIGCSE Technical Symposium on Computer Science Education. ACM, pp. 81–86.
                    
                    
                        Davidson, C. (2015). Why yack needs hack (and vice versa): From digital humanities to digital literacy. 
                        Svensson, P. , Goldberg, DT, Ed: 131–45.
                    
                    
                        Garcia, D., Harvey, B. and Barnes, T. (2015). The beauty and joy of computing. 
                        ACM Inroads, 
                        6(4). New York, NY, USA: ACM: 71–79.
                    
                    
                        Harvey, B. and Mönig, J. (2015). Lambda in blocks languages: Lessons learned. In, 
                        2015 IEEE Blocks and Beyond Workshop (Blocks and Beyond). pp. 35–38.
                    
                    
                        Kölling, M., Brown, N. C. C. and Altadmri, A. (2015). Frame-based editing: Easing the transition from blocks to text-based programming. In, 
                        Proceedings of the Workshop in Primary and Secondary Computing Education. (WiPSCE ’15). New York, NY, USA: ACM, pp. 29–38.
                    
                    
                        Maloney, J., Resnick, M., Rusk, N., Silverman, B. and Eastmond, E. (2010). The scratch programming language and environment. 
                        Trans. Comput. Educ., 
                        10(4). New York, NY, USA: ACM: 16:1–16:15.
                    
                    
                        Montfort, N. (2016). 
                        Exploratory Programming for the Arts and Humanities. Cambridge: MIT Press.
                    
                    
                        Papert, S. (1980). 
                        Mindstorms: Children, Computers, and Powerful Ideas. New York, NY, USA: Basic Books, Inc.
                    
                    
                        Ramsay, S. (2011). On building. In Melissa Terras, Julianne Nyhan, (ed), 
                        Defining Digital Humanities: A Reader. London: Routledge, pp. 243–45.
                    
                    
                        Resnick, M., Maloney, J., Monroy-Hernández, A., Rusk, N., Eastmond, E., Brennan, K., Millner, A., et al. (2009). Scratch: Programming for all. 
                        Commun. ACM, 
                        52(11). New York, NY, USA: ACM: 60–67.
                    
                    
                        Sinclair, S., Rockwell, G. and Others (2016). Voyant tools. 
                        URL: Http://Voyant-Tools. Org/[September 5, 2016].
                    
                    
                        Wing, J. M. (2006). Computational thinking. 
                        Commun. ACM, 
                        49(3): 33–35.
                    
                    
                        Yang, G. L. and Holmes, M. (2015). 
                        Secret Coders. New York: Macmillan.
                    
                
            
        
    

        
            IsiXhosa is an Nguni language classified in the south-eastern geographical zone of South Africa (Guthrie, 1971:33). It is one of the official South African languages, and one of the most widely spoken (after isiZulu) with approximately eight million mother-tongue speakers. In terms of natural language processing, particularly computational morphology, the Nguni languages including isiXhosa belong to the lesser-studied languages of the world and can be classified as under-resourced languages. Nguni languages are characterised by a rich agglutinating morphological structure, based on two principles: the nominal classification system and the concordial agreement system (Bosch &amp; Pretorius, 2008:97).
            The principal author’s masters study focused on the representation of women protagonists by male and female authors in isiXhosa dramas. The whole analysis process was done manually, mainly because digitised isiXhosa literature books were not available. This limited the study to only four books. The analysis focused on gender inequality in the way women are represented by male authors, as opposed to the way in which women authors represent women protagonists, and also on patriarchal traces found in isiXhosa dramas.
            
                When examining the representation of female protagonists in isiXhosa dramas, similar works from other scholars are noteworthy. The first contribution that narrates the same viewpoint as the one investigated here is by Ngqase (2002), in which she examines the representations of women in four isiXhosa drama books. The study highlights the interplay between culture and women's social space. The second contribution by Peter (2010) expresses female character portrayal in various drama works written by males. He concludes that many male writers are unwilling to portray female characters in their totality and true complexity, which is evident in the way some writers have resorted to the use of stereotypes (Peter, 2010:15).
            
            As an isiXhosa language researcher at the South African Centre for Digital Language Resources (SADiLaR), the principal author has been introduced to computational methods which could afford new ways to approach the research topic described. 
            Assessing and reporting on the usability of computational tools when analysing isiXhosa texts 
            
                This presentation reports on the same research topic, with the focus on computational methods to analyse the texts instead of manual approaches. The computational tools which were utilised include, Voyant Tools and regular expressions (regular expressions) as well as testing the feasibility of BookNLP when used conjunctively with written languages. 
            
            
                The creators of 
                Voyant Tools
                 note that it supports analysis in any language since it mostly operates on character sequences; however, limited language-specific support is available (Sinclair &amp; Rockwell, 2019). Capitalisation in isiXhosa is of special importance in the proposed study as the language follows a pattern where the second letter of a word is capitalised instead of the first. For instance, the “Context” tool in Voyant Tools produces search terms only in lower case.
            
            
                With 
                BookNLP
                , which is specifically built for English texts, the authors will now focus on how successfully the sub-processes in its pipeline fare with a non-Western language and how it could be adapted and/or how a similar pipeline could be developed for isiXhosa using tools developed by SADiLaR. BookNLP was developed by Bamman, Underwood and Smith (2014). The study follows similar approaches to those utilised by Algee-Hewitt, Porter and Walser (2016).
            
            
                Finally,
                 regular expressions
                 will be used as well, as it allows to match patterns and search for very specific character sequences more effectively.
            
            Operationalisation of the research questions
            
                The study has two parts. First, by reporting on the performance of the computational tools on the isiXhosa drama corpus versus an English equivalent. The specific steps for Voyant Tools and differences when using regular expressions will be provided and compared. Second, in terms of research questions focusing on the representation of Xhosa protagonists by male and female authors, regular expressions was used as the main investigation tool. 
            
            
                The paper reports on:
            
            
                How can computational tools used to analyse Western languages be used for conjunctively written South African languages? 
                Are authors of isiXhosa literature influenced or led by their gender when writing? 
                
                    Do authors conceptualise their work with the intention to uplift one gender while diminishing the other?
                
                How can the gap caused by inequality between sexes be bridged through written literature? 
            
            A practical example:
            
                If data from an English corpus is analysed using Voyant Tools, the tool would automatically be able to provide word frequencies and links between words. However, with a conjunctive language like isiXhosa, this would only be possible by making use of special search options, because the generic frequency table will be skewed owing to the difference in semantic properties of the words. For example, gender association in isiXhosa depends solely on the prefix. Only through the prefix will one be able to confirm whether a noun is referring to a single person or a group of people. Furthermore, only through contextualisation will one know whether that person is male or female, as isiXhosa prefixes are also unisex. 
            
            
                E.g.:
                 uPeter 
                u
                sela amanzi
            
            Peter (he) is drinking water
            
                uSammy 
                u
                phunga iti.
            
            Sammy (she) is drinking tea.
            This research also aims to provide a point of departure for new scholars interested in analysing isiXhosa literary works using computational approaches.
            
                Key words
                : Conjunctive language, Nguni language, computational methodologies, voyant tools, regular expressions, BookNLB
            
        
        
            
                
                    Bibliography
                    
                        Algee-Hewitt, M., Porter, J., Walser, H
                        . (2016). Representations Of Race: Mining Identity In American Fiction, 1789-1964. In Digital Humanities 2016: Conference Abstracts. Jagiellonian University &amp; Pedagogical University, Kraków, pp. 111-112.
                    
                    
                        Bamman, D., Underwood, T. and Smith, N. A.
                         (2014). A Bayesian Mixed Effects Model of Literary Character. 
                        Proceedings of the 52nd Annual Meeting of the Association for Computation Linguistics.
                         Baltimore, Maryland, pp. 370-79.
                    
                    
                        Bosch, S., Pretorius, L. and Fleisch, A.
                         (2008). Experimental Bootstrapping of Morphological Analysers for Nguni Languages. 
                        Nordic Journal of African Studies
                         17(2):66-88.
                    
                    
                        Guthrie, M.,
                         (1969). 
                        Comparative Bantu, Farnborough
                        : Gregg, vol. 4.
                    
                    
                        Guthrie, M.,
                         (1970). Contributions from Comparative Bantu studies to the prehistory of Africa.  
                        Language and history in Africa.
                         (Dalby ed.), 1:1-27.
                    
                    
                        Ngqase, F.F.,
                         (2002). 
                        The way in which women are portrayed in isiXhosa dramas. 
                        B.A Thesis. University of Stellenbosch. Available at: (Accessed: 24 April 2019)
                    
                    
                        Peter, Z.W., 
                        (2010). 
                        The depiction of female characters by male writers in selected isiXhosa drama works. 
                        BA Thesis. Nelson Mandela Metropolitan University. Available at:
                        
                            http://hdl.handle.net/10948/1482
                        
                         (Accessed: 24 April 2019)
                    
                    
                        Sinclair, S. and Rockwell, G.
                         (2019) Languages.-
                        
                            https://voyant-tools.org/docs/#!/guide/languages Date of access: 24 April 2019.
                        
                    
                
            
        
    

        
            Urban exploration or urbex is the exploration of human-made spaces that are generally inaccessible and hidden away from the general public. Recording the visit of these ‘forgotten’ spaces through photography is a main component of this phenomenon which has resulted in a wealth of urban exploration photos and videos of abandoned sites. 
            Urbex destinations are located worldwide and include a wide range of abandoned sites. Belgium has been a very popular destination for urban explorers and Château de Noisy, a neo-gothic castle in Belgium dating back to the 19
                th century was a very famous destination which was demolished in 2017. There is a rich collection of urbex materials on this building which urban explorers have shared through various online platforms, such as personal websites, Facebook, YouTube, and Flickr. The latter has become a significant repository of urban exploration photographs. 
            
            Regardless of the social and political complexities of this phenomenon, urban exploration is intertwined with abandoned historic sites and in recent years the potential of urban exploration for preservation of heritage has been brought to the attention of academia. Considering that urban exploration is becoming increasingly popular, the importance and possible contribution of this activity and its records for research on abandoned heritage sites cannot be neglected.
            This research focuses on the documentation and information management of abandoned heritage sites and looks into the potentials of the rich collection of existing digital urbex resources for their preservation by exploring their content and new means of representation and engagement. The unique value of such iconographic data can be attributed to the fact that normally these abandoned sites are inaccessible to the general public. Hence these photos and videos can shed light on these unknown places, and with the right utilization can not only document and digitally preserve some aspects of the valuable heritage but also can bring public attention to heritage sites that may still be saved from deterioration and revived. 
            To explore the potentials of urbex produced materials for heritage preservation, concentrating on the rich collection of urbex data of numerous abandoned sites, this research aims to gain insights into the urbex scene and its evolution. Moreover, focusing on Château de Noisy, considering that the prevalent methods of documentation of a historic site which require physical access and presence are not applicable, it aims to explore the potential of ‘distant documentation’ by investigating the application of existing tools and software to create a new approach for the preservation of abandoned and even demolished heritage sites and their story. To reach these objectives, focusing on Flickr and using the Flickr API service, two Flickr Dataset are collected: One of general photos related to urban exploration on Flickr (from 2000 to 2017) and another which includes the specific photos of Château de Noisy on Flickr. To collect, prepare, visualize, analyse, create and present the data for this study multiple tools and methods are employed: Python Scripting Language (collection and preparation), Tableau Desktop (visualization and analysis), Voyant (textual analysis), ContextCapture (creation/reconstruction) and WebStorm (presentation via the creation of a website). 
            Terminology of the urbex Flickr photo titles and visualizing the distribution of the urbex Flickr photos, led to interesting insights into the urbex scene. Furthermore, the collected and downloaded images of Château de Noisy from Flickr offer insights into this abandoned building carrying information on diverse aspects such as its function, materials (and pathology), structure and context over the course of many years. 
            Château de Noisy was demolished without being given the chance for detailed documentation through advanced 
                in situ techniques. Using the ContextCapture software and a selection of the images and videos of the castle that were identified through retrieving the images of the Château de Noisy Flickr Dataset, a 3D mesh model of the building and its immediate context is created. This scalable 3D reconstructed model can allow a flexible interactive experience of the site and can be used to curate and create an immersive experience of the exterior of the building and its immediate context while providing additional heritage information. A digital reconstruction of the building and subsequent narration that builds upon this vessel can create an engaging and immersive experience for the public and digitally preserve the ‘fairytale castle’ building that once stood in Celles. The experience of such distant documentation of Château de Noisy can also be implemented in other heritage sites which are demolished or inaccessible. For buildings that still exist, raising awareness of its current state and heritage values can lead to their potential preservation and revival.
            
            
                
            
            Figure 1. Image extract from the reconstructed 3D model of Château de Noisy
        
        
            
                
                    Bibliography
                    
                        Arboleda, P. (2016). Heritage views through urban exploration: The case of ‘Abandoned Berlin’. 
                        International Journal of Heritage Studies, 22(5): 368-381.
                    
                    
                        Bennett, L. (2013). Who goes there? Accounting for gender in the urge to explore abandoned military bunkers. 
                        Gender, Place &amp; Culture, 20(5): 630-646.
                    
                    
                        De Liedekerke Beaufort, C. (n.d.). 
                        Le Château de Noisy. [Historic 16 page text on Château de Noisy available at Augustijns Historisch Instit. in Heverlee, Belgium]
                    
                    
                        DeSilvey, C. (2006). Observed decay: Telling stories with mutable things. 
                        Journal of Material Culture, 11(3): 318-338.
                    
                    
                        Garrett, B. (2010). Urban explorers: Quests for myth, mystery, and meaning. 
                        Geography Compass, 4(10): 1448-1461.
                    
                    
                        Garrett, B. (2013). 
                        Explore Everything: Place-Hacking the City. London: Verso Books.
                    
                    
                        Garrett, B. (2014). Undertaking recreational trespass: urban exploration and infiltration. 
                        Transactions of the Institute of British Geographers. 39(1): 1-13.
                    
                    
                        Kindynis, T. (2017). Urban exploration: From subterranea to spectacle. 
                        British Journal of Criminology, 57(4): 982-1001.
                    
                    
                        Mott, C. and Roberts, S.M. (2014). Not everyone has (the) balls: Urban exploration and the persistence of masculinist geography. 
                        Antipode, 46(1): 229-245.
                    
                    
                        Ninjalicious [Chapman, J.] (2005). 
                        Access All Areas: A User’s Guide to the Art of Urban Exploration. Toronto: Infilpress.
                    
                    
                        Novel, C., Kervien, R., Graindorge, P. and Poux, F. (2016). Comparing aerial photogrammetry and 3d laser scanning methods for creating 3d models of complex objects [White paper], https://www.bentley.com/en/products/brands/contextcapture
                    
                    
                        Paiva, T. (2008). 
                        Night Vision: The Art of Urban Exploration. San Francisco: Chronicle Books.
                    
                    
                        Palombini, A. (2017). Storytelling and telling history. Towards a grammar of narratives for Cultural Heritage dissemination in the Digital Era. 
                        Journal of Cultural Heritage, 24: 134-139.
                    
                    
                        Pinder, D. (2005). Arts of urban exploration. 
                        Cultural Geographies, 12(4): 383-411.
                    
                    
                        Sansivero, B. (2015). Belgium's Abandoned Fairytale Castle. 
                        Atlas Obscura, https://www.atlasobscura.com/articles/belgiums-abandoned-fairytale-castle 
                    
                    
                        Spyrou, E. and Mylonas, P. (2016). A survey on Flickr multimedia research challenges. 
                        Engineering Applications of Artificial Intelligence, 51(C): 71-91.
                    
                    
                        Stones, S. (2016). The value of heritage: Urban exploration and
                        the historic environment. 
                        The Historic Environment: Policy &amp; Practice, 7(4): 301-320.
                    
                
            
        
    

        
            
                Introduction and motivation
                In a 2020 talk entitled “A Hornbook for Digital Book History”, Whitney Trettien weaves together many of the strands that have led book history, bibliography, media studies, and the digital humanities to have become deeply entangled in recent years. She convincingly argues for the potential of Book History done digitally “to build connective tissue across scattered collections” and advocates “using the digital tools at our disposal in order to see the big picture of the past”.
                    
                        
                            https://rarebookschool.org/rbs-online/a-hornbook-for-digital-book-history/. She shares this vision of a continuum of print and digital with other influential voices at the intersection of book history, media studies, and the digital humanities, among them Henrike Laehnemann, Sarah Werner, and Matt Kirschenbaum to name but a few.
                        
                    
                
                It is in this vein that this paper presents the motivation for and realisation of a new open-access open scholarship platform (currently in public beta) named PRISMS.
                    
                        
                            https://www.prisms.digital/
                        
                     The aim of the PRISMS Open Scholarship platform is two-fold:
                
                
                    It offers a publication platform for digital scholarly editions, with full-text (preferably encoded in TEI) and facsimiles, and any accompanying materials, such as introduction, editorial statement, critical apparatus, contextual source materials, bibliography, and indices;
                    It facilitates the semantic annotation of these editions and their related scholarship (in any format) by enabling easy-to-perform formal ontological modelling (based on the CIDOC-CRM family of ontologies
                        
                            
                                http://www.cidoc-crm.org/
                            
                        ), and thus hopes to contribute to providing the abovementioned “connective tissue” not only for scattered collections, but to overcome the artificial print/digital divide.
                    
                
                PRISMS was born out of the realization that digital editions do not break with the historicity or materiality of the sources they organize and present, but instead remediate and extend them in ways that enable new forms of access, engagement, presentation, and analysis. PRISMS conceptualizes digital editions as living entities that perform rather than merely document the remediation they engage in.
                The scholarship that underpins each digital edition provides the essential context for these remediation processes, and collectively they sustain the knowledge network that supports all academic engagement with the texts from any disciplinary viewpoint. PRISMS is designed to allow for the collaborative and collective modelling of this continuum of digital editions and scholarship by placing digital editions, their material and contextual basis, and the resulting academic engagement in a linked context, building on the standards and tools provided by the Semantic Web.
                We believe that this type of formalization is beneficial for the purposes of this project in at least three ways: firstly, ontologies facilitate modelling with reduced reliance on implicit knowledge through an explicit, shared conceptualization of the domain. Secondly, formal models encourage collaboration as they can be shared, re-used, adapted (forked), enhanced, aggregated, and developed collaboratively. Thirdly, as a form of knowledge representation, visualization, and preservation, formal models support computational processing and ultimately reasoning, and can develop alongside the mental models and human reasoning we engage in as scholars. PRISMS facilitates scholarship that is based on these principles
                    
                         Ground-breaking research projects in this domain include the 
                            ResearchSpace platform and the 
                            Sphaera CorpusTracer project.
                        
                    .
                
            
            
                Approach and implementation
                The PRISMS Open Scholarship platform integrates the task of publishing digital editions with the need for analytical and modelling tools to perform the type of knowledge representation that connects the material, digital, and the scholarship that builds on them. PRISMS aims to support digital editors, book historians, experts in media and cultural studies, librarians, literary scholars, and of course digital humanists, to ensure a wide range of domain expertise, disciplinary practices, and methodological approaches are reflected in the platform. To this end, the PRISMS platform hosts a variety of tools alongside the digital editions, which can be categorized as component tools (such as text-based tools, image-based tools, XML-based tools, etc.) and workbench tools (those available across document types and editions).
                
                    
                    Figure 1 
                        Some of the built-in analysis and visualization tools in PRISMS. Voyant-Tools is shown alongside a relation being made between two editions, and some highlighted annotations
                    
                
                The former category of tools is useful for any type of close scholarly work, and in PRISMS these tools include a bookmarking tool, an annotation tool (initially focussing on texts and images, but with a vision to extend annotation capabilities across all media types), the ability to keep research records (and other forms of note-taking, e.g. transcriptions, translations etc.) in the form of notebooks, and integration of Voyant Tools
                    
                        
                            https://voyant-tools.org/
                        
                     for statistical analysis and a variety of visualisations of texts. The latter category includes the ability to participate in the shaping of the knowledge graph by modelling concepts and relationships, an easy way to organize research materials, and the ability to download, share, and publish contributions for the benefit of all.
                
                
                    
                    Figure 2 
                        Modelling both the material legacies and digital remediation processes with Linked Data technologies and Cytoscape.js
                    
                
                All semantic modelling work, e.g. with regard to the provenance of the material and digital manifestations of an edition, can be performed both directly in a visual representation of the graph using Cytoscape.js or via a set of customizable HTML forms. All resulting triples are stored in an RDF-native graph database (using the abovementioned ontologies) for long-term preservation, collaboration, and re-use. Every user of PRISMS has both read access to this global graph that underpins the platform and unlimited access (via SPARQL Update operations) to a private graph. By default, everything in PRISMS is private. Everything contributors add is immediately visible to them, and they can conduct their scholarship in complete privacy, without delay or interference. Only when the user decides to publish their contributions will they be made available to everyone. Depending on the type of contribution made, there are options to share, download, and/or publish them. All contributions made to the PRISMS platform are stored in standard formats, e.g. the W3C Web Annotation Data Model for annotations and relations.
            
            
                Contribution and further work
                PRISMS has been launched with corpora from the EEBO-TCP
                    
                        
                            https://textcreationpartnership.org/tcp-texts/eebo-tcp-early-english-books-online/
                        
                    , ECCO-TCP
                    
                        
                            https://textcreationpartnership.org/tcp-texts/ecco-tcp-eighteenth-century-collections-online/
                        
                    , EVANS-TCP
                    
                        
                            https://textcreationpartnership.org/tcp-texts/evans-tcp-evans-early-american-imprints/
                        
                    , the DTA extended core corpus
                    
                        
                            https://www.deutschestextarchiv.de/
                        
                    , and the Taylor Editions
                    
                        
                            https://editions.mml.ox.ac.uk/
                        
                     scholarly editions platform. And it is easy to add new editions to the platform either as part of a dedicated digital scholarly editing process, for which training is provided, or by simply adding a IIIF manifest and using a built-in XML-aware or standard text-editor to start transcribing and adding contextual materials. The platform already supports the addition and semantic annotation of a wide range of primary and secondary materials, such as facsimiles in IIIF, a transcription of a source text, a PDF of a journal article, a video of a theatrical performance, an audio book, an image, or a 3D-model of a sculpture mentioned in a text, etc. 
                
                
                    
                    Figure 3 
                        A view of the PRISMS workbench, with three editions of Faust loaded, and an aggregation of primary and research materials in support of a performance analysis, with a facsimile, two videos, and an audio book
                    
                
                Moving forward, we will continue to work on integrating the digital research and tools important to PRISMS’ users (e.g. reference manager, images taken in reading rooms, items deposited in institutional repositories). With end of the beta phase, the project also intends to provide access to the entire PRISMS knowledge graph through regular data dumps and a public SPARQL endpoint. We envision that over time, PRISMS will evolve both as a powerful discovery tool and a personal research tool.
            
        
        
            
                
                    Bibliography
                    Ciotti, F. (2015) “Digital methods for Literary Criticism.” Lecture slides. University of Rome Tor Vergata, 
                        http://didattica.uniroma2.it/files/scarica/insegnamento/161783-Informatica-Umanistica-Lm-Per-Il-Llea/37175-Slide
                    
                    Ciula, A. and Marras, C. (2016) “Circling around texts and language: towards 'pragmatic modelling' in Digital Humanities.” 
                        
                            Digital Humanities Quarterly (DHQ)
                         10.3 
                        http://www.digitalhumanities.org/dhq/vol/10/3/000258/000258.html
                    
                    Ciula, A. and Eide, Ø. (2107) “Modelling in digital humanities: Signs in context.” 
                        
                            Digital Scholarship in the Humanities
                         32: i33–i46. 
                        https://doi.org/10.1093/llc/fqw045
                    
                    Ciula, A., Eide, Ø, Marras, C. and Sahle, P. (2018) 
                        Models and Modelling between Digital and Humanities — A Multidisciplinary Perspective. 
                        Historical Social Research (HSR) Supplement 31.
                    
                    Eide, Ø. (2015)
                         Media Boundaries and Conceptual Modelling: Between Texts and Maps. Pre-print manuscript, 
                        https://www.oeide.no/research/eideBetween.pdf
                    
                    Kirschenbaum, M. and Werner, S. (2014) “Digital Scholarship and Digital Studies: The State of the Discipline.” 
                        Book History 17, 406-458 
                        https://www.academia.edu/15995371/Digital_​Studies_​and_​Digital_​Scholarship_​The_​State_​of_​the_​Discipline
                    
                    Kräutli, F. and Valleriani, M. (2018) “CorpusTracer: A CIDOC database for tracing knowledge networks.” 
                        Digital Scholarship in the Humanities 33(2): 336-346. 
                        https://pure.mpg.de/rest/items/item_2472866_10/component/file_3002633/content
                    
                    Laehnemann, H. (2022) “History of the Book blog.” 
                        https://historyofthebook.mml.ox.ac.uk/
                    
                    Oldman, D., Doerr, M. and Gradmann, S. (2016) “Zen and the Art of Linked Data: New Strategies for a Semantic Web of Humanist Knowledge.” In Schreibman, S., Siemens, R., Unsworth, J. (eds.) 
                        A New Companion to Digital Humanities. Malden, MA: Wiley Blackwell, 251-273.
                    
                
            
        
    



        
            
                Introduction
                It is difficult to identify named entities like people and places in long texts and even more difficult to connect the entities that you find to the rich network of information available on the web. In this paper we describe work supported by the LINCS (Linked Infrastructure for Networked Cultural Scholarship) project to make named entity recognition available to scholars through Voyant and its extension Spyral. In this talk we will:
                First, describe the development of NSSI, a set of named entity recognition (NER) tools that are also available as web services for other tools like Voyant to use.
                Second, describe how Voyant can use NSSI as a web service to process a text by adding named entity recognition.
                Third, describe how Spyral, the notebook programming extension of Voyant, can be used for more sophisticated control of the process of named entity recognition, extraction, and use in Voyant. 
                Finally, we will conclude by discussing how NSSI and Spyral will be linked into the LINCS infrastructure to allow scholars to connect their enriched data to that of others.
                Background on LINCS
                Humanists tend to be interested in named people, named places and particular organizations over time. NER tools let humanists identify mentions in text referring to the people, places, organizations and other entities discussed in large collections without having to manually comb through them. Good tools like the Stanford Named Entity Recognizer (Finkel et al. 2005) have been available for some time, but are difficult to use if you are not familiar with command line tools and not connected with other resources.
                The LINCS project, led by Susan Brown at the University of Guelph, is funded by the Canadian Foundation for Innovation to develop shared infrastructure for linked open data. To that end LINCS is working with teams at the University of Alberta and McGill University to develop new NER tools and to connect them to easy-to-use text analysis environments like Voyant.
            
            
                NSSI
                NSSI, or NERVE Secure Scalable Infrastructure, is an application that bundles natural language processing tools, making them simple to use and combine into workflows common to the digital humanities (Zafar 2021). This framework was developed as part of the LINCS project, with the intent to decouple the backend NER tools from the existing Named Entity Recognition Vetting Environment (NERVE) user interface developed by the Canadian Writing Research Collaboratory. This separation allows us to continue using those NER services for NERVE, while making them accessible to other tools such as Voyant and Spyral.
                NSSI’s design focuses on modularity, with each tool connected as a service that can be used individually or within a larger set of steps. For NER in particular, we have integrated Stanford NER which otherwise requires programming knowledge to use, since it does not come with its own API. With NSSI, a tool such as Spyral can make an API call that includes input text or XML and retrieve the named entities when processing completes. In the presentation we will briefly describe the NSSI infrastructure.
                
                    
                    Figure 1: Experimental RezoViz NER Interface in Voyant
                
            
            
                Voyant and Spyral
                Voyant Tools is a suite of text analysis and visualization tools that are widely used with over 100,000 users in the last six months. The tools are available in the browser so they don’t need to be installed, though you can download them and run them locally (Rockwell & Sinclair 2016). In the presentation we will show how Voyant can call the NER tools in NSSI and display the found entities as a list for further use. We will also describe the usability testing conducted on ResoViz through the LINCS project.
                
                    
                    Figure 2: ResoViz Social Network Visualization
                
                Voyant is also being extended with a notebook programming environment called Spyral (Land et al. 2021; Rockwell et al. 2021). Spyral is, like Observable, an in-browser notebook programming environment that uses JavaScript as the programming language. The difference between Spyral and other notebook environments like Mathematica or Google Colab is that a) the notebooks are maintained on the server so that, again, there is no installation needed and b) Spyral is an extension to Voyant. This means that you can save what you see in Voyant as a notebook with an interactive panel of results embedded in the notebook. Then you can document your results, add more interactive panels, and process the results. In the presentation we will show how Spyral can be used to extend the work with NSSI possible with Voyant and to edit and document results.
            
            
                Next Steps
                The paper will conclude by describing the next steps in the larger project, and those are to allow users to connect named entities in their texts to other data about the entities available through the LINCS triple store and other open data resources like Wikidata (Vrandečić 2012). The ultimate goal is to provide scholars with linked infrastructure where data about entities like people or novels can be annotated and connected with that of other projects.
            
            
                Links
                Google Colaboratory (Colab): https://colab.research.google.com/ 
                LINCS project: https://lincsproject.ca/
                Stanford Named Entity Recognizer: https://nlp.stanford.edu/software/CRF-NER.html 
                Voyant Tools: https://voyant-tools.org and Spyral: https://voyant-tools.org/spyral
            
        
        
            
                
                    Bibliography
                    Finkel, J. R., Grenager, T., and Manning C. (2005). Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling. Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370. http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf (accessed 21 May 2022).
                    Zafar, H. (2021). Linked Data Conversion using Microservices [video file]. Zenodo. https://doi.org/10.5281/zenodo.6551465 (accessed 21 May 2022).
                    Land, K., MacDonald, A. and Rockwell, G. (2021). Spyral Notebooks as a Supplement to Voyant Tools. CSDH-SCHN 2021 conference online. http://dx.doi.org/10.17613/2bsr-xp53 (accessed 21 May 2022).
                    Rockwell, G. and Sinclair, S. (2016). Hermeneutica: Computer-Assisted Interpretation in the Humanities. Cambridge, Massachusetts, MIT Press.
                    Rockwell, G., Land, K., and MacDonald, A. (2021). Social Analytics Through Spyral. Pop! Public. Open. Participatory. no. 3 (2021-10-31). https://popjournal.ca/issue03/rockwell (accessed 21 May 2022).
                    
                        Vrandečić, D. (2012). Wikidata: A new platform for collaborative data collection. In Proceedings of the 21st international conference on world wide web, pp. 1063-1064.
                    
                
            
        
    



        
            Digital literary text analysis is increasingly becoming an integral part of literary studies. However, many tools designed for performing such analysis remain inaccessible to researchers without significant coding and computing skills. Voyant Tools was designed in part to address this gap. Spyral Notebooks are an extension of Voyant Tools and allow researchers to expand upon their findings from Voyant in a notebook environment. Unlike other notebook environments, Spyral Notebooks are accessible without downloading any programs or advanced set-up. Spyral Notebooks are available in an entirely online format. To use Spyral Notebooks, one needs only a connection to the Internet. The notebooks are easily adaptable, shareable, and editable. 
            Spyral is a notebook development environment that is integrated into Voyant Tools. Notebook environments can be thought of as both extensions of traditional research notebooks and as novel tools that integrate documentation, active analysis and presentation of results. At their core, notebooks are made up of three types of blocks or cells that a user can add or delete in a sequence. 
            
                There are text cells that can contain headings and other text elements found in word processors or browser editors (usually based in HTML) for typing unstructured text. Depending on the notebook environment, the text blocks can be simple or more sophisticated. Spyral Notebooks use HTML for text and offer an in- browser WYSIWYG HTML editor for the text blocks. 
                There are code cells where the user inputs code, be it Python, the Wolfram language used in Mathematica, or JavaScript, which is used in Spyral. The code cells can be run in sequence or individually as you debug your code. Code cells can contain as much or as little code as the user desires. 
                There are output cells which produce the output of the code you input in the associated code cell. It is important to recognize that the output of the code is dependent on what you have instructed the computer to do; that is, it is not a printout of the code cell but the results of running your code. You thus have to instruct the computer to print out the desired results. 
            
            In our tutorial we introduce participants to Spyral Notebooks. We illustrate how to create a corpus for textual analysis from Voyant Tools or directly in Spyral Notebooks. After walking through the basic mechanisms for using Spyral Notebooks including saving, editing, and sharing notebooks, we move on to more specific features available in Spyral. Participants will learn how to enhance the capabilities of Voyant and go deeper with their textual analysis using Spyral. Finally we provide participants with several tutorial notebooks designed to highlight some of Spyral’s advanced features such as categories for use in sentiment analysis. 
            Spyral Notebooks are a welcome addition to the field of digital humanities as they provide an accessible notebook environment specifically designed for literary text analysis. Spyral Notebooks are thoughtfully designed to serve researchers with limited coding skills who want to take their analysis from Voyant one step further. We especially envisage Spyral proving useful for digital humanities instructors. Spyral provides a useful platform for student work, allowing students to embed their analysis from Voyant, perform more complex analysis using JavaScript, and annotate their code with their thought processes.
        
    



        
            The introduction of digital text analysis tools and methodologies in (non-digital) humanities undergraduate courses has been sparsely documented in the literature. Furthermore, most of the times we encounter it, it is done in the context of semester-long or mid-term projects (Boyle and Hall 2016; Ficke 2014), where the stakes for the students are very high. Other times, they include a session on text analysis but no practical application of the tools and methodologies discussed in the course, other than a follow along demonstration.
            This short paper introduces a middle point between these two extremes through the introduction of low stakes activities and assignments to help student discover and use digital text analysis tools and methodologies.
            Besides giving students the opportunity to interact with the material in a safe and relaxed manner, low stakes activities help with student retention, confidence, and relationship building (Hamilton 2020; Meer and Chapman 2014). Low stakes activities are also a useful tool to assess comprehension and instruction when the person delivering the lesson is not the regular or official instructor in the course, such as the case of a librarian or a guest speaker. Furthermore, these types of activities are particularly useful for digital humanities instruction because they contribute to scaffolding, a method that has been identified as ideal in this type of instruction (Griffin and Taylor 2017; Isuster 2020; Sample and Schrum 2013; Tracy and Hoiem, 2018).
            In the context of a Hispanic Studies course, a librarian offered a workshop series on digital text analysis and the web-based reading and analysis environment Voyant Tools. Interspersed with instruction there were a series of low stakes assessments that helped students understand and apply the content of the workshops. Working with the class readings, the librarian created activities that did not rely on having a single answer but encouraged students to discuss and interrogate both the methods and the information used. For example, when preparing a text for text analysis, students debated how different research questions necessitate different text preparation. The activities were completed in groups and were not graded. Results were discussed within the class.
            The short paper presentation will explore the process of creating and implementing low stakes activities for digital text analysis and other digital humanities instruction. It will discuss the benefits of these types of activities as they pertain to digital humanities instruction and engagement and will share best practices and tips to help attendees create these kinds of activities in their own classrooms, including assignment design and sourcing materials.
        
        
            
                
                    Bibliography
                    Boyle, M. and Hall, C. (2016) ‘Teaching “Don Quixote” in the Digital Age: Page and Screen, Visual and Tactile’, 
                        Hispania, 99(4), pp. 600–614.
                    
                    Ficke, S.H. (2014) ‘From Text to Tags: The Digital Humanities in an Introductory Literature Course’, 
                        CEA Critic, 76(2), pp. 200–210. 
                        10.1353/cea.2014.0012.
                    
                    Griffin, M. and Taylor, T.I. (2017) ‘Shifting expectations: Revisiting core concepts of academic librarianship in undergraduate classes with a digital humanities focus’, 
                        College & Undergraduate Libraries, 24(2–4), pp. 452–466. 
                        10.1080/10691316.2017.1325346.
                    
                    Hamilton, M. (2020) ‘Implementation of a low-stakes daily assessment in a large introductory LAC course’, 
                        Teaching and Assessment Symposium [Preprint]. Available at: 
                        https://digscholarship.unco.edu/posters_2020/4.
                    
                    Isuster, M.Y. (2020) ‘From students to authors: Fostering student content creation with Scalar’, 
                        College & Undergraduate Libraries, 27(2-4), pp. 133–148. 
                        10.1080/10691316.2020.1830908.
                    
                    Meer, N.M. and Chapman, A. (2014) ‘Assessment for confidence: Exploring the impact that low-stakes assessment design has on student retention’, 
                        The International Journal of Management Education, 12(2), pp. 186–192. 
                        10.1016/j.ijme.2014.01.003.
                    
                    Sample, M. and Schrum, K. (2013) ‘What’s Wrong with Writing Essays: A Conversation’, in Cohen, D.J. and Scheinfedlt, J.T. (eds) 
                        Hacking the academy : new approaches to scholarship and teaching from digital humanities. Ann Arbor, MI: University of Michigan Press, pp. 87–96.
                    
                    Tracy, D.G. and Hoiem, E.M. (2018) ‘Scaffolding and Play Approaches to Digital Humanities Pedagogy: Assessment and Iteration in Topically-Driven Courses’, 
                        Digital Humanities Quarterly, 11(4). Available at: 
                        http://digitalhumanities.org:8081/dhq/vol/11/4/000358/000358.html.
                    
                
            
        
    



        
            Not all Humanities have been equally touched by the digital. For textual scholarship, history and linguistics, for instance, we can have a substantial number of scholarly contributions, particularly when we include experiences embodied in projects and resources. However, comparatively speaking, digital literary criticism has had few followers. An exception are Computational Literary Studies (CLS) that apply quantitative methods to large amount of literary and bibliometric data. Linked to the methods of distant reading [Moretti, 2005], this approach enjoys great success today, while web resources like Voyant, software like Gephi, and programming environments like R, have made text mining very accessible, even for those with limited computer skills. Linked to this approach, stylometry and authorship attribution are also thriving. Particularly mediatized researches are the initiatives that led to "unmasking" the identies of Robert Galbraith, a pseudonym of J.K. Rowling, and Elena Ferrante [Joula, 2015; Tuzzi and Cortelazzo, 2018]. However, literary criticism connected to close reading seems almost absent from the DH radar. The CATMA tool, designed to define personalized tagsets for (mainly) literary analysis [Meister 2020], represents a bright exception. Meister, in fact, is one of the few scholars that has engaged with digital literary criticism and digital hermeneutics; the latter has been explored also by Van Zundert (2016) and Ramsey (2011), but from a quantitative perspective. Relatively few scholars in DH have to addressed literary criticism with qualitative approaches, which are, conversely, among the most important for non-digital literary scholars.
            The reasons for this absence are probably to be found in the controversies about the use of markup within texts that have inflamed the scholarly community since the Eighties. The act of adding explicit markers in the text has been subjected to scrutiny, as it is perceived (rightly) as a harbinger of interpretation and this fact has been (and is, to a certain extent, still) perceived as an invasion, a disfigurement of the text; Cummings (2008) gives a vivid account of the debate and reflects on how it has limited the use of TEI for literary criticism. The argument goes that once the text is marked up, it cannot be reused by others because the interpretation added by the encoder would make it unusable. According to this vision, digital texts must be made available in their most neutral and objective form, and any form of annotation, including editorial, must be avoided. Sperberg-McQueen 1991 and Cummings 2008, amongst others, have tried to address the issue, and I have argued elsewhere on the hermeneutic fallacy of the category of objectivity [Pierazzo 2015]; but these methods remain far from impacting “the Humanities at large” and in particular the literary scholars [Meister 2020]. However, in order to contextualize this debate, one should go back to when this controversy was born. The urgency of those years was to put texts online, to create literary corpora for concordances and the study of word frequencies; at the time, digital acquisition of texts, the transformation of the printed into sequences of characters to be analyzed by computers (Machine Readable Form) was mostly done by hand, with an enormous expenditure of time and energy. The emphasis was therefore on making texts available and on the need of not repeating work. Researchers did not want to work with texts full of manually added codes which then had to be removed just as manually in order to reuse the texts.
            It is worth noting, though, how this discourse hides the concept of DH as a service: the goal was thought produce resources for others to do “real” research. This argument is not only dangerous, condemning DH to a mere service, but also wrong, as text, any text, can only be the result of the dialectical compromise between the source documents that contains it and scholars that interpret it (even when they “only” transcribe it), and therefore no text can ever be considered objectively neutral [Pierazzo, 2015]. Today conditions have changed: most literary texts are digitally available in many versions, not to mention the plethora of tools and methods to “get rid of” markup, therefore the objections do not stand in the same way.
            Another obstacle for the uptake of DH in literary studies is the conviction that close reading and critical interpretation only require a reader, a text, and a (printed) essay, and therefore computers, in this context, are useful as typewriters [Kirschenbaum, 2016]. Yet, the lack of experimentation and engagement of the scholarly community in DH for literary analysis does not allow for a clear assessment of the epistemological added value of using computers for one or few texts at a time. But shouldn’t be this the moment for rethinking Digital Literary Studies? Couldn’t we at least try to use markup, ontologies and other methods to understand a text, or answer questions about interpretation?
            The paper will present some experiences at the University of Tours using TEI markup for the history of ideas, and ontologies and databases for analysis of fictional entities (people and places). We have applied these methods to works by Boccaccio, the Vite by Vasari, and to a small corpus of librettos of the 17th century. These experiments are showing promising results, not only in literary terms, but also on a largely methodological perspective, with colleagues and researchers finding themselves challenged and enticed by DH heuristics.
            Conditions are ripe for experiences and discussions in order to evaluate the impact of DH in literary studies, particularly in the light of the advancements in HTR and other types of CLS that have the potentials of bringing a large amount of unknown and understudied texts into the literary arena. This could truly change our perspectives and understandings on literature, but we need to sharpen our hermeneutical tools first.
        
        
            
                
                    Bibliography
                    
                        Cummings, J., 2008. The text encoding initiative and the study of literature. In 
                        A companion to digital literary studies (pp. 451-76), Blackwell.
                    
                    
                        Kirschenbaum, M.G., 2016. What is digital humanities and what’s it doing in English departments?. In 
                        Defining Digital Humanities (pp. 211-220). Routledge.
                    
                    
                        Juola, P., 2015. The Rowling case: A proposed standard analytic protocol for authorship questions. 
                        Digital Scholarship in the Humanities, 30(1): 100-113.
                    
                    
                        Pierazzo, E., 2015. 
                        Digital scholarly editing: Theories, models and methods. Routledge.
                    
                    
                        Ramsay, S., 2011. 
                        Reading Machines: Toward and Algorithmic Criticism. University of Illinois Press.
                    
                    
                        Sperberg-McQueen, C.M., 1991. Text in the electronic age: Texual study and textual study and text encoding, with examples from medieval texts. 
                        Literary and Linguistic Computing, 6(1): 34-46.
                    
                    
                        Tuzzi, A. and Cortelazzo, M.A., 2018. What is Elena Ferrante? A comparative analysis of a secretive bestselling Italian writer. 
                        Digital Scholarship in the Humanities, 33(3): 685-702.
                    
                    
                        Van Zundert, J.J., 2016. Screwmeneutics and hermenumericals: the computationality of hermeneutics. 
                        A companion to digital humanities. (pp. 331-347) Blackwell.
                    
                
            
        
    



        
            This short paper introduces LEAF (the Linked Editorial Academic Framework virtual research environment), an enhanced and expanded collaborative editorial platform that supports a variety of digital scholarly projects through a pipeline of integrated tools for collaborative production and publication of scholarly and documentary collections. Funded through the Canada Foundation for Innovation and the Andrew W. Mellon Foundation, LEAF aims to address the challenges that face many who undertake and maintain large-scale collaborative DH projects now: namely, the need to ensure that these projects can remain operational and available to editors and audiences over the long-haul. It is only by sharing physical, software, and human infrastructures across institutions that this can be accomplished. In so doing we can support scalability, interoperability, and preservation while allowing for dynamic, iterative, and collaborative editing, and therefore ensure that our materials, collections, and editions will remain viable and accessible. The LEAF team aims to do this by integrating best practices for text encoding, annotation, and metadata standards. This short paper will report on the development of LEAF and the functionalities that it will provide. 
            The implementation, and dissemination of LEAF is built upon a collaboration to extend the Canadian Writing Research Collaboratory (CWRC) built by the Universities of Alberta and Guelph (Susan Brown) with Bucknell University (Diane Jakacki), and Newcastle University (James Cummings) as founding partners. This work enhances CWRC’s functionality through collaborative software development that will ultimately support multiple instances of the LEAF platform in Canada, the US, and the UK. At Bucknell, this work will inform the Liberal Arts Based Digital Edition Publishing Cooperative and the Bucknell Digital Press, funded by an Andrew W. Mellon Digital Publishing Cooperative Implementation grant that will support an expanding portfolio of peer-reviewed digital editions and edition clusters. 
            
                The LEAF platform combines hardware, software, and personnel. LEAF is being built on a solid foundation in terms of its data models, core functionality, and code management, so that it is positioned for extension and long-term sustainability. The platform is based on the Islandora 8 framework, which combines Drupal 8 with a Fedora 5 repository for long-term preservation. The LEAF repository will customize and enhance Islandora to enable digital humanities workflows and publication needs. Enhancements include an innovative web-based editing tool that allows users to employ TEI XML along with Web Annotation and IIIF standards-compatible Linked Open Data annotations that enhance discoverability and interoperability. 
            
            
                The founding LEAF institutions are collaborating to upgrade the existing CWRC environment and produce a fully modular platform that will also be hosted on Bucknell’s servers, further tested at Newcastle University, and offered as containerized open-source code freely available for download and installation by other institutions. In particular, LEAF will facilitate the production and publication of dynamic digital scholarly editions and collections, offering multilingual transcription, translation, and image markup. Entirely browser-based, its functionality includes an in-browser XML markup editor, XML rendering tools, built-in text and data visualization tools including the Voyant Tools suite and its Dynamic Table of Contexts Browser. Overall the LEAF platform will provide a sophisticated interface for digital editions in which the XML markup is leveraged for navigation and active reading, and enhanced with Linked Open Data.
            
        
    

