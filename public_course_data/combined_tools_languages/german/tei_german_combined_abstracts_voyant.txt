

      
         
            CLARIN-D: Eine Forschungsinfrastruktur für die Geistes- und Sozialwissenschaften 
            Auffinden, Auswerten, Aufbewahren: Die Forschung mit digitalen Sprach- und Textdaten stellt die Sozial- und Geisteswissenschaften vor neue, fächerübergreifende Herausforderungen:
            
               Rohdaten und Ergebnisse sollen für Nachvollziehbarkeit und weitere Analysen zentral und einfach auffindbar sein,
               für die Auswertung können computergestützte Werkzeuge vielfältig eingesetzt werden – möglichst entlang methodischer Standards, und
               Forschungsergebnisse müssen nachvollziehbar und langfristig verfügbar gemacht werden.
            
            Die
          Forschungsinfrastruktur CLARIN-D (Common Language Resources and Technology Infrastructure in Deutschland) unterstützt die Geistes- und Sozialwissenschaften dabei, ihre digitalen Ressourcen in nachhaltiger, offener und interoperabler Weise zur Verfügung zu stellen (www.clarin-d.de). Über neun CLARIN-D-Zentren mit unterschiedlichen Arbeitsschwerpunkten und einem breiten Angebot an Webservices stehen der deutschen Forschungslandschaft umfangreiche Angebote zur Forschung mit Sprachdaten zur Verfügung. Insofern bei den Angeboten die Perspektive von nicht computerlinguistisch vorgebildeten Fachwissenschaftler_innen als Zielgruppe im Vordergrund steht, sind die CLARIN-D Angebote für die Digital Humanities von besonderem Interesse, zielen sie doch darauf ab, die Arbeit mit digitalen Sprachdaten durch Vereinheitlichung von Standards und Bereitstellung von Werkzeugen und Webservices zu erleichtern.
        
            Beim Auffinden von Ressourcen geht es darum, Zugang zu Daten zu erhalten, die der wissenschaftlichen Gemeinschaft zur Verfügung gestellt wurden. Diese Daten werden zitiert und können so gefunden und in anderen Forschungskontexten und zur Reproduktion von Ergebnissen verwendet werden.
            Werkzeuge zur Analyse von Forschungsdaten sind über das Web zugänglich und können dadurch ohne zusätzlichen Aufwand verwendet werden. CLARIN-D macht Werkzeuge für die Geistes- und Sozialwissenschaften verfügbar, so dass unterschiedliche Werkzeuge für neue Forschungsfragestellungen zusammen verwendet werden können.
            Daten, die in Forschungsprojekten entstehen oder die anderen Forschern zur Verfügung gestellt werden, können mit Hilfe der CLARIN-D-Zentren langfristig aufbewahrt und somit archiviert werden. Sie erhalten dabei eine eindeutige Referenz und können ähnlich wie Bücher und Artikel zitiert werden. Außerdem werden dadurch Anforderungen von Förderungsorganisationen zur Vorhaltung von Daten und Ergebnissen über Projektlaufzeiten hinaus gewährleistet.
            Mit diesen Services ist CLARIN-D für eine Vielzahl von Fachdisziplinen, welche sich im Bereich der Digital Humanities bewegen, von großen Interesse. Um den Anforderungen und Bedürfnissen dieser Fachcommunities gerecht zu werden, haben sich (potenzielle) Nutzer_innen der Infrastruktur innerhalb von CLARIN-D in sogenannten Fach-Arbeitsgruppen (F-AGs) organisiert. In zehn F-AGs, welche von Germanistik und anderen Philologien über diverse Teilbereiche der Linguistik bis hin zu Sozial- und Geschichtswissenschaften reichen, sind mittlerweile ca. 200 Wissenschaftler_innen organisiert, welche sich über Möglichkeiten, Bedarfe und methodische Standards bei der Arbeit mit digitalen Sprachdaten austauschen. Zudem werden im Rahmen von sogenannten “Kurationsprojekten” wichtige fachwissenschaftliche Ressourcen aufbereitet und für die Forschungscommunity zugänglich gemacht.
            Im geplanten Panel sollen exemplarisch zwei Use Cases dargestellt und dokumentiert werden (Abschnitt 2). Ein weiterer Beitrag befasst sich mit den Prinzipien und Möglichkeiten der Dokumentation von fachwissenschaftlichen Nutzungsszenarien von Clarin-D-Ressourcen (Abschnitt 3).
         
         
            
               Fachwissenschaftliche Use Cases der Infrastruktur
        
            Das Panel “Fachwissenschaftliche Nutzungsszenarien der CLARIN-D Infrastruktur” stellt zwei Use Cases aus unterschiedlichen fachwissenschaftlichen Perspektiven vor, bei denen Services bezüglich der drei zentralen Leistungen Auffinden, Auswerten und Aufbewahren zur Anwendung kommen. Über den Erkenntnisgewinn der Darstellung der einzelnen Use Cases hinaus wird so im Rahmen des Panels der Nutzen der Infrastruktur für unterschiedliche Disziplinen insgesamt sichtbar gemacht. Dazu stellen die Einzelvorträge die Generalisierbarkeit ihrer Ansätze an zentralen Punkten heraus um zu verdeutlichen, wie andere Forschungsfragen mit ähnlichen Methoden und Werkzeugen bearbeitet werden können. Zudem wird im Rahmen der Diskussionen zu den einzelnen Vorträgen vor allem auf Erfahrungen und Generalisierbarkeit der Ansätze fokussiert, um so den projekt- und fächerübergreifenden Mehrwert der Nutzung von Komponenten der Infrastruktur aufzuzeigen und die Anwendung für eigene Projekte zu ermutigen. Vorgestellt werden drei Nutzungsszenarien aus dem Bereich der Politikwissenschaften, der Neueren Geschichte und der Germanistik, wobei neben den Projekten vor allem die Reflexion des Einflusses der CLARIN-D Infrastruktur auf Forschungsmöglichkeiten und Dokumentation von Forschungsabläufen im Vordergrund steht.
            
               
                  Beitrag 1: Aufbereitung und Analyse von Parlamentsprotokollen als öffentliche Sprachressource der Demokratie
          
               Im deutschsprachigen Raum besteht ein Mangel an frei verfügbaren, annotierten Korpora politischer Texte. Dadurch wird der Einstieg in die DH-Forschung massiv gehemmt. Eine mögliche Lösung bieten Plenarprotokolle als öffentliches Archiv des politischen Zeitgeschehens einer Demokratie, welche im Rahmen der CLARIN-D Infrastruktur aufbereitet und verfügbar gemacht werden. Plenarprotokolle des Bundestags, der Landtage oder auch des Europaparlaments dokumentieren über große Zeiträume das gesamte Spektrum politischer Aktivität. Dies ist zugleich, ohne eine thematische Klassifikation von Debatten, ein Nachteil: Plenarprotokolle decken, wenn nicht Subkorpora nach inhaltlichen Kriterien gebildet werden können, das politische Geschehen für viele Auswertungszwecke zu undifferenziert ab. Für eine Vielzahl sozialwissenschaftlicher Fragestellungen ist es relevant, dass themen- bzw. politikfeldspezifische Subkorpora gebildet werden können. Dafür ist eine Annotation von Debatten und deren Klassifikation erforderlich. Eine entsprechende Aufwertung der
            Ressource “Plenarprotokollkorpus” wird im Kurationsprojekt der F-AG 8 vorgenommen.
          
               Im Rahmen des Panel-Beitrags wird zuerst der Workflow dargestellt, wie auf Basis der bei Parlamenten verfügbaren Plenarprotokolle (im txt- und pdf-Format) XML-Dokumente aufbereitet werden, die TEI-Standards entsprechen. Anschließend zeigen wir, wie mit CLARIN-Tools Annotationen für die Korpusaufbereitung vorgenommen werden. Redebeiträge sollen auf Basis der manuellen Annotation automatisch in bestimmte Themenkategorien klassifiziert werden. Im Beitrag wird auch dargestellt, wie die Qualitätssicherung der (halb-)automatischen Aufbereitung durch Intercoderreliabilitäten und Qualitätskriterien maschineller Sprachverarbeitung von Seiten der klassischen Politikwissenschaft als auch von Seiten der Informatik realisiert werden kann. Versionierung der Daten und Issue Tracking werden dabei gleichermaßen realisiert. Im Ergebnis steht der sozialwissenschaftlichen Community (und selbstverständlich auch anderen Wissenschaftler_innen) ein nach Themengebieten differenziert auswertbares Korpus zur Verfügung.
            
            
               
                  Beitrag 2: CLARIN-kompatible Aufwertung OCR-erfasster Texte aus der Lehrbuchsammlung des GEIs und deren Integration in die CLARIN-D-Infrastruktur: Ein Fazit aus fachwissenschaftlicher Sicht
          
               Welchen Aufwand bringt eine Integration historischer Quellen in die CLARIN-D Infrastruktur mit sich? Welche Mehrwerte ergeben sich daraus für die historische Forschung? Diese Fragen sollte das Projekt
            „Quellen des Neuen: Realkundliches und naturwissenschaftliches Wissen für Dilettanten und Experten zwischen Aufklärung und Moderne“ der 2014 gegründeten CLARIN-D-Facharbeitsgruppe „Neuere Geschichte“ klären. Zu diesem Zweck sollten im Projekt Korpora aus verschiedenen Projektkontexten über die CLARIN-Infrastruktur miteinander verknüpft und interoperabel gemacht werden. Im Blick waren dabei (1) das digitale Schulbuchkorpus des GEI (Georg-Eckert-Institut Braunschweig; GEI), (2) das Korpus des Deutschen Textarchivs (Berlin-Brandenburgischen Akademie der Wissenschaften; BBAW) sowie (3) die gedruckten Publikationen des Universitätsgelehrten Johann Friedrich Blumenbach (1752–1840) (Akademie der Wissenschaften Mainz). Durch Verknüpfung dieser Korpora sollten Untersuchungen zum Zusammenhang und Verhältnis von schulischer Lehre mit der Wissensproduktion und -vermittlung im universitären Umfeld ermöglicht werden. Die CLARIN-Infrastruktur ermöglicht zudem den Rückgriff auf gegebene Tools für die vergleichende Analyse der gegebenen Korpora.
          
               Die Herausforderung des Projekts bestand darin, die in Qualität und Form heterogenen Daten der verschiedenen Korpora CLARIN-konform aufzubereiten und miteinander interoperabel zu machen. Von mehreren möglichen Integrationsszenarien wurde in Zusammenarbeit mit dem CLARIN-D-Servicezentrum an der BBAW eine sehr genaue und vergleichsweise aufwändige Methode realisiert, mit dem Ziel, adäquate Forschungsdaten für die Wissensgeschichte und andere historisch arbeitende Disziplinen zu erhalten. Unter Nachnutzung bzw. Anpassung bestehender Workflows der BBAW wurden exemplarisch Schulbücher verschiedener Fächer und Zeiträume dem Auszeichnungslevel der DTA-Korpora und der bereits integrierten „Blumenbach-Online“-Ressourcen angepasst. In verschiedenen ‘Stadien’ der Textaufbereitung wurde zudem die Anwendbarkeit von CLARIN-D-Tools auf Teilmengen der verfügbaren Daten getestet.
               Der Panel-Beitrag erläutert den Workflow zur CLARIN-konformen Aufbereitung der GEI-digital-Ressourcen und führt den Mehrwert dieser Arbeiten für die Forschung beispielhaft vor. 
            
            
               
                  Beitrag 3: Möglichkeiten und Prinzipien der Dokumentation von fachlichen Nutzungsszenarien von Clarin-D-Ressourcen
          
               Für die fachliche Nutzung der Ressourcen (Daten, Werkzeuge), die in einer Infrastruktur angeboten werden, ist die Frage von zentraler Bedeutung, wie man typische wissenschaftliche Anwendungsszenarien, bei denen Daten und Werkzeuge für eine spezifische fachliche Aufgabenstellung genutzt werden, so darstellen kann, dass die Dokumentationen für unterschiedliche Zielgruppen hilfreich und im besten Fall auch stimulierend sind. Dabei sind einerseits unterschiedliche Nutzertypen (z. B. Doktorand_innen, Studierende, erfahrene und weniger erfahrene Forscher_innen), andererseits die ganz unterschiedlichen fachlichen Fragestellungen in verschiedenen Disziplinen (z. B. Germanistik) und Unterdisziplinen (z. B. Syntax, Wortschatzforschung) zu berücksichtigen. Gegenstand des Panel-Beitrags sind zunächst drei Typen der Dokumentation, die jeweils von einer fachlichen Fragestellung ausgeht, sodann die Ermittlung von Daten und Werkzeugen umfasst, die zielorientierte Anwendung von Daten und Werkzeugen beschreibt und mit dem Hinweis auf ein fachliches Resultat endet, das auf die ursprüngliche fachliche Fragestellung rückzubeziehen ist. Die drei Typen sind:
               
                  gedruckte und bebilderte Anleitungen;
                  Screencasts, bei denen eine fachliche Anwendung digitaler Ressourcen von einer Stimme aus dem Off kommentiert wird;
                  Experten-Interviews, die vor Ort oder als virtuelles Video-Meeting aufgezeichnet werden und dann als Filme / Podcasts dauerhaft zugänglich sind. Hier vertritt ein/e Interviewer/in die Nutzerinteressen, eine Expertin oder ein Experte stellt die Ressourcen und ihre Anwendung dar.
               
               Der Beitrag wird zunächst die Prototypen vorstellen und dann allgemeine Überlegungen zu Zielen, Verfahrensweisen, Prinzipien (z. B. Usability, Zielgruppenorientierung), Darstellungsformen und zum Repertoire von Darstellungsmitteln anschließen.
            
         
         
            
               Fächerübergreifende Perspektiven
        
            Wir versprechen uns durch die multiperspektivische Reflexion der hier vorgestellten Nutzungsszenarien nicht nur eine Verbesserung des Verständnisses für den Nutzen einer Forschungsinfrastruktur, sondern hegen insbesondere die Hoffnung, einen wichtigen Beitrag zur Methodendiskussion in den Digital Humanities zu liefern. In der gemeinsamen Nutzung und Weiterentwicklung von Technologien zum Auffinden, Auswerten und Aufbewahren digitaler Sprachdaten liegt der Schlüssel zur Etablierung von Standards, zum Herabsetzen von Zugangsschwellen und damit letztlich zur Ermöglichung eines breiten Austausches in diesem noch vergleichsweise jungen Forschungsfeld.
         
         
            
               Ablauf
        
            Das Panel wird moderiert von Gregor Wiedemann, Koordinator der Fach-AGs im CLARIN-D Projekt. Der inhaltliche Ablauf wird wie folgt gestaltet:
            
               Einführung zur
            CLARIN-D Infrastruktur: Gregor Wiedemann (Universität Leipzig, Koordinator für die CLARIN-D Fach-AGs), Vortragszeit: 5 Minuten
          
               Beitrag 1, Vortragender:
            Prof. Dr. Andreas Blätte (Universität Duisburg/Essen, Lehrstuhl für Public Policy und Landeskunde), Vortragszeit: 10 Minuten
          
               Beitrag 2, Vortragende:
            Dr. Maret Keller (Georg-Eckert-Institut – Leibniz-Institut für internationale Schulbuchforschung), Susanne Haaf (Berlin-Brandenburgische Akademie der Wissenschaften), Kay-Michael Würzner (Berlin-Brandenburgische Akademie der Wissenschaften), Vortragszeit: 10 Minuten
          
               Beitrag 3, Vortragender:
            Prof. Dr. Thomas Gloning (Universität Gießen, Professur für Germanistische Sprachwissenschaft), Vortragszeit: 10 Minuten
          
               Panel-Diskussion zu fächerübergreifenden Problemen bei der Arbeit mit digitalen Sprachdaten und Lösungsansätzen im Rahmen der CLARIN-D Infrastruktur, Diskussionszeit: 25 Minuten
               Publikumsdiskussion Möglichkeiten, Perspektiven aber auch (Einstiegs-)Hürden bei der Anwendung virtueller Forschungsinfrastrukturen. Neben Verständnisfragen sollen vor allem Bedarfe seitens der (potenziellen) Anwender_innen und Möglichkeiten zur Generalisierung der präsentierte Forschungsabläufe diskutiert werden. Diskussionszeit: 30 Minuten
            
         
      
   



      
         
            Stand der Dinge
            Digitale Editionen machen in den Digital Humanities das „Brot- und
          Buttergeschäft“ aus. Doch während sich der methodisch-theoretische Hintergrund
          digitaler Editionen zusehends konsolidiert und sich diese neue Form der
          Publikation von Forschungsergebnissen im (fach)wissenschaftlichen Diskurs
          bereits etabliert hat, fehlt es nach wie vor an umfassend dokumentierten und
          selbstkritisch reflektierten Best-Practice-Beispielen von Frameworks und
          Workflows zur Erstellung und / oder Publikation von digitalen Editionen, welche
          als Blaupausen für künftige digitale Editionsprojekte herangezogen werden
          können. Das Resultat ist so bekannt wie unerfreulich und kann – nur geringfügig
          überspitzt – auf folgende Formel gebracht werden: So gut wie jedes Projekt
          erfindet das Rad – das technische Grundgerüst der Edition – wieder neu.
            Die wichtigsten Gründe für diese Entwicklung lassen sich rasch benennen:
            
               Digitale Editionen sind nach wie vor eine sehr junge Publikationsform und darüber hinaus abhängig von der raschen Weiterentwicklung gegenwärtiger Webtechnologien und -standards.
               Digitale Editionen erfordern technische Kompetenzen, welche jene traditioneller Geisteswissenschaftler_innen meist übersteigen, weshalb Kooperationen mit Entwickler_innen notwendig sind.
               Digitale Edition werden häufig in Form von Einzelprojekten realisiert, weshalb nicht auf bestehende Lösungsansätze anderer Institutionen zurückgegriffen wird. 
               In so gut wie allen Fällen fehlen die Ressourcen, manchmal wohl leider auch der Wille, die Eigenentwicklung, das geschaffene technische Grundgerüst hinreichend gut zu dokumentieren und in einer Form zu veröffentlichen, so dass andere den Quellcode, Workflows, Stylesheets etc. nachnutzen zu können.
               Digitale Editionen sind – wenigstens in der Selbstwahrnehmung – häufig hochgradig speziell und unterscheiden sich in Inhalt, Form und Funktion von allen bereits bestehenden Editionsprojekten, womit eine völlige Neuentwicklung des technischen Grundgerüstes gerechtfertigt wird. 
            
         
         
            Ablauf:
            Im Rahmen des Panels sollen einige der aktivsten Institutionen aus dem Bereich der digitalen Editionen an einen Tisch gebracht werden. Diese erhalten im Vorfeld der Tagung einen Fragebogen zur Vorbereitung einer kurzen (pro Teilnehmer ca. fünf Minuten) Vorstellung ihrer Systeme, wobei darin der Fokus auf dem Thema Reusability der in den Projekten verwendeten Technologien und Workflows liegen sollte. Konkret sollen die Teilnehmer_innen des Panels auf folgende Punkte eingehen:
            
               Kurze Vorstellung der eigenen Frameworks und Workflows, vor allem hinsichtlich einer Einschätzung über die Stärken und Schwächen der eigenen Lösungsansätze, aus welcher Tradition/Disziplin (z. B. Philologie, Geschichtswissenschaften) sie kommen und welche konkreten Projekte damit realisiert wurden. 
               Gibt es ein weitgehend standardisiertes Prozedere im Falle von Kooperations- bzw. Nachnutzungsanfragen (inklusive der dafür notwendigen Ressourcen)?
               Wer soll die Angebote nutzen; gibt es fachlich, institutionell, qualitativ, budgetär, regional, national, zeitlich oder anderweitig konstituierte Zielgruppen?
            
            Ein Ziel dieser Vorstellungsrunde soll es sein, potenziell interessierten Nutzer_innen im Auditorium einen kompakten Überblick über bestehende Angebote zur Erstellung und / oder Veröffentlichung von digitalen Editionen zu vermitteln.
         
         
            Podiumsdiskussion (ca. 30 Minuten)
            Im Anschluss an diese Kurzvorstellung erfolgt eine moderierte Podiumsdiskussion, worin folgende Punkte weiter thematisiert werden:
            
               Dokumentation von Technologie und Workflows: Wie gut sind die technischen Aspekte dokumentiert? Ist es für Dritte möglich, anhand dieser Dokumente ähnliche Projekte zu realisieren? Welche technische Infrastruktur ist dafür notwendig?
               Veröffentlichung von Code: Ist der für das System geschriebene oder adaptierte Code für andere nachnutzbar veröffentlicht (z. B. auf GitHub)?
               Wird das entwickelte Framework auch als Service angeboten?
               Ressourcen und Organisation der Entwicklung: Wie groß war / ist der Aufwand der Entwicklung des technischen Grundgerüstes, eventuell in Personenmonaten. Woher stammt das verwendete Know-How (Eigenentwicklung oder Adaption bestehender Konzepte)?
               Wie generisch ist das verwendete technische Framework bzw. wie aufwändig sind die Änderungen, die bei der Adaption an ein neues Projekt notwendig werden? Sprich: Wie leicht und wie weit kann das System an projektspezifische Bedürfnisse angepasst werden (unterstützte Datenformate, Funktionalitäten)? Welche Kompetenzen sind notwendig, um Anpassungen auf unterschiedlichen Ebenen (Anzeige, Projektstruktur usw.) vorzunehmen?
               Wie groß ist der laufende Aufwand für Wartung einzelner Projekte bzw. des Frameworks an sich?
            
            Es sollen gemeinsame Problemfelder identifiziert und reflektiert werden. Auf dieser Basis kann dann über mögliche (gemeinsame) Lösungen diskutiert werden. 
         
         
            Publikumsdiskussion (ca. 30 Minuten)
            Im letzten Drittel des Panels wird die Diskussion zum Publikum hin geöffnet werden. Dabei sollen vor allem potentielle Nutzer_innen die Möglichkeit bekommen, gezielt konkrete und ggf. eigene Projekte betreffend Fragen zu stellen und direkt mit möglicherweise zukünftigen Projektpartnern ins Gespräch zu kommen.
         
         
            Teilnehmer
            Bei der Auswahl der Teilnehmer wurde einerseits darauf geachtet, vornehmlich etablierte Institutionen anzusprechen, die sich als Dienstleister im Bereich digitaler Editionen profiliert haben, deshalb an möglichst generischen Lösungen zur Erstellung und Publikationen von digitalen Editionen interessiert sind und dafür selbst Frameworks und Workflows entwickelt haben. Außerdem wurde versucht, bei der Auswahl der Teilnehmer möglichst den gesamten deutschsprachigen Raum abzudecken.
            
               ACDH-ÖAW
               Matej Durco und Peter Andorfer
               Das ACDH verwendet ein eXistdb-basiertes Framework zur Veröffentlichung digitaler Editionen names cr-xq-mets. cr-xq-mets basiert lose auf SADE. Die Idee von cr-xq-mets ist die konsequente Trennung von Code und einzelnem Projekt, mit dem Ziel einen hohen Grad an generischer Projektentwicklung bei gleichzeitig geringem Aufwand an Projektmaintainance zu erreichen. 
               Das ACDH übernimmt auch die Organisation und Moderation des Panels. 
            
            
               Herzog August Bibliothek
               Thomas Stäcker
               Die Herzog August Bibliothek (HAB) hat für ihre digitalen Editionsprojekte
              die Reihe Editiones Electronicae Guelferbytanae gegründet  ), in der bisher 20 Werke erschienen sind. Hinzu kommen zahlreiche
              kleinere und umfangreichere Editionen, die außerhalb dieser Reihe erschienen
              sind. Hervorzuheben sind die beiden im Langzeitförderprogramm der DFG (je 12
              Jahre) erscheinenden Editionen der Kritische Gesamtausgabe der
              Schriften und Briefe Andreas Bodensteins von Karlstadt und die
              Digitale Edition und Kommentierung der Tagebücher des Fürsten Christian II.
                von Anhalt-Bernburg (1599-1656). Die Kodierung
                erfolgt in TEI (nach den übergreifenden Festlegungen der HAB). Die Anzeige
                der Webdarstellung und Suchfunktionaltäten basieren auf PHP sowie eXistdb. 
            
            
               TextGrid
               Sibylle Söring
               
                  TextGrid ist eine Virtuelle
                Forschungsumgebung für die text- und quellenbasierten Geisteswissenschaften,
                die u. a. die Erstellung digitaler Editionen mithilfe Open Source-basierter
                Tools und Dienste unterstützt. Neben der Software, dem TextGrid Laboratory,
                bietet TextGrid mit dem TextGrid Repository die Möglichkeit, vielfältige
                Forschungsdaten - u. a. XML / TEI-kodierte Texte, Bilder und Datenbanken -
                langfristig zu speichern sowie nach internationalen Standardformaten
                zitierbar zu publizieren und zur Nachnutzung zur Verfügung zu stellen, wie
                etwa zur Recherche und Visualisierung. Mit TextGrid wurden und werden
                verschiedene Editionsvorhaben umgesetzt, so u. a. die Digitale
                genetisch-kritische Edition von Theodor Fontanes Notizbüchern
                und die
                Bibliothek der Neologie. 
            
            
               BBAW – DTA, CLARIN-D
               Christian Thomas
               Das DFG-geförderte Projekt Deutsches Textarchiv der Berlin-Brandenburgischen Akademie
                der Wissenschaften (BBAW) erstellt bzw. publiziert digitale Editionen von
                derzeit (September 2015) 1665 Werken im DTA-Kernkorpus unter Verwendung
                eines selbst entwickelten, auf Open-Source-Software basierenden Frameworks.
                Zu diesem Framework gehört mit dem Modul DTAE (DTA-Erweiterungen)
                ein elaborierter Workflow zur Integration
                hochwertiger Textressourcen aus externen Editions- und Forschungsprojekten.
                Über DTAE konnten als Ergänzungen des DTA-Kernkorpus bislang weitere 1097
                Einzelwerke sowie der gesamte Bestand zweier herausragender Zeitschriften
                des 19./20. Jahrhunderts, das von J. G. Dingler begründete Polytechnische
                Journal (346 Bände, 1820–1931) und die Zeitschrift Die
                Grenzboten (270 Bände, 1841–1922) in das DTA integriert werden. Der
                Textbestand aus DTA und DTAE umfasst ca. 200 Mio. Tokens und ca. 1,2 Mrd.
                Zeichen. Die DTA-Korpora sind einheitlich gemäß dem TEI-basierten und
                ausführlich dokumentierten DTA-Basisformat (DTABf) kodiert und werden
                mit Hilfe computerlinguistischer Werkzeuge automatisch annotiert, was unter
                anderem spezifizierte Suchanfragen nach bestimmten Metadatenfeldern,
                Wortarten, grammatischen Kategorien, X-Pfaden etc. ermöglicht. Zudem wird
                der historische Textbestand automatisch in Richtung moderner Orthographie
                ‘normalisiert’, was schreibweisentolerante Suchen über das gesamte Korpus
                ermöglicht (siehe allgemein zur Suche im DTA www.deutschestextarchiv.de/doku/DDC-suche_hilfe). Die
                Qualitätssicherung sämtlicher Korpusressourcen geschieht kollaborativ in der
                webbasierten Umgebung DTAQ, in der derzeit 866
                registrierte Nutzer mögliche Fehler auf der Text-, Annotations- und
                Metadatenebene melden und, je nach Rechtestatus, auch direkt online
                korrigieren können. 
            
            
               Trier Center for Digital Humanities
               Thomas Burch, Vera Hildenbrandt
               Das TCDH kann inzwischen auf eine mehr als langjährige Erfahrung in der
                Planung, Durchführung und Betreuung von Projekten im Bereich der Digital
                Humanities verweisen. Neben dem Schwerpunkt im Bereich der Erstellung und
                Publikation digitaler Editionen verfügt das Zentrum über eine ausgewiesene
                Expertise in der Entwicklung von Software-Umgebungen für
                geisteswissenschaftliche Großvorhaben. In mehreren von der DFG, dem BMBF
                sowie im Rahmen des Akademienprogramms geförderten Projekten entstanden und
                entstehen am TCDH digitale Editionen wie z. B. das
                Heinrich-Heine-Portal, das
                Christian-Dietrich-Grabbe-Portal, das Cusanus-Portal, die
                  elektronische Publikation der Korrespondenz August-Wilhelm Schlegels sowie die digitale Rekonstruktion
                    der Textgenese und Entstehungsgeschichte von Wolfgang Koeppens ‚Jugend'. Im
                    Bereich der Softwareentwicklung konzipiert, betreut und entwickelt das Team
                    des TCDH virtuelle Arbeitsumgebungen für Projekte mit hohen Anforderungen an
                    Workflow und grafische Benutzerschnittstellen. Hervorgehoben seien hier
                    Systeme wie das internetbasierte Artikelredaktionssystem für die Produktion
                    und Publikation von Wörterbüchern in dezentralen Arbeitsstellen, das
                    gemeinsam mit dem Forschungszentrum Europa und dem Sonderforschungsbereich
                    600 entwickelte "Forschungsnetzwerk und Datenbanksystem" (FuD 2015), die
                    Redaktions- und Publikationsplattform zur Europäischen Geschichte Online
                    oder das
                    interaktive Werkzeug “Transcribo” zur Erstellung von Transkriptionen. 
            
            
               ZIM – ACDH Graz
               Hubert Stigler
               Das ZIM hat im Rahmen einer Vielzahl von Editionsprojekten
                      forschungsgetrieben ein objektorientiertes Framework auf Basis
                      von FEDORA Commons und weiteren Open-Source-Projekten (Apache Cocoon,
                      Blazegraph u. a.) entwickelt, das Aspekte der Publikation von Digitalen
                      Editionen mit jenen der Langzeitarchivierung von Forschungsdaten zu
                      verbinden sucht. Als österreichischer Beitrag zu DARIAH steht es einer
                      breiten Öffentlichkeit zur Nachnutzung zur Verfügung.
            
            
               DH Lab Basel 
               Lukas Rosenthaler
               Während die anderen Teilnehmer vor allem an Lösungen für XML / TEI-basierte
                        digitale Editionen arbeiten, legt SALSAH (System for Annotation
                        and Linkage of Sources in Arts and Humanities) den Schwerpunkt (zurzeit
                        noch) auf die Verknüpfung und Verlinkung von vornehmlich digitalen
                        Faksimiles. 
            
         
      
   



      
         
            Einleitung und Projekthintergrund
            Die Kommunikation im Internet bzw. mit sozialen Medien hat in den vergangenen zwei Jahrzehnten in den geisteswissenschaftlichen Disziplinen eine zunehmende Aufmerksamkeit erfahren. Zahlreiche sprach-, sozial- und medienwissenschaftliche Analysen haben die sprachlichen und interaktionalen Besonderheiten bei der Kommunikation in Chats, Foren, Weblogs und sozialen Netzwerken, per SMS und WhatsApp als einen neuen Gegenstand geisteswissenschaftlicher Forschung erschlossen. Durch ihre digitale Verfügbarkeit sind Sprachdaten aus solchen Genres – im Gegensatz etwa zu Aufzeichnungen von Gesprächen – einfach zu gewinnen und für Forschungszwecke speicherbar. Trotzdem gibt es bislang wenige Korpora zur Sprachverwendung in sozialen Medien, die für Analysezwecke im Bereich der Digital Humanities aufbereitet sind und die der Scientific Community zur Nutzung zur Verfügung stehen. Das hat zum einen mit unklaren rechtlichen Rahmenbedingungen in Bezug auf die Nutzung und Bereitstellung digitaler Kommunikationsdaten für Forschungszwecke zu tun, zum anderen mit dem Fehlen geeigneter Standards für die Strukturbeschreibung und linguistische Annotation von Social-Media-Genres sowie der Notwendigkeit, automatische Annotationswerkszeuge für Daten dieses Typs anzupassen.
            In unserem Beitrag präsentieren wir Ergebnisse aus dem Projekt „ChatCorpus2CLARIN“, das als Kurationsprojekt der fachspezifischen Arbeitsgruppe F-AG 1 „Deutsche Philologie“1. von Mai 2015 bis Februar 2016 vom BMBF gefördert wird. Ziel des Projekts ist es, das
          Dortmunder Chat-Korpus, ein existierendes Korpus zur Sprachverwendung und Sprachvariation in der deutschsprachigen Chat-Kommunikation, in die Korpus-Infrastrukturen der CLARIN-D-Zentren an der Berlin-Brandenburgischen Akademie der Wissenschaften (BBAW) und am Institut für Deutsche Sprache (IDS) Mannheim zu integrieren. Dabei geht es insbesondere um die Herstellung einer Interoperabilität der Zielressource mit Korpora zur gesprochenen und geschriebenen Sprache (DWDS-Korpora, DeReKo, FOLK), die an der BBAW und am IDS bereits vorhanden sind. Die Bereitstellung des Chat-Korpus in CLARIN-D soll einen systematischen, korpusgestützten Vergleich der Sprachverwendung in Chats mit der Sprachverwendung in mündlichen Gesprächen und in redigierten Texten erlauben und der empirischen, sprachdatengestützten Forschung zur Sprache und Interaktion in sozialen Medien somit neue Möglichkeiten eröffnen.
        
            Um Interoperabilität mit existierenden CLARIN-D-Ressourcen herzustellen und es
          Forscher_innen zu ermöglichen, die unterschiedlichen Ressourcen im
          Forschungsprozess vernetzt zu nutzen, wird das Chat-Korpus bei der Integration
          unter Rückgriff auf Standards im Bereich der Digital Humanities remodelliert und
          um zusätzliche linguistische Annotationen erweitert. Der Beitrag beschreibt die
          Modellierung der Ressource und ihre Integration in CLARIN-D und zeigt, welche
          Mehrwerte sich für Nutzer des Korpus durch die Integration und die zusätzlichen
          Annotationen ergeben.
         
         
            Die Ausgangsressource
            Das
            Dortmunder Chat-Korpus (Beißwenger 2013) ist eine Sammlung von Chat-Mitschnitten aus vier verschiedenen Handlungsbereichen (Freizeit, Bildung, Beratung, Medien), die ca. 140.000 Chatter-Beiträge und 1,06 Mio. Token umfasst und die 2002–2008 am Lehrstuhl für Linguistik der deutschen Sprache und Sprachdidaktik der TU Dortmund aufgebaut wurde. Die Daten sind in einem XML-Format repräsentiert, das zentrale Strukturelemente von protokollieren Chatverläufen (sog. ‚Logfiles‘) abbildet, unterschiedliche Typen von Chat-Beiträgen unterscheidet und ausgewählte Stilelemente internetbasierter Kommunikation erfasst. Teile des Korpus werden seit 2005 über die Website
            http://www.chatkorpus.tu-dortmund.de zusammen mit einem einfachen, Java-basierten Abfragewerkzeug zur Verfügung gestellt. Das Korpus wird in diversen linguistischen und computerlinguistischen Projekten sowie im Bildungskontext (Schule und Hochschule) als Ressource in Forschung und Lehre genutzt.
          
         
         
            Interoperabilität durch Anschluss an Standards im Bereich der Digital Humanities
            
               Strukturmodellierung und Repräsentation in TEI
               Für die Repräsentation der im Korpus dokumentierten Chat-Verläufe greifen wir auf die Formate der
              Text Encoding Initiative (
              TEI) zurück. In den TEI-Guidelines (TEI-P5) gibt es bislang keine Modelle für die Darstellung von Social-Media-Genres, dafür umfangreiche Module für die Strukturrepräsentation von Textgenres und von transkribierten Gesprächen. Die in den Guidelines vorgesehene Möglichkeit der
              customization macht das Encoding-Framework aber flexibel genug, um es an die Erfordernisse auch von (neuen) Genres anzupassen.
            
               Seit 2013 beschäftigt sich in der TEI eine Special Interest Group (SIG)
              „Computer-mediated communication“ 2 mit der Entwicklung eines Standards für die Modellierung von
              Social-Media-Genres (Beißwenger et al. 2012; Chanier et al. 2014; Margareta
              / Lüngen 2014). Das Projekt greift den aktuellen Stand der in der SIG
              diskutierten Schemaentwürfe auf, testet diese an den Daten des Chat-Korpus
              sowie an Ausschnitten ausgewählter weiterer Social-Media-Genres
              (Wikipedia-Diskussionsseiten, WhatsApp-Dialoge, News-Diskussionen, Tweets)
              und entwickelt sie weiter. Das dabei entstehende TEI-Schema wird in Form
              eines ODD3dokumentiert und bildet die Grundlage für die TEI-Modellierung des
              kompletten Korpus. Zugleich wird das ODD, dessen Fertigstellung für Herbst
              2015 vorgesehen ist, in die weitere Arbeit der SIG eingespielt. 
            
            
               Linguistische Basisannotation mit „STTS 2.0“
               Um die Recherchemöglichkeiten im Korpus zu verbessern, wird der
                Ausgangsressource eine zusätzliche Annotationsebene hinzugefügt, deren Kern
                Part-of-speech-Informationen (PoS) bilden. Das im Projekt verwendete
                PoS-Tagset („STTS 2.0“, Beißwenger et al. 2015) verwendet die Kategorien des
                Stuttgart-Tübingen Tagset (STTS, Schiller et al.
                1999) und erweitert diese einerseits um Tags für typische Einheiten bei der
                schriftlichen Sprachverwendung in Social-Media-Genres (u. a. Emoticons,
                Hashtags, Adressierungen) sowie um Einheiten für die Darstellung von
                Phänomenen, die typisch sind für Kontexte informeller, dialogischer
                Kommunikation (u. a. Abtönungs- und Intensitätspartikeln, Diskursmarker).
                Die Erweiterungen sind abgestimmt auf Erweiterungen, die am IDS für die
                PoS-Annotation des FOLK-Korpus zur gesprochenen Sprache zum Einsatz kommen. 
               Um die Annotationen nach STTS 2.0 zu erzeugen, wurde das komplette
                  Chat-Korpus 2015 mit einem POS-Tagger annotiert, für den im BMBF-Projekt
                  “Analyse und Instrumentarien zur Beobachtung des Schreibgebrauchs im
                  Deutschen” (IDS 2014-2016) neue Taggermodelle speziell für den Umgang mit
                  Social-Media-Genres entwickelt wurden (Horbach et al. 2014). Um das Ergebnis
                  der automatischen Annotation manuell nachzukorrigieren und zusätzlich
                  einzelnen Tokens normalisierte Formen zuzuordnen, wurde das Werkzeug OrthoNormal (Schmidt 2012) auf die Bearbeitung von
                  Chat-Daten angepasst. 
            
         
         
            Zielressource und Mehrwerte
            Die Integration in die Infrastrukturen der beteiligten CLARIN-D-Zentren umfasst die Archivierung in den Repositorien an der BBAW und am IDS, die Aufnahme der Metadaten in das Virtual Language Observatory (VLO), die Einbindung der Daten in die korpusübergreifende Suchmaschine
                CLARIN Federated Content Search sowie die Bereitstellung über Webservices.
              
            Die rechtlichen Bedingungen der Bereitstellung werden über ein Rechtsgutachten
                geklärt. Je nach Ergebnis kommen für die Ressource unterschiedliche
                Lizenzmodelle in Frage: Als Idealfall wird eine CLARIN-Endnutzer-Lizenz vom Typ
                PUB („publicly available“, Oksanen et al. 2010) angestrebt, gegebenenfalls aber
                auch der Lizenztyp ACA-NC (akademische, nicht-kommerzielle Nutzung zum
                vollständigen Kopieren / Download freigegebener Ressourcen) oder, falls
                erforderlich, eine Beschränkung auf eine Nutzung über eine
                Korpusrecherchesoftware durch bei CLARIN registrierte Nutzer (Lizenztyp QAO-NC,
                gemäß Vorschlag in Kupietz / Lüngen 2014).
            Nach der Integration wird die Zielressource für Nutzer im Bereich der Digital Humanities gegenüber der Ausgangsressource die folgenden Mehrwerte aufweisen:
            
               
                  Erweiterung der Möglichkeiten des Zugriffs und der Durchsuchbarkeit der Ressource.
                  
               
                  Interoperabilität auf der Ebene der Dokumentstruktur (TEI): Durch die Remodellierung in einem TEI-Format wird die Ressource interoperabel mit anderen in TEI repräsentierten Sprachressourcen und Annotations- bzw. Analysewerkzeugen.
                  
               
                  Linguistische Annotation: Die Anreicherung um zusätzliche linguistische Basisannotationen wird die Möglichkeiten zur Nutzung der Ressource für die korpusgestützte Sprachanalyse erweitern und anspruchsvollere linguistische Suchanfragen ermöglichen.
                  
               
                  Interoperabilität auf der Ebene der linguistischen Annotation (STTS): Durch die Kompatibilität der Part-of-speech-Annotationen mit STTS wird die Ressource interoperabel mit anderen nach STTS annotieren Sprachressourcen.
                  
               
                  Vernetzung mit Korpusressourcen anderen Typs: Durch die Integration in CLARIN-D und die genannten Interoperabilitätsmerkmale werden die Möglichkeiten zu einem korpusgestützten Vergleich sprachlicher Besonderheiten im Chat-Korpus mit Korpora gesprochener Sprache und Korpora redigierter Schriftlichkeit verbessert.
                  
               
                  Verbesserte Auffindbarkeit der Ressource durch die Bereitstellung standardisierter Metadaten und die Aufnahme in das VLO.
                  
            
            Die Ergebnisse aus dem Projekt können zum gegenwärtigen Zeitpunkt z. T. nur
                  perspektivisch formuliert werden. Zum Termin der Konferenz werden die
                  Projektarbeiten abgeschlossen sein und die Ergebnisse vorliegen.
         
      
      
         
            Für weitere Informationen siehe http://www.clarin-d.de/de/wissenschaftsbereiche/germanistik
            
            Sie hierzu die Webseite der TEI unter http://www.tei-c.org/Activities/SIG/CMC/.
                
            Siehe http://www.tei-c.org/Guidelines/Customization/odds.xml.
                
         
         
            
               Bibliographie
               
                  Beißwenger, Michael (2013): "Das Dortmunder
                      Chat-Korpus", in: Zeitschrift für germanistische
                      Linguistik 41, 1: 161-164. Erweiterte Fassung online: http://tinyurl.com/chatkorpus [letzter Zugriff 18. September
                      2015]. Beißwenger, Michael / Ermakova, Maria / Geyken,
                      Alexander / Lemnitzer, Lothar / Storrer, Angelika (2012): "A TEI
                      Schema for the Representation of Computer-mediated Communication", in: Journal of the Text Encoding Initiative (jTEI) 3.
                      http://jtei.revues.org/476 [letzter Zugriff 18. September
                        2015].
               
                  Beißwenger, Michael / Bartz, Thomas / Storrer, Angelika /
                          Westpfahl, Swantje (2015): Tagset und Richtlinie
                          für das PoS-Tagging von Sprachdaten aus Genres internetbasierter
                          Kommunikation. https://sites.google.com/site/empirist2015/home/annotation-guidelines
                          [letzter Zugriff 18. September 2015].
               
                  Chanier, Thierry / Poudat, Celine / Sagot, Benoit /
                              Antoniadis, Georges / Wigham, Ciara / Hriba, Linda / Longhi, Julien /
                              Seddah, Djamé (2014): "The CoMeRe corpus for French: structuring
                              and annotating heterogeneous CMC genres", in: Journal of
                              Language Technology and Computational Linguistics 2: 1-30. http://www.jlcl.org/2014_Heft2/1Chanier-et-al.pdf [letzter
                              Zugriff 18. September 2015].
               
                  Horbach, Andrea / Steffen, Diana / Thater, Stefan / Pinkal,
                                  Manfred (2014): "Improving the Performance of Standard
                                  Part-of-Speech Taggers for Computer-Mediated Communication", in: Proceedings of KONVENS 2014 171-177. 
               
                  IDS = Institut für Deutsche Sprache (2014-2016): Projekt Schreibgebrauch. Analyse und Instrumentarien
                                  zur Beobachtung des Schreibgebrauchs im Deutschen http://www.schreibgebrauch.de/index.html.
               
                  Kupietz, Marc / Lüngen, Harald (2014): "Recent
                                    developments in DeReKo", in: Calzolari, Nicoletta / Choukri, Khalid /
                                    Declerck, Thierry / Loftsson, Hrafn / Maegaard, Bente / Mariani, Joseph /
                                    Odijk, Jan / Piperidis, Stelios (eds): Proceedings of the
                                    Ninth International Conference on Language Resources and Evaluation
                                    (LREC 2014), Reykjavik, Iceland. 
               
                  Margaretha, Eliza / Lüngen, Harald (2014): "Building
                                      Linguistic Corpora from Wikipedia Articles and Discussions", in: Journal of Language Technology and Computational
                                      Linguistics 2: 59-82. http://www. jlcl.org/2014_Heft2/3MargarethaLuengen.pdf [letzter
                                      Zugriff 18. September 2015]. 
               
                  Oksanen, Ville / Lindén, Krister / Westerlund, Hanna
                                        (2010): "Laundry Symbols and License Management: Practical Considerations
                                        for the Distribution of LRs based on experiences from CLARIN", in: Proceedings of LREC 2010: Workshop on Language Resources:
                                        From Storyboard to Sustainability and LR Lifecycle Management,
                                        Malta. 
               
                  Schmidt, Thomas (2012): "EXMARaLDA and the FOLK tools –
                                          two toolsets for transcribing and annotating spoken language", in: Proceedings of LREC2012
                  http://www.lrec-conf.org/proceedings/lrec2012/pdf/529_Paper.pdf
                                            [letzter Zugriff 18. September 2015]. 
               
                  Schiller, Anne / Teufel, Simone / Stöckert, Christine
                                            (1999): Guidelines für das Tagging deutscher Textcorpora
                                            mit STTS (Kleines und großes Tagset). Universität Stuttgart:
                                            Institut für maschinelle Sprachverarbeitung. 
               
                  TEI Consortium (eds.) (2007): TEI P5:
                                              Guidelines for Electronic Text Encoding and Interchange
                  http://www.tei-c.org/Guidelines/P5/ [letzter Zugriff 18.
                                                September 2015]. 
            
         
      
   



      
         In dieser Posterpräsentation soll das am Exzellenzcluster Asien und Europa der Universität Heidelberg angesiedelten Projekt "Standardisierte Arbeitsabläufe zur Retrodigitalisierung am Fallbeispiel der Grabungsdokumentation Kastell Heidelberg-Neuenheim" (RetroDig) vorgestellt werden. Im Rahmen dieses einjährigen (04/2015 - 03/2016) Disziplin übergreifenden Forschungsprojekts entwickelt die JRG Digital Humanites and Digital Cultural Heritage zusammen mit der Heidelberg Research Architecture (HRA) eine Reihe von strikt generischen Komponenten für den Einsatz in einem interdisziplinären Forschungsverbund.
         Außeruniversitärer Partner bei diesem Vorhaben ist das Kurpfälzische Museum der Stadt Heidelberg, das eine bisher unpublizierte archäologische Grabungsdokumentation aus den 1970er für einen exemplarischen Retrodigitalisierungs-Workflow bereitstellt.
         Im Fallbeispiel werden sämtliche analogen Artefakte der vom Verfall bedrohten Dokumentation (handgezeichnete Pläne, handschriftliches Grabungstagebuch und Papierabzüge von Fotos) von Mitarbeitern der JRG Digital Humanities digitalisiert, erschlossen und annotiert. Dazu wird am Exzellenzcluster vorhandene digitale Infrastruktur verwendet und wo nötig erweitert.
         Das Kernstück der digitalen Forschungsumgebung des Clusters ist das
        Metadaten-Ökosystem Tamboti1. Es handelt sich dabei um ein auf eXist DB basierendes Open Source System,
      dass von der HRA zusammen mit ihren Partnern bereits seit mehreren Jahren entwickelt
      und schrittweise ausgebaut wird. Größere Komponenten für Tamboti werden
      grundsätzlich an Forschungsfragen orientiert im Rahmen von kleinen thematischen
      Fallbeispielen entworfen und implementiert. Nach Projektabschluss werden die
      entwickelten Softwarekomponenten von der HRA in den Regelbetrieb in Forschung und
      Lehre überführt. 
         Im hier vorgestellten Projekt RetroDig sollen darüber hinaus auch Einsatzmöglichkeiten im Museumsbereich evaluiert werden. Die generierten Datensätze und Softwarekomponenten werden die Grundlage für eine inhaltliche Aufbereitung der RetroDig Ergebnisse in einem zukünftigen digitalen Editionsprojekt der JRG Digital Humanities and Digital Cultural Heritage sein.
         Im derzeit laufenden Prokjekt wurden bereits die analogen Dokumente im Medialab der
        HRA digitalisiert. Im ersten Erschließungsschritt werden die materiellen
        Gesichtspunkte der Artefakte betrachtet und dabei die Objektmetadaten im VRA Core 4
        Standard aufgenommen. Dazu wird der formbasierter Zizphus VRA Editor2  verwendet, der die VRA-XML Datensätze direkt in einer Kollektion Tamboti
      speichert, wo die Daten bereits durchsucht, mit anderen Nutzern geteilt oder für
      Präsentationszwecke im integrierten Atomic-Wiki aufbereitet werden können. 
         Mitunter aus arbeitsökonomischen Gesichtspunkten werden beim Erstellen der Metadaten auch Beschriftungen transkribiert und vorerst im VRA  aufgenommen. Erst einmal ohne dabei qualitative Aussagen über deren Inhalt zu treffen.
         Dieses Vorgehen erlaubt es der HRA parallel zur Erfassung der Daten an der
        Entwicklung einer mehrteiligen Annotationskomponente zu arbeiten. Diese besteht 1.)
        aus einem DOM-nahen SVG Editor, der auf OpenSeadragon aufsetzt, 2.) einem
        semantischen Verlinkungsmechanismus (zum Zeitpunkt der Einreichung des Posters
        werden mehrere Möglichkeiten der Informationsmodellierung und Umsetzung diskutiert
        und ausprobiert. Bei der finalen Posterpräsentation wird sowohl auf die Diskussion,
        als auch auf die Implementierung Bezug genommen werden) und 3.) der Integration von
        TEIAN3,
        einem Editor, der es erlaubt beliebige Subsets von XML Vokabularen über eine
        graphische Nutzeroberfläche auf einen Text anzuwenden. Mit diesen Komponenten wird
        es u. a. möglich sein: Annotationen im Sinne des Open Annotation Data Model (OADM)
        zu erstellen, das im ersten Erfassungsschritt bereits transkribierte Material z. B.
        in TEI auszuzeichnen und mit dem SVG-Editor Grabungspläne so nachzuzeichnen, dass
        sie im Detail annotiert und in ein Geoinformationssystem eingehängt werden können. 
         Im vorgestellten Projekt haben wir uns aufgrund der kurzen Projektlaufzeit bewusst
          gegen die Entwicklung einer projektspezifischen Präsentationsoberfläche entschieden.
          Stattdessen konzentrieren wir uns auf Arbeitsabläufe zum Erstellen standardisierter
          Daten und der Frage wie diese möglichst zukunftssicher modelliert werden können. Die
          Datenanzeige soll derweil über Tamboti oder per IIIF-P über Mirrador oder andere
          standardkonforme Viewer erfolgen können. Die Daten können über Tamboti für weitere
          Nutzergruppen freigegeben werden und so z. B. im Unterricht mit Atomic Wiki oder
          HyperImage4 aufbereitet werden. Ein Tutorenprogramm5 und eine
          ausführliche Dokumentation dafür wurden in den letzten Semestern bereits etabliert
          und regelmäßig in unterschiedlichen Fachbereichen eingesetzt. 
      
      
         
            Die
      Produktivinstanz des Exzellenzclusters Asien und Europa ist unter http://tamboti.uni-hd.de zu
      erreichen. Der Quellcode ist unter https://github.com/exc-asia-and-europe/tamboti veröffentlicht.
    
             Ziziphus ist ein integrierter Bestandteil von
        Tamboti und wird von der HRA in Zusammenarbeit mit betterFORM und eXist
        solutions entwickelt. Der Quellcode ist unter https://github.com/exc-asia-and-europe/ziziphus/ veröffentlicht.
      
             Der Quellcode ist unter https://sourceforge.net/projects/teian/ veröffentlicht. 
             Derzeit wird die Community
        Edition des HyperImage Authoring Environment in der Version 3.0.beta2
        eingesetzt.
             Das Tutorenprogramm wurde im Wintersemester 2013 / 14,
        unterstützt durch Mittel aus dem „Willkommen in der Wissenschaft“
        Förderprogramm, vom Lehrstuhl für Visuelle und Medienanthropologie (Christiane
        Brosius) initiert und gemeinsam mit der HRA implementiert. Der kontinuierliche
        Ausbau des Programms wird von der Abteilung Schlüsselkompetenzen und
        Hochschuldidaktik der Universität Heidelberg begleitet.
         
      
   



      
         
            Ziel: Erfassung und Analyse literarischer Topographien
            Der Fokus der Untersuchung liegt auf der literarischen Repräsentation von Raum.
          Bisherige Untersuchungen ihres Werks haben erwiesen, dass Aichingers Bezugnahmen
          auf Orte und Ereignisse in Wien zentrale Bedeutung zukommt (Fässler 2013). Dabei
          fällt auf, dass Aichinger Raumbezüge in verschiedenen Phasen ihres Werks auf
          ganz unterschiedliche Weise elaboriert: Der Wienbezug ihres ersten Romans, Die größere Hoffnung (1948), ist für den Leser / die
          Leserin unzweifelhaft erkennbar, obwohl Aichinger konsequent auf die Nennung
          identifizierbarer Ortsnamen verzichtet. Im mittleren Werk werden Ortsbezüge
          zunehmend abstrakt; in ihren spätesten Texten hingegen häufen sich exakte
          Ortsangaben im Stadtraum Wien. Ziel des Projekts ist es, grundlegende Strukturen
          in Aichingers Referenzierung auf Orte zu ermitteln und deren Zusammenhang zur
          historischen Erfahrung herauszuarbeiten, deren Darstellung im Zentrum ihres
          Werks steht. Zu diesem Zweck sollen alle Angaben zu Ort, Zeit und Person in
          ihren Texten so codiert werden, dass sie einer maschinellen Abfrage zugänglich
          und damit sowohl systematisch als auch vollständig evaluiert werden können.
         
         
            Arbeitsschritte
            
               Digitale Texterfassung
               Textgrundlage ist die achtbändige Ausgabe der Werke Ilse Aichingers (S.
              Fischer Verlag 1991) sowie die danach erschienenen Einzelbände. Diese Bände
              wurden gescannt und mittels OCR (Optical Character Recognition) erfasst und
              dadurch maschinenlesbar gemacht. Als Vergleichskorpora sollen zusätzlich die
              davon abweichenden Textfassungen der Erstausgabe des Romans sowie der
              zwischen 2000 und 2004 in Tageszeitungen publizierten Texte erfasst
              werden.
            
            
               Digitale Texterschließung
               Im zweiten Arbeitsschritt wird eine TEI-konforme Datei erstellt, in der die
                Texte mithilfe von Standards wie RDF (Resource Description Framework), XML
                (Extensible Markup Language) und PoS (Part-of-Speech-Tagging) codiert und
                dadurch der maschinellen Abfrage durch Abfragesprachen wie SPARQL (SPARQL
                Protocol And RDF Query Language) zugänglich gemacht werden. Im Hinblick auf
                den primären Fokus der Untersuchung, die Erfassung und Analyse der
                literarischen Topographien Aichingers, werden vorrangig Personennamen sowie
                Orts- und Zeitangaben kodiert. Außerdem ist eine Analyse anhand semantischer
                Felder geplant, wofür eine Vernetzung mit unterschiedlichen Datenbanken (z.
                B. Dornseiff) vorgesehen ist. Von vornherein soll so gearbeitet werden, dass
                die Möglichkeit weitere bzw. speziellere Codierungen zu ergänzen offen
                bleibt.
            
            
               Erhebung und Einpflege zusätzlicher Daten / Metadaten
               Ergänzend zur digitalen Erfassung und Erschließung der Texte werden weitere
                  Metadaten eingebracht. Dies können textgenetisch relevante Daten sein wie
                  Entstehungs- und Publikationsdaten, oder Sacherläuterungen, wie sie in
                  Apparaten wissenschaftlicher Editionen oder Kommentaren üblich sind, sowie
                  Hinweise auf Varianten, Querverweise, Illustrationen etc. Ein Teil dieser
                  Daten ist durch Recherchen in Wiener Archiven oder am Aichinger-Vorlass im
                  Deutschen Literaturarchiv (DLA) in Marbach zu erheben. Die Auszeichnung
                  durch RDF ermöglicht aber auch die Verlinkung mit online Datenbanken und
                  damit den Anschluss an das semantic web (Hitzler et al. 2008; Ivanovic /
                  Frank 2015). 
            
         
         
            Darstellung der Ergebnisse
            Das Textkorpus :aichinger soll die Basis bilden für die Durchführung von Abfragen
                  und Analysen, die eine präzise, systematische und vollständige Evaluierung der
                  Raumbezugnahmen im Gesamtwerk der Autorin ermöglichen soll. Erfassbar werden
                  dadurch beispielsweise Personenkonstellationen in Verbindung mit Orten sowie
                  Frequenzen der Nennung bestimmter Orte resp. Wege in Korrelation zur
                  beschriebenen Zeit wie zur Zeit der Textabfassung. Diese Ergebnisse verlangen
                  unterschiedliche Darstellungsformen. So sind Diagramme möglich, oder Wordclouds,
                  die wiederum Häufungen oder Übereinstimmungen bzw. Korrelationen darstellen
                  können. Auf der Basis der RDF Codierung lassen sich z. B. maschinell Karten
                  generieren, in denen die erwähnten Orte oder Wege der in den Texten genannten
                  Figuren u. a. aufscheinen. Die kartographische Darstellung ermöglicht es darüber
                  hinaus Leerstellen ihres Werkes (nie genannte Orte oder Zonen) oder verdeckte
                  Strukturen (die Wientopographie, die der für Aichinger maßgebliche Film Der
                  Dritte Mann darstellt) sichtbar zu machen. Insbesondere anhand solcher Karten
                  wird das Poster das Konzept und die Analysemöglichkeiten unseres Projektes
                  darstellen.
         
         
            Prototyp
            Das Projekt dient der Sichtbarmachung und besseren Analyse der raumrelevanten Strukturen im Werk Aichingers und deren Relevanz für die erinnerungskulturell motivierte Schreibweise, der die Autorin verpflichtet ist. Das Projekt hat insofern paradigmatischen Charakter, als die an diesem Beispiel entwickelten Methoden den Status eines Prototyps haben und auch bei der Analyse anderer Textkorpora Anwendung finden sollen.
         
      
      
         
            
               Bibliographie
               
                  Aichinger, Ilse (1991): Werke in acht
                      Bänden. Herausgegeben von Richard Reichensperger (Die größere
                      Hoffnung / Der Gefesselte / Eliza Eliza / Schlechte Wörter / Kleist, Moos,
                      Fasane / Auckland / Zu keiner Stunde / Verschenkter Rat). Frankfurt / Main:
                      S. Fischer. 
               
                  Fässler, Simone (2011): Von Wien
                      her, auf Wien hin. Ilse Aichingers „Geographie der eigenen
                      Existenz.“ Wien: Böhlau.
               
                  Hitzler, Pascal / Krötzsch, Markus
                        / Rudolph, Sebastian (2008): Semantic
                        Web. Berlin / Heidelberg: Springer. 
               
                  Ivanovic, Christine / Frank,
                          Andrew (2015): „Auf der Suche nach dem erfüllten Raum: Digitale
                          Korpusanalyse in der Literaturwissenschaft am Beispiel Ilse Aichinger“, in:
                          Tagung der Digital Humanities im deutschsprachigen
                            Raum, Graz. 
            
         
      
   



      
         
            Einleitung
            In unserem Poster möchten wir unsere neueste FinderApp GoetheFind vorstellen, die mit
          computerlinguistischen Methoden die Originalausgabe von Goethes Faust
          durchsucht. GoetheFind hat einen neuen browser-basierten Faksimile-Viewer, der
          die Schaltstelle der multimedialen Ausgabe der Suchtreffer darstellt: Die
          Treffer werden im Faksimile der Originalausgaben gehighlighted dargestellt und
          mit einer gesprochenen Faust-Hörbuchausgabe verlinkt. Bei Faust I werden die
          Treffer mit der entsprechenden Szene des Videos der Bühnenaufführung vom
          Hamburger Schauspielhaus mit Gustav Gründgens (1960) verlinkt. GoetheFind
          entstand aus unserer FinderApp WiTTFind (Hadersbeck / Pichler; Hadersbeck et al. 2014), die den
          öffentlich zugänglichen Teil des Nachlasses von Ludwig Wittgenstein durchsucht
          und mit der wir im Sommer 2014 den EU-AWARD des EU-Projekt Digitised Manuscripts to Europeana (cf.
          Ploeger 2014) gewannen. 
            In unserer neuen FinderApp GoetheFind, setzen wir Ideen des „Standoff-Markups“ um, damit „overtagged“-XML vermieden wird. Wir entwickelten eine reduzierte „XML-TEI-P5 anchor-key“ Edition und speichern die Metainformatione in einer „NoSQL-mongo“-Datenbank. Alle relevanten Editions-, OCR- und Transkriptionsinformationen zur multimedialen Trefferausgabe sind in der Datenbank gespeichert.
         
         
            Modellierung der Editionsdaten 
            
               Anstatt „overtagged“ XML ein reduziertes „anchor-key“ XML-TEI-P5 mit „NoSQL“-Database
               Grundlage unserer FinderApp GoetheFind ist die XML-TEI-P5 Textedition im
              DTABf Format vom Deutschen Textarchiv (DTA) der Berlin-Brandenburgischen
              Akademie der Wissenschaften (BBAW 2013; Haaf et al. 2015), dazu die
              Bilddigitalisate der Staatsbibliothek zu Berlin (BBAW 2013) und dem freiem
              Hochstift Frankfurt des Frankfurter Goethe-Hauses (Signatur: III B / 23).
              Da wir zur multimedialen Ausgabe der Suchtreffer zahlreiche
              Metainformationen benötigen und "overtagged" XML des Editionstexts vermeiden
              wollten, verwenden wir Ideen des „Standoff-Markups“ und lagern alle
              notwendigen Meta-Informationen in der „NoSQL“ mongo-Datenbank GoetheDB aus.
              Eine eindeutige Referenz der Datenbankeinträge zum Editionstext lösen wir
              über den XML-TEI-P5 Tag , der an Seitenanfängen in die
              Edition eingefügt ist. Die Trefferpositionen werden über ein
              XML-Attributtrippel (Seite, Zeile, Token) genau spezifiziert. 
            
            
               Vorverarbeitung der Edition, Faksimile, Videoaufführung und Audioaufnahme
               Da unsere FinderApp die Suchanfragen regelbasiert mit Hilfe von lokalen
                Grammatiken im Kontext eines Satzes realisiert, verwenden wir als wichtigste
                Strukturierungsebene Sätze. Goethes Faustdrama bettet Sätze in Rede und
                Gegenrede, sogenannte Repliken ein, die die zweite Strukturierungsebene
                darstellt. Zur visuellen Hervorhebung und multimedialen Ausgabe der
                gefundenen Textstellen im Faksimile ermitteln wir für die Replike
                geometrische Informationen mit Hilfe eines von uns entwickelten
                semiautomatischem OCR-Correction Tools. Die Bühnen- und Audioaufnahme werden
                mit Hilfe des Clarin-Tools: „Munich Automatic Segmentation System WebMAUS“
                (CLARIN) semiautomatisch transkribiert. 
            
         
         
            Computerlinguistische Methoden zur Textinterpretation 
            Mit Hilfe unseres Speziallexikons GoetheLEX, angereichert um historische Sprachvarianten, Part of Speech Tagging und lokalen Grammatiken implementierten wir eine Partikelverb- und Semantische Suche. Bei der Eingabe von Suchanfragen verwenden wir eine symmetrische Autovervollständigung mit Informationen zur Häufigkeit des Auftretens im Text.
         
         
            Treffer im Faksimile-Viewer und multimediale Ausgabe
            Ähnlich wie bei Google-Docs entwickelten wir einen Browser basierter Faksimile-Viewer mit dem man in einem doppelseitigen Buchlesemodus durch das Dokument blättern kann und die gefundenen Textstellen farblich hervorgehoben werden. GoetheFind vernetzt alle Treffer mit der entsprechenden Replik in der Bühnenaufführung und der Hörbuchausgabe. Sobald der Nutzer auf die Mulitmediabuttons des Treffers drückt, startet im Browser ein Videoviewer oder eine Ausdioausgabe ab dieser Stelle. 
         
         
            Danksagung
            Wir danken dem Deutschen Textarchiv für die gute Zusammenarbeit und die
                freundliche Verfügungsstellung der Editionsdaten von Goethes Faust (BBAW 2013).
                Der Staatsbibliothek zu Berlin – Preußischer Kulturbesitz und dem Freien
                Deutschen Hochstift, in Frankfurt danken wir für die Wiedergaberechte der
                Bilddigitalisate der Originalausgabe Goethes Faust.
         
      
      
         
            
               Bibliographie
               
                  BBAW (2013): Das DTA-Basisformat
                    DTABf. Berlin-Brandenburgische Akademie der Wissenschaften (BBAW)
                    http://www.deutschestextarchiv.de/doku/basisformat [letzter
                      Zugriff 09. Januar 2016]. 
               
                  CLARIN (o. J.): CLARIN-D
                      WebMAUS. Automatic Segmentation of Speech. https://www.clarin.eu/movies/clarin-d-webmaus-automatic-segmentation-speech
                      [letzter Zugriff 08. September 2015]. 
               
                  Haaf, Susanne / Geyken, Alexander / Wiegand, Frank
                        (2015): "The DTA 'Base Format': A TEI Subset for the Compilation of a Large
                        Reference Corpus of Printed Text from Multiple Sources", in: Journal of the Text Encoding Initiative 8 https://jtei.revues.org/1114
                        [letzter Zugriff 09. Januar 2016]. 
               
                   Hadersbeck, Max / Bruder, Daniel / Capsamun, Roman / Conforti, Costanza / 
                            Eder, Elisabeth / Eichfeldt, Nora / Fink, Florian / Herteis, Simeon / Höps, Raphael /
                            Lindinger, Matthias / Ling, Jennifer / Mittelhammer, Katharina / Schmidt, Katharina / 
                            Schweter, Stefan
                  FinderApp GoetheFind.
                          Centrum für Informations- und Sprachverarbeitung (CIS), Ludwig Maximilians Universität
                          München
                          http://goethefind.cis.uni-muenchen.de/
                           [18.02.2016]. 
               
                  Hadersbeck, Max / Pichler, Alois (eds.) (o. J.) FinderApp WiTTFind. Centrum für Informations- und
                          Sprachverarbeitung (CIS), Ludwig Maximilians Universität München &
                          Wittgenstein Archives, University of Bergen http://wittfind.cis.uni-muenchen.de/ [letzter Zugriff 09. Januar
                          2016].
               
                  Hadersbeck, Max, Pichler, Alois, Fink, Florian , Gjesdal,
                              Øyvind Liland (2014): "Wittgenstein's Nachlass. WiTTFind and
                              Wittgenstein advanced search tools (WAST)", in: Digital
                              Access to Textual Cultural Heritage (DaTeCH 2014), Madrid 91-96
                              http://dblp.uni-trier.de/db/conf/datech/datech2014.html#HadersbeckPFG14
                                [letzter Zugriff 09. Januar 2016]. 
               
                  Hadersbeck, Max / Pichler, Alois / Fink, Florian / Bruder,
                                  Daniel / Arends, Ina / Baiter, Johannes (2015): "Wittgensteins
                                  Nachlass. Erkenntnisse und Weiterentwicklung der FinderApp WiTTFind", in:
                                  2. Tagung der Digital Humanities im deutschsprachigen
                                    Raum 23.-27.2.2015, Graz.
               
                  Ploeger, Lieke (2014): "Open Humanities Awards round 2
                                      – Winners announced", in: DM2E. Digitised Manuscripts to
                                      Europeana
                  http://dm2e.eu/open-humanities-awards-round-2-winners-announced/
                                        [letzter Zugriff 09. Januar 2016]. 
               
                  TEI (2015): "Stand-off Markup", in: TEI Guidelines for Electronic Text Encoding and Interchange,
                                        Version 2.9.1, 16.9 http://www.tei-c.org/release/doc/tei-p5-doc/en/html/SA.html#SASO
                                        [letzter Zugriff 09. Januar 2016].
            
         
      
   



      
         Ambiguitäten allerorten: Ambiguität ist ein integraler Bestandteil menschlicher
        Kommunikation. Sie kann unbeabsichtigt produziert werden, wie in (1a), oder zu
        strategischen Zwecken eingesetzt werden, z. B. für komische Effekte wie in (1b). 
         1a. William isn’t drinking because he’s unhappy (vgl. Hirschberger / Avesani
          1997).
         1b. …men who can sheer sheep and women with long hair… (vgl. Cutting from BBC News
            Website).
         Ambiguität findet sich vornehmlich in sprachlichen Ausdrücken, kann jedoch auch in
              der Interaktion mit Bildern oder in Bildern selbst zu finden sein, sowie auf
              nicht-sprachliche Kommunikation übertragen werden. Daher eröffnet das Thema
              Ambiguität ein interdisziplinäres Forschungsfeld, an dem neben Sprach- und
              Literaturwissenschaft auch Rhetorik, Psychologie, Theologie, Rechtswissenschaft und
              Medienwissenschaften größtes Interesse bekunden (siehe beispielsweise Klein /
              Winkler 2010; Winkler 2015).
         Das Datenbankprojekt TInCAP (Tübingen Interdisciplinary Corpus of Ambiguity Phenomena), das im Rahmen des interdisziplinären Graduiertenkollegs 1808 „Ambiguität: Produktion und Rezeption“ entsteht, zielt darauf ab, Belege von Ambiguität verschiedener Provenienz zu sammeln und diese interdisziplinär zu annotieren und nachhaltig zu speichern. Dabei stehen drei Ziele im Vordergrund: (a) die interdisziplinäre Auseinandersetzung mit dem Thema Ambiguität durch die Erstellung eines gemeinsamen Annotationsschemas, (b) die Nachhaltigkeit der gesammelten Daten und (c) die Zugänglichkeit der Datensammlung für die nationale und internationale Forschungsgemeinschaft.
         
            ANNOTATION. Jeder Beleg zur Ambiguität kann interdisziplinär im
                Hinblick auf fünf verschiedene Aspekte annotiert werden. Die (i) Kommunikationsebene
                legt fest auf welcher Ebene die Ambiguität in der Kommunikation eine Rolle spielt,
                beispielsweise auf der Ebene der fiktiven Charaktere vs. Erzähler-Leser vs. konkrete
                Kommunikation. Wir unterscheiden außerdem (ii) zwischen strategischem vs.
                nicht-strategischem Einsatz der Ambiguität in Produktion und Rezeption. Darüber
                hinaus wird (iii) sowohl die Ebene des Auslösers der Ambiguität (z. B. auf
                Wortebene, Phrasenebene etc.) annotiert als auch ihre Reichweite, d. h. bis zu
                welcher Ebene sie für die Interpretation relevant ist. Als weiteren Punkt
                kennzeichnet (iv) eine qualitative Annotation wie sich die unterschiedlichen
                Lesarten zueinander verhalten: Sind sie voneinander abgeleitet oder komplett
                unabhängig? Spielt Vagheit eine Rolle? Nicht zuletzt sieht die Datenbank die
                Möglichkeit vor, (v) disziplininterne Begrifflichkeiten zur Beschreibung des
                behandelten Phänomens zu verwenden. Dieses interdisziplinär erarbeitete und
                anwendbare Schema erlaubt es uns langfristig Korrelationen in den Daten zu finden,
                die über die unterschiedlichen Modi (unterschiedliche Typen von Ambiguität in
                unterschiedlichen Texttypen / Bildern) hinweg gelten, sodass wir damit ein genuin
                interdisziplinäres Forschungsergebnis erreichen können. 
         
            NACHHALTIGKEIT. Ein wichtiges Ziel der Datenbank ist es, die gesammelten Daten langfristig und nachhaltig zu speichern. Dazu haben wir ein XML-Schema entwickelt, das weitestgehend TEI-konform [5] ist. Diese XML-Dateien können im Rahmen der universitären Infrastruktur langfristig gespeichert, katalogisiert und zugänglich gemacht werden. Bei Video-, Audio- und Bilddateien halten wir uns an die üblichen Standards für nachhaltige Datenformate.
                
         
            INTERFACE. Für die aktive Arbeitsphase mit der Datenbank im
                  Rahmen des GRK 1808 und für die Zugänglichkeit für die (inter)nationale
                  Forschergemeinschaft haben wir eine Datenbankanwendung spezifiziert, die von einem
                  externen Dienstleister implementiert wird. Dabei setzen wir auf die
                  objektorientierte hierarchische Datenbanktechnologie LDAP (vgl. Zeilenga 2006), die
                  bereits im BMBF-Projekt RiR eingesetzt wurde. So lässt sich nicht nur die
                  XML-Hierarchie bestens abbilden, sondern es wird auch eine sichere und feingranulare
                  Zugriffskontrolle ermöglicht. Mittels einer entsprechend angepassten
                  Synchronisierungssoftware konnte die Datenbank während der XML-Erfassungsphase
                  ständig aktualisiert werden. Eine webbasierte Benutzeroberfläche ermöglicht u.a.
                  komplexe Suchen in den verschiedenen Hierarchieebenen, wobei mehrere Einträge in der
                  Hierarchie (also z. B. ein Haupteintrag sowie mehrere Annotationseinträge und
                  bibliographische Einträge) als ein virtueller Eintrag zusammengezogen werden. Über
                  die Benutzeroberfläche können neue Einträge erstellt und vorhandene Einträge
                  modifiziert werden, wobei die Zugriffskontrolle erlaubt, auch nur Teile eines
                  solchen virtuellen Eintrags sichtbar/bearbeitbar zu machen. Gleichzeitig erlaubt der
                  Export einzelner bzw. aller Datensätze, im XML-Format die Nachhaltigkeit der
                  eingegebenen Daten auch über einen längeren Zeitraum hinweg sicherzustellen. 
         Damit zeigt das Datenbankprojekt, wie sich interdisziplinäre inhaltliche Arbeit innovativ mit den Zielen der Nachhaltigkeit verknüpfen lässt, ohne die Benutzerfreundlichkeit in der aktiven Arbeitsphase zu vernachlässigen.
      
      
         
            
               Bibliographie
               Cutting from BBC News Website, quoted BBC 4, Friday Night Comedy, the News
                        Quiz, Series 82, Episode 2; Broadcasted: 15.Nov 2013.
               
                  Hirschberg, Julia / Avesani, Cinzia (1997): „The role
                          of prosody in disambiguating potentially ambiguous utterances in English and
                          Italian“, in: Botinis, Antonis / Kouroupetroglou, Georgios / Carayannis,
                          George (eds.): Intonation. Theory, Models and
                          Applications 189–192. 
               
                  Klein, Wolfgang / Winkler, Susanne (eds.) (2010): Ambiguität. Zeitschrift für Literaturwissenschaft und
                            Linguistik 40, 158. Stuttgart: Metzler. 
               
                  RiR (2012-2015): Relationen im
                              Raum
                  
                              http://www.steinheim-institut.de/wiki/index.php/RiR [letzter
                              Zugriff 15. Februar 2016]. 
               
                  TEI Consortium (eds.):  Guidelines
                                for Electronic Text Encoding and Interchange
                  http://www.tei-c.org/P5/
                                [letzter Zugriff 15. Februar 2016]. 
               
                  Winkler, Susanne (ed.) (2015): Ambiguity. Language and Communication. Berlin / New York: de
                                  Gruyter. 
               
                  Zeilenga, Kurt (2006): Lightweight
                                    Directory Access Protocol (LDAP). Directory Information Models,
                                    IETF RFC 4512, June 2006. 
            
         
      
   



      
         
            Das Vorhaben
            Die Uwe Johnson Werkausgabe ist ein Vorhaben der Berlin Brandenburgischen
          Akademie der Wissenschaften an der Universität Rostock. Erstmals wird mit Uwe
          Johnson ein Autor des 20. Jahrhunderts in einem Akademienvorhaben ediert. Die
          Werkausgabe gliedert sich in die Abteilungen Werke, Schriften und Briefe und wird
          sowohl als Buch wie auch digital erscheinen. Für beide Medien bilden gemäß TEI
          (P5) ausgezeichnete Texte die gemeinsame Datengrundlage, im Digitalen ergänzt um
          die digitalisierten Materialien des Uwe Johnson-Archivs. Um „lesbare“ Bücher zu
          ermöglichen, werden textkritische Apparate, Kommentare und Erläuterungen in
          einer exemplarisch sinnstiftenden Auswahl in den Druck aufgenommen, digital wird
          historisch-kritische Vollständigkeit angestrebt. 
            Zu den Eigenheiten des Autors Uwe Johnson gehört eine textsortenübergreifende Arbeitsmethode: In Briefen formuliert er Prosaversuche, aus Zeitungen übersetzt er in seine Bücher, Personen, Dokumente und Ereignisse der Zeitgeschichte treten in seinen Romanen auf.
         
         
            Aus der Werkstatt
            Aus dieser Arbeitsweise ergibt sich beinahe zwingend, dass im Digitalen mit den
            Buchdeckeln auch die Grenzen zwischen den drei die Buchausgabe gliedernden
            Abteilungen durchlässig werden. Anhand einiger Beispiele soll Johnsons Arbeiten
            gezeigt werden: welche Brücken zwischen Texten und unterschiedlichen Medien er
            schlägt, und welche Auswirkungen das auf einen Editionsprozess hat, der diesen
            Pfaden nachgehen will, um sie den Nutzern zu präsentieren. Hinzu kommen Fragen
            der Einbindung externer Ressourcen, audiovisuellen Materials und der
            Berücksichtigung von Urheberrechtsfragen.
            Durch die beiden Zielmedien ergibt sich ein unterschiedlicher Bedarf in der Tiefe der TEI-Auszeichnung der jeweiligen Dokumente. Die Ansprüche an die Auszeichnung für ein Buch sind selbstverständlich andere als die, für eine digitale Edition, in der die Grenzen lediglich durch die editorischen Kriterien gezogen werden. Das Erforderliche kann aufgenommen werden, während auf zu weit Entferntes oder Weiterführendes nur hingewiesen wird. Aus diesem Grund wird für jedes Werk eine vollausgezeichnete „Masterdatei“ erstellt, aus der sich beide Versionen speisen und somit dieselbe Textgrundlage haben, wodurch eine doppelte Datenhaltung vermieden wird. Neben der Erfassung der verschiedenen Varianten finden sich in dieser Datei sowohl der text- als auch der historisch-kritische Kommentar. Das Ziel der Buchfassung, nämlich eine lesbare Ausgabe zu sein, die das zum Textverständnis Notwendige enthält, steht dem der digitalen Edition gegenüber, welche losgelöst der analogen Grenzen funktioniert. Hier kann vom Optimum ausgegangen werden und dann durch den Nutzer entsprechend seiner Fragestellung reduziert werden. Durch Zuweisung von Attributen ist es möglich per XSL Transformation das gewünschte Endprodukt zu generieren. Dazu werden einerseits einige der typischen TEI-Elemente als „Filtersignal“ genutzt, andererseits ein „Digital“- oder „Print“-Attribut, welches in dem Prozess wieder entfernt wird. Die so generierte print-XML wird per XSL-FO in PDF umgewandelt, um eine orientierende Vorlage für den Verlag zu erhalten, der diese beiden Dateien für den Satz erhält. Nach dem gleichen Prinzip wird die Digital-XML als Grundlage der digitalen Edition generiert.
            Künftig wird eine weitere Auszeichnungsebene hinzukommen. In der praktischen Arbeit hat sich gezeigt, dass eine zusätzliche semantische Auszeichnung sinnvoll ist. Daher konzipieren wir zurzeit eine Möglichkeit neben der TEI- auch eine semantische Auszeichnung vorzunehmen. In welcher Ebene dies geschieht ist noch in der Überlegung. Anzunehmen ist ein erneutes Transformationsszenario.
            Johnsons Arbeitsmethoden bieten einen sehr großen Anreiz zur Vernetzung und Visualisierung in der digitalen Präsentation. Intertextualität und Collage sind nur zwei von vielen Termini, die sich auf ihn anwenden und im digitalen Raum auf eine Art und Weise präsentieren lassen, wie es in einem Buch nicht möglich wäre. So können Zusammenhänge sicht- und erfahrbar gemacht werden, die sonst nur Abstrakt zu erfassen wären.
            Dazu gehört auch die angestrebte Multimedialität. Johnson hielt vielerlei Lese- und sonstige Reisen ab, welche zum Teil als Video- oder Audiodokument vorliegen. Im Zusammenspiel mit seinen Manuskripten – er hat oft dokumentiert, wo er was geschrieben hat – lässt sich die Entstehung seines Œuvres auch räumlich nachverfolgen und etwaige Koinzidenzen erforschbar machen.
            Die spätere Präsentation dieser Ergebnisse wird sich der Nutzer gemäß seinen Bedürfnissen komplett selbst anpassen können. Die Textvarianten, die von Interesse sind, werden sich nach eigenem Wunsch anordnen, zu- oder abschalten lassen. Die einzelnen Anzeigeelemente werden also nicht in einem Layout „gefangen“ sein, sondern können durch den Nutzer selbst angeordnet werden. Entscheidungen, welche Varianten wie nebeneinander gelegt werden und ob ein Faksimile hinzugeschaltet werden soll liegen in der Hand des Benutzers, der seinen Schreibtisch in der Art anordnen kann, wie es seiner Arbeit dienlich ist.
         
      
   



      
         
            Untersuchungsgegenstand und Zielsetzung
            In der neueren deutschen Literatur steht Stefan George (1868-1933) wie kein anderer für die außergewöhnliche Beschäftigung eines Autors mit bzw. für die Verwendung von Typografie. Ab 1904 werden seine Werke in
          Stefan-George-Schrift (St-G) gedruckt. Der Formenkanon der serifenlosen Type basiert auf Georges Buchschrift1 sowie auf der
          Akzidenz-Grotesk der Schriftgießerei Berthold und zeigt zudem Einflüsse historischer Schriften wie der Unziale und der karolingischen Minuskel. Bis zu der vom Dichter autorisierten
          Gesamt-Ausgabe der Werke (1927-1934) entwickelt sich das Typenrepertoire der Schriftart, sodass die St-G-Schrift nicht in einer, sondern in mehreren Fassungen vorliegt.
        
            Eine serifenlose Schrift inmitten der in Deutschland tobenden Antiqua-Fraktur
          Debatte zu verwenden, ihr Design an der eigenen Handschrift zu orientieren und
          gleichzeitig auf historische Vorbilder zu referieren – lediglich ausschnitthaft
          verdeutlichen diese Aspekte die große Relevanz von Typografie für Georges Werk.
          Umso verwunderlicher ist es, dass bisherige Editionen2 keine tiefentypografische Analyse der Drucke
          vornehmen. Die Abhandlungen der einschlägigen Forschung zur Gestaltung und
          Genese der Type sowie zu ihrer Verwendung und Wirkung sind dementsprechend
          dürftig. Daher ist das Ziel des Projekts Stefan George Digital
        (StGD) die erstmalige Edition der Drucküberlieferung der Georgeschen Lyrik,
        wobei der Schwerpunkt auf der Erschließung typografischer Formen mittels eines
        semantischen Modells liegt.3
            
         
         
            Vorgehen und Methodik
            Das Editionskorpus StGDs besteht aus 29 Druckausgaben der insgesamt 11 lyrischen
        Werke4 Georges, in denen die Anwendung und die Entwicklung der typografischen
        Gestaltung sichtbar werden. Die digitalen Volltexte werden größtenteils aus
        bestehenden Repositorien (z. B. Deutsches Textarchiv und TextGrid
        Repository) semi-automatisch in ein projektspezifisches XML/TEI Schema
        konvertiert. Da der Schwerpunkt der Edition auf der buchwissenschaftlichen
        Erschließung des Materials liegt, werden die Daten entsprechend mit Metadaten
        (z. B. FRBR, METS) angereichert. Schließlich werden digitale Faksimiles,
        vereinzelt aus bestehenden Repositorien (z. B. ULB Düsseldorf), größtenteils jedoch erstmalig hochauflösend
        digitalisiert, auf einen IIIF basierten Imageserver mit angepassten Viewern
        (Open Seadragon, Mirador) in die Edition integriert. Die digitalen Bilder werden
        sowohl parallel zum edierten Text als auch separat typografisch annotiert.
        Während die Ebenen der Meso- (Schrift in der Fläche), Makro- (Organisation von
        Schrift) und Paratypografie (Materialität und Technik) im Rahmen der digitalen
        Edition weitestgehend mit bestehenden Datenmodellen erfasst werden können, wird
        das Modell zur Erschließung der Mikrotypografie (Formausstattungsmerkmale) erst
        im Rahmen des Projekts entwickelt (zu den typografischen Ebenen vgl. Stöckl
        2004). 
            Die Modellierung der typografischen Detailformen erfolgt in Form einer Ontologie, welche ihre eindeutige Identifikation, formalisierte Beschreibung und Zitation ermöglicht. Damit wird eine netzwerkartige Erschließung und Verknüpfung unterschiedlicher Aspekte und Charakteristika von Schrift unternommen, welche sowohl für Mensch als auch für Maschine interpretierbar sind. Die Technologien des Semantic Web zur Wissensrepräsentation in Thesauri (SKOS), Klassenmodelle (RDFs) und Ontologien (OWL) können dafür ebenso verwendet werden wie Methoden der Daten- und Softwaremodellierung (UML). 
            Sowohl die Digitale Edition als auch die Ontologie zur Beschreibung von Typografie werden unter CC BY-SA Lizenz auf der
          GAMS, der technischen Infrastruktur des
          Grazer Zentrums für Informationsmodellierung, bereitgestellt.
        
         
         
            Kontextualisierung in den Digitalen Geisteswissenschaften
            Das Projekt ist vorrangig für die digitale Editorik relevant, die seit geraumer Zeit
          verstärkt auch die Materialität von Dokumenten zu erschließen versucht. Statt den Weg der
          Abbildung von Schrift mittels Faksimiles oder ihrer Rekonstruktion im Rahmen der
          Transkription (z. B. Schriftfaksimile ) zu gehen, wählt das Projekt die formale
          Modellierung und macht die Informationen so analysierbar. In diesem Zusammenhang trägt
          StGD auch zur Bildung einer 5 noch kaum
          bestehenden Digitalen Buchwissenschaft und Typenkunde bei, für welche der Ansatz der
          semantischen Modellierung von Typenformen ebenfalls neu ist.6
            
            Schließlich kann das StGD auch als exemplarisch für den Einsatz und Umgang mit projektspezifischen Datenmodellen gelten. Um die Ontologie für die breitere Forschungscommunity nutzbar zu gestalten, wird die Übertragbarkeit des Modells auf verschiedene Arten von Typen, wie beispielsweise bewegliche Lettern und frühneuzeitliche Typen, getestet. Außerdem ist der Versuch eines mappings des Modells auf Handschriften in Zusammenarbeit mit
          DigiPal vorgesehen.
        
         
         
            Zentrale Aspekte auf dem Poster
            Neben einer Gesamtpräsentation des Projekts StGD, wird das Poster vorrangig drei aktuelle Herausforderungen illustrieren:
            
               Modellierung mikrotypografischer Formen basierend auf einer stabilen
            Terminologie, festgelegten Beschreibungskategorien (z. B. Form und Stil)
            sowie zentraler Unterscheidungsmerkmale (z. B. Serifen und Strichdicke). 
               Vernetzung der vier typografischen Ebenen Mikro-, Meso, Makro- und Paratypografie, welche mit unterschiedlichen Datenmodellen an unterschiedlichen Orten der Edition erfasst werden.
               Visualisierung typografischer Genese sowie gestalterischer Brüche und Kontinuitätslinien über das lyrische Gesamtwerk Georges hinweg. 
            
         
      
      
         
            
            Ab circa 1896 stilisierte George seine Kursivhandschrift verstärkt zu einer Buchschrift, welche in der Folgezeit auch von Mitgliedern des George-Kreises verwendet wurde.
          
             Neben den Editionen einzelner Werke v.a.: Sämtliche Werke in 18. Bänden. Herausgegeben von der
            Stefan George-Stiftung; bearbeitet von Peter Landmann und Ute Oelmann.
            Stuttgart 1981-2013. 
             Das Projekt
            Stefan George Digital entsteht im Rahmen von DiXiT (Digital Scholarly
            Editions Initial Training Network), als Teil der Marie Curie Actions im 7.
            Rahmenprogramm der Europäischen Kommission. 
            Die Bandzählung und -aufteilung folgt der
            Veröffentlichung der Werke in der Gesamt-Ausgabe (1927-1934). „Der
            Teppich des Lebens“ ist beispielsweise vier Mal (1900/1,1901/2,1904/3,
            1932/Gesamt-Ausgabe), „Der Stern des Bundes“ nur zwei Mal (1914/1,
            1928/Gesamt-Ausgabe) im Korpus repräsentiert. Im Laufe des Projekts
            werden außerdem auch einzelne Gedichte, welche im Vorfeld in der von
            George gegründeten Zeitschrift Blätter für die
              Kunst (1892-1919) veröffentlicht wurden, in das Editionskorpus
            aufgenommen.
            Vgl. die „stilisierte Ursprungsschrift“
            (Tool Dreifachlupe der Universität Würzburg).
            Das Typenrepertorium des Gesamtkatalogs der
            Wiegendrucke beschreibt die formalen Aspekte von Typen beispielsweise in
            Prosaform (vgl. Inkunabelreferat der Staatsbibliothek zu Berlin –
            Preußischer Kulturbesitz).
         
         
            
               Bibliographie
               
                  Eide, Øyvind (2015): "Ontologies, Data Modeling, and
              TEI", in: Journal of the Text Encoding Initiative 8.
              http://jtei.revues.org/1191 [letzter Zugriff 13. Oktober 2015]. 
               
                  Inkunabelreferat der Staatsbibliothek zu Berlin –
                  Preußischer Kulturbesitz (o.J.): TW.
                  Typenrepertorium der Wiegendrucke http://tw.staatsbibliothek-berlin.de/ [letzter Zugriff 15.
                  Februar 2016]. 
               
                  Lucius von, Wolf D. (2012): "Buchgestaltung und
                    Typographie bei Stefan George", in: Aurnhammer, Achim / Braungart, Wolfgang
                    / Breuer, Stefan / Oelmann, Ute (eds.): Stefan George und
                    sein Kreis. Ein Handbuch, 1. Berlin / Boston: De Gruyter 467-491. 
               
                  Stöckl, Hartmut (2004): "Typographie: Körper und Gewand
                      des Textes. Linguistische Überlegungen zu typographischer Gestaltung", in:
                      ZfAL Zeitschrift für Angewandte Linguistik 41:
                      5-48. 
               
                  Stokes, Peter A. (2011): Describing
                        Handwriting. Part I: http://www.digipal.eu/blog/describing-handwriting-part-i/
                        [letzter Zugriff 13. Oktober 2015]. 
               
                  Universität Würzburg (o. J.): Dreifachlupe
                  http://vb.uni-wuerzburg.de/ub/lskd/dreifachlupe.html [letzter
                          Zugriff 15. Februar 2016].
               
                  Wehde, Susanne (2000): Typographische
                            Kultur. Eine zeichenhistorische und kulturgeschichtliche Studie zur
                            Typographie und ihrer Entwicklung. Tübingen: Niemeyer. 
            
         
      
   



      
         Der Poster beschreibt den gegenwärtigen Stand des Projekts WALDI (World Atlas of Little Data Infrastructure), das am Institut für Empirische Sprachwissenschaft gemeinsam mit dem StudiumDigitale seit Mai 2015 realisiert wird. 
         Durch empirische Arbeit in der Sprachforschung entstehen verhältnismäßig kleine
        Daten, die i. d. R. von einzelnen Forschern erhoben werden und für die aktuelle
        Zwecke lokal (d. h. nicht-netzbasiert) abgespeichert. Die Verwendung von solchen
        „Little Data“ ist durch eine konkrete Fragestellung und das Forschungsinteresse
        beschränkt. Aus diesem Grund geraten sie nach dem Auslaufen einer Forschungsphase
        schnell in Vergessenheit. 
         Das primäre Ziel des Projektes ist die Konstruktion eines webbasierten Tools, das den linguistischen Feldforschern erlaubt lokal abgespeicherte Sprachdaten auf einem digitalen Atlas abzubilden und die Existenz dieser Daten für die Nutzer des Atlasses bekanntzugeben. 
         Eine webbasierte Anwendung erlaubt die freie Nutzeranmeldung auf einer digitalen
          Weltkarte (basierend auf Google Maps). Die Nutzer_innen editieren die Karte, indem
          sie die elizitierten Daten der dokumentierten Sprache (kleine Sprachkorpora)
          geographisch fixieren (pinnen) und den fixierten Pin mit einem TEI-ähnlichen Header
          versehen. Das Pinnen geschieht anhand von GPS-Angaben, die durch einfache
          Mauszeigerbewegungen generiert werden (man kann für viele Sprachen z. B. im Amazonas
          oder kaukasischen Hochland keine Orte in Google Maps lokalisieren). Der Header
          beinhaltet alle relevanten Sprachdateninformationen, die in folgende 5
          Reiter-Kolumnen unterteilt sind: 1. Korpus Information (Autoren, Institutionen,
          Release, Größe etc.); 2. Sprache (Status, Schrifttum, Typologie etc.); 3. Die Art
          und der Aufarbeitungsstand der Daten (Transkription, Annotation etc.); 4. Erfasste
          Texte (Art, Epoche, Genre etc.); 5. Zugänglichkeit und Kontakt.
         Zur technischen Realisierung des Tools kommen die Elemente aus Google Maps, JavaScript, MySQL zum Einsatz.
      
      
         
            
               Bibliographie
               
                  Bubenhofer, Noah (2014): Visual
              Linguistics - Sprache sehen. http://www.visual-linguistics.net/ [letzter Zugriff 14. Februar
              2016].
               
                  Dryer, Matthew S. / Haspelmath, Martin (2013): World Atlas of Language Structures Online (WALS).
                Leipzig: Max Planck Institute for Evolutionary Anthropology http://wals.info/ [letzter Zugriff 14.
                Februar 2016].
               
                  Krämer, Sybille (2009): „Operative Bildlichkeit. Von
                  der ‘Grammatologie’ zu einer ‘Diagrammatologie’? Reflexionen über
                  erkennendes Sehen“, in: Hessler, Martina / Mersch, Dieter (eds.): Logik des Bildlichen. Zur Kritik der ikonischen
                  Vernunft. Bielefeld: transcript 94-123. 
            
         
      
   



      
         
            Die digitale Edition
            Das Theodor-Fontane-Archiv konzipiert eine digitale, kritische Edition aller Briefe von und an Theodor Fontane. Hierbei handelt es sich um etwa 10.000 Briefe, die bislang unvollständig und nach heutigen editionswissenschaftlichen Standards unzureichend veröffentlicht sind. Ein Großteil des Briefnachlasses befindet sich im Archiv, doch sind weitere Bestände aus anderen Institutionen und Privatbesitz zu berücksichtigen. Somit wird der gesamte, stark verstreute Briefnachlass virtuell zusammengeführt und erstmalig als ein Korpus recherchierbar, wodurch die Voraussetzung für die systematische Erforschung dieses zentralen Werkbestandes geschaffen wird.
            Verschiedene editorische Herausforderungen stellen sich bezüglich der Briefe.
          Neben den Originalhandschriften sind mitunter mehrfache Abschriften sowie
          bisherige Drucke einzubeziehen. Somit werden mehrere Überlieferungsträger eines
          Briefes verzeichnet und in der Edition präsentiert, wodurch sich die Frage einer
          sinnvollen Darstellung von Überlieferungsvarianten stellt. Daneben muss eine
          Lösung für die digitale Umsetzung materialspezifischer Besonderheiten gefunden
          werden, z. B. die häufig beschriebenen Briefränder. Diese Textbestandteile sind
          oft seitenübergreifend und stehen in den unterschiedlichsten Winkeln zur
          regulären Beschreibrichtung. In der Online-Präsentation werden neben den
          Digitalisaten und Transkriptionen die Metadaten, die Kommentierung sowie die XML
          / TEI P5-Auszeichnung verfügbar gemacht. Für die individuelle Benutzbarkeit soll
          die Anzeige der genannten Daten flexibel anzupassen sein. Aufgrund der
          besonderen Schriftbildlichkeit müssen die Digitalisate drehbar und im Falle von
          mehreren Textzeugen soll deren parallele Anzeige möglich sein. Auf die
          Verwendung von Standards (z. B. XML / TEI P5) und Normdaten (z. B. GND für
          Personen) und die Erstellung von projektinternen Indizes (zu Personen,
          Institutionen, Orten, Werken, Periodika) wird besonderer Wert gelegt. So können
          etwa personelle und institutionelle Netzwerke, an denen Fontane teilhatte,
          rekonstruiert und intertextuelle Verbindungen nachvollzogen werden.
         
         
            Das Fontane-Forschungsportal
            Die Edition der Briefe steht im Zusammenhang mit dem ebenfalls in der
            konzeptionellen Entwicklung befindlichen Fontane-Forschungsportal. Dessen Ziel
            ist die Präsentation der Digitalen Sammlungen des Archivs und optional anderer
            Bestandshalter. Die aufbewahrten Handschriften liegen digitalisiert vor, die
            Verknüpfung der Digitalisate mit dem technischen und bibliographischen
            Metadatensatz wird über den METS / MODS- bzw. METS / EAD-Standard erfolgen. Die
            Handschriften- und Bibliothekskataloge des Archivs, bisher intern als allegro-C-
            und allegro-HANS-Datenbanken geführt, werden ebenfalls über das Portal als OPACs
            zugänglich gemacht. Somit stellt das Portal die Verknüpfung von archivalischen
            Quellen- und Erschließungsdaten und von Forschungsprimärdaten her.
            Neben der virtuellen Zusammenführung des zerstreuten Nachlasses ist das zweite Ziel des Portals, alle Forschungsressourcen zu Fontane schnell zugänglich bereitzustellen. Die Fontane-Aktivitäten außerhalb des Archivs sollen hier gebündelt werden und das Portal als Kommunikationsplattform für die verschiedenen Akteure dienen. Dafür werden technische Schnittstellen für Datenaustausch sowie Kooperationen mit anderen Projekten geschaffen. Doch richtet sich das Portal nicht nur an die Wissenschaft, sondern auch an die breite Öffentlichkeit, der hier ein umfassender Zugang zu Fontane, seinem Leben und Werk ermöglicht werden soll.
         
         
            FuD und CMS
            Die technische Umsetzung der digitalen Edition und des Forschungsportals wird im Rahmen der virtuellen Forschungsumgebung FuD („Forschungsnetzwerk und Datenbanksystem“) erfolgen, die am Kompetenzzentrum für elektronische Erschließungs- und Publikationsverfahren in den Geisteswissenschaften an der Universität Trier entwickelt wurde. Die in FuD bereitgestellten Tools werden an die projektspezifischen Bedürfnisse angepasst und weiterentwickelt, etwa für die Verzeichnung unterschiedlicher Textzeugen. Diese Weiterentwicklungen sollen von anderen Institutionen nachgenutzt werden können.
            Im Hintergrund stehen eine Neustrukturierung der gesamten Datenstruktur des Theodor-Fontane-Archivs und die Überführung in ein Content Management System, das ebenfalls in Zusammenarbeit mit dem Kompetenzzentrum Trier erarbeitet wird. Die Datenmodellierung des CMS, auf das Edition und Portal gleichermaßen zugreifen, muss gewährleisten, dass unterschiedliche Arten von Daten in FuD zusammengeführt, angereichert und weiterverarbeitet werden können. Hierzu gehören bibliographische Metadaten von Handschriften und Bibliotheksbeständen des Archivs, bibliographische Metadaten anderer Institutionen, Digitalisate plus deren Metadaten, Normdaten, die Transkriptionen der Edition, die TEI-Textauszeichnung, editorischen Kommentare und die erstellten Indizes. Auch werden Schnittstellen zu anderen Projekten und zu Archivdatenbanken hergestellt und der Zugang per Open Access gewährleistet.
         
         
            Edition und Portal als Forschungsumgebung
            Die digitale Edition und das Forschungsportal werden auf Grundlage der gemeinsamen Datenbasis so konzipiert und miteinander verbunden, dass zwischen ihnen Datenaustausch möglich wird. So wird der jeweilige Rechercherahmen erweitert, eine umfassendere Kontextualisierung der Informationen erreicht und eine digitale Arbeits- und Forschungsumgebung geschaffen, die den Erfordernissen eines Archivs in seinen Aufgaben des Sammelns, Erschließens und Langzeitarchivierens sowie den Erfordernissen einer Forschungseinrichtung gleichermaßen gerecht wird. Dies und die Vernetzung mit verschiedenen Forschungsvorhaben ermöglicht nicht nur neue Erkenntnisse für die Fontane-Forschung selbst, sondern auch zur Literatur- und Geistesgeschichte des 19. Jahrhunderts. Für weitere, interdisziplinäre und neue Fragestellungen wird die Struktur offen und flexibel angelegt. Zudem soll die entwickelte Arbeitsumgebung in ihrer Datenstruktur auch Modellcharakter für kleinere Archive, Museen und Sammlungen haben.
            Im Poster werden der derzeitige Konzeptionsstand von Edition und Portal vorgestellt, Probleme und offene Fragen aufgezeigt sowie Lösungsansätze zur Diskussion gestellt.
         
      
   



      
         Der Beitrag „explore.bread.AT! Die österreichische Brotkultur dialektal“ informiert
        über eine Pilotstudie, die synchron und diachron Lemmata für Brot und Gebäck
        betrachtet. Die Studie bettet sich in das Projekt „exploreAT! exploring austrias
        culture through the language glass“ (Wandl-Vogt 2015) am Austrian Centre for Digital
        Humanities der Österreichischen Akademie der Wissenschaften (ACDH-ÖAW) ein. Das
        Projekt „exploreAT!“ baut auf dem „Wörterbuch der bairischen Mundarten des
        Österreichischen“ (WBÖ) und der dazugehörigen „Datenbank der bairischen Mundarten
        des Österreichischen“ (DBÖ) auf, welche Daten aus der Zeit von 1911-1998 der
        (ehemaligen) Habsburger Monarchie beinhalten. Die DBÖ umfasst u. a. auf bestimmte
        Fragen genannte Lemmata. Eine genaue Aufstellung der Ressourcen stellen Siemund et
        al. (in Vorbereitung) dar. 
         Für die Studie werden Fragen aus den Fragebögen „30. Brot backen (III)“ sowie „31. Weißgebäck“ der WBÖ verwendet. Hierin befinden sich unter anderem Fragen nach der Benennung bestimmter Brotsorten, ob handgebackenes Brot anders genannt wird als Bäckerbrot, welche sonstigen Brotsorten bekannt sind und Verwendung finden, wie einzelne Teile sowie Eigenschaften des Brotes genannt werden, aber auch zur Brotzubereitung sowie zu Redensarten, welche in irgendeiner Form im Zusammenhang mit Brot stehen. Die verschiedenen benutzten Wortstämme der genannten Lemmata werden miteinander verglichen und Tendenzen der Verbreitung – zunächst im österreichischen Raum – aufgezeigt. Ein weiterer Untersuchungsschwerpunkt liegt in der Ermittlung der regional vorkommenden Brot- und Gebäcksorten anhand des Auftretens ihrer entsprechenden Lemmata. Im historischen Verlauf kann daraus geschlossen werden, inwiefern sich die Brotkultur innerhalb von Österreich unterscheidet und im Laufe der Zeit angepasst beziehungsweise weiter voneinander abgegrenzt hat. Ähnlich geartete Fragen anderer Sprachatlanten wie beispielsweise dem Sprachatlas von Oberösterreich (SAO) werden analysiert, sodass die Lemmata der DBÖ mit ihnen in Verbindung gesetzt werden können. Außerdem werden weitere Lexika und Atlanten wie der österreichische Volkskundeatlas (
          Burgstaller et al. 1959-1981) zu Rate gezogen, um die Ergebnisse in einen kulturhistorischen Kontext einbetten zu können. Langfristiges Ziel ist es, die Betrachtungen auf den deutschsprachigen Bereich sowie deren Grenzregionen auszuweiten und somit Gemeinsamkeiten, aber auch Unterschiede zwischen den einzelnen Ländern sichtbar zu machen sowie die Entwicklung mit Migrationsströmen zu vergleichen.
        
         Technisch werden die verwendeten Lemmata aus den Datenbanken exportiert und in XML
          modelliert. Dafür sollen die XML-Auszeichnungssprachen TBX und TEI genutzt werden.
          Es ist vorgesehen die genutzten Daten in die Eweiterung von Standards für die
          Kombination von TBX und TEI einfließen zu lassen. Die Ergebnisse sollen in
          Kooperation mit Roberto Theron (Universidad de Salamanca)
          visualisiert werden. Einblicke in die Arbeit, aber auch praktische Realia wie
          Brotrezepte sollen auf dem Projektblog http://brot.linguence.de (Siemund 2015-*) veröffentlicht werden, um die
          Forschung nicht der Wissenschaft exklusiv zu halten, sondern auch interessierte
          Laien partizipieren zu lassen und sich im Citizen Science-Kontext des Projekts
          „exploreAT!“ einzubetten. Das Blog ist im Rahmen des 10. World Bread Day am
          16.10.2015 mit einem Rezept für Kletzenbrot online gegangen und hat sich bei einer
          Bloggerinitiative (zorra
          25.09.2015) zu selbigem Tag beteiligt, um möglichst publikumswirksam zu starten. Es ist anvisiert, die
          Forschungsergebnisse in verschiedene europäische Initiativen und Netzwerke
          einzubetten. Eigentlich nicht erwähnt werden muss, dass die Forschungsergebnisse
          open access unter creative commons-Lizenz zur Verfügung gestellt werden sollen. 
         Im Fokus des Posters liegen zum einen die technischen Vorgehensweisen, zum anderen die Methoden des Citizen Science.
      
      
         
            
               Bibliographie
               
                  Burgstaller, Ernst / Wolfram, Richard / Helbok, Adolf
                (eds.) (1959-1981): Österreichischer Volkskundeatlas.
                Österreichische Akademie der Wissenschaften. 
               
                  Siemund, Melanie (2015-*): explore.bread.AT!. Österreichische Akademie der Wissenschaften,
                  Austrian Centre for Digital Humanities http://brot.linguence.de/
                  [letzter Zugriff 08. Januar 2016]. 
               
                  Wandl-Vogt, Eveline (2015): exploreAT! exploring austrias culture through the language glass
                  https://acdh.oeaw.ac.at/dha/de/node/78 [letzter Zugriff 17.
                    Februar 2016].
               
                  zorra (25.09.2015): "World Bread Day 2015 – Invitation
                      / Einladung", in: 1x umrühren aka kochtopf. Food-Blog
                      http://www.kochtopf.me/world-bread-day-2015-invitation-einladung
                        [letzter Zugriff 08. Januar 2016]. 
            
         
      
   



      
         
            Einleitung
            Seit 2014 arbeitet das Projekt „Repositorium Steirisches Wissenschaftserbe“ an der digitalen Erschließung, Archivierung und Veröffentlichung von für den Regionalraum Steiermark bedeutsamem Quellenmaterial. Das Projekt vereint nicht nur zahlreiche Konsortialpartner sondern auch unterschiedliche Quellengattungen, von Museumsobjekten, über Handschriften, bis zu Postkarten oder historischen Glasdias. Die homogene Beschreibung dieser Mischung aus objekt-, bild- und textzentrierten Ressourcen ist eine Herausforderung. Das Poster zeigt die ersten Zwischenergebnisse des Projektes. Dabei wird nicht nur der Kernteil der digitalen Erschließung, Langzeitarchivierung und Dissemination vorgestellt, sondern auch Aspekte, denen im Vorhinein oft weniger Beachtung geschenkt wird, die aber für einen erfolgreichen Projektverlauf ebenso zentral sind.
         
         
            Fachlich-inhaltliche Aspekte
            In einem ersten Schritt wurden disziplinenspezifische Fragestellungen an
          Einzelbestände eruiert, sowie die Funktionalitäten der geplanten gemeinsamen
          Webplattform und die dafür benötigten Daten mit den Partnern festgelegt. Als
          größte Herausforderung aus der Sicht der Digital Humanities ergab sich dabei die
          Homogenisierung der Metadaten: Die Inhalte der unterschiedlichen Objekttypen
          werden im Projekt zunächst mit domänenspezifischen und international anerkannten
          XML-Metadatenstandards (etwa TEI, LIDO, EAD) beschrieben. Die Datenerfassung und
          Erschließung erfolgt dabei nach eigens entworfenen bestands- und
          disziplinenübergreifenden Richtlinien, um eine möglichst konsistente Datenbasis
          zu schaffen. Das umfasst auch die Verwendung von kontrollierten Vokabularien und
          Thesauri (z. B. Geonames, AAT, GND) für die semantische Anreicherung der
          Quellen. Für das gemeinsame Portal werden aus dieser Datenbasis festgelegte
          Metadaten-Kernkategorien (Person, Ort, Zeit, Objekttyp, Medientyp und
          Kurzbeschreibung) extrahiert und auf Dublin Core und Europeana Data
          Model-Kategorien gemappt. Dieser Ansatz soll der Diversität der Quellen Rechnung
          tragen und eine qualitativ hochwertige Einzelbeschreibung mit generischen
          Beschreibungskategorien verbinden, die die Grundlage für einen gemeinsamen
          Suchraum bilden. Die Langzeitarchivierung der digitalen Forschungsdaten erfolgt
          in bereits vorhandenen institutionellen Repositorien der universitären Partner.
          1
            
         
         
            Finanzielle, organisatorische und rechtliche Aspekte
            Kooperationen zwischen Universitäten und Gedächtnisinstitutionen sind oft schwierig formal abzusichern. Neben der inhaltlichen Bereitschaft zur Partnerschaft müssen sich alle Beteiligten zunächst auch auf ein gemeinsames Geschäftsmodell einigen. Während an den wissenschaftlichen Forschungseinrichtungen immer mehr auf Open Access gesetzt wird, sind Gedächtnisinstitutionen traditionell stärker auf persönliche BesucherInnen fokussiert, und sehen die Onlinepräsentation ihrer Sammlungen als potentielle Konkurrenz zu analogen Ausstellungen. Da die meisten Partnerinstitutionen öffentlich gefördert werden und öffentliches Gut verwahren, ist ein freier Zugang zu den Online-Ressourcen jedoch wünschenswert. In das entstehende gemeinsame Projektportal werden daher per Definition nur kostenfrei zugängliche digitale Objekte aufgenommen.
            Doch nicht nur finanzielle Interessen können für einen beschränkten Zugang zu Ressourcen verantwortlich sein sondern auch rechtliche Aspekte. Archive müssen gesetzliche Sperrfristen beachten, Fragen des Datenschutzes sowie Persönlichkeitsrechtes spielen oft bei Korrespondenzen und Bildsammlungen eine Rolle. Selbstverständlich müssen alle Institutionen auf die Urheberrechte der WerkschöpferIn Rücksicht nehmen.
            Während fachlich-inhaltliche Nachhaltigkeit durch bereits vorhandene Infrastruktur und entsprechendes Know-How bei den jeweiligen Partnern relativ mühelos umgesetzt werden kann, ist finanzielle und organisatorische Nachhaltigkeit schwieriger sicherzustellen. Gerade bei institutionsübergreifenden Projekten mit begrenzter Laufzeit stellt dies ein Problem dar. In diesem Fall kann die Projektarbeit nur als Anschubfinanzierung für den Aufbau einer Infrastruktur und von Arbeitsabläufen verstanden werden, die darauffolgend in den Regelbetrieb übergehen sollten.
         
         
            Kooperativ-kommunikative Aspekte
            Differenzen zwischen Wissenschafts- und Gedächtnisinstitutionen können nur durch kontinuierliche Gespräche überbrückt, Ziele und Lösungsansätze nur gemeinsam erarbeitet und umgesetzt werden. Spezielle Begrifflichkeiten, die auf divergierende disziplinäre Hintergründe zurückzuführen sind, sind von Beginn an explizit zu machen. Vorstellungen und Erwartungen bezüglich des Projekt-Ergebnisses oder der visuellen Umsetzung unterscheiden sich häufig gravierend. Das zentrale Augenmerk der Digital Humanities muss hier auf der Vermittlerrolle liegen: Die Kommunikation mit den unterschiedlichen Institutionen, Disziplinen und FachwissenschaftlerInnen erfordert oft Fingerspitzengefühl, da nicht nur die Kooperation selbst hinterfragt wird, sondern auch die Vorteile der digitalen Komponente des Projektes. 
         
         
            Ausblick
            Speziell in den Geisteswissenschaften darf nicht vergessen werden, dass viele der zentralen Forschungsobjekte in BAM-Institutionen gepflegt und aufbewahrt werden. Unter diesem Gesichtspunkt ist die enge Kooperation zwischen der (digitalen) Wissenschaft und den Gedächtnisinstitutionen nicht nur wünschenswert sondern unabdinglich. Die inhaltliche, organisatorische und technische Vernetzung birgt Synergien, die im Idealfall über den begrenzten Zeitraum eines Projektes hinausgehen und in nachhaltigere Formen der Kooperation überführt werden. Beide Bereiche profitieren von dieser Zusammenarbeit und können damit nicht zuletzt der Öffentlichkeit auch einen Teil der eigenen Geschichte und Kultur besser vermitteln und zugänglich machen.
            Nachdem im ersten Projektabschnitt die Grundlagen für eine erfolgreiche Zusammenarbeit gelegt und die Daten der Partner nach den festgelegten Richtlinien erfasst und angereichert wurden, folgt in der nächsten Phase die technische Umsetzung der Projektplattform und des Suchportals.
         
      
      
         
            Die Karl-Franzens-Universität Graz und die
            Kunstuniversität Graz betreiben jeweils eine auf der open source
            Software FEDORA Commons basierende Archivierungslösung: GAMS ( http://gams.uni-graz.at) und
            PHAIDRA ( http://phaidra.kug.ac.at).
         
         
            
               Bibliography
               
                  GAMS (2014-*): GAMS:
              Geisteswissenschaftliches Asset Management System. Zentrum für
              Informationsmodellierung - Austrian Centre for Digital Humanities,
              Karl-Franzens-Universität Graz http://gams.uni-graz.at/ [letzter Zugriff: 08. Januar 2016]. 
               
                  Phaidra (o. J.): Phaidra: Permanent
                Hosting, Archiving and Indexing of Digital Resources and Assets.
                Universität Wien, Bibliotheks- und Archivwesen https://phaidra.kug.ac.at/
                [letzter Zugriff: 08. Januar 2016]. 
            
         
      
   



      
         
            Textarchiv und Umfang der Studie
            Das DFG-geförderte Projekt „DeutschesTextarchiv“ (DTA) der
          Berlin-Brandenburgischen Akademie der Wissenschaften (BBAW) stellt
          deutschsprachige Drucke als Bilddigitalisate und TEI-XML-annotierte Volltexte
          aus mehr als 300 Jahren, vom Beginn des 17. bis zum frühen 20. Jahrhundert, über
          das Internet zur freien Nutzung bereit. Das DTA mit seinen Erweiterungskorpora
          umfasst derzeit knapp 2800 Dokumente mit mehr als 630 000 digitalisierten Seiten
          und ca. 1,1 Mrd. Zeichen (Stand: 7.10.2015). Neben dem Anspruch, vielseitig
          nutzbare und qualitativ hochwertige Primärquellen frei verfügbar zumachen, liegt
          der Fokus des DTA-Projekts auf der korpus- bzw. computerlinguistischen Analyse
          der elektronischen Volltexte. Alle Quellen stehen in verschiedenen Formaten zum
          Herunterladen und auch zum „Harvesten“ über eine API bereit (BBAW 2007-2015). 
            Im Rahmen des DTA wurden in den letzten beiden Jahren verschiedene automatische und semiautomatische Ansätze zur Erkennung von Personen- und Ortsnamen evaluiert. Immer wieder bedeuteten dabei die Heterogenität des Korpus und die große sprachliche Varianz innerhalb des Textkorpus eine Herausforderung für die Tools: Je früher die Entstehungszeit eines Textes liegt, desto größer werden sprachliche und sachliche Differenz bei der Benennung von Eigennamen. Der Fokus wird im Folgenden auf Ortsnamen in historischen Texten des DTA liegen.
            Das Ziel der Studie besteht darin, die Verteilung der im DTA erwähnten Ortsnamen darzustellen, um ein synthetisches Bild der Sammlung zusammenzustellen und gleichzeitig Rückschlüsse auf den Inhalt zu ermöglichen. Sie erfolgt im Rahmen einer Kooperation zwischen der Berlin-Brandenburgischen Akademie der Wissenschaften (Zentrum Sprache) und der Österreichischen Akademie der Wissenschaften (ICLTT, Institut für Corpuslinguistik und Texttechnologie), beide Zentren verfügen über digitalisierte historische Textkorpora.
         
         
             Erkennung von Ortsnamen
            Spezialisierte Werkzeuge aus dem Gebiet der Computerlinguistik werden im Rahmen
            dieser Studie eingesetzt. Erstens wird für die Tokenisierung (Segmentierung in
            Wortformen) die Software WASTE (Jurish / Würzner 2013) benutzt, die speziell für
            Texte verschiedener Epochen im Rahmen des DTA entwickelt worden ist. So lassen
            sich Sprachqualitäten besser annähern, die von den heutigen Standards
            abweichen.
            Die deutsche Version des „Wikiwörterbuchs“ Wiktionary der Wikimedia Stiftung, das von Internetnutzern gepflegt wird, wird verwendet, um lexikalische Informationen über Wörtern zu sammeln. Ziel dieses Vorgehens ist es unter anderem, solche Token zu erkennen, die mit Sicherheit keine Eigennamen sind. Ein weiteres mögliches Problem besteht bei Eigennamen, die keine Ortsnamen sind, jedoch aus verschiedenen Gründen als solche ausgezeichnet worden sind, u. a. Namen von bekannten Autoren so wie fiktive Namen und Vornamen. Listen werden also benutzt, um bereits bekannte Eigennamen auszugrenzen.
            Die Erkennung von Ortsnamen beruht oft auf Verfahren aus der künstlichen
              Intelligenz sowie named-entity recognition (Leidner / Lieberman 2011).
              Wissensbasierte Methoden zeigen jedoch auch vielversprechend Ergebnisse, so wie
              zum Beispiel anhand von Datenbanken aus Wikipedia (Hu et al. 2014).
            Unsere Erkennung der Ortsnamen erfolgt über Datenbanken, die die Vorteile von geisteswissenschaftliche Sorgfalt und Opportunismus aus Big Data Herangehensweisen kombiniert. Über ein gleitendes Fenster wird nach Treffern (einschließlich Mehrwortausdrücken) gesucht. Aus der passenden Datenbank werden Koordinaten und gegebenenfalls weitere geographisch relevante Informationen extrahiert, diese Daten werden wiederum in einer weiteren für das gesamte Verfahren angelegten Datenbank zusammengefasst. Falls mehrere Möglichkeiten bestehen, ist ein Disambiguierungsverfahren nötig, das Informationen wie Distanz, Kontext und aktuelle Bevölkerungszahlen benutzt.
            Die Erkennung erfolgt über die Durchsuchung von Listen unterschiedlichen Ranges:
                als Erstes wird nach aktuellen sowie ehemaligen Ländern und vergleichbaren
                Hoheitsgebieten gesucht (z. B. Österreich-Ungarn), dann wird die Suchanfrage um
                Regionen oder regionale Landschaften erweitert (z. B. Schwaben), bei einem
                negativen Ergebnis wird anschließend nach Städten und schließlich nach
                geographischen Merkmalen wie Flüssen oder Bergen gesucht. Die dafür nötigen
                Informationen wurden zum Teil händisch (Staaten und Regionen) und zum Teil
                automatisch gesammelt und händisch zusammengefasst oder überprüft (Städte und
                Geographie). Da mancherorts die Staatsgrenzen bis ins 20. Jahrhundert instabil
                geblieben sind und da gewisse Staaten sich durchaus als multinational verstehen
                lassen, wurden insbesondere für Mitteleuropa Listen einschließlich der aktuellen
                oder ehemaligen deutschen Namen erstellt, u. a. anhand von bereits im Web
                auffindbaren Listen wie zum Beispiel Kategorien oder Listen von Wikipedia.
            Jedem eindeutigen Ortsnamen wurden dann Koordinaten hinzugefügt, entweder durch automatische Abfrage von Wikipedia und Wikidata oder händisch unter Heranziehungen historischer Beschreibungen oder Atlanten. Bei politischen Entitäten wurde bisher Europa im 19. und 20. Jahrhundert in Betracht gezogen. Die Listen werden regelmäßig erweitert, sie umfassen derzeit 78 Hoheitsgebiete, 858 Regionen, 9.846 Städte und 13.962 geographische Merkmale.
            Wenn kein Treffer in den Listen gefunden wird, werden größere, automatisch erstellte Ortsregister in Betracht gezogen. Geographische Informationen über Orte stammen dann aus den Geonames-Datenbanken, die zum Beispiel von dem Openstreetmap Projekt benutzt werden, und dessen Creative Commons Attribution Lizenz eine Wiederverwendung der Daten ermöglicht. Alle Datenbanken für aktuelle europäische Länder sind gesammelt und verarbeitet worden: gewisse Ortstypen (nämlich Region und bewohnter Ort) sind ausgewählt worden, und existierende Varianten in diversen europäischen Alphabeten sind extrahiert worden, um mögliche Änderungen im Laufe der Geschichte zu reflektieren.
         
         
            Projektion auf einer Karte
            Schließlich werden die Ergebnisse auf eine Karte Europas projiziert, die die tatsächliche politische Lage dieser Zeit spiegelt. Dafür werden die Grenzen von 1914 gezeigt. Der quantitative Schwerpunkt des Korpus liegt nämlich auf dem 19. Jahrhundert liegt und der Stand vor dem ersten Weltkrieg gibt ein vernünftiges Bild von Europa während des 'langen 19. Jahrhunderts'. Die Qualität der Daten so wie des graphischen Resultats wurde in mehreren Durchläufen geprüft, dabei wurden jeweils verbliebene Fehler eliminiert: die Karte bzw. die Projektion der Daten wird so sukzessive verbessert und feiner justiert.
            Zur Projektion wird die Kartographieumgebung TileMill benutzt, die eine Anpassung anhand der Stylesheet-Sprache CartoCSS ermöglicht. So können wichtige Punkte im Graphen hervorgehoben werden. Die Wahl verschiedener Farben erleichtert den Überblick über das visualisierte Ergebnis und dessen Interpretation, da im Feld der Visualisierungsstudien bekannt ist, dass das menschliche Auge instinktiv unterscheiden und klassifizieren kann (Bertin 1967).
         
         
            Karte
            
               
            
            Karte von Europa in den Grenzen von 1914 (Hoheitsgebiete in Gelb, Regionen in Orange, Städte sowie aus Geonames extrahierte Ortsnamen in Grün, geographische Merkmale in Blau).
            
               
            
            Zoom: Karte von Mitteleuropa in den Grenzen von 1914 (Hoheitsgebiete in Gelb, Regionen in Orange, Städte sowie aus Geonames extrahierte Ortsnamen in Grün, geographische Merkmale in Blau).
         
         
            Diskussion
            Wir hegen die Hoffnung, dass solche Visualisierungsstudien den Weg nach einer größeren Sichtbarkeit von digitalem Kulturerbe und von literarischer Forschung im digitalen Zeitalter ebnen. Genauer betrachtet glauben wir, dass detailreiche Annäherungsweisen gefragt werden, die sowohl auf technischer Kompetenz als auch auf historisches und literarisches Wissen aufbauen. In diesem Sinne planen wir, mehr Metadaten einzubeziehen sowie vielseitige Visualisierungen zu erzeugen.
            Es sollte immer berücksichtigt werden, dass die linguistischen Korpora, die als Basis für die Karte benutzt werden, immer schon ein Konstrukt sind, woraus folgt, dass die auf diesen Daten basierenden Projektionen ebenso Konstrukte sind: Auch wenn sie unmittelbar interpretierbar scheinen, spielen Qualität der Daten, Spezialisierungsgrad der Verarbeitungskette und Qualitätsprüfung eine maßgebende Rolle. Deswegen sind wir der Meinung, dass eine gewisse Dekonstruktion des Prozesses nötig ist, im Sinne einer Öffnung der black box, die dem Betrachter das originelle Moment der Entzückung vielleicht wegnimmt, aus wissenschaftlicher Sicht jedoch wünschenswert ist. So möchten wir keine Fehler kaschieren, eventuelle Verzerrungen nicht verschweigen, und für die Reproduzierbarkeit des gesamten Prozesses sorgen, einerseits durch detailreiche Dokumentierung des Prozesses, andererseits durch die Herausgabe möglichst aller dabei verwendeten Tools und Komponenten als Open Source Software.
            So können unmögliche, in diesem Kontext falsche Verbindungen vermieden werden: es
                  gibt zum Beispiel einen Ort namens "Hermann" in Norwegen, was Fakt und Datum
                  zugleich ist. Es ist jedoch nötig, sich nicht allein auf diese Datengrundlage zu
                  verlassen, wenn man nach Orten sucht, sonst wird das Endprodukt – d. h. die
                  Karte – verfälscht.
            Interessanterweise bietet die Karte für einen möglichen Hermeneuten genau diese Fälle in Perspektive, die Existenz eines möglicherweise falschen Knotens bleibt nicht unbemerkt und wirft Fragen auf. Auf dieser Weise ist uns zum Beispiel ein systematischer Fehler mit den Vornamen aufgefallen. Durch diese hermeneutische Schleife wird die Analyse nach und nach verschärft.
            Die Säuberung der Daten ist in dieser Hinsicht entscheidend, die Anzahl und Vielfalt von Filtern, die eingesetzt werden, erheben unsere Arbeit von einer massiven Datensammlung und -Analyse auf das Niveau einer Studie in Digital Humanities, die Rücksicht auf Besonderheiten einer Sprache und einer Epoche nimmt.
         
      
      
         
            
               Bibliographie
               
                  BBAW (2007-2015): Deutsches
                      Textarchiv. Berlin: Berlin-Brandenburgische Akademie der
                      Wissenschaften http://www.deutschestextarchiv.de/ [letzter Zugriff 30. Januar
                      2016]. 
               
                  Bertin, Jacques(1967): Sémiologie
                        graphique. Paris: Mouton / Gauthier-Villars. 
               
                  Hu, Yingjie / Janowicz, Krzysztof / Prasad, Sathya
                          (2014): „Improving Wikipedia-Based Place Name Disambiguation in Short Texts
                          Using Structured Data from Dbpedia“, in: Proceedings of
                          the 8th ACM Workshop on Geographic Information Retrieval, Dallas,
                          Texas 8–16. 
               
                  Jurish, Bryan / Würzner, Kay-Michael (2013): „Word and
                            Sentence Tokenization with Hidden Markov Models“, in: Journal for Language Technology and Computational Linguistics 28,
                            2: 61–83.
               
                  Leidner, Jochen L. / Lieberman, Michael D. (2011):
                              „Detecting Geographical References in the Form of Place Names and Associated
                              Spatial Natural Language“, in: SIGSPATIAL Special 3,
                              2: 5–11. 
            
         
      
   



      
         Dank Bibliotheken und Archiven konnten große Bestände mittelalterlicher Handschriften über die Jahrhunderte erhalten werden. Zur Beantwortung zentraler Fragestellungen der buchhistorischen Forschung werden die Handschriften analysiert und insbesondere die verschiedenen Bestandteile des Layouts – beispielsweise Seitengröße, Schrift- und Bildraum – vermessen. Die meist manuelle Analyse ist jedoch sehr zeit- und arbeitsintensiv, so dass nur eine geringe Anzahl mittelalterlicher Handschriften auf diese Art und Weise untersucht werden kann. Mit Hilfe digitaler Methoden und Werkzeuge können vorhandene Digitalisate der Handschriftenseiten automatisch oder halbautomatisch ausgewertet werden. Im Gegensatz zum manuellen Ansatz kann in diesem Fall mit einer signifikanten Verbesserung im Hinblick auf Schnelligkeit, Genauigkeit sowie Reproduzierbarkeit gerechnet werden. Bisher mangelt es jedoch an grafischen Oberflächen, die die entstehenden hochdimensionalen Metadaten großer, elektronisch erfasster Bestände dynamisch visualisieren können. Als Mehrwert wird es Geisteswissenschaftlern ermöglicht, Zusammenhänge zwischen Handschriften einfach zu erkennen und neue Erkenntnisse aus den Daten zu gewinnen.
        Aus diesem Grund entwickelt das vom BMBF geförderte Verbundprojekt „eCodicology“ (http://www.ecodicology.org) einen Software Workflow zur automatischen Annotation und Visualisierung von makro- und mikrostrukturellen Layoutmerkmalen mittelalterlicher Handschriften.
         In diesem Kontext präsentieren wir das Visualisierungsframework CodiVis, das die Erforschung von Korrelationen im abstrakten Merkmalsraum digitalisierter mittelalterlicher Handschriften vereinfacht und unterstützt. Die Datengrundlage von CodiVis sind mittelalterliche Handschriften, die zwischen dem achten und neunzehnten Jahrhundert in der Bibliothek der Trierer Benediktinerabtei St. Matthias aufbewahrt wurden.
          Der Bestand wurde im Rahmen des Projektes „Virtuelles Skriptorium St. Matthias“ (http://www.stmatthias.uni-trier.de) digitalisiert und mit bibliografischen Metadaten, wie Datierung, Beschreibstoff, Format und inhaltlichen Informationen, in TEI P5 konformen XML-Dateien angereichert. Nach Einspeisung der Digitalisate in das Datenrepositorium CodiStore können mit Hilfe von SWATI (Software Workflow for the Automatic Tagging of Images) verschiedene Layoutmerkmale der Handschriftenseiten bestimmt sowie die bibliografischen Metadaten extrahiert werden. 
         
            
            
               Abb. 1: CodiVis Prototyp, der beide Metadatenarten zur
            Visualisierung nutzt.
         
         Für einen schnellen Überblick über den gesamten Datenbestand und eine gleichzeitige Darstellung der zugehörigen Handschriftendetails wird eine Kombination zweier Visualisierungsformen angeboten. Auf der linken Seite ist der Bestand mit Hilfe eines radialen Baumdiagramms illustriert, geordnet nach dem Jahrhundert der Entstehung. Auf der rechten Seite werden die extrahierten Merkmale Schriftraum, Seitenhöhe und Seitenbreite mittels paralleler Koordinaten dargestellt. Die verschiedenen Linien repräsentieren dabei die spezifischen Ausprägungen der Layoutmerkmale einzelner Handschriften. Zur Untersuchung der Korrelationen werden Markierungen im Radialbaum automatisch in die Ansicht der parallelen Koordinaten übernommen. 
         Bisherige Evaluationen des Visualisierungsframeworks zeigen, dass der überwiegende Teil der Nutzer durch die interaktive Zugangsweise erfolgreich Zusammenhänge zwischen ähnlichen Handschriften, fehlerhafte Informationen und Ausreißer erkennen konnte. Darüber hinaus eröffnet CodiVis neue Fragestellungen im Hinblick auf die Visualisierung von Unsicherheiten in den bibliografischen Daten sowie in den automatischen Messungen, die in einem nächsten Schritt zusätzlich zu Visualisierungsmöglichkeiten einzelner Seiten integriert werden. Insgesamt können durch CodiVis neue Möglichkeiten der intuitiven Erkundung historischer Daten aufgezeigt werden. 
         Die Präsentation gibt einen Einblick in die Entwicklung und Benutzung von CodiVis im
            Rahmen des Projektes eCodicology mit einem Ausblick auf die mögliche Weiternutzung
            mit anderen Beständen. Zum Ende der Projektlaufzeit ist die Veröffentlichung als
            Teil des DARIAH Portals geplant.
      
      
         
            
               Bibliographie
               
                  DARIAH: DARIAH-EU. Digital
                Research Infrastructure for the Arts and Humanities http://www.dariah.eu/ [letzter
                Zugriff 11. Februar 2016].
               
                  Technische Universität Darmstadt / KIT Karlsruhe /
                  Universität Trier (2014): eCodicology.
                  Algorithmen zum automatischen Tagging mittelalterlicher Handschriften http://www.ecodicology.org
                  [letzter Zugriff 16. Februar 2016].
               Kompetenzzentrum für elektronische Erschließungs- und
                    Publikationsverfahren in den Geisteswissenschaften (2010-2014): Virtuelles
                    Skriptorium St. Matthias. Der mittelalterliche Bibliotheksbestand der
                    Trierer Abtei St. Matthias digital im Netz. Universität Trier: http://www.stmatthias.uni-trier.de/index.php [letzter Zugriff 16.
                    Februar 2016].
            
         
      
   



      
         
            Forschungskontext
            Im Rahmen des Projekts „travel!digital“
            1 wird erstmals der Versuch unternommen, die vielfältige Terminologie in historischen Reiseführern systematisch zu erschließen. Der Fokus auf
            Menschen und
            Monumente stellt zwei dominante semantische Felder in den Mittelpunkt, die sowohl Schlüsselelemente der Textsorte Reiseführer als auch wesentliche Komponenten kultureller Narrative darstellen. Damit sind Reiseführer keineswegs bloße historische Quellen, sondern vielmehr als bedeutende diskurshistorische Artefakte
            2 zu betrachten. Zudem eignet sich das in ihnen enthaltene enzyklopädische Wissen in besonderem Maße für die Datenmodellierung. Vor diesem Hintergrund erstaunt das Fehlen systematischer diachroner Forschung auf diesem Gebiet mindestens genauso wie der eklatante Mangel an digitalen Ressourcen, die für historische Untersuchungen geeignet wären.
            3
            
            Mit dem Baedeker Corpus, einer digitalen Sammlung
              deutschsprachiger Reiseführer aus dem Zeitraum 1875–1914, zielt das Projekt
              nicht nur darauf ab, wertvolles kulturelles Erbe mit den Methoden der Digital Humanities nachhaltig und langfristig
              sicherzustellen, sondern besonders darauf, semantische Technologien verstärkt
              zur Erforschung des deutschsprachigen Repertoires kultureller Diskurse an der
              Wende vom 19. zum 20. Jahrhundert einzusetzen. Als „komplexe Intertexte“ (vgl.
              Wierlacher 1997; Koshar 2000), die dominante Diskurse (re)produzieren bzw.
              (re)konstruieren, stellen Reiseführer „kodifizierte und autorisierte Versionen
              lokaler Kultur und Geschichte“ dar (vgl. Jaworski / Pritchard 2005). Die in den
              Reiseführern transportierten zeitgenössischen Lesarten zu Tourismus und
              kulturellem Erbe sowie Orientalismus und kolonialem Diskurs sind kultur- und
              diskurshistorisch von besonderem Interesse. Darüber hinaus lassen sich
              strukturell, linguistisch und insbesondere semantisch erschlossene digitale
              Reiseführer im Rahmen vergleichender literaturwissenschaftlicher Forschung, der
              historischen Lexikographie und Linguistik, der historischen Geographie und
              Kulturanthropologie nutzen. 
            Das vorliegende Korpus vereint alle Erstauflagen zu
              außereuropäischen Reisezielen aus dem Hause Baedeker, die vor der Zäsur des
              Ersten Weltkriegs erschienen sind. Es umfasst mehr als 1,5 Mio. running words und deckt eine Vielzahl an Regionen ab.
                4 Mit Blick auf differenzierte Analysemöglichkeiten stehen neben der
                linguistischen Basis-Annotation der Volltexte (Lemmatisierung, Part-of-Speech-Tagging) der Aufbau kontrollierter Vokabulare und deren
                Anbindung an LOD-Ressourcen im Mittelpunkt. Die semantischen Repräsentationen
                werden mithilfe des RDF-basierten Simple Knowledge
                Organization System SKOS und dessen Erweiterung SKOS-XL realisiert, mit dem sich auch ambige lexikalische Einheiten
                und nicht-hierarchische Relationen sinnvoll organisieren lassen. Zur Erstellung
                der SKOS-Repräsentation des Baedeker
                  Corpus wird der OpenSKOS Editor 5 eingesetzt; eine Entwicklung des Meertens
              Institute, die bereits für CLAVAS (CLARIN OpenSKOS
              Vocabulary Service) verwendet wird, das u. a. die ISO-639-3 language codes im SKOS Format
              enthält. Auch die DARIAH-EU Arbeitsgruppe Thesaurus Maintenance verwendet OpenSKOS für eine erste Version der SKOS-Repräsentation des in Entwicklung befindlichen Backbone Thesaurus. Der OpenSKOS Editor sichert
              daher die Kompatibilität mit aktuellen Standards und Entwicklungen. 
         
         
            bdk:ConceptScheme(s)
            Neben Personennamen beinhalten Reiseführer auf Seiten der
                Menschen eine Vielzahl an Gruppenbezeichnungen, die generische Referenzen und als solche, Subjekte charakterisierender Eigenschaftszuschreibungen darstellen (vgl. Schmidt-Brücken 2015). Diese für diskurshistorische Analysen relevanten Belege des Sprachgebrauchs werden jeweils von einem
                bdk:Descriptor, einer Unterklasse von
                skos:Concept, repräsentiert. Ihnen zugeordnet sind einzelne Terme als
                skosxl:prefLabel und
                skosxl:altLabel. Mithilfe der
                Properties hasTranslation / isTranslationOf und
                hasVariant / isVariantOf finden die in den Reiseführern enthaltenen Übersetzungen und Varianten auf Ebene der Terme Berücksichtigung. Auf diese Weise wird der Wortschatz vollständig erfasst und in einem
                skos:ConceptScheme zusammengefasst. Der Strukturierung dieses
                bdk:ConceptSchemeGroups dienen sechs mit
                skos:topConceptOf zugeordnete Kategorien: 1) allgemeine Sammelbegriffe, 2) ethnisch/nationale Gruppen, 3) geographische Konzepte im weitesten Sinne, 4) Berufsgruppen, wozu auch politische, religiöse und wirtschaftliche Funktionen sowie Lebensstile zählen, 5) Religionsgemeinschaften und 6) soziale Gruppierungen. Aufgenommen werden als eigene Konzepte nicht nur Nomen, sondern auch Adjektive, wobei die entsprechenden Konzepte mit
                skos:related aufeinander bezogen sind. Abbildung 1 listet die
                skos:topConcept(s) und ihre Definitionen auf und gibt einige Beispiele der ihnen zugeordneten Konzepte und Labels sowie deren Beziehungen.
              
            
               
               
                  Baedeker Group Taxonomy. Farbliche Hervorhebungen Spalte 2: Konzepte, die in Beziehung zu anderen Konzepten stehen; farbliche Hervorhebungen kursiv Spalte 3 und 4:
                  Properties für Übersetzungen und Varianten auf Ebene der Terme.
                
            
            Ein ähnlich vielfältiges Bild ergibt sich im Bereich der Monumente und Sehenswürdigkeiten, die als Gegenstand wertender Beschreibung und Einordnung eine zentrale Rolle in Reiseführern einnehmen. Aufgrund ihrer Vielzahl werden zunächst nur jene Sehenswürdigkeiten strukturiert, die mit Baedeker-Sternen als besonders sehenswert gekennzeichnet sind. Das Spektrum reicht von Architektur
                6 und Kunst
                7 bis zu Unterkünften, Landschaften und atemberaubenden Aussichten. Diese Kategorien strukturieren jeweils als
                skos:topConcept(s) das
                bdk:ConceptSchemeMonuments. Derzeit wird an einer Lösung gearbeitet, die es erlaubt, zwischen profaner und sakraler Kunst und Architektur zu unterscheiden und, sofern sakral, die jeweilige Religion zuzuordnen.
              
         
         
            Ausblick
            Die linguistische Annotation des
                Baedeker Corpus und die Erstellung der vorgestellten kontrollierten Vokabulare stellen die Voraussetzung für eine weitergehende systematische Analyse der Textsorte Reiseführer dar. Neben der Identifikation der in den untersuchten Baedeker-Bänden genannten historischen Persönlichkeiten mithilfe der
                Virtual International Authority Files und der
                Deutschen Biographie erscheint insbesondere die Anbindung der hier beschriebenen Taxonomien an Ressourcen wie die
                DBpedia, den
                GESIS Thesaurus Sozialwissenschaften, den
                AAT Art & Architecture Thesaurus des
                Getty Research Institute oder den
                UNESCO Thesaurus erfolgversprechend, zumal die Verschränkung der lexikalischen Bestandsaufnahme mit der Kontextualisierung dieser externen Quellen neue Perspektiven auf den Text zu eröffnen vermag. Vor allem aber kann das hier vorgestellte Datenmodell die granulare Analyse jener semantischen Komponenten unterstützen, die das Sprechen über sowohl das ‚Fremde‘ als auch das ‚Eigene‘ bestimmen, und erschließt somit mit rezenten Technologien die Funktionsweise eines Diskurses, dessen Wirkungsgrad bis heute weit über das Feld der Reiseliteratur hinausreicht.
              
            Die digitalen Texte inklusive der Faksimiles und die SKOS-Vokabulare werden zu Projektende in Form einer Web-Applikation
                8 zur Verfügung gestellt. Die Navigations- und Abfragemöglichkeiten sowohl in den Volltexten als auch der linguistischen Annotation werden ergänzt durch Register der Wortformen, der Lemmata sowie der semantischen Komponenten. Die vorgestellten SKOS-Vokabulare zu
                  Menschen und
                  Monumenten fungieren zudem als Verbindung der konkreten Belegstellen im
                  Baedeker Corpus und externen LOD-Ressourcen.
                
         
      
      
         
            Das Projekt „travel!digital. Exploring
                  People and
                  Monuments in Baedeker Guidebooks (1875–1914)” wird im Rahmen der Plattform
                  
                  Digital Humanities Austria
                gefördert.
             Dominique Maingueneau (2014: 437) nennt Reiseführer explizit als
                Vertreter sogenannter Diskursgenres, „[…] das
                heißt, soziohistorisch variierende Kommunikationsdispositive“.
            Gründe dafür finden sich u. a. darin, dass die Textwissenschaften in der
                Vergangenheit stets der Reiseliteratur den Vorrang gegenüber den
                Gebrauchstextsorten eingeräumt haben. Koshar, der das Fehlen einer
                allgemeinen Geschichte des Genres beklagt, verweist auf den schlechten
                Ruf der Reiseführer und bringt dies u. a. damit in Zusammenhang, dass
                die Textsorte äußerst variabel und daher konzeptuell schwer fassbar ist
                (Koshar 2000: 15-16). Eine Einschätzung, die eineinhalb Jahrzehnte
                später noch immer zutreffend ist. Als erwähnenswerte Ausnahme sei die
                Arbeit von Sabine Müller (2012) genannt. Dass die digitalen
                Geisteswissenschaften bisher kaum zur Verbesserung der Lage beigetragen
                haben, hängt vermutlich damit zusammen, dass die (Retro-)Digitalisierung
                der komplex strukturierten historischen Bände sehr aufwändig ist.
                Derzeit stehen wenige exemplarische Analysen kleiner analoger Korpora
                mit Fokus auf entweder Textsortenmerkmalen (vgl. Gorsemann 1995; Pretzel
                1995; Ramm 2000; Mittl 2007) oder der Entwicklung einzelner Regionen
                (Gorsemann 1995: Island; Pretzel 1995: Rheinland; Forschungsgruppe
                Tüschau 16 1998: Polen; Epelde 2004: Indien; Bock 2010: Rheinland), noch
                wenigeren computerlinguistischen Arbeiten gegenüber. Die letzteren
                basieren zwar auf umfangreicheren digitalen Datenmengen, nachdem jedoch
                ausschließlich rezentes Material herangezogen wird, bleiben historische
                Aspekte unberücksichtigt (vgl. Neumann 2003; Lam 2007; Gandin 2013,
                2014).
             Für die breite Untersuchung kultureller Narrative war es naheliegend,
                Reiseführer zu außereuropäischen Destinationen in das Korpus
                aufzunehmen. Es war Fritz Baedeker, dritter Sohn des Verlagsgründers
                Karl Baedeker und seit 1869 Leiter des Hauses, der das Verlagsprogramm
                um außereuropäische Titel erweiterte: Palästina und Syrien (1875),
                Unter- (1877) und Ober-Ägypten (1891), Nordamerika und Mexiko (1893),
                Konstantinopel und Kleinasien (1905), die afrikanische Mittelmeerküste
                (1909) und Indien und Ceylon (1914). Die strukturelle und deskriptive
                XML-Annotation nach TEI-Richtlinien (Version P5) konnte bereits für das
                gesamte Korpus abgeschlossen werden.
            Aktuell befindet sich der OpenSKOS Editor in
                Überarbeitung. Die neue um SKOS-XL Komponenten
                erweiterte Version soll mit Jahresende zur Verfügung stehen.
            Unterschieden werden: Kapelle, Kirche, Kloster, Mausoleum, Friedhof, Bildungs- und Wissenschaftseinrichtung, Gesundheits- und Sporteinrichtung, Museum, Sammlung, Palast, Theater, Industriebau, Inneneinrichtung, Verkehrsbau, Ensemble, Park.
                
             Unterschieden werden: Denkmal, Skulptur, Gemälde, anderes Kunstwerk, Sammlung.
            Die Applikation basiert auf der 
                  corpus_shell
                (Ďurčo et al.), einem modularen
                Framework für die Publikation von Sprachressourcen. Zum FCS /
                SRU-Protokoll siehe CLARIN ERIC sowie Stehouwer et al. 2012.
         
         
            
               Bibliographie
               
                  Bock, Benedikt (2010): Baedeker &
                    Cook — Tourismus am Mittelrhein 1756 bis ca. 1914. Bern / Berlin /
                    Frankfurt am Main / New York / Paris / Wien: Peter Lang. 
               
                  CLARIN ERIC (o. J.): Federated
                      Content Search (CLARIN-FCS). https://www.clarin.eu/content/federated-content-search-clarin-fcs
                      [letzter Zugriff 10.09.2015]. 
               
                  Digital Humanities Austria: http://clarin.arz.oeaw.ac.at/dha/.
               
                  Ďurčo, Matej / Mörth, Karlheinz / Schopper, Daniel / Siam,
                          Omar (o. J.): corpus-shell. https://clarin.oeaw.ac.at/corpus_shell [letzter Zugriff 10.
                          September 2015]. 
               
                  Epelde, Kathleen R. (2004): Travel
                          Guidebooks to India. A Century and a Half of Orientalism. PhD,
                          University Wollongong http://ro.uow.edu.au/cgi/viewcontent.cgi?article=1195&context=theses
                          [10. September 2015]. 
               
                  Forschungsgruppe Tüschau 16 (1998): Die Darstellung anderer Kulturen. Ermittlung von Stereotypen in
                          deutschen Polen-Reiseführern (der Jahre 1990-1996). Oberhausen: Athena
                          Verlag.
               
                  Gandin, Stefania (2013): “Translating the Language of
                            Tourism. A Corpus Based Study on the Translational Tourism English Corpus
                            (T-TourEC)”, in: Procedia — Social and Behavioral
                            Sciences 95: 325-335. 
               
                  Gandin, Stefania (2014): “Investigating loan words and
                              expressions in tourism discourse: a corpus driven analysis on the BBC-Travel
                              Corpus”, in: European Scientific Journal10, 2: 1-17. 
               
                  Gorsemann, Sabine (1995): Bildungsgut
                                und touristische Gebrauchsanweisung. Produktion, Aufbau und
                                Funktion von Reiseführern. Münster / New York: Waxmann. 
               
                  Jaworski, Adam / Pritchard, Anette  (eds.) (2005): Discourse, communication and tourism. Clevedon:
                                  Channel View Press. 
               
                  Koshar, Rudy (2000): German Travel
                                    Cultures. Oxford: Berg. 
               
                  Lam, Peter Y. W. (2007): “A corpus-driven
                                      lexico-grammatical analysis of English tourism industry texts and the study
                                      of its pedagogic implications in English for Specific Purposes”, in:
                                      Hildalgo, Encarnación / Quereda, Luis / Santana, Juan (eds.): Corpora in the Foreign Language Classroom. Amsterdam
                                      / New York: Rodopi 71-88. 
               
                  Maingueneau, Dominique (2014): „Diskurs und
                                        Äußerungsszene. Zur gattungsspezifischen Kontextualisierung eines
                                        Zeitungsartikels zum unternehmerischen Bildungsdiskurs“, in: Angermüller,
                                        Johannes / Nonhoff, Martin / Herschinger, Eva / Macgilchrist, Felicitas /
                                        Reisigl, Martin / Wedl, Juliette / Wrana, Daniel / Ziem, Alexander (eds.):
                                        Diskursforschung. Ein interdisziplinäres
                                        Handbuch. Bielefeld: transcript 433-453. 
               
                  Mittl, Katja (2007): Baedekers
                                          Reisehandbücher Funktionen und Bewertungen eines Reisebegleiters
                                          des 19. Jahrhunderts (= Alles Buch. Studien der Erlanger Buchwissenschaft
                                          22). Friedrich-Alexander-Universität Erlangen-Nürnberg. 
               
                  Müller, Sabine (2012): Die Welt des
                                            Baedeker. Eine Medienkulturgeschichte des Reiseführers 1830-1945.
                                            Frankfurt / New York: Campus Verlag. 
               
                  Neumann, Stella (2003): Textsorten
                                              und Übersetzen. Eine Korpusanalyse englischer und deutscher
                                              Reiseführer. Bern / Berlin / Frankfurt am Main / New York / Paris / Wien:
                                              Peter Lang. 
               
                  Pretzel, Ulrike (1995): Die
                                                Literaturform Reiseführer im 19. und 20. Jahrhundert.
                                                Untersuchungen am Beispiel des Rheins. Bern / Berlin / Frankfurt am Main /
                                                New York / Paris / Wien: Peter Lang. 
               
                  Ramm, Wiebke (2000): “Textual Variation in Travel
                                                  Guides”, in: Ventola, Eija (ed.): Discourse and
                                                  Community. Doing Functional Linguistics. Tübingen: Gunter Narr
                                                  147-165. 
               
                  Schmidt-Brücken, Daniel (2015): Verallgemeinerung im Diskurs. Generische Wissensindizierung in
                                                    kolonialem Sprachgebrauch. Berlin / München / Boston: Walter de Gruyter. 
               
                  Stehouwer, Herman / Durco, Matej / Auer, Eric Auer /
                                                        Broeder, Daan (2012): "Federated Search: Towards a Common Search
                                                        Infrastructure", in: Proceedings of the Eighth
                                                        International Conference on Language Resources and Evaluation (LREC
                                                        2012): 3255–3259. 
                
               
                  Wierlacher, Alois (1997): „Verfehlte Alterität. Zum
                                                          Diskurs deutschsprachiger Reiseführer über fremde Speisen“, in: Teuteberg,
                                                          Hans Jürgen / Neumann, Gerhard / Wierlacher, Alois (eds.): Essen und kulturelle Identität. Europäische Perspektiven. Berlin:
                                                          Akademie Verlag 498-509. 
            
         
      
   



      
         Auch in Zeiten von „Big Data“ haben relativ kleine, auf eine spezifische Fragestellung hin zugeschnittene und aufbereitete Korpora ihre Bedeutung. In diesem Beitrag beschreiben wir die Aufbereitung eines solchen Korpus für die nachhaltige Langzeitarchivierung und skizzieren die sich daraus ergebenden Möglichkeiten zur explorativen Analyse.
         Das Korpus „Diskurs in der Weimarer Republik“ (DWR) wurde im Rahmen des Projektes
        „Demokratiediskurs 1918-1925“ (Kämper 2014) zur Dokumentation und Analyse des
        sprachlichen Wandels im Umbruch von der Monarchie zur Demokratie erstellt. Es
        umfasst 779 Dokumente im Zeitraum von 1912 bis 1933, davon 641 zwischen 1918 und
        1925. 551 Dokumente sind (u. a.) nach Themenbereich und Textsorte klassifiziert (s.
        Tabelle 1).
         
            
            
               Tab. 1: Themenbereiche und Auswahl an Textsorten im DWR
         
         Ursprünglich wurde das Korpus im Rich-Text-Format (RTF) bzw. MS-Office (DOC) erstellt, und die Metadaten in einer Oracle-Datenbank verwaltet. Im Rahmen des LIS-Projektes „Zentrum für germanistische Forschungsprimärdaten“
          1 wurde das Korpus für die Langzeitarchivierung aufbereitet. Im Einzelnen wurden folgende Schritte durchgeführt:
          
         
            Alignierung und Bereinigung der Metadaten: Die Verknüpfung von Metadaten mit
              Dokumenten war über Dateinamen repräsentiert, die teilweise nicht einheitlich
              enkodiert waren. Diese wurden entsprechend normalisiert, um einen eindeutigen
              Bezug herzustellen. Darüber hinaus wurden die Wertebereiche der einzelnen
              Metadatenfelder von Tippfehlern (z. B. Poitik vs. Politik) und
              Enkodierungsproblemen weitestgehend bereinigt.
            Validierung und Kuration der Datenformate: Die vorhandenen RTF-Versionen und DOC-Versionen wurden mithilfe von Open-Office-Macros in valides RTF transformiert. Zur besseren Nachnutzbarkeit wurde zusätzlich mit Hilfe des TEI Open-Office Pakets
                teioop5 eine valide TEI-P5-XML-Version erstellt, die mit Metadaten für Autor, Titel und Erscheinungsjahr angereichert wurde. Zudem wurde auch eine PDF-Leseversion erzeugt.
              
            Extraktion zusätzlicher Metadaten: Die in den Dokumenten vorhandenen bibliographischen Quellenangaben wurden mit Hilfe heuristischer Regeln extrahiert und in die Metadaten integriert.
            Generierung von CMDI-Metadaten: Die Metadaten wurden in das CLARIN-Metadatenframework CMDI (Broeder et al. 2011) transformiert.
         
         Das aufbereitete Korpus
              2 ist im Langzeitarchiv des IDS
              3 (Fankhauser et al. 2013) abgelegt.
              
         Zur Exploration sprachlicher Variation im Korpus wurde das Korpus zudem für ein am Institut für Deutsche Sprache entwickeltes System zur kontrastiven Visualisierung von Korpora (Fankhauser et al. 2014a, 2014b) aufbereitet.
         Dafür wurde das Korpus an Hand der Metadaten für Themenbereiche und Textsorten in Teilkorpora aufgeteilt, und für die einzelnen Teilkorpora Frequenzlisten aller Wörter (ohne Lemmatisierung oder Stopwortausschluss) erstellt. Diese Frequenzlisten, repräsentiert als multinomiale Verteilungen über das Vokabular, werden mit Hilfe der Kullback-Leibler Divergenz verglichen. Auf dieser Basis wird die Distanz zwischen Teilkorpora in Form von Heatmaps visualisiert, und der Beitrag einzelner Wörter zu der jeweiligen Distanz mit Hilfe von Wortwolken.
         Zur Exploration sprachlicher Variation im Korpus wurde das Korpus zudem für ein am Institut für Deutsche Sprache entwickeltes System zur kontrastiven Visualisierung von Korpora (Fankhauser et al. 2014a, 2014b) aufbereitet.
         Dafür wurde das Korpus an Hand der Metadaten für Themenbereiche und Textsorten in Teilkorpora aufgeteilt, und für die einzelnen Teilkorpora Frequenzlisten aller Wörter (ohne Lemmatisierung oder Stopwortausschluss) erstellt. Diese Frequenzlisten, repräsentiert als multinomiale Verteilungen über das Vokabular, werden mit Hilfe der Kullback-Leibler Divergenz verglichen. Auf dieser Basis wird die Distanz zwischen Teilkorpora in Form von Heatmaps visualisiert, und der Beitrag einzelner Wörter zu der jeweiligen Distanz mit Hilfe von Wortwolken.
         Abbildung 1 zeigt die Distanz zwischen Themenbereichen sowie zwischen Textsorten innerhalb eines Themenbereichs (grün für geringe, purpur für große Distanz). Es wird deutlich, dass der Themenbereich
                Kirche (KI) sich am deutlichsten von den anderen Themenbereichen abhebt. Innerhalb der Themenbereiche zeigt sich, dass die Textsorten - soweit für einen Themenbereich mit Dokumenten belegt - im Themenbereich
                Frauen deutlich stärker ausdifferenziert sind als im Themenbereich
                Politik. Inbesondere die Textsorten
                Stellungnahme (S) und
                Kundgebung (K) heben sich deutlicher von den anderen Textsorten ab als im Themenbereich
                Politik.
              
         
            
            
               Abb. 1: Heatmaps für den Vergleich von Themenbereichen (links)
                und Textsorten innerhalb eines Themenbereichs ( Politik:
              mitte, Frauen: rechts). 
         
         Abbildung 2 zeigt den Beitrag einzelner Wörter zu der Distanz zwischen Teilkorpora in Form von Wortwolken. Groß dargestellte Wörter sind hierbei besonders typisch für ein Teilkorpus, die Farbe korrespondiert mit der relativen Häufigkeit eines Wortes im Teilkorpus (blau für selten, purpur für häufig). Die Wortwolke links vergleicht
                Frauen mit dem restlichen Korpus. Sie wird sowohl auf begrifflicher Ebene (
                Frau/Mann) als auch auf grammatischer Ebene (
                die, ihre, sie, …) vom allgemeinen Diskursgegenstand
                Frauen dominiert. Die Wortwolke in der Mitte zeigt die typischen Wörter von
                Zeitungsartikeln im Vergleich zu
                Essays innerhalb des Themenbereichs
                Frauen, die Wortwolke rechts typische Wörter im umgekehrten Vergleich. Hier wird deutlich, dass
                Zeitungsartikel sich im wesentlichen um die politisch/öffentliche Stellung der Frau drehen (
                Wahlrecht, Frauenstimmrecht, politische) und
                Essays um die private Welt der Frau (
                Beziehung, Moral, Erotik). Ein sehr deutlicher Unterschied zeigt sich auch im Numerus von
                Frau: Plural in
                Zeitungsartikeln und Singular in
                Essays.
              
         
            
            
               Abb. 2: Wortwolken für die typischen Wörter des Themenbereichs
              Frauen im Vergleich mit dem restlichen Korpus (links) und
              in den Textsorten Zeitungsartikel vs. Essay im Themenbereich Frauen (mitte und rechts). 
         
         Dieser kurze explorative Überblick kann natürlich nur einen kursorischen Eindruck über Inhalt und Vielfalt des Korpus geben. Technisch wurde er erst möglich durch die konsequente Kuration der Metadaten und Daten an Hand der generellen Richtlinien der CLARIN Infrastruktur.
      
      
         
            
              Das Zentrum für
                germanistische Forschungsprimärdaten, wird gefördert von der DFG
              im Rahmen des Programms „Informationsinfrastrukturen für Forschungsdaten“.
             Korpus: „Diskurs in der Weimarer Republik“
                PID:
                http://hdl.handle.net/10932/00-01B9-43B3-1E1D-7B01-6
            
            Siehe IDS-Repositorium.
         
         
            
               Bibliographie
               
                  Broeder, Dan / Schonefeld, Oliver / Trippel, Thorsten / Van
                      Uytvanck, Dieter / Witt, Andreas (2011): "A pragmatic approach to
                      XML interoperability – the Component Metadata Infrastructure (CMDI)", in:
                      Proceedings of Balisage. The Markup Conference
                      2011 (= Balisage Series of Markup Technologies 7). 
               
                  Fankhauser, Peter / Fiedler, Norman / Witt, Andreas
                        (2013): "Forschungsdatenmanagement in den Geisteswissenschaften am Beispiel
                        der germanistischen Linguistik", in: Zeitschrift für
                        Bibliothekswesen und Bibliographie (ZfBB) 60, 6: 296-306. 
               
                  Fankhauser, Peter / Knappen, Jörg / Teich, Elke
                          (2014a): "Exploring and Visualizing Variation in Language Resources", in:
                          Proceedings of the Ninth International Conference on
                            Language Resources and Evaluation (LREC'14)
               
               
                  Fankhauser, Peter / Kermes, Hannah / Teich, Elke
                            (2014b): "Combining Macro- and Microanalysis for Exploring the Construal of
                            Scientific Disciplinarity", in: Proceedings of the Digital
                            Humanities 2014. 
               
                  Institut für Deutsche Sprache (IDS): Zentrum für germanistische Forschungsprimärdaten
                  http://www1.ids-mannheim.de/fi/projekte/lis.html [letzter Zugriff
                              11. Februar 2016].
               
                  Institut für Deutsche Sprache (IDS): IDS Repository
                  https://repos.ids-mannheim.de/ [letzter Zugriff 11. Februar
                                2016].
               
                  Kämper, Heidrun (2015): "Demokratiediskurs 1918-1925"
                                  http://www1.ids-mannheim.de/lexik/zeitreflexion18.html [letzter
                                    Zugriff 14. Oktober 2015]. 
            
         
      
   



      
         In diesem Poster wird die digitale Edition der Excerpta
        Constantiniana (im Weiteren Excerpta), einer
        byzantinischen Geschichtsenzyklopädie, die im 10. Jahrhundert in Konstantinopel in
        Altgriechisch verfasst wurde, vorgestellt. 
         Zugrunde liegt ein disziplinspezifisches Forschungsprojekt im Bereich Klassische und Byzantinische Philologie, dessen Ziel darin besteht, die Edition einer wichtigen Quelle der byzantinischen Geschichtsschreibung vorzulegen. Das Ziel dieses Posters ist fachübergreifend und besteht darin, die Rolle des Herausgebers sowie das Konzept der Präsentation einer historischen Quelle in digitaler Umgebung zu definieren.
         Bei den Excerpta handelt es sich um ein groß angelegtes,
        mehrere Bände umfassendes Werk. Es besteht aus mehreren Tausend einzelnen Auszügen
        (Exzerpten), die inhaltlich aus etwa drei Dutzend antiken und byzantinischen
        Geschichtswerken stammen. Die erhaltenen Reste umfassen etwa 560 000 Wörter (es wird
        vermutet, dass fast das Zehnfache verlorengegangen ist). Die beiden erhaltenen
        Originalhandschriften der Excerpta (je ein Band) zeichnen
        sich durch ein bemerkenswertes Layout aus: zum Zweck der Navigation durch den Inhalt
        wurden an deren Rändern mehrere hundert Notizen und Piktogramme angebracht (s.
        Abbildung 2). 
         Die Edition des Gesamtwerks befindet sich in der Vorbereitungsphase; exemplarisch
          wurde bereits ein Abschnitt aus den Excerpta ediert, und zwar
          24 Seiten aus der Originalhandschrift Vaticanus graecus 73
          (ca. 9 000 Wörter). Bei der Handschrift handelt es sich um einen Palimpsest: der
          Text der Excerpta wurde etwa im 14. Jahrhundert ausradiert
          und mit einem anderen Text überschrieben, sodass der frühere Text heute nur mühsam
          lesbar ist (s. Abbildung 1). 
         Die Standardlösung wäre es, das Faksimile der Handschrift zu publizieren und eine historisch-kritische Edition des Texts anzufertigen. Die Publikation von dermaßen beschädigten Seiten erwies sich jedoch als wenig ergiebig. Auch die traditionelle Art der Textgestaltung im Rahmen einer historisch-kritischen Edition war kaum für die Wiedergabe der
            Excerpta geeignet. So bedurfte es beispielsweise einer originalgetreuen Reproduktion der Notizen und Piktogramme an den Rändern, die einen wichtigen Beitrag für das Verständnis des Textes leisten.
          
         Die Art der Gestaltung der Excerpta war ausschlaggebend bei
          der Entwicklung des Konzepts dieser Digitaledition. Es wurde eine pluralistische Herangehensweise an den Text zugrunde gelegt, die
          gleichzeitig mehrere Ansichten (Wiedergabemöglichkeiten)
          desselben Textes ermöglicht. Dabei wurden drei Grundansichten ausgewählt: (1) die
          digitale Rekonstruktion der Handschrift (topographische
          Edition), (2) die diplomatische Abschrift (diplomatische
          Edition) sowie (3) die normalisierte, historisch-kritische Version des
          Textes (digitale historisch-kritische Edition). 
         
            
               Topographische Edition: Die topographische Edition umfasst
              die digitale Rekonstruktion des Originals und die zweidimensionale, detaillierte
              Darstellung der Oberfläche. Das bedeutet, dass der ausradierte Text der
              Handschrift, stark vergrößert, auf Touchscreen mit Stylus nachgemalt wird (s.
              Abbildungen 1 und 2). Diese Methodik wurde hier m.W. zum ersten Mal angewendet
              und wird daher im Poster vorgestellt. Sie kombiniert menschliches Expertenwissen
              mit den aktuellen technischen Möglichkeiten. 
            
               Diplomatische Edition: Unter einer diplomatischen Edition
                wird eine möglichst originalgetreue Abschrift einer Handschrift verstanden. In
                dieser Ansicht wird die Gestaltung der Originalhandschrift, d. h. vor allem das
                Layout und die Navigationselemente des ursprünglichen Texts, visualisiert (s.
                Abbildung 2). Nach Möglichkeit wird die ursprüngliche Orthographie wiedergeben.
                Hier ist auch die Option vorgesehen, innerhalb derselben Ansicht auf
                normalisierte Orthographie umzuschalten (dies ermöglicht z. B. die Wahl zwischen
                mittelalterlicher und moderner Zeichensetzung, zwischen der Schreibweise mit
                Abbreviaturen oder mit deren Auflösung u. ä.) 
            
               Digitale historisch-kritische Edition: Diese Ansicht hat
                  das Layout einer modernen Edition, die Orthographie wird weitgehend
                  normalisiert. Außerdem wird unter dieser Ansicht als Option die Möglichkeit der
                  Hervorhebung unterschiedlicher Textinhalte angeboten, wie z. B. von Zitaten,
                  Orten, Personennamen, Völkerbezeichnungen usw. In diese Ansicht gehören ferner
                  der kritische Apparat sowie Indices mit Namen, Orten usw. 
         
         Technisch wird das folgenderweise implementiert. Die topographische Edition wird in Form von Bildern hergestellt. Die Transkription der Handschrift für die diplomatische und historisch-kritische Edition wird in TEI-XML angefertigt. Erscheinungen, die mit XML-Tags kodiert werden, werden in größere
                  Blöcke aufgeteilt. Die wichtigsten Blöcke sind:
                
         
            
               Physischer Zustand der Handschrift und physische Struktur des
                      Texts: physische Schäden der Handschrift, Lesbarkeit des Textes,
                      Seiten- und Zeileneinteilung an den Stellen, wo sie nicht mit der logischen
                      Struktur des Texts in Verbindung stehen (s. u.); 
            
               Logische Struktur des Texts sowie alle Elemente des Layouts,
                          welche die Navigation im Text unterstützen: Einheiten wie Bände,
                          Kapitel, Exzerpte; Elemente der Gestaltung, die auf diese Einteilung verweisen
                          (z. B. größere Leerräume im Text); Piktogramme und Randnotizen; 
            
               Orthographie der Handschrift: originale Zeichensetzung, Akzentuierung, Abbreviaturen und Ligaturen;
                          
            
               Normalisierte Orthographie: moderne Zeichensetzung, Worttrennung (fehlt in der Handschrift), Groß- und Kleinschreibung nach modernen Normen.
                          
            
               Inhalte im Text: Zitate, Namen, Orte, Völker u. a. 
         
         Die Webdarstellung wird auf der Basis von XSLT erstellt. Für jede Ansicht wird einzeln modelliert, wie die einzelnen Blöcke der Tags transformiert werden sollen. So ist beispielsweise für das Layout der diplomatischen Edition der physische Zustand der Handschrift und die physische Struktur des Texts maßgebend, während für das Layout der historisch-kritischen Edition die logische Struktur des Texts entscheidend ist. Die endgültige Darstellung wird über Cascading Stylesheets (CSS) gestaltet. Es ist geplant, die Edition bis Mitte des Jahres 2016 online zu stellen.
                            
         
         
            
            
               Abb. 1: Der Prozess der graphischen Rekonstruktion des
                            Palimpsests
         
         
            
            
               Abb. 2: Die topographische Edition
         
         
            
            
               Abb. 3: Die diplomatische Edition
         
         
            
            
               Abb. 4: Die historisch-kritische Edition
         
      
   



      
         Die hier unter dem Arbeitstitel CFDB vorgestellte Datenbank
      stellt den derzeit einzigartigen Versuch dar, Fragen zur paläographischen
      Entwicklung der babylonischen Keilschrift im ersten vorchristlichen Jahrtausend mit
      den Mitteln der Digital Humanities zu beantworten. Sie wird im Zuge des unter der
      Leitung von Michael Jursa an der Universität Wien durchgeführten Projekts Diplomatik und Paläographie neu- und spätbabylonischer
      archivalischer Dokumente ( FWF P 26104 ) am Austrian Centre for Digital
      Humanities der Österreichischen Akademie der Wissenschaften entwickelt.
      Diese Datenbank stellt ein dynamisches und flexibles Untersuchungsinstrument dar,
      das im Hinblick auf die neu- und spätbabylonische Epigraphik eine beträchtliche
      Lücke in der Forschung zu schließen beabsichtigt. 
         Verglichen mit alphabetischen Zeichensystemen weist die Keilschrift auf der Ebene des Schriftduktus eine hohe Zahl an potentiell objektivierbaren Merkmalen auf: Eigenschaften wie Schreibwinkel, Drucktiefe, Reihenfolge, Anordnung und Clustering von Keilen, Zeichengröße und andere Kriterien eignen sich ausgezeichnet für paläographische Untersuchungen. Vorrangiges Interesse dieser Seite des Forschungsprojekts ist die Identifizierung von gehäuft auftretenden standardisierten Zeichenformen (auf der Ebenen von Einzelzeichen und Ligaturen) bzw. die Frage, ob sich Abweichungen davon an die Größen Datum, Entstehungsort, Archiv oder Schreiber rückbinden lassen.
         CFDB ist analog zu konventionellen assyrologischen Zeichenliste strukturiert: Jedem Graphem dieser Liste wird ein Korpus attestierter Formen (Allographen) in Form von annotierten Bildsegmenten beigestellt. Die Allographen werden anhand ihrer Datierung und Periodisierung sowie ihres Herkunftsortes, des Archives, der Textsorte und, sofern möglich, des Schreibers klassifiziert. Somit erlaubt CFDB die Untersuchung dieses Zeichenkorpus einerseits hinsichtlich des Verhältnisses verschiedener Allographen zueinander in einer diachronen Perspektive sowie andererseits in Bezug auf die oben genannten unterschiedlichen Aspekte des Schreibduktus, und zwar über die Ebene von Einzelzeichen oder -tafeln hinaus. Auf einer Metaebene bietet die Datenbank die Möglichkeit, in der Literatur häufig anzutreffende, allerdings lediglich impressionistisch begründete Differenzierungen zwischen „formalen“, „kalligraphischen“ oder „kursiven“ Schriftduktus zu objektivieren und zur Diskussion zu stellen. 
         Die Implementierung der Applikation, die sich derzeit im Beta-Stadium befindet und
        deren Quellcode im Laufe der Entwicklung der Forschungscommunity Open Source
        verfügbar gemacht werden wird, basiert auf der XML-Datenbank exist-db. Eine integrierte, browserbasierte
        Arbeitsumgebung erlaubt die Eingabe und Manipulation der Metadaten zu einzelnen
        Tafeln, die Bearbeitung der digitalen Standardzeichenliste, Upload und Verwaltung
        von einem oder mehreren Faksimiles einzelner Tafeln sowie die manuelle
        Bildsegmentierung nach Einzelzeichen und deren Annotation. Die Verwendung von XForms
        für die Dateneingabe einerseits und von REST-Endpoints für die Kommunikation
        zwischen Annotierungsoberfläche und Server anderseits ermöglichen es, Teile der
        Anwendung auch in verändertem technischen Kontext (bspw. vor einer relationalen
        Datenbank) wiederverwenden zu können. Die Applikation sieht sich damit auch als
        kleiner Beitrag zum Aufbau einer nachhaltigen, da modularen Forschungsinfrastruktur. 
         Das Datenmodell von CFDB beruht auf den aktuellen Guidelines
        der Text Encoding Initiative und verwendet Bestandteile
        der Module transcr (Kapitel 11: Representation of Primary
        Sources) für das Markup der Bild-Text-Relation sowie des gaiji-Moduls
        (Kapitel 5: Characters, Glyphs, and Writing Modes). Die
        digitale Version der Standardzeichenliste, die im Zuge des Projekts erstellten
        Bildsegmente, Transkriptionen sowie die zugehörigen Metadaten und Annotationen
        werden nach Projektende sowohl eingebettet in cfdb als auch als Datenset in TEI-XML
        der Forschungsöffentlichkeit zur Verfügung gestellt. Die Anbindung an relevante
        Initiativen zur Digitalisierung und zum Korpusaufbau von Keilschrifttexten
        (Cuneiform Digital Library Initiative CDLI, Neo-Babylonian Cuneiform Corpus NaBuCCo)
        ist geplant. 
         CFDB ist als ein dynamisches Werkzeug für Untersuchungen zur Keilschrift konzeptualisiert, das zur einfachen Referenz in Forschung und Lehre dient und mit Blick auf die Nachhaltigkeit von Forschungsdaten entwickelt wurde. Gleichzeitig kann die Applikation leicht für analog strukturierte Forschungsprojekte in anderen Bereichen der Bild-Text-Korrelation adaptiert werden.
         
            
         
         
            
         
         
            
         
      
      
         
            
               Bibliographie
               
                  Biggs, Robert D. (1973): "On regional cuneiform
              handwritings in Third Millennium Mesopotamia", in: Orientalia 42: 39-46.  
               
                  Daniels, Peter (1995): "Cuneiform calligraphy", in:
                Mattila, Raija (ed.): Nineveh, 612 BC. The glory and
                fall of the Assyrian Empire. Helsinki: Helsinki University Press 81-90.  
               
                  Devecchi, Elena (ed.) (2012): Palaeography and Scribal Practices in Syro-Palestine and Anatolia in
                  the Late Bronze Age. Papers read at a symposium in Leiden, 17–18
                  December 2009 (= PIHANS 119). Leiden: Nederlands Instituut voor het Nabije
                  Oosten  
               
                  Jursa, Michael (in Vorbereitung): "Late Babylonian
                    Epigraphy: a Case Study", in: Current Research in
                    Cuneiform Palaeography. Proceedings of the Workshop organized at
                    the 60th Rencontre Assyriologique Internationale, Warsaw 2014 
               
                  Powell, Marvin A. (1981): "Three problems in the
                      history of cuneiform writing: Origins, Direction of Script, Literacy", in:
                      Visible Language 15, 4: 419-440. 
               
                  Sallaberger, Walther (2001): "Die Entwicklung der
                        Keilschrift in Ebla", in: Meyer, Jan-Waalke / Novák, Mirko / Pruß, Alexander
                        (eds.): Beiträge zur Vorderasiatischen Archäologie
                        Winfried Orthmann gewidmet. Frankfurt am Main: Johann Wolfgang
                        Goethe-Universität 436-445.
            
         
      
   



      
         Das digitale Handbuch der Höfe und Residenzen im spätmittelalterlichen Reich ist die
        online-Ausgabe einer Druckpublikation, die zwischen 1998 und 2011 von Autorinnen und
        Autoren der Hof- und Residenzenforschung erstellt und von der Residenzen-Kommission
        der Akademie der Wissenschaften zu Göttingen konzeptionell und redaktionell betreut
        wurde. Das ca. 5000 Seiten und mehrere hundert Abbildungen umfassende Werk besteht
        aus über 1000 Einzelartikeln, die dynastisch, topographisch und sachlich gegliedert
        und vielfältig miteinander verknüpft sind. Der topographische Schwerpunkt liegt auf
        Höfen und Residenzen des Heiligen Römischen Reichs Deutscher Nation in der Zeit
        zwischen etwa 1200 bis 1650 (Hirschbiegel / Wettlaufer 1999, 2000, 2002; Wettlaufer
        2005). In einer Zusammenarbeit mit der SUB Göttingen wurde ein Konzept für eine
        Online Präsentation der Druckdaten, die vom Thorbecke Verlag im TEI-Format zur
        Verfügung gestellt wurden, entworfen und umgesetzt.
         In dem Projekt spielen Datenmodellierung, Vernetzung und Visualisierung eine entscheidende Rolle. Aufbauend auf einen SOLR - Index wird ein Zugriff auf Text und Bilder über eine string-basierte Suche sowie über eine topographischen Zugriff mittels Open-Street-Map Karten und dem leaflet Javascript Framework angeboten, die die ortsgebundenen Informationen zu den Artikeln erschließen. 
         
            
         
         Eine Facettierung zu Handbuchteilen, Autoren und Artikeln erlaubt die Einschränkung der Suchergebnisse auf bestimmte Teilmengen. Die Texte und Bilder werden sowohl über eine HTML-basierte Ansicht als auch im PDF Format für die Benutzer präsentiert. Abkürzungen werden zur besseren Verständlichkeit bei mouse-over Events aufgelöst. Quellen und Literatur zu den einzelnen Artikeln können bei Bedarf ausgeklappt und angezeigt werden. Ein Zitierlink erlaubt die Referenzierung zur seitenidentischen Druckausgabe des Handbuchs. Eine besondere Herausforderung stellte die Verlinkung der Artikel untereinander dar, die über eindeutige Identifikatoren für alle Texte und Bilder miteinander verknüpft sind. Aufgrund der Datengrundlage waren die End- und Zielpunkte der zu verknüpfenden Strings, die nur durch einen Pfeil im Fließtext gekennzeichnet sind, nicht leicht zu bestimmen. Trotzdem konnte eine über 90% korrekte Verlinkung durch automatisierte string-matching Verfahren mit Hilfe des kontrollierten Lemmavokabulars erreicht werden. 
         
            
         
         Die Architektur des Projekts ist auf eine leichte Archivierbarkeit und langfristige
          Nachnutzung hin optimiert, da nach dem Auslaufen von Akademieprojekten in der Regel
          keine Mittel mehr für eine Pflege von digitalen Projektergebnissen mehr zur
          Verfügung stehen (Wettlaufer 2012). Über XML TEI stehen die Textdaten langfristig
          lesbar bereit und durch Apache SOLR können die Daten mit Hilfe einer etablierten
          Technologie effizient gesucht und angezeigt werden. Die Nutzung von Typo3 und der
          Extension "Find" erlauben die Integration in die schon bestehende digitale
          Infrastruktur der Göttinger Akademie. Die "Find"-Erweiterung der SUB
          Göttingen schafft eine Schnittstelle zu beliebigen SOLR-Indizes in Typo3 und
          zeichnet sich durch eine leichte Konfigurierbarkeit und erweiterte
          Templating-Fähigkeiten aus, mit denen komplexe Ansichten der Suchergebnisse
          realisiert werden können. Das Hosting an der Göttinger Staats- und
          Universitätsbibliothek lassen die langfristige Pflege und Verfügbarkeit der
          Handbücher im Rahmen einer bestehenden Kooperationsvereinbarung erwarten. 
         Für die Zukunft sind neben einer Erweiterung des Userinterface mit zusätzlichen Materialien und einer Verknüpfung der Artikel mit prosopographischen Datensätzen von Hofinhabern auch eine Bereitstellung der Metadaten der Einzelartikel als Linked Open Data sowie eine Verknüpfung mit weiteren Handbüchern zu einem ähnlichen Themenbereich aus einem Nachfolgeprojekt geplant. Das digitale Handbuch wird voraussichtlich im Frühjahr 2016 öffentlich zur Verfügung stehen.
         Projektseite:
            http://adw-goe.de/forschung/abgeschlossene-forschungsprojekte-aus-dem-akademienprogramm/hof-und-residenz/
         
      
      
         
            
               Bibliographie
               
                  Hirschbiegel, Jan / Wettlaufer, Jörg (1999):
                "Materialien zum Werk 'Fürstliche Höfe und Residenzen im
                spätmittelalterlichen Reich. Ein dynastisch-topographisches Handbuch'.
                Zusammengestellt von Jan Hirschbiegel und Jörg Wettlaufer", in: Mitteilungen der Residenzen-Kommission der Akademie der
                Wissenschaften zu Göttingen. Sonderheft 3 http://resikom.adw-goettingen.gwdg.de/MRK/SH3.pdf [letzter
                Zugriff 15. Oktober 2015]. 
               
                  Hirschbiegel, Jan / Wettlaufer, Jörg (2000):
                  "Projektdatenbank: Fürstliche Höfe und Residenzen im spätmittelalterlichen
                  Reich. Ein dynastisch-topographisches Handbuch", in: Mitteilungen der Residenzen-Kommission der Akademie der Wissenschaften
                  zu Göttingen 11, 2: 9-14 http://resikom.adw-goettingen.gwdg.de/MRK/MRK11-2.pdf [letzter
                  Zugriff 15. Oktober 2015]. 
               
                  Hirschbiegel, Jan / Wettlaufer, Jörg (2002):
                    "Fürstliche Höfe und Residenzen im spätmittelalterlichen Reich. Bilder und
                    Begriffe", in: Mitteilungen der Residenzen-Kommission der
                    Akademie der Wissenschaften zu Göttingen 12, 1: 12-18 http://resikom.adw-goettingen.gwdg.de/MRK/MRK12-1.pdf [letzter
                    Zugriff 15. Oktober 2015]. 
               
                  SUB Göttingen (o. J.): TYPO3
                    extension providing a frontend for Solr indexes
                  https://github.com/subugoe/typo3-find [letzter Zugriff 12.
                      Februar 2016].
               
                  Wettlaufer, Jörg (2005): "Höfe und Residenzen im
                        spätmittelalterlichen Reich. Erste Ergebnisse des Handbuchprojekts der
                        Residenzen-Kommission der Akademie der Wissenschaften zu Göttingen", in:
                        Pils, Susanne / Niederkorn, Jan Paul (eds.):Ein
                        zweigeteilter Ort? Hof und Stadt in der frühen Neuzeit (=
                        Forschungen und Beiträge zur Wiener Stadtgeschichte 44). Wien: Böhlau 7-26. 
               
                  Wettlaufer, Jörg (2012): "Das digitale Handbuch der
                          Höfe und Residenzen im spätmittelalterlichen Reich. Probleme und Erfahrungen
                          einer digitalen Bereitstellung von kollaborativen Werken in Open Access nach
                          dem Projektende", Vortrag auf dem Workshop Rechtliche
                          Rahmenbedingungen der Akademievorhaben der Akademie der Wissenschaften
                          zu Göttingen und der Union der deutschen Akademien der Wissenschaften AG
                          „Elektronisches Publizieren“. 8. und 9. Oktober 2012, Göttingen,
                          Historische Sternwarte http://www.digihum.de/agep/docs/wettlaufer_2012_agep.pdf [letzter
                          Zugriff 15. Oktober 2015]. 
            
         
      
   



      
         Das 2013 gestartete Projekt
        Die Schule von Salamanca. Eine digitale Quellensammlung und ein Wörterbuch ihrer juristisch-politischen Sprache umfasst eine Sammlung von über 100 Texten iberischer Theologen und Juristen des 16. und 17. Jahrhunderts zu politischen und rechtlichen Themen. Diese Sammlung wird ergänzt durch ein Handbuch, in dem neben biografischen Informationen zu den in der Edition vertretenen Autoren die Entwicklung zentraler Begriffe der europäischen Rechts- und politischen Ideengeschichte in diesem Diskussionszusammenhang erschlossen wird. Beide Teile werden vollumfänglich und frei online zugänglich sein; zu Beginn des Jahres 2016 werden die ersten Daten der Quellensammlung und des Wörterbuchs zur Verfügung gestellt. Dies sind insbesondere die TEI-XML Dateien der Volltexte der ersten Werke sowie die dazugehörigen Digitalisate. Parallel besorgen wir die Bereitstellung der Daten als Linked Open Data in Verbindung mit einem SPARQL-Endpoint.
      
         Zu Beginn des DHd-Vortrags wird das Projekt und seine Website kurz vorgestellt. Eine wichtige Facette besonders der künftigen Projektentwicklung sehen wir aber in der Einbindung unserer Daten in eine Linked Data Infrastruktur. Der Vortrag soll daher vor allem dafür genutzt werden, um einen Einblick in die Arbeitsschritte, Entscheidungen und Implementierungen zu geben, die in dieser Hinsicht erfolgt sind und noch ausstehen.
         1. Wie werden die TEI-Daten in einer Linked-Data-Umwelt angeboten: welche Elemente
        oder Attribute werden welchen Objekten und Prädikaten welcher Ontologien zugeordnet?
        Wie wird diese Zuordnung in Anschlag gebracht, um die Daten als semantische Daten
        anzubieten? Wie ist die zeitliche Dimension vieler biographischer Angaben
        abzubilden? Wie ist mit alternativen Werten, z. B. konkurrierenden Angaben zu
        Geburtsdaten, umzugehen? Gibt es Rückwirkungen auf unser „Kerngeschäft“, i. e. das
        TEI Schema und die Datenerfassung?
         Wir werden semantische Daten zu den Werken und zu den in der Edition vertretenen Autoren anbieten, dabei hauptsächlich auf die foaf-, bio-, relationship-, und SPAR-Ontologien zurückgreifen und die TEI Daten über den xtriples Webservice (xTriples 2015) in RDF umwandeln.
         2. Wie werden die Daten in unseren eigenen Diensten vernetzt genutzt, wie sollen sie
          zur externen Nachnutzung und Vernetzung angeboten werden: Welche Dienste und
          Ressourcen sollen – direkt oder indirekt – über das Projekt angeboten werden? 1 Welche Möglichkeiten ergeben sich durch die Vernetzung unserer Daten
          mit denen anderer Anbieter, Forschungsfragen in neuer Weise zu bearbeiten? In
          welcher Form können oder sollen solche erweiterten Möglichkeiten auf der Projektsite
          öffentlich angeboten werden, wie verhält es sich mit Rechtemanagement und
          Qualitätssicherung in föderierten Abfragen/Daten? Welche anderen Datenanbieter sind
          interessante Kooperationspartner, welches sind die einschlägigen Normdatenbanken?
          2 Was sind die
            Erfahrungen mit den verfügbaren Schnittstellen dieser Anbieter? 
         Wir werden sowohl auflösbare Entitäten URIs als auch einen SPARQL Endpoint anbieten.
              Föderierte Abfragen zur Bearbeitung spezieller Forschungsfragen werden im Vortrag
              vorgestellt. Auch für uns bislang noch ungelöste Probleme, die sich z. B. aus der
              heterogenen Struktur und Qualität der Daten ergeben, werden angesprochen.
         Über die infrastrukturelle Seite dieser Vernetzung hinaus gilt es hier die Benutzerperspektive nicht aus den Augen zu verlieren: Wie und wo sollen welche Art von Benutzern Vernetzungen und Abfragen selbständig definieren können? Weit mehr als eine Frage der technischen Möglichkeiten ist dies ein Problem der Widmung und des Einsatzes von Entwicklungsressourcen, denn sie betrifft die Entwicklung von Oberflächen, deren Nutzung weit über die Kernaufgaben der meisten Projekte hinausgeht.
                3
         
         3. Visualisierungsmöglichkeiten, aufsetzend auf verschiedene Services: Ein Problem bei der Gewinnung vieler Daten aus dem Semantic Web ist ihre Aufbereitung in einer Form, die den Anwendenden dazu befähigt, neue Entdeckungen zu machen, vielleicht sogar neue wissenschaftliche Erkenntnisse zu generieren. Je größer dabei die Quantität und die Heterogenität der Daten ist, desto anspruchsvoller wird die Aufgabe, eine gute Visualisierung zu erzeugen.
         Es gibt bereits verschiedene Open Source Programme, die sich als Visualisierungstools
                eignen und bei denen es sich lohnt, über einen Einsatz nachzudenken. Neben
                generischen Visualisierungs-Tools (z. B. NodeBox 2015, Sgvizler 2015, d3SPARQL
                2015), Google Heatmaps (Heatmaps 2015) und
                Zeitverlaufsanzeigen (z. B. über Timeline 2015; HDAT 2015) etwa durch Abfragen von
                Koordinaten aus dem TGN, lohnt es auch, einen Blick auf RelFinder (2015) zu werfen. RelFinder visualisiert die Beziehungen
                zwischen zwei Ressourcen und gibt auch den jeweiligen Inhalt der Ressource aus. Für
                alle diese Werkzeuge gilt es jedoch das mögliche Einsatzszenario sorgfältig zu
                entwerfen. Überlegungen und Kriterien für ein allgemeines Verfahren solcher Entwürfe
                sollen vorgestellt werden. 
         Die Zukunftsperspektiven für die Bereiche Vernetzung und Visualisierung werden im Salamanca-Projekt stets mitbedacht, und so umreißt der Vortrag bereits erbrachte Leistungen und Implementierungen, getroffene Entscheidungen aber auch noch offene und zu erprobende Themen: Insbesondere in Punkt 1 gehen viele Aspekte schon im Frühjahr 2016 in „production use“, in Punkt 2 werden einige konzeptuelle Fragen angesprochen, die wir für uns noch nicht gelöst haben, in Punkt 3 werden wir beschreiben, wie wir von einem eher zufälligen Spiel mit verschiedensten technischen Möglichkeiten zu einem systematischen Prozess der Reflexion über aktuelle und neue Entwicklungsziele übergehen.
      
      
         
            Z.B. Auflösung von Ressourcen-URLs, content
                negotiation, triple-store dump, Linked Data Fragments, SPARQL Endpoint
                usw.
            Projekte: z. B. Scholasticon (Schmutz 2009) mit biographischen Informationen, die
                  Sammlung Post-Reformation Digital Library 
               Scholastica (PRDL 2013) mit Informationen zu
                  Universitäten, Fakultäten und Lehrstühlen der Frühen Neuzeit oder Early Modern Letters Online (EMLO 2015) mit Daten zu
                  frühneuzeitlichen Korrespondenzen. Normdatenbanken: z. B. Getty Thesaurus of Geographic Names (TGN 2015), Gemeinsame Normdatenbank der Deutschen Nationalbibliothek (GND 2015)
                  oder der CERL Thesaurus (CERL 2015).
             Die Anwendung
                  Linked Data Fragments Client (LDFC 2015) zeigt etwa Beispiele, wie sich Nutzende Ressource(n) und Abfragen zusammenklicken können. Eine graphische Oberfläche für (begrenzte) Abfragen ermöglicht einen effizienten Umgang mit Linked Open Data, ohne dafür die Abfragesyntax beherrschen zu müssen. Auch über
                  LodLive (LodLive 2012) ließe sich ein entsprechendes Abfrage-Interface realisieren.
                  
         
         
            
               Bibliography
               
                  CERL (2015): CERL Thesaurus
                  http://thesaurus.cerl.org
                      [letzter Zugriff 14. Oktober 2015 ]. 
               
                  d3SPARQL (2015): d3sparql.js
                        Utilities for visualizing SPARQL results with the D3 library http://biohackathon.org/d3sparql/ [letzter Zugriff 13. Oktober
                        2015 ]. 
               
                  EMLO (2015): Early Modern Letters
                          Online
                  http://emlo.bodleian.ox.ac.uk/ [letzter Zugriff 14. Oktober 2015
                            ]. 
               
                  GND (2015): Gemeinsame Normdatei
                  http://www.dnb.de/DE/Header/Hilfe/kataloghilfe.html#doc207526bodyText73
                                [letzter Zugriff 14. Oktober 2015 ]. 
               
                  HDAT (2015): Historical Dutch-Asiatic
                                  Shipping
                  http://app.thebrownmap.nl/
                                  [letzter Zugriff 13. Oktober 2015 ]. 
               
                  Heatmaps (2015): Google Maps
                                    Javascript API
                  https://developers.google.com/maps/documentation/javascript/heatmaplayer
                                      [letzter Zugriff 13. Oktober 2015 ]. 
               
                  LDFC (2015): Linked Data Fragments
                                        client
                  http://client.linkeddatafragments.org/ [letzter Zugriff 13.
                                          Oktober 2015 ]. 
               
                  LodLive (2012): LodLive Browsing
                                            the Web of Data http://en.lodlive.it/ [letzter Zugriff 13. Oktober 2015 ]. 
               
                  NodeBox (2015): Meet NodeBox 3
                  https://www.nodebox.net/node/ [letzter Zugriff 13. Oktober 2015]. 
               
                  PRDL (2013): Scholastica Early
                                                  Modern Academies & Universities http://www.prdl.org/schools.php [letzter Zugriff 14. Oktober 2015
                                                  ]. 
               
                  RelFinder (2015): RelFinder
                                                    Interactive Relationship Discovery in RDF Data http://www.visualdataweb.org/relfinder.php [letzter Zugriff 13.
                                                    Oktober 2015 ]. 
               
                  Schmutz, Jacob  (2009): Scholasticon Ressources en ligne pour l'étude de la scolastique
                                                      moderne (1500-1800): auteurs, sources, institutions http://scholasticon.ish-lyon.cnrs.fr [letzter Zugriff 14. Oktober
                                                      2015 ]. 
               
                  Sgvizler (2015): Sgvizler
                  http://dev.data2000.no/sgvizler/ [letzter Zugriff 13. Oktober
                                                          2015]. 
               
                  TGN (2015): The Getty Thesaurus of
                                                            Geographic Names (TGN) http://vocab.getty.edu/ [letzter Zugriff 13. Oktober 2015 ]. 
               
                  Timeline (2015): Leaflet.timeline
                  https://github.com/skeate/Leaflet.timeline [letzter Zugriff 13.
                                                                Oktober 2015]. 
               
                  xTriples (2015): Xtriples A
                                                                  generic webservice to extract RDF statements from XML resources http://xtriples.spatialhumanities.de/index.html [letzter Zugriff
                                                                  13. Oktober 2015 ]. 
            
         
      
   



      
         
            Einleitung
            In diesem Beitrag stellen wir erste Einsichten aus einer quantitativen Analyse von Dramen vor, sowie unsere Konzeption für eine darauf aufbauende interaktive Werkbank, die einen Anstoß für eine Diskussion zur Tool-Unterstützung quantitativer Dramenanalyse geben soll. Die Werkbank unterstützt interessierte Forscherinnen und Forscher beim Einlesen von Dramen aus TEI-basierten Quellen und befindet sich noch in Entwicklung
          1. Neben den in Dramen schon explizit kodierten strukturellen Informationen (Wer spricht was?) stellt die Werkbank insbesondere Möglichkeiten zur Verfügung mit Werkzeugen zur maschinellen Sprachverarbeitung auch den Inhalt der Figurenrede zu analysieren. Inspektions- und Aggregationswerkzeuge und -sichten erlauben auch die Analyse größerer Korpora.
        
            Um die Anwendungsgebiete der Werkbank aufzuzeigen, skizzieren wir – anhand einer
          Pilotstudie zur Analyse des Verhältnisses von
          dramatischer Figur zur dramatischen Handlung – den Problemhorizont quantitativer
          Literaturwissenschaft. Dabei interessieren uns insbesondere diese Fragen: Gibt
          es einen Zusammenhang zwischen angenommenen prototypischen Rollen (Protagonist,
          Intrigant, König usw.) und Länge bzw. Häufigkeit der Redebeiträge oder der
          Referenz auf die Figur? Wird über bestimmte Figuren(-rollen) auf bestimmte Arten
          gesprochen (abwertend / aufwertend, ...)? Gibt es
          Figuren(-rollen)konstellationen, die häufig kookurrieren, und zwar in Bezug auf
          ihren eigenen Rede- und Bühnenbeitrag als auch im Bezug auf die Referenzen auf
          die Figuren? 
         
         
            Dramenanalyse: Basics
            Dramentexte unterscheiden sich insbesondere durch zwei zusammenhängende
            Eigenschaften von Prosatexten: a) Dramatische Texte sind im Gegensatz zu vielen
            anderen Textsorten auf allen Ebenen (Akt- bis Redefolge) ausgesprochen gut
            strukturiert und ermöglichen somit eine verhältnismäßig unaufwändige
            Datenerhebung. Die Kehrseite der guten Strukturiertheit ist dass dramatische
            Texte damit nicht dem Prototyp eines Textes entsprechen, wie er von vielen
            Werkzeugen zur Sprachverarbeitung angenommen wird. Die maschinelle
            Sprachverarbeitung auf dramatischen Texten ist damit nicht durch existierende
            Werkzeuge „out of the box“ zu leisten. b) Die dramatischen Figuren sprechen unvermittelt. Unterscheidungen zwischen Erzähler- und
            Figurenrede und -denken spielen in Dramen keine Rolle. Während Ansätze der
            Stilometrie, das Figurensignal vom Erzähler- und jenes wiederum vom
            Gattungssignal zu trennen (Jannidis 2014), noch in den Kinderschuhen stecken,
            muss sich die (teil-)automatische quantitative Dramenanalyse diesen
            interpretativen Problemen nicht stellen. Sie hat vor allem technisch- methodische Probleme zu lösen: a)
            Erfassung und Einlesen der Daten und b) (teil-)automatische Textanalyse in
            Dramen. Zu letzterem gehört auch der adäquate Einsatz von interpretierbaren
            Maßen und transparenten Verfahren sowie visuellen Repräsentationen von
            Ergebnissen. 
         
         
            Erfassung und Einlesen der Daten: TEI-Integration
            Eine automatisierte Erfassung der Oberflächenstruktur inklusiver aller relevanten
              Metadaten dramatischer Texte ist die Grundvoraussatzung einer quantitativen
              Textanalyse im oben genannten Sinne. TEI / XML ist als Standard etabliert, um
              Texte und Korpora möglichst genau entsprechend der/einer gedruckten Edition
              digital zu kodieren (cf. TextGrid; DTA). Insbesondere erlaubt TEI auch die
              Kodierung von Seitenzahlen, Formatierungen, Zeilenumbrüchen, Kopf- und Fußzeilen
              und vieles mehr, was über den reinen Textinhalt hinausgeht.
            Wie Trilcke et al. (2015) auch schon festgestellt haben, ist die Extraktion der inhaltlichen Textstruktur aus den TEI-Daten keineswegs trivial. Für Netzwerkanalyse ist die eindeutige Identifizierung von Figuren besonders relevant, für eine (maschinelle, computergestützte) Analyse des Inhaltes und der Häufigkeit der Figurenrede kommen o.g. Formatierungsmarkierungen noch als Herausforderung hinzu. In unserer Werkbank bieten wir einen Plausibilitätscheck an, der es erlaubt, Fehler im Importprozess (die sowohl durch Fehlannahmen im Importmodul als auch durch Fehlkodierungen in den Quelldaten verursacht werden können) direkt zu erkennen und zu beheben. Einmal identifizierte und behobene Fehler fließen in die Quelldaten zurück.
         
         
            (Automatische) Textanalyse in Dramen
            In den bereits existierenden Arbeiten zur Stilometrie auf Dramen werden komplette
                Dramen verglichen (z. B. durch Vorverarbeitung mit DIGIVOY). Ein differenzierter
                Vergleich, bei dem einzelne Figuren oder Gruppen von Figuren betrachtet werden,
                ist so noch nicht möglich gewesen.
            Andere Projekte gehen genau den gegenteiligen Weg und verwerfen alle Dialoginhalte und beziehen ihre Netzwerkanalyse nur auf die Interaktion der jeweils in der Szene aktiven Figuren (cf. Trilcke et al.). Uns ist kein verfügbares System bekannt, das diese Lücke schließt und eine inhaltliche Analyse erlaubt, die sowohl die Interaktion der aktiven Figuren als auch deren Redeinhalt einbezieht. 
            In unserer Werkbank erfolgt die Textanalyse mit computerlinguistischen
                  Werkzeugen, welche durch die CLARIN-D Infrastruktur (Mahlow et al. 2014)
                  bereitgestellt werden. Der Aufbau von Dramen erfordert eine spezielle
                  Herangehensweise bei der Textanalyse, da die in der Computerlinguistik oft
                  getroffene Annahme, dass Texte aus vollständigen und grammatikalisch
                  wohlgeformten Sätzen bestehen, in Dramen nicht zutrifft (wie auch in Texten aus
                  sozialen Medien oder in gesprochener Sprache). Daneben weisen Dramen die oben
                  genannte spezifische Struktur auf, die eine adäquate Vorverarbeitung bedingt. Um
                  eine Verarbeitung mit einer nicht modifizierten CL-Verarbeitungskette zu
                  ermöglichen, wird das Drama vorher in passende Textsegmente zerlegt. Segmente,
                  die zu einem Dialog gehören müssen nach der Verarbeitung wieder der jeweiligen
                  Figur zugeordnet werden. Im Kontext der Figurenanalyse sind insbesondere
                  Eigennamenerkennung und Koreferenzresolution von Interesse. Wenn man den
                  stilometrischen Blick weitet und auch syntaktische Konstruktionen (verwendet
                  eine Figur mehr oder weniger komplexe Satzstruktur?) untersuchen möchte, sind
                  auch andere linguistische Verarbeitungsschritte möglich. 
            Die Ergebnisse dieser Verarbeitung werden nicht fehlerfrei sein, deswegen bietet
                    die Werkbank Möglichkeiten, die Ergebnisse zu korrigieren. Insbesondere die
                    Zusammenführung von unterschiedlich genannten oder geschriebenen (z. B. „Emilia“
                    vs. „Emilie“ oder „die Soldaten“ vs. „erster Soldat“) Figuren ist nicht trivial
                    und teilweise nur durch zusätzliches Weltwissen realisierbar. Damit dieser
                    Schritt vereinfacht wird kommt hier ein halb-automatischer Figurenabgleich zum
                    Einsatz. Das überarbeitete und manuell geprüfte Drama kann in einem
                    TEI-konformen Format exportiert werden, damit die so kuratierte Ressource wieder
                    der Community zur Verfügung gestellt werden kann. Linguistische Annotationen,
                    die in TEI nicht direkt repräsentiert werden können, werden in einem geeigneten
                    stand-off-Format exportiert.
            
               Pilotstudie
               In einer Pilotstudie haben wir anhand eines einzelnen Dramas exploriert, wie
                        der Zusammenhang von (der zentralen) Dramenfigur zur dramatischen Handlung
                        automatisiert sichtbar gemacht werden kann. Die (zentrale) Stellung im
                        Figurennetzwerk wird dabei nicht (wie in der aktuellen Forschung gängig;
                        vgl. Moretti 2011) lediglich durch häufige Präsenz oder Interaktion auf der
                        Bühne repräsentiert, sondern durch differenziertere Analysen der
                        Figurenaktivität. Wie häufig eine Figur spricht, wie viel sie spricht und wie häufig
                        über sie gesprochen wird, sind dabei die Kerndaten der
                        quantitativen Analyse, auf der weiter vorzustellende Analysen beruhen. Eine
                        manuelle Datenerfassung übersteigt jedoch selbst bei einzelnen Dramen
                        schnell den vom Menschen leistbaren Zeiteinsatz (wie die in Abbildung 1
                        manuell erstellte Erfassung der Redeteile in Emilia
                        Galotti zeigt): 
               
                  
                  
                     Abb. 1: „ Token von
                        x“ = Gesamthäufigkeit der Nennung jeder einzelnen Namensvariante.
                        Figurennennung = Nennung der Namensvarianten in der
                        Rede anderer Figuren. Redehäufigkeit = Wie oft spricht
                        eine Figur. Gesamtzahl der Wörter… = Redelänge in
                        Wörtern. (Aktivitäts)Quotient = Summe der
                        Redehäufigkeit geteilt durch die Summe der Figurennennung: X > 1 = Aktiv
                        (Redet häufiger als über sie geredet wird); X 
               
            
         
         
            NLP-Unterstützte Analysemöglichkeiten in Dramen
            Die Kombination von in Dramen vorhandenen strukturellen Informationen und durch automatische Verarbeitung ermittelte inhaltlich-semantische Information erlaubt neue, feinkörnige Analysen von Dramen. Die im Folgenden genannten sollen durch die Werkbank unterstützt werden, entweder durch Integration existierender oder durch Entwicklung neuer Tools.
            
               Oberflächenanalyse der Figuren
               Möglich ist eine automatische Auswertung der Figurenreden nach inhaltlichen
                          Kriterien. Ohne Vorwissen bereitstellen zu müssen, lassen sich wichtige
                          Begriffe, durch deren Verwendung sich eine Figur von anderen unterscheidet,
                          mit Verfahren wie TF*IDF ermitteln und z. B. als Tabelle oder als Wortwolke
                          darstellen. Komplexere Verfahren wie topic modeling (Blei et al. 2003) oder
                          Wortfeldanalysen können natürlich auch auf den Redeinhalt einer Person (ggf.
                          auf Akte / Szenen o. ä. eingeschränkt) angewendet werden, erfordern aber
                          zumindest die Einstellung von Parametern (z. B. Anzahl der topics im topic
                          modelling) oder das Spezifizieren von Wortfeldern. Automatische Methoden zur
                          Erweiterung von Wortfeldern (angelehnt an z. B. Query Expansion, vgl.
                          Manning et al. 2008) können diesen Prozess unterstützen und sollen im
                          Rahmen der Werkbank erprobt und integriert werden. Abbildung 2 zeigt eine
                          visuelle Auswertung dieser Analyse.
               
                  
                  
                     Abb. 2: Strukturelle und inhaltliche
                          Analyse von Schillers Johanna von Orleans. Unten: Figurenaktivität. Oben:
                          Prominenz ausgewählter semantischer Räume in der Figurenrede (Frankreich,
                          Gott, Militär). 
               
               
                  
                  
                     Abb. 3: Anhand der Häufigkeit der
                          Figurennennung („Emilia“ vs. „Tochter“) kann der (bisher kaum erforschte)
                          Diskursverlauf im Sinne einer Unterscheidung in private und öffentliche
                          Konversation sehr gut nachvollzogen werden. 
               
            
            
               Stilometrische Analysen von Figurenreden
               Stilometrische Analysen werden durch eine Schnittstelle ermöglicht, durch die
                            man Figurenrede als Datenstrukturen in R abrufen und dann nach diversen
                            Kriterien untersuchen kann, etwa mit Hilfe von stylo (Eder et al. 2013). Es
                            ließe sich z. B. untersuchen, ob Könige bei Schiller anders sprechen als bei
                            Lessing, oder ob Bürgerfiguren in einem bestimmten Dramenkorpus anders
                            sprechen als Adelsfiguren:
               
                  
                  
                     Abb. 4: Figurenreden, extrahiert aus 34
                            Dramen; nach Standeszugehörigkeit benannt.
               
            
            
               Sentiment-Analyse
               Durch Methoden aus der Sentiment-Analyse (die zur automatisierten Analyse von
                              Produktreviews eingesetzt wird) ließe sich z. B. analysieren, wie und ob
                              bestimmte Figuren über andere sprechen. Neben positiv / negativ wären auch
                              feinere, dramenspezifische Unterscheidungen denkbar (Feigling, Hahnrei,
                              ...).
            
            
               Kombination mit Netzwerkanalyse
               Die Kombination dieser Techniken mit Netzwerkanalyseverfahren würde es
                                erlauben, im Netzwerk auch Entitäten darzustellen über die geredet wird,
                                ohne dass sie direkt im Drama vorkommen (z. B. Gott), Kanten zwischen Knoten
                                können dann (z. B. durch Farben) auch inhaltliche, relationale Informationen
                                kodieren (X spricht viel / positiv über Y).
               Eine Netzwerkdarstellung, in der die Position der Figuren nicht mehr zufällig
                                  (oder durch Layout-Algorithmen gesteuert) ist ist ebenfalls denkbar
                                  (Abbildung 4). Dabei werden prototypischen Figurenrollen feste Positionen in
                                  einem Raster zugewiesen, so dass große Mengen an Netzwerken schnell und
                                  direkt verglichen werden können.
            
         
      
      
         
            
                             http://www.ims.uni-stuttgart.de/short/dramen
         
         
            
               Bibliographie
               
                  Blei, David / Ng, Andrew Y. / Jordan, Michael I.
                                    (2003): „Latent Dirichlet Allocation“, in: Journal of
                                    Machine Learning Research 3: 993–1022. 
               
                  Eder, Maciej / Kestemont, Mike / Rybicki, Jan (2013):
                                    „Stylometry with R: a suite of tools“, in: Digital
                                    Humanities 2013 Conference Abstracts 487-89. 
               
                  Jannidis, Fotis (2014): „Der Autor ganz nah.
                                    Autorstil in Stilistik und Stilometrie“, in: Schaffrick, Matthias / Marcus
                                    Willand (eds.): Theorien und Praktiken der
                                    Autorschaft. Berlin: De Gruyter 169-195. 
               
                  Mahlow, Cerstin / Eckart, Kerstin / Stegmann, Jens /
                                      Blessing, Andre / Thiele, Gregor / Gärtner, Markus / Kuhn, Jonas
                                      (2014): „Resources, Tools, and Applications at the CLARIN Center Stuttgart“,
                                      in: Akten der 12. Konferenz zur Verarbeitung natürlicher
                                      Sprache (KONVENS 2014) 11-21. 
               
                  Moretti, Franco (2011): Network
                                      Theory, Plot Analysis. LiteraryLab Pamphlet 2: http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf [letzter
                                      Zugriff 20. August 2014]. 
               
                  Manning, Christopher D / Raghavan, Prabhakar / Schütze,
                                          Hinrich (2008): Introduction to Information
                                          Retrieval. Cambridge: Cambridge University Press. 
               
                  Trilcke, Peer / Fischer, Frank / Kampkaspar, Dario
                                          (2015): „Digital Network Analysis of Dramatic“, in: Digital Humanities 2015 Conference Abstracts: http://dh2015.org/abstracts/xml/FISCHER_Frank_Digital_Network_Analysis_of_ Dramati/FISCHER_Frank_Digital_Network_ Analysis_of_Dramatic _Text.xml
                                          [letzter Zugriff 16. Februar 2016].
            
         
      
   



      
         
            Einleitung 
            Dieser Vortrag präsentiert Analysen und Visualisierungen eines derzeit im Aufbau befindlichen Corpus an graphischen Romanen (oder „Graphic Novels“, einer Unterform des Medium Comics) und stellt den für die Annotation dieser multimodalen Erzählform entwickelten Editor vor, der zeitgerecht zur DHd-Jahrestagung in Leipzig für den Download zur Verfügung stehen wird. Während sich die Analyse literarischer Text-Corpora bereits seit mehreren Jahren im Fokus der Digitalen Geisteswissenschaften befindet, stehen Bestrebungen zur Erforschung visueller Erzählformen wie Theater, Comics, Film, Fernsehen, sowie Computerspiele, oft eine Randerscheinung in den DH dar und vor einer Reihe ungelöster Herausforderungen. Diese bestehen sowohl in technischer - etwa in Bezug auf die automatisierte Erkennung visueller Objekte und die Annotation komplexer Bild-Text- Kombinationen – als auch in methodischer Hinsicht. Angesichts der Dominanz visueller Erzählformen seit dem frühen 20. Jahrhundert, sowie noch verstärkt in der Gegenwartskultur, stellt dies eine außerordentliche Forschungslücke dar. 
            In einer kurzen Einleitung wird der Vortrag den derzeit im Aufbau befindlichen Corpus sowie die Zielsetzungen der vom deutschen Bundesministerium für Bildung und Forschung (BMBF) geförderten Nachwuchsgruppe „Hybride Narrativität: Digitale und Kognitive Methoden zur Erforschung Graphischer Literatur“ erläutern. Darauf folgt die Vorstellung der für die Annotation entwickelten XML-Beschreibungssprache und des graphischen Editors. Im zweiten Teil des Vortrages stellen wir einige Methodenkombinationen vor, die es ermöglichen sollen, die Bild-­Text-­Verbindungen multimodaler Kulturformen, sowie deren Beitrag zur spezifischen Narrativität graphischer Romane, zu verstehen. 
         
         
             GNML-Editor: Werkzeuge zur (Halb-)Automatischen Annotation 
            Während die Analyse von Textcorpora oft bereits automatisiert möglich ist, bleibt eine automatische Analyse multimodaler Narrative derzeit eine Zukunftsvision. Im Fall von Comics und gezeichneter, sowie aus anderen Gründen nicht perspektivischer, Bilder gelingt die Objekt-Identifikation (etwa die Wiedererfassung eines vorab bekannten Charakters) nur mit viel Trainingsaufwand und recht hohen Fehlerzahlen. Auch bei der automatischen Erkennung Handschriften-ähnlicher Fonts versagen übliche Standard-OCRs. Daher führt der Weg über eine Annotation des Bild-­Materials mit anschließender Analyse der Annotationen und Bild-Daten. Hierzu wird im Rahmen unseres Forschungsprojektes die XML-Sprache „Graphic Narrative Markup Language“ (GNML) entwickelt, welche die visuellen und textuellen Aspekte Graphischer Literatur beschreibt. GNML baut auf der „Text Encoding Initiative“ (TEI), und damit auf etablierten Standards, auf. Basierend auf den GNML-Annotationen können die in der graphischen Literatur enthaltenen Texte analysiert werden, Auswertungen der Bildinhalte vorgenommen, oder deren Kombination analysiert werden. 
            Um die Fehleranfälligkeit bei der Annotation gering zu halten wird ein graphischer GNML-Editor entwickelt. Dieser unterstützt Fachwissenschaftler bei der effizienten Annotation mit Mechanismen wie Autovervollständigung von Charakter-Namen oder integrierten Rechtschreibprüfungen. Durch eine halb-automatische Erfassung wird die Annotation beschleunigt und so erst der Aufbau eines größeren Corpus ermöglicht. Teil des Editors ist eine Erkennung der Panel-Strukturen, sowie Werkzeuge, welche die Eckpunkte einer Sprechblase oder eines Textkästchens automatisch ermitteln. Ergänzt werden diese Werkzeuge um eine effiziente Charakter-Erfassung.
            Da sich die Konzepte des Editors (visuelle Objekte mit graphischen und textuellen Eigenschaften) nicht nur auf Comics beschränken sondern auch auf andere Bild-Text- Kombinationen anwendbar sind, lässt sich der Editor auf eine Obermenge solcher Formate erweitern. Diese Generalisierung erlaubt es, eine XML-basierte Annotationssprache zu hinterlegen und automatisch einen entsprechenden Editor zu generieren, sowie Daten in der hinterlegten Annotationssprache zu erfassen. Damit kann der Editor auch in der Annotation anderer multimodaler Medien Anwendung finden.
         
         
             Analysen und Visualisierungen eines Corpus Graphischer Romane
            Der zweite Teil des Vortrags stellt Ansätze vor, die exemplarisch für einige strukturelle Bestandteile von Comics (und insbesondere des graphischen Romans) Methoden der Bild- und Textanalyse miteinander verbinden. Für die digitale Literaturwissenschaft entwickelte Zugänge wie das Topic Modelling sind für solche Kulturformen aufgrund ihrer Bildlastigkeit nur von beschränkter Relevanz. Ansätzen zur computergestützten Bilderkennung und der Analyse großer Bildmengen fehlt hingegen bisher oft das narrative Erkenntnisinteresse. Erschwerend kommt noch hinzu, dass die Konzepte der Narratologie meist für literarische Texte entwickelt wurden und den Spezifika multimodalen Erzählens häufig nicht gerecht werden.
            In einer ersten Analyse des derzeit noch in Erstellung befindenden Gesamtkorpus
          von rund 300 graphischen Romanen vergleichen wir die historische Entwicklung der
          visuellen und Textebenen ihrer Buchcovers. Dazu gehören sowohl grammatische und
          semantische Auswertungen der Romantitel mit Hilfe des Stanford Parser (vgl. De
          Marneffe et al. 2006), als auch der farblichen und stilistischen Gestaltung. In
          einem weiteren Schritt widmen wir uns detaillierteren Analysen eines ersten
          Sub-Corpus, der aus den zehn meist zitierten Titeln des Gesamtcorpus besteht.
          Zwar lassen sich aufgrund der geringen Zahl hier keine Genre-Vergleiche
          anstellen, oder stichhaltig historische Entwicklungen nachverfolgen.
          Beispielhaft können allerdings narrative Entwicklungen dargestellt werden: so
          kombinieren wir Netzwerkanalysen der Figuren mit deren visueller Prominenz und
          zugeordnetem Textanteil, sowie mit stilistischen Analysen dieser Figurentexte.
          Weiters stellen wir, im Anschluss an Arbeiten von Lev Manovich (vgl. u. a.
          Manovich 2012), explorative Visualisierungen aller Einzelseiten im Gesamtverlauf
          der Erzählung vor.
            Abschließend wendet sich der Vortrag der Frage zu, ob die Text-Bild-Verbindungen
            multimodaler Narrative mit solchen Methodenkombinationen aus der digitalen
            Literatur- und Bildwissenschaft zu erfassen sind, oder sich durch die
            Operationalisierung alternativer Ansätze aus der intermedialen Narratologie,
            etwa Rick Altmans Konzept des „Following“ (vgl. Altman 2008), eigenständige
            Analysemethoden entwickeln lassen.
         
      
      
         
            
               Bibliographie
               
                  Manovich, Lev (2012): „How to Compare One Million Images?", in: Berry, David (ed.): Understanding Digital Humanities Basingstoke: Palgrave Macmillan 249-278.
               
                  Altman, Rick (2008): A theory of narrative. Columbia University Press.
               
                  De Marneffe, Marie-Catherine / MacCartney, Bill / Manning, Christopher D. (2006): „Generating Typed Dependency Parses from Phrase Structure Parses“, in: Proceedings of Language Resources and Evaluation (LREC) 6: 449-454.
            
         
      
   



      
         
            Digitale
      Korpora entstehen unter bestimmten Voraussetzungen, werden von verschiedensten
      Institutionen gefördert und haben unterschiedliche Ziele in Bezug auf Qualität und
      Quantität. In vielen Fällen müssen Forscherinnen weitere Verbesserungen vornehmen,
      bestehende Daten erweitern und verbessern, im schlimmsten Fall auch neu erheben. In
      diesem Paper beschreiben wir eine Vielzahl einfach(st)er Probleme, die es zu
      bewältigen gilt, will man ein Korpus bestehend aus möglichst vielen genuin
      deutschsprachigen Dramen computergestützt analysieren. Ausgehend von den Beständen
      des TextGrid Repository (TextGrid
      Konsortium 2015) soll mittels einer simplen grafischen Oberfläche – abrufbar in
      jedem Webbrowser – ein Programm zur Verfügung gestellt werden, das sich
      spielähnlicher Mittel bedient: die Nutzer auffordert, mehrere Level zu durchlaufen,
      Punkte zu sammeln und mit jeder Eingabe das Korpus zu verbessern, um schließlich
      Ausgangsmaterial für eine Vielzahl von Fragestellungen zu bieten. 
          Crowdsourcing, Social Editing und viele verwandte Begriffe sind Konzepte, die
        innerhalb der Digital-Humanities-Community in den vergangenen Jahren einen kleinen
        Hype erfahren haben. An Umsetzungen mangelt es, während man sich noch über die
        Definitionen streitet. Dabei sind die Lösungsansätze sehr vielversprechend – allen
        voran die von Zooniverse etablierten
        Projekte, die sich nun auch geisteswissenschaftlichen Themen widmen. Es werden alte
        Texte transkribiert und jede Person, die über Computer und Internetanschluss
        verfügt, kann einen aktiven Beitrag leisten und die Forschung unterstützen.
        Erfahrungen aus bereits gut etablierten Ansätzen, wie dem Projekt DigitalKoot, entwickelt von der
        Finnischen Nationalbibliothek, zeigen, dass die Anwendung und Umsetzung
        spielerischer Verfahren durchaus einen Mehrwert für die Wissenschaft generieren
        können. Darüber hinaus können Aufgaben gemeistert werden, die durch einen Einzelnen
        oder auch eine kleinere Forschungsgruppe niemals würden selbst bewältigt werden
        können. Voraussetzung dafür ist das Interesse und die Teilnahme vieler Personen an
        der zu bewältigenden Aufgabe. Diese so zu isolieren und danach zu vereinfachen, dass
        auch Laien damit umgehen können, stellt die Herausforderung dar. Viel Resonanz
        bekommen Projekte wie EyeWire oder GalaxyZoo, beides
        naturwissenschaftliche Citizien-Science-Vorhaben. Im Projekt GalaxyZoo geht es um
        die Klassifizierung und Beschreibung von Galaxien. Das Projekt war in der Lage, 50
        Millionen Klassifizierungen zu sammeln, und das innerhalb seines ersten
        Betriebsjahres (2007, vgl. Prestopnik 2011: 2). 
         Methodisch betrachtet bieten Konzepte, die auf der zunehmenden Beteiligung der
          sogenannten Crowd aufbauen, ein großes Potential zur
          Weiterentwicklung oder Herausbildung neuer Forschungsfragen und -themen. Das bereits
          erwähnte Crowdsourcing  kann dabei als Überbegriff für
          verschiedene Ansätze gesehen werden, die zwar z. T. deutlich voneinander abzugrenzen
          sind, in manchen Bereichen jedoch deutliche Ähnlichkeiten aufweisen. In einem groben
          Kategorisierungsversuch kann eventuell eine Zweiteilung vorgenommen werden, um einen
          besseren Überblick über diese verschiedenen und doch ähnlichen Methoden und Konzepte
          zu gewinnen. Serious Games, Games with a
          Purpose und Meaningful Play wollen das Spiel als
          Medium nutzen um bestimmte Inhalte oder Absichten zu transportieren und dem Nutzer
          ein bestimmtes Problem näher zu bringen. Ansätze wie Gamification, Gameful Design, Social Editing oder Human Computation nutzen
          gezielt einzelne Elemente aus dem Spieldesign, um einen eigentlich spielfremden
          Kontext anzureichern und durch die Schaffung einer neuen Atmosphäre attraktiver für
          potentielle Nutzer_innen zu gestalten.  
         Während Play(s) sich methodisch eher in der zweiten genannten Kategorie wiederfinden
            soll, ist wichtig zu betonen, dass der Erfolg solcher Projekte eng mit der
            Entwicklung und dem Design selbst zusammenhängt. Die Oberfläche sollte
            beispielsweise ansprechend gestaltet bzw. angemessen bezogen auf die Zielgruppe und
            einfach zu bedienen sein. Als  Spielelemente können Levels, das Sammeln von Punkten
            in Kombination mit einfachen Spielanweisungen dienen. Neben der tatsächlichen
            Entwicklung einer neuen Anwendung hängt ein großer Teil des Erfolgs von der zu
            tätigenden Handlung der Teilnehmer_innen ab. Die Aufgabe, die im Rahmen eines
            Crowdsourcing oder Citizien-Science-Projektes von den Teilnehmer_innen bearbeitet
            werden soll, ist im besten Fall einfach zu verstehen und in simple Teilbereiche
            unterteilt. Gleichzeitig unterliegt die Einbindung von freiwilligen, fachfremden
            Teilnehmer_innen gewissen eher impliziten und wenig ausgesprochenen Regeln. So
            sollten die Teilnehmenden generell als Partner oder Mitarbeiter_innen betrachtet
            werden und nicht als günstige Arbeitskräfte. Zudem sollten sie nicht zur Bewältigung
            von Aufgaben angehalten werden, die eigentlich einfacher und besser von einem
            Computer ausgeführt werden könnten. Diese Grundethik sollte bei jeder Umsetzung
            einer neuen Idee zumindest mitbedacht werden, um künftige Teilnehmer_innen nicht zu
            verärgern oder zu verschrecken.
         Die wenigen bisher gesammelten und verfügbaren Erfahrungen aus Projekten für die Geisteswissenschaft sollen nun ausgewertet werden und in die Umsetzung einer neuen Projektidee eingebracht werden. "Play(s)" ist der Name der Anwendung, die sich damit befassen soll, ein literaturwissenschaftliches Volltext-Korpus anzureichern. Das TextGrid-Repositorium bietet dafür optimale Voraussetzungen: alle Texte sind im TEI-Format erfasst und diese Quelle ist frei zugänglich.
         In diesem Projekt knüpfen wir an die von einer Projektgruppe (vgl. Trilcke et al. 2015) bereits herausgefilterten Dramen des Repositoriums an. Dabei wurden bereits in einem manuellen Durchgang allen Sprecherinstanzen im Auswahlkorpus eindeutige Namen (IDs) zugewiesen, um eine Ausgangsbasis für Netzwerkanalysen zu schaffen. In diesen Vorarbeiten wurden die genuin deutschen Texte ausgewählt und dabei aus den insgesamt 666 Dramen auf 465 Werke zurückgegriffen. Um diese Analysen mit einer quantitativ und qualitativ erweiterten Quellenbasis zu vertiefen, bedarf es einer noch genaueren Referenzierung. So sollten zum Beispiel die als Sprecher auftretenden Personengruppen aufgelöst werden und zu diesen die beteiligten Akteure genannt werden. Auch eine Klassifizierung des Geschlechtes, der sozialen Stellung und weitere Features sind denkbar, um differenzierte Analysen tätigen zu können. Hier wird deutlich, dass jeder Text einer bestimmten Aufbereitung bedarf, die aber in vielen kleinen Einzelschritten erfolgen kann, da die Informationen und einzelnen semantischen Anreicherungen in ihren Kategorien unabhängig voneinander sind.
          Innerhalb des TextGrid-Korpus beschränken wir uns auf die Betrachtung der Dramen und
              innerhalb derer sind es die Strukturinformationen, die auf Grundlage des XML-Codes
              Netzwerkanalysen auf Basis des gemeinsamen Auftretens in einer Szene ermöglichen.
              Gemeinsames Auftreten heißt in diesem Fall, dass innerhalb einer Szene alle
              Sprecher_innen in Verbindung gebracht werden. Dazu gilt es die einzelnen Akteure
              ausfindig zu machen, da das Korpus selbst keine Information, wie man sie im
              TEI-Attribut who (TEI Consortium 2015) erwarten kann, mitliefert. Betrachtet man als
              Beispielfall das Drama "Fraw Wendelgard" von Nicodemus Frischlin, finden sich
              innerhalb der Sprecherbenennung drei verschiedene Schreibweisen, die alle auf die
              Gräfin Wendelgard verweisen: Wendelgard, Wendelgart und Wendelgardt. In einem
              anderen Werk taucht in einem Dialog zwischen Faust und Mephistopheles ein einziges
              Mal der Sprecher "Mephistoph" auf. Häufig beobachtet man Akteure, die mit einem
              unbestimmten Artikel eingeführt werden, im Folgenden aber mit bestimmtem oder ohne
              Artikel angegeben werden, wobei offensichtlich ist, dass es sich um die vorangehende
              genannte Entität handelt. 
         Die Ursache kann drucktechnisch bedingt in den Buchausgaben liegen, in denen Sprechernamen abgekürzt werden, um Platz und Papier zu sparen, es können auch schlicht Fehler im Satz auftauchen und eine weitere Fehlerquelle kann der Digitalisierungsprozess sein. In all diesen Fällen ist die Korrekturaufgabe denkbar simpel: man muss jene Sprecher zusammenführen, bei denen es sich offensichtlich um die gleiche Person handelt. Getreu der Buchausgaben handelt es sich dabei nicht um Fehler, das Encoding muss hier schlicht um semantische Information erweitert werden, wie es das Attribute who in den TEI Guidelines vorsieht. Dazu zählen auch Fälle, in denen das Markup innerhalb des TextGrid-Korpus fehlerhaft ist. Das betrifft leere speaker-Elemente, solche, in denen noch Teile der Bühnenanweisung mit einfließen und auch jene, die noch ein leeres Element stellvertretend für zum Beispiel einen Seitenumbruch beinhalten und dadurch als Auswertung des Inhaltes von tei:speaker ein Leerzeichen voran steht.
          Man findet außerdem bei gemeinsam sprechenden Personengruppen unterschiedliche
                Nennungen. In einem Drama Friedrich Kaisers ist eine solche Aggregation mit “HELFER
                UND ROBERT”  benannt, später folgt aber "ROBERT UND HELFER”. Eine bestimmte vom
                Autor intendierte Hierarchie soll das Datenmodell nicht abdecken und somit gilt es
                die verschiedenen Zeichenketten als eine Entität zu betrachten. Zudem soll die
                Tiefenauszeichnung dieser Elemente weiter gehen und jeder Gruppe die einzelnen,
                sofern bestimmbaren, Akteure zugewiesen werden. 
         Diesen Beobachtungen folgt die Spielstruktur.
         In einem ersten Level gilt es die unterschiedlich benannten aber in der fiktiven Welt gleichen Sprecher zu identifizieren. Dazu werden alle unterschiedlichen Zeichenketten innerhalb der tei:speaker-Elemente eines Dramas zunächst in der Reihenfolge ihres ersten Auftretens gelistet. Mutmaßlich gleiche Namen sind nacheinander auswähl- und abspeicherbar. Ist dies für ein Drama vollständig geschehen, kann dieses Drama als “gelöst” markiert werden.
         Weiterhin gilt es Aggregationen ausfindig zu machen (Level 2).
         Diese Aggregationen sollen schließlich aufgelöst werden (Level 3). Dazu sind nicht nur die an einer Gruppe beteiligten Akteure zu nennen, sondern auch deren Vollständigkeit zu deklarieren. Es kann zum Beispiel das Volk sprechen und weiterhin einzelne Personen aus dem Volk auftreten. Diese sind Teil des Volkes, die Gruppe selbst ist aber eine weitaus größere und daher unvollständig durch die einzelnen Akteure belegt. Sprechen zwei auch näher bestimmte Einzelpersonen gemeinsam, so kann diese Gruppe vollständig aufgelöst werden.
         Die Geschlechter der Akteure sind in Level 4 zu bestimmen. Dabei ist zu wählen aus male, female, both, und unknown. Die letzte Gruppe umfasst dann schließlich auch metaphysische Konstrukte, die personifiziert auftreten.
         In Level 5 sollen diese dann genauer spezifiziert werden. Dabei stehen die Kategorien
                  Tier, metaphysisches Wesen (z. B. Gottheit, Hexen und Magier) und Eigenschaft /
                  Gefühl / Moral zur Auswahl.
         Schließlich lässt sich noch der soziale Status bestimmen, sofern Berufsbezeichnungen, Adelstitel oder andere Indikatoren ausfindig zu machen sind.
          Zwischen den einzelnen Levels gilt es die Eingaben anderer Spieler zu verifizieren
                    oder auch zu falsifizieren. Diese Eingabe wirkt sich auf die eigenen Punkte immer
                    positiv aus, die jeweils anonym bleibende begutachtete Spielerin wird bei
                    Fehleingaben aber Punktabzüge bekommen. Da die Dramen immer zufällig gewählt werden
                    und auch mehrfach erfasst werden, stehen damit verschiedene Qualitätskontrollen zur
                    Auswahl, die auch kontinuierliche nicht sinnvolle Eingaben erkennen lassen. Die
                    betreffenden Spielerinnen können weiterspielen, finden aber nur noch eine
                    persönliche Highscoreliste vor, während sie aus den Highscorelisten anderer getilgt
                    werden und ihre Eingaben auch nicht in den weiteren Forschungsprozess Einzug halten.
                    Zudem stehen die Daten für 465 Dramen im LINA Zwischenformat (vgl. Trilcke et al. 2015) zur Verfügung, die mit den in
                    Level 1 erfassten Eingaben übereinstimmen sollten. Zudem können alle hier
                    getätigten Erhebungen ebenfalls in das Zwischenformat einfließen, womit sie dann
                    für die Netzwerkanalysen des Projektes zur Verfüggun stünden. Bei Bedarf ließen
                    sich die Ergebnisse sogar direkt in die Quelldokumente übernehmen.
                  
          Kritisch betrachtet stammen aus der Welt der Computerspiele die Levelstruktur und
                    einzelne Elemente, wie Avatar und Highscoreliste. Tatsächlich ist das Angebot eines,
                    das Social Editing auf einfachste Fragestellungen hin anwendet und jeder Spielerin
                    die Möglichkeit bietet, aktiv an der Tiefenerschließung von literarischen Texten
                    mitzuwirken. Außerdem gibt es einen didaktischen Aspekt, da komplexe Probleme im
                    Hinblick auf Korpuserstellung implizit aufgezeigt werden. 
      
      
         
            
               Bibliographie
               
                  
                  Prestopnik, Nathan R. (2011): Citizen
                          Science Case Study. Galaxy Zoo / Zooniverse http://citsci.syr.edu/system/files/galaxyzoo.pdf [letzter Zugriff
                          15. Oktober 2015]. 
               
                  TEI Consortium (2015): TEI P5.
                          Guidelines for Electronic Text Encoding and Interchange. Version 2.9.0.
                          Updated on 9th October 2015. http://www.tei-c.org/Guidelines/P5/ [letzter Zugriff 15. Oktober
                          2015]. 
               
                  TextGrid Konsortium (2015): Die
                          Digitale Bibliothek bei TextGrid
                  https://textgrid.de/digitale-bibliothek [letzter Zugriff 15.
                            Oktober 2015].
               
                  Trilcke, Peer / Fischer, Frank / Göbel, Mathias /
                              Kampkaspar, Dario (2015): Network Analysis of
                              Dramatic Texts
                  https://dlina.github.io/about/ [letzter Zugriff 15. Oktober
                                2015].
            
         
      
   



      
         Sprachwissenschaftler_innen und Jurist_innen haben gemein, dass sie mit Texten
        arbeiten. Der juristische Umgang mit Texten ist allerdings geprägt und überformt von
        den Verfassungsgeboten der Rechtssicherheit und Vorhersehbarkeit der Interpretation
        von Normtexten, die eine disziplinäre Standardisierung erfordern: „Im Gegensatz zur
        grundsätzlich nicht normierbaren Alltagssprache oder zur Offenheit
        literaturwissenschaftlicher Interpretationen ist die Sprache des Rechts auf
        weitestgehende Verbindlichkeit, Deutlichkeit und Disziplin (zumindest)
        angelegt“ (Jeand'Heur 1998: 1287). Juristische Fachtexte lassen sich deshalb nur mit
        einem stark spezialisierten fachsprachlichen Sach- und (impliziten) Methodenwissen
        adäquat verstehen (vgl. hierzu Vogel 2012b: 34ff.). 
         Das spezialisierte Fach(sprach)wissen in der Jurisprudenz hat mindestens drei Funktionen: Erstens soll es juristische Entscheidungsarbeit valide und zuverlässig organisieren; zweitens soll es die Komplexität der Lebenswelt auf ‚rechtsrelevante’ und verfahrenssichere, also in juristischen Kategorien verarbeitbare, Ausschnitte reduzieren; drittens stiftet es binnendisziplinäre Identität (Ingroup): Wer die Sprache und die ‚Denke’ der Jurisprudenz nicht beherrscht, hat vor Gericht schlechte Karten.
         All diese in der Regel für Laien nicht erkennbaren Funktionen stehen hinter sog. „
          
            Subsumtionen“, also der juristischen Auslegungsmethode. Damit ist kein rechtspositivistisches ‚Anwenden‘ eines objektiv oder subjektiv vorgegebenen ‚Gesetzesinhalts‘ gemeint. Die juristische „Auslegung“ von Normen ist vielmehr ein komplexer Prozess der Ko(n)textualisierung von Lebenswelt (zu beurteilender Sachverhalt, „Fall“) und Textwelt (inter- und intratextuelle Verknüpfung von Norm- und dogmatischen Texten). Lebens- und Textwelt sind dabei nicht lediglich ‚gegeben’ und „im Sinne eines kybernetischen Informationsübertragungsmodells“ im Hinblick auf ‚die’ Norm zu „decodieren“ (so noch
          Baden 1977: 14ff.; vgl. dazu kritisch
          Busse 2005). Sie ‚geben’ dem hermeneutisch tätigen Rechtsarbeiter vielmehr sinnlich wahrnehmbare Hinweisreize
          (Gumperz 1982: 131f.), die gemeinsam mit bereits bestehendem, institutionalisiertem juristischen Norm(sprach)wissen in mentalen Modellen Sinn-voll gemacht werden können
          (Hörmann 1980). Rechtsnormen sind also keine absoluten Entitäten, sondern Ergebnis konstruktiver Textarbeit mit unterschiedlichen versprachlichten Eingangsdaten und Geltungsansprüchen
          (Müller et al. 1997; Felder 2003).
        
         Seit rund 30 Jahren widmet sich die Rechtslinguistik als gemeinsame Teildisziplin von
          Rechts- und Sprachwissenschaft diesen Vertextungsverfahren im Recht (vgl. Vogel 2016). Juristische
          und linguistische Untersuchungen erfolgten dabei bislang ausschließlich mittels
          qualitativer Zugänge und auf Basis weniger hundert Texte. Die Ergebnisse geben
          wichtige Einblicke in die Mikroprozesse unseres sprachbasierten Rechtssystems, sei
          es vor Gericht, in der Verwaltung oder in der Gesetzgebung (Überblick bei Felder / Vogel 2016). In der
          frühen Rechtskybernetik und heutigen Rechtsinformatik hingegen wird das Recht meist
          als logisch operierendes Ontologiesystem zu formalisieren versucht, das die
          semantisch Struktur seiner realen performativen Bearbeitung jedoch vernachlässigt
          (vgl. zur Kritik am „Subsumtionsautomaten 2.0“ Kotsoglou 2014; Vogel 2015).
          Erst neuere Ansätze einer „evidenzbasierten Jurisprudenz“ (Hamann 2014) und
          rechtstheoretisch fundierten Korpuslinguistik in den USA (Mouritsen 2010, 2011) sowie
          in Deutschland (Vogel
          2012a; Vogel et al. 2015; Hamann 2015) versprechen praxisnahe Analysen und
          Einsichten in die ‚Makroökonomik‘ juristischer Fachsprache und -kommunikation. 
         An dieser Stelle setzt ein seit 2014 laufendes und von der Heidelberger Akademie der
            Wissenschaften finanziertes Projekt zur Konzeption und Auswertung eines
            „Juristischen Referenzkorpus“ (JuReko) an (Vogel / Hamann 2015). Ziel
            des Projektes, das den Kern der „International Research Group Computer Assisted
            Legal Linguistics“ (CAL 2 2014-2016) bildet, ist im ersten Schritt der Aufbau eines
            kontrollierten, zunächst statischen Fachtext-Korpus, das alle wichtigen Textsorten
            aus Judikative, Legislative und Rechtswissenschaft umfasst (v.a. Aufsätze aus
            juristischen Fachzeitschriften, Entscheidungstexte und Normtexte; Zielgröße: rund
            eine Milliarde fortlaufender Wortformen). Die Textdaten werden zunächst im
            html-Format gewonnen und anschließend in mehreren Konvertierungschritten
            TEI-P5-konform kodiert. Dafür kommen xsl-Transformationen zum Einsatz, die auf die
            unterschiedlichen Webseitenstrukturen angepasst werden. Im Anschluss werden die
            Texte mit Part-of-Speech und weiteren Annotationen und Metadaten angereichert, wobei
            die speziellen Anforderungen einer rechtslinguistischen Textanalyse und
            -verarbeitung im Vordergrund stehen. 
         Das Korpus bildet im zweiten Schritt die Grundlage für die Erprobung neuer computerlinguistischer Methoden zur Analyse insbesondere juristischer Semantik bzw. Dogmatik sowie zur Beschreibung von Wortschätzen und grammatischen Mustern in verschiedenen Rechtsbereichen auf Basis geeigneter Metriken. In Zusammenarbeit mit Praktikern aus Gesetzgebung und Rechtsprechung werden weitere Untersuchungsprojekte abgeleitet und vorbereitet. Hierzu zählt etwa die Entwicklung von Werkzeugen für die rechtslinguistisch wie korpusstatistisch-empirisch fundierte Optimierung der Gesetzesredaktion.
         Der Vortrag stellt das Infrastrukturvorhaben „JuReko“ vor und diskutiert Möglichkeiten und Grenzen des durch die Projektgruppe entwickelten Ansatzes der „Computergestützten Rechtslinguistik“ als komplementären Beitrag zur qualitativen, juristischen Hermeneutik. Dabei wird anhand von Beispielen sowohl auf die textlinguistischen als auch technischen Details des Projektes eingegangen. Im Ausblick steht die Erweiterung des JuReko um Rechtstexte des britischen Case Law als Ausgangspunkt für ein Europäisches Rechtskorpus (European Law Corpus) und damit eine weltweit einzigartige Grundlage für rechts(sprach)kulturvergleichende Studien.
      
      
         
            
               Bibliographie
               
                  Baden, Eberhard (1977): Gesetzgebung
                  und Gesetzesanwendung im Kommunikationsprozess. Studien zur jur.
                  Hermeneutik u. zur Gesetzgebungslehre. Baden-Baden:
                  Nomos-Verlagsgesellschaft. 
               
                  Busse, Dietrich (2005): „Ist die Anwendung von
                    Rechtstexten ein Fall von Kommunikation? Rechtslinguistische Überlegungen
                    zur Institutionalität der Arbeit mit Texten im Recht“, in: Lerch, Kent D.
                    (ed.): Die Sprache des Rechts. Recht Vermitteln:
                    Strukturen, Formen und Medien der Kommunikation im Recht. 3 Bände. Berlin:
                    Walter De Gruyter 23–54. 
               
                  CAL2
                   (2014-2016): International Research Group: Computer Assisted Legal
                      Linguistics. University of Freiburg http://www.cal2.eu/ [letzter Zugriff
                      08. Januar 2016]. 
               
                  Felder, Ekkehard (2003): Juristische
                        Textarbeit im Spiegel der Öffentlichkeit. Berlin / Boston: De
                        Gruyter. 
               
                  Felder, Ekkehard / Vogel, Friedemann (eds.) (2016): Handbuch Sprache im Recht. Berlin / Boston: De
                          Gruyter Mouton 
               
                  Gumperz, John Joseph (1982): Discourse strategies. Cambridge: University Press. 
               
                  Hamann, Hanjo (2014): Evidenzbasierte
                              Jurisprudenz. Methoden empirischer Forschung und ihr Erkenntniswert für
                              das Recht am Beispiel des Gesellschaftsrechts. Tübingen: Mohr
                              Siebeck. 
               
                  Hamann, Hanjo (2015): „Der "Sprachgebrauch" im
                                Waffenarsenal der Jurisprudenz. Die Rechtspraxis im Spiegel der
                                quantitativ-empirischen Sprachforschung“, in: Vogel, Friedemann (ed.): Zugänge zur Rechtssemantik. Interdisziplinäre Ansätze
                                im Zeitalter der Mediatisierung zwischen Introspektion und Automaten. Berlin
                                / New York: Walter De Gruyter 184-204. 
               
                  Hörmann, Hans (1980): „Der Vorgang des Verstehens“, in:
                                  Kühlwein, Wolfgang (ed.): Sprache und Verstehen.
                                  Tübingen: Narr 17–29. 
               
                  Jeand'Heur, Bernd (1998): „Die neuere Fachsprache der
                                    juristischen Wissenschaft seit der Mitte des 19. Jahrhunderts unter
                                    besonderer Berücksichtigung von Verfassungsrecht und Rechtsmethodik“, in:
                                    Hoffmann, Lothar / Burkhardt, Armin / Ungeheuer, Gerold / Wiegand, Herbert
                                    Ernst / Steger, Hugo / Brinker, Klaus (eds.): Fachsprachen: ein internationales Handbuch der Fachsprachenforschung
                                    und Terminologiewissenschaft. (= Handbücher zur Sprach- und
                                    Kommunikationswissenschaft 14.1). Berlin: De Gruyter 1286–1295. 
               
                  Kotsoglou, Kyriakos N. (2014): „Subsumtionsautomat 2.0.
                                      Über die (Un-)Möglichkeit einer Algorithmisierung der Rechtserzeugung“, in:
                                      Juristenzeitung 69, 9 451–457. 
               
                  Mouritsen, Stephen C. (2010): „The Dictionary Is Not a
                                        Fortress: Definitional Fallacies and a Corpus-Based Approach to Plain
                                        Meaning“, in: Brigham Young University Law Review
                                        1915–1980 http://www.lawreview.byu.edu/archives/2010/5/10Mouritsen.pdf
                                        [letzter Zugriff 07. November 2012]. 
               
                  Mouritsen, Stephen C. (2011): „Hard Cases and Hard
                                          Data: Assessing Corpus Linguistics as an Empirical Path to Plain Meaning“,
                                          in: The Columbia. Science and Technology Law Review
                                          8: 156–205 http://www.stlr.org/cite.cgi?volume=13&article=4 [letzter
                                          Zugriff 07. November 2012]. 
               
                  Müller, Friedrich / Christensen, Ralph / Sokolowski,
                                              Michael (1997): Rechtstext und Textarbeit (=
                                              Schriften zur Rechtstheorie). Berlin: Duncker & Humblot. 
               
                  Vogel, Friedemann (2012a): „Das Recht im Text.
                                                Rechtssprachlicher Usus in korpuslinguistischer Perspektive“, in: Felder,
                                                Ekkehard / Müller, Marcus / Vogel, Friedemann (eds.): Korpuspragmatik. Thematische Korpora als Basis
                                                diskurslinguistischer Analysen. Berlin / Boston: De Gruyter 314–353. 
               
                  Vogel, Friedemann (2012b): Linguistik
                                                  rechtlicher Normgenese. Theorie der Rechtsnormdiskursivität am
                                                  Beispiel der Online-Durchsuchung (= Sprache und Wissen 9). Berlin / Boston:
                                                  De Gruyter. 
               
                  Vogel, Friedemann (2015): „Zwischen Willkür, Konvention
                                                    und Automaten: Die interdisziplinäre Suche nach Bedeutungen in Recht und
                                                    Gesetz“, in: Vogel, Friedemann (ed.): Zugänge zur
                                                    Rechtssemantik. Interdisziplinäre Ansätze im Zeitalter der
                                                    Mediatisierung zwischen Introspektion und Automaten. Berlin / New York:
                                                    Walter De Gruyter. 
               
                  Vogel, Friedemann (2016): „Rechtslinguistik: Zur
                                                      Bestimmung einer Fachrichtung“, in: Felder, Ekkehard / Vogel, Friedemann
                                                      (eds.): Handbuch Sprache im Recht (= Handbücher
                                                      Sprachwissen 12). Berlin / Boston: De Gruyter Mouton. 
               
                  Vogel, Friedemann / Christensen, Ralph / Pötters,
                                                          Stephan (2015): Richterrecht der Arbeit –
                                                          empirisch untersucht. Möglichkeiten und Grenzen computergestützter
                                                          Textanalyse am Beispiel des Arbeitnehmerbegriffs. Berlin: Duncker
                                                          & Humblot. 
               
                  Vogel, Friedemann / Hamann, Hanjo (2015): „Vom corpus
                                                            iuris zu den corpora iurum – Konzeption und Erschließung eines juristischen
                                                            Referenzkorpus (JuReko) “, in: Jahrbuch der Heidelberger
                                                            Akademie der Wissenschaften für 2014. Heidelberg: Winter. 
            
         
      
   



      
         
            Einleitung
            In diesem Beitrag werden die Probleme skizziert, die sich aus Praktiken
          Österreichischer Archive bei der Umsetzung von online-Projekten ergeben. Die
          Beschränkung auf Österreich ergibt sich, um die Beispiel-Palette überschaubar zu
          halten; die Problembereiche und Lösungsvorschläge lassen sich allerdings
          allgemein anwenden. State-of-the-art Projekte von Bibliotheken und Archiven im
          deutschsprachigen Raum, die den neusten Stand der Forschung umsetzen, werden in
          einem ersten Schritt beschrieben. Der zweite Abschnitt skizziert Gründe dafür
          und Konsequenzen daraus, dass diese Standards häufig nicht herangezogen werden.
          Schließlich werden Lösungsvorschläge präsentiert und eine Agenda vorgeschlagen,
          die die Situation nachhaltig verbessern könnte. Diese wird im Rahmen des
          Vortrags auf der DHd 2016 im Zentrum stehen. Dort werden auch die hier allgemein
          beschriebenen Schwierigkeiten anhand mehrerer Beispielprojekte illustriert.
         
         
            Forschungsstand / Vorbildprojekte
            Vor allem im Bereich der digitalen Edition haben sich im deutschsprachigen Raum
            auf breiter Ebene Standards und “best practices” entwickelt, die von einer
            etablierten Community umgesetzt werden. Umfangreiche Bibliotheken publizierter /
            rechtefreier Werke bieten etwa das 
                  Deutsche Textarchiv
               
            oder der Forschungsverbund 
                  TextGrid
               . Diese Plattformen greifen zur Annotation der zur Verfügung gestellten
            Texte auf die Auszeichnungssprache XML und das Datenformat TEI zurück, die sich
            in den DH als Standards etabliert haben, und machen auch ihre entsprechenden
            Tools verfügbar. Die Texte selbst sind creative-commons-lizenziert und können
            von der Website des Deutschen Textarchivs downgeloadet werden, ebenso wie aus
            dem TextGrid Repository, das darüberhinaus als
            Langzeitarchiv fungiert. 
            Im Bereich der Beforschung von Archivbeständen überwiegen im digitalen Raum
              ebenfalls Editionsprojekte, die die erwähnten Standards zur Anwendung bringen.
              Beispiele hierfür sind das 
                  Heinrich-Heine-Portal
               , das u. a. vom Deutschen
              Literaturarchiv Marbach unterstützt und vom Trier Centre for
              Digital Humanities umgesetzt wird, oder die 
                  Digitale Edition der Korrespondenz August Wilhelm Schlegels
               ,
              an der letztgenanntes Zentrum ebenfalls beteiligt ist. Die Herzog August
              Bibliothek Wolfenbüttel erarbeitet für ihre in der Reihe 
                  Editiones Electronicae Guelferbytanae
               
              publizierten digitalen Editionen ebenfalls neue Editionstechniken auf Basis der
              TEI und stellt dazu Dokumentation zur Verfügung. Ein positives Beispiel mit
              Österreichischer Beteiligung (eine Kooperation zwischen dem Literaturarchiv der
              ÖNB und dem Institut für Germanistik der Universität Hamburg) ist die
              Hybridedition des Briefwechsels August Sauer – Bernhard Seuffert (ÖNB 2015), die die Metadaten der online
              edierten Briefe entsprechend TEI Standards codiert. 
            Wegweisend speziell für die Arbeit mit literarischen Nachlässen ist auch das
                Virtual Research Environment (VRE) 
                  SALSAH 
                (System for Annotation and Linkage of
                Sources in Arts and Humanities) des Digital Humanities
                Lab der Universität Basel, das ähnlich dem System von Susan Schreibmans
                
                  versioning machine
                funktioniert.
                
            Im Bereich der Archivierung und Bereitstellung von elektronischen Publikationen,
                  Multimedia-Objekten und anderen digitalen Daten wird in Österreich das Projekt
                  
                  e-Infrastructures Austria
                umgesetzt, das u. a. mit Horizon
                    2020 verbunden ist und wichtige Impulse für Forschungswebsitearchivierung
                    bringen könnte. 
         
         
            Problemanalyse
            Die in Österreichischen Literaturarchiven aufbewahrten Bestände werden im Rahmen von wissenschaftlichen Projekten mit direkt an der Institution angesiedelten Mitarbeitenden erforscht und publiziert. Aufgrund der Vergabepolitik des FWF Forschungsfonds, der in den allermeisten Fällen Geldgeber dieser Unternehmen ist, haben die betreffenden Projekte mittlerweile häufig eine digitale Komponente. Projektleitende und Mitarbeitende sind zumeist literaturwissenschaftlich ausgebildet. Sie konzipieren und entwerfen, wie die digitale Repräsentation ihrer Arbeit strukturiert wird und erarbeiten das wissenschaftliche Konzept, das Inhalt und Funktionalität zugrundeliegt. Für die technische Umsetzung werden meist erst nach Abschluss der konzeptionellen Arbeit externe Auftragnehmende engagiert, oft privatwirtschaftliche IT-Unternehmen, die von den Möglichkeiten, die im Bereich der DH bereits verfügbar wären, nur eingeschränkte Kenntnis haben.
            Aus dieser Situation ergeben sich Probleme in mehreren Bereichen:
            
               Langzeitarchivierung von Scans
               Die im Rahmen von Projekten erstellten Scans sollten in einer digitalen Langzeitarchivierung der projekttragenden Institution abgelegt werden, was fallweise versäumt wird. Gründe:
               
                  fehlender Speicherplatz 
                  fehlende Arbeitszeit (sowohl auf Projekt- als auch auf Institutionsseite)
                  pragmatische Lösungen: Langzeitarchivierungstaugliche Scans bedeuten einen aufwändigeren Arbeitsprozess
                  die Materialien werden häufig nicht in größere Projekte eingespeist
                          (z. B. Europeana)
               
            
            
               Datenmodellierung
               Die Projektzuständigen haben aufgrund ihrer Ausbildung meist einen editorischen oder von archivarischen Ordnungsprinzipien geprägten Zugang zur Modellierung und Strukturierung der Projektdaten. Gründe und Konsequenzen:
               
                  die gewählten Datenmodelle sind selbst innerhalb einzelner Geisteswissenschaften nicht homogen
                  wie konkret die gewählten Daten zu notieren und annotieren sind, wird in jedem Projekt individuell entschieden
                  selbst bei Orientierung an vorhandenen Standards ergeben sich unterschiedliche Auslegungen
                  keine ausreichenden Kenntnisse von TEI und anderen Standards
                  “Selbstverständlichkeiten” werden im Datenmodell oft vergessen (z. B.
                            die Angabe der Verfassenden, wenn ein Projekt sich mit dem Werk einer
                            Einzelperson beschäftigt.)
                  keine transdisziplinäre (Wieder-)Verwendung der Daten
               
            
            
               Technische Umsetzung, Vernetzung, Visualisierung
               Forschungsprojekte werden häufig in Zusammenarbeit mit Firmen umgesetzt, die nicht (primär) mit wissenschaftlicher Klientel arbeiten, deren Wünsche und Methoden daher nicht im Detail verstehen und nicht mit bereits existierenden DH-Tools und Ressourcen vertraut sind. Konsequenzen:
               
                  keine semantische Annotation, Vernetzung mit verfügbaren Datensätzen bzw. LOD
                  keine projektinterne Vernetzung der Daten
                  keine strukturierte Visualisierung des Datensatzes
               
            
            
               Langzeitarchivierung / Verfügbarmachung von Daten
               An hostenden Institutionen werden kaum personelle Ressourcen zur Wartung abgeschlossener online-Projekte einkalkuliert. Projekt-Websites sterben daher oft nach wenigen Jahren, mit ihnen die Daten. Auch in Projektfinanzierungsplänen wird dieser Aspekt bislang nicht berücksichtigt. Konsequenzen:
               
                  Daten werden nicht als LOD zur Verfügung gestellt
                  Trägerinstitutionen archivieren die Daten nicht sachgemäß
               
            
         
         
            Lösungsansätze
            Der skizzierten Situation muss auf allen Ebenen begegnet werden:
            
               Institutionen
               Seitens der hostenden Institutionen muss stärker daran gearbeitet werden, für langfristige Datensicherung Möglichkeiten zu entwickeln und anzubieten oder Kooperationen mit Langzeitdatenarchiven einzugehen. Dafür müssen sowohl substanzielle finanzielle als auch personelle Ressourcen explizit dieser Aufgabe zugeordnet werden. Die Postionen, die die
                            AG Datenzentren im Verband DHd 2015 formuliert hat, sind dafür wertvolle Impulse und sollten stärker an Institutionen herangetragen werden. Da die betreffenden Institutionen meist Bibliotheken bzw. an solche angeschlossen sind, scheint es auch gewinnbringend, die institutionsinterne Kommunikation zu intensivieren: "digitale Biliothek"-Abteilungen haben für viele der skizzierten Probleme bereits gute Lösungsansätze entwickelt, wie das umfangreiche Vortragsprogramm zu diesem Thema am Österreichischen Bibliothekartag 2015 gezeigt hat. Auch die ÖNB hat mittlerweile eine wichtige Initiative in Angriff genommen: Eine interne Arbeitsgruppe Digital Humanities soll dort Lösungen für die digitale Arbeit entwickeln. Diese Herangehensweise kann für Bibliotheken allgemein als Vorbild dienen, sollte aber auch mit einer Initiative verbunden sein, die im Idealfall kollaborativ erarbeiteten (technischen) Lösungen mit anderen Bibliotheken und Digital Humanists zu teilen.
                          
            
            
               Fördergebende
               Bei der Bewilligung von Projektanträgen sollten Fördergebende die skizzierten Probleme ernsthaft berücksichtigen und Projekte, die keine ausreichenden Ressourcen für die Arbeit an der digitalen Repräsentation der Projektergebnisse vorsehen, ablehnen - anstatt die Praxis, utopische Ziele in Projektanträge einzubauen, zu unterstützen. (Inter)Nationale DH-Plattformen sollten es sich zur Aufgabe machen, ein entsprechendes Empfehlungspapier zur Verfügung zu stellen. Bedenkenswert ist auch die Forderung nach einem “offenen Lebenszyklus” von Forschungsprojekten, die etwa von der Plattform
                            
                     digital humanities austria
                  
                            gestellt wird.
                          
            
            
               Forschende
               Das größte Potential zur nachhaltigen Verbesserung der Situation liegt im
                            Bereich der Projektangestellten. Geisteswissenschaftlich Forschende erfahren
                            im Rahmen des Studiums unzureichende Ausbildung zur Arbeit im digitalen Raum
                            und haben in der Folge entsprechende Hemmungen, mit digital humanists in
                            Austausch zu treten. Deshalb werden DH von nicht primär im digitalen Raum
                            arbeitenden Forschenden noch immer als eigene Disziplin wahrgenommen anstatt
                            als Teil und Methode des geisteswissenschaftlichen Forschens an sich. Hier
                            muss Bewusstsein geschaffen und Skepsis abgebaut werden, indem die Lehrpläne
                            grundlegend überarbeitet und gegenwärtigen Standards angepasst werden.
                            Dadurch würden viele der umrissenen Probleme gar nicht erst entstehen. Neben
                            den Forschenden der Zukunft, die man so erreichen kann, müssen kurz- bis
                            mittelfristig auch die Forschenden der Gegenwart stärker animiert werden,
                            sich mit seriösen Methodiken und Frameworks für Forschungsprojekte im
                            digitalen Raum auseinanderzusetzen, indem sie dort, wo sie mit ihrem Wissen
                            stehen, abgeholt werden. Dafür kann etwa die Vorgehensweise des 
                     Austrian Centre
                            for Digital Humanities
                   der Österreichischen Akademie der
                            Wissenschaften als vorbildlich genannt werden: Hier steht den Forschenden
                            ein digitaler Helpdesk zur Verfügung, der über Möglichkeiten und potentielle
                            Partner für Projekte informiert. Auch das Outreach-Programm, in dessen
                            Rahmen Vorträge, Workshops und eine jährliche DH-Konferenz veranstaltet
                            werden, bietet die Möglichkeit, sich über digitale Methoden zu informieren
                            und mit digital humanists in Kontakt zu treten. Ebenso zu begrüßen sind die
                            Outreach Programme des 
                     Zentrum für Informationsmodellierung – ACDH
                   der
                            Universität Graz, das vor allem im Bereich der Lehre ein Österreichweites
                            Vorbild sein sollte, und von e-Infrastructures
                            Austria.
               
               Die Arbeit in den Bereichen Helpdesk und Outreach zeigt, dass für eine zeitnahe Verbesserung der Situation Adaptionen in der Ausbildung der Forschenden der Zukunft alleine nicht ausreichen; um die Forschenden der Gegenwart zu erreichen, die nicht in digitaler Methodik ausgebildet wurden und sich (noch) nicht damit auseinandergesetzt haben, braucht es “Übersetzende”, die die Kommunikation zwischen rein geisteswissenschaftlich und rein digital Denkenden erleichtern und Brücken bauen. Die Wichtigkeit solcher Bindeglieder, die “beide Sprachen sprechen”, kann nicht hoch genug eingeschätzt werden, da sie allen Beteiligten Frustration, Zeit, überflüssige Arbeit und letztlich auch Geld ersparen können. Zentren, Institute und Verbände, die in den Digital Humanities arbeiten, sollten ihre Aufmerksamkeit vermehrt auf diesen neuen Arbeitsbereich der digital Übersetzenden richten und ihre Aktivitäten gezielt in diese Richtung lenken.
            
         
         
            Conclusio
            Die unzureichende Vernetzung geisteswissenschaftlich Forschender mit der DH Community führt zu technischen Unzulänglichkeiten in abseits davon ambitionierten digitalen Projekten, die ihre Nachhaltigkeit gefährden. Gegenseitige Annäherung über Outreach-Programme und die Adaption der Lehrpläne geisteswissenschaftlicher Studienrichtungen, vor allem aber verbesserte interne und externe Kommunikation sind notwendig, um zu nachhaltiger Verbesserung der Situation zu gelangen.
         
      
      
         
            
               Bibliographie
               
                  AG Datenzentren der DHd (2015): "Was sind und was
                          sollen Datenzentren in den Geisteswissenschaften?", in: Panel der AG Datenzentren im Verband DHd. Verbandstagung der DHd
                          in Graz, 26.02.2015 https://www.conftool.pro/dhd2015/index.php?page=browseSessions&form_session=37
                          [letzter Zugriff 09. Februar 2016].
               
                  Berlin-Brandenburgische Akademie der Wissenschaften
                          (2007-2015): Deutsches Textarchiv
                  www.deutschestextarchiv.de.
               
                  Digital Humanities Lab der Universität Basel
                            (2009-2016): SALSAH. System for Annotation and
                            Linkage of Sources in Arts and Humanities http://salsah.org/ [letzter Zugriff 09. Februar 2016].
               
                  Europeana Foundation (o. J.): Europeana Collections. http://www.europeana.eu/portal/ [letzter Zugriff 09. Februar
                            2016].
               
                  Herzog August Bibliothek Wolfenbüttel (o. J.): Editiones Electronicae Guelferbytanae
                  http://www.hab.de/de/home/bibliothek/digitale-bibliothek-wdb/digitale-editionen.html
                              [letzter Zugriff 09. Februar 2016].
               
                  Karl-Franzens-Universität Graz (2016): Zentrum für Informationsmodellierung – Austrian Centre for
                              Digital Humanities
                  https://informationsmodellierung.uni-graz.at/ [letzter Zugriff 09
                                Februar 2016].
               
                  Österreichische Akademie der Wissenschaften (2015):
                                Austrian Centre for Digital Humanities
                  http://www.oeaw.ac.at/acdh/
                                [letzter Zugriff 09. Februar 2016].
               
                  Österreichische Akademie der Wissenschaften / Universität
                                  Wien / Universität Graz (o. J.): digital
                                  humanities austria
                  http://digital-humanities.at/ [letzter Zugriff 09. Februar
                                    2016].
               
                  Österreichische Nationalbibliothek (2011-2015): Handkeonline. http://handkeonline.onb.ac.at/ [letzter Zugriff 09. Februar
                                    2016].
               
                  ÖNB = Österreichische Nationalbibliothek (2015): Briefwechsel Sauer - Seuffert. http://sauer-seuffert.onb.ac.at/ [letzter Zugriff 09. Februar
                                    2016].
               
                  Susan Schreibman (2010): Versioning
                                    Machine V4.0
                  http://v-machine.org/.
               
               
                  TextGrid Konsortium (2006–2015): TextGrid. Digitale Bibliothek. Göttingen https://textgrid.de/digitale-bibliothek.
               
                  Trier Centre for Digital Humanities (2014): Digitale Edition der Korrespondenz August Wilhelm
                                    Schlegels (beta-Version)
                  http://august-wilhelm-schlegel.de/briefedigital/ [letzter Zugriff
                                      09. Februar 2016].
               
                  Trier Centre for Digital Humanities /
                                        Heinrich-Heine-Institut Düsseldorf (2004-2009): Heinrich-Heine-Portal
                  http://www.hhp.uni-trier.de/Projekte/HHP/ [letzter Zugriff 09.
                                          Februar 2016].
               
                  Universität Wien (2014-1016): e-Infrastructures Austria
                  http://e-infrastructures.at/ [letzter Zugriff 09. Februar
                                            2016].
            
         
      
   



      
         Der vorgeschlagene Beitrag dokumentiert das Ineinandergreifen philologischer und informatischer Fragestellungen und Entscheidungen bei Aufbau und Aufbereitung eines digitalen Korpus für vergleichende quantitative Stilanalysen von Franz Kafkas Prosa.
         In den letzten Jahren haben digitale Ressourcen wie TextGrid, das Deutsche Textarchiv [DTA], und Gutenberg-DE reichhaltige digitale Korpora von historischen Texten (literarischer und nichtliterarischer Art) zur Verfügung gestellt. Kafkas Werk selbst ist zudem fast vollständig digitalisiert. Dennoch liegen derzeit weder ein vollständiges Kafka-Kernkorpus noch ein „Kafka-Referenzkorpus“ vor, das eine sinnvolle quantitative Analyse seines Sprachgebrauchs durch den Vergleich mit ausreichend großen Stichproben anderer Texte zulässt. Unser Projekt möchte diese Lücke füllen und ein Kafka/Referenzkorpus vorstellen, das sowohl philologisch als auch informatisch solide aufbereitetet ist, und eine hypothesengetriebene aber auch explorative quantitative Stilistik ermöglicht. 
         Bei der Konzeption des Kafka/Referenzkorpus verfolgen wir einen autororientierten Ansatz der digitalen Stilistik. Ausgehend von der Hypothese, dass der Stil eines Autors durch von ihm rezipierte Texte beeinflussbar ist, und dass Stil quantitativ beschreibbar ist (vgl. Herrmann / van Dalen-Oskam / Schöch 2015), gehen wir zunächst vom faktischen textuellen Input Kafkas aus und ergänzen diesen durch Stichproben kanonischer und populärer zeitgenössischer Texte. Der Aufbau des Kafka/Referenzkorpus wird von drei Kriterien geleitet: 
         (a) Vollständigkeit von Kafkas Schriften in der Originalfassung (=Kafka-Kernkorpus); 
         (b) Abbildung von Texten, die Kafkas Schreibprozess beeinflusst haben könnten / Abbildung von Texten, die eine näherungsweise Repräsentativität der Epoche der klassischen Moderne herstellen (=Kafka-Referenzkorpus); 
         (c) eine hohe Akkuratheit bzw. Konsistenz bei informatischer Vorverarbeitung wie Normalisierung, linguistischer Annotation (
        Part of Speech), Metadaten und Textmarkup (XML-TEI) in einem
        Stand-Off Korpus, das einen hohen Grad an Forschungsflexibilität ermöglicht.
      
         Das Kafka-Kernkorpus (je nach Zählart ca. 120 Texte) wurde dabei intern in die Dimensionen Kafka_Publikation (zu Lebzeiten/posthum) und Kafka_Genre (Literarisch, Brief, Tagebuch, Amtliche Schriften) unterteilt. Das Referenzkorpus (ca. 8.000 Texte) wurde hauptsächlich aus TextGrid, DTA, Gutenberg-DE und Gutenberg.org extrahiert, und beinhaltet Metadaten zu Autor (Name, Gender), Publikationsdatum und -Ort, sowie Gattung. Es umfasst literarische Texte der Kategorien „kanonisch“ und „populär“ ebenso wie Gebrauchstexte. Neben Kinder- und Jugendliteratur die Kafka rezipierte sind hier auch Sach- und Fachliteratur von Interesse, nicht zuletzt weil Kafkas Stil durch Elemente der Fachsprache, aber auch ein hochsprachliches „Prager Deutsch“ ohne sozio- oder dialektale Einflüsse geprägt sein soll (vgl. Nekula 2003). Zur Korpuszusammensetzung wurden Aufzeichnungen zu Kafkas Lesegewohnheiten untersucht, wobei Zeugnisse über seine Bibliothek, biographische Berichte, aber auch Dokumente zur zeitgenössischen Rezeption sowie Autor- und Werk-Indices in Literaturgeschichten konsultiert wurden (Blank 2001; Born 1990; Born / Koch 1983; Born / Mühlfeit 1979). Das Ergebnis dieses Forschungsschrittes ist eine Liste von 765 Titeln, die das Metadatum „in Kafkas Bibliothek“ tragen, und einen Schwerpunkt zu Kafkas Lebzeiten setzen, aber eben auch Werke von älteren Autoren wie Goethe und Kleist, sowie Flaubert und Dostojewski (in Übersetzung) beinhalten. Dass die von uns einbezogenen Online-Repositorien hinsichtlich der editionsphilologischen Textqualität variieren ist ein hinzunehmendes Übel, dem wir zum einen pragmatisch (Wahl der bestmöglichen verfügbaren Ausgabe; Ziel, die Fehlermarge unter 2% zu halten), zum anderen unter Hinweis auf die flexible Struktur des Korpus (Austausch durch eine qualitativ hochwertigere Version ist möglich) begegnen. Durch die nahtlose Dokumentation des Korpus wird zudem die nötige Transparenz gewährleistet um auch Nachnutzern flexible Kontrolle der Daten zu ermöglichen.
         Die Hauptaufgabe der informatischen Dimension des Projektes
        besteht neben der Einbettung in einen praktikablen und anschlussfähigen Workflow
        (eXist Datenbank) und der Homogenisierung und informatischen Aufbereitung der
        Ausgangsdaten (Tokenisierung, Normalisierung, Lemmatisierung) in einer reliablen
        linguistischen Annotation auf POS (STTS Tagset). Wortarten gelten als verlässliche
        Indikatoren für Register und Genrevariation (vgl. z. B. Biber / Conrad 2009), und
        sind im Vergleich mit anderen Variationsmarkern durch eine relativ akkurate
        automatische Annotation besonders praktikabel. Obwohl bei der POS-Annotation gute
        Accuracy für das gegenwärtige Standarddeutsch mithilfe von Hidden-Markov-Modellen und Markov-Modellen erzielt
        wird (RF-tagger, Tree-Tagger), wurden diese Tagger auf Zeitungstexten trainiert und
        erfordern deshalb in unserem Korpus manuelles Fehlermanagement: Ein Ausschnitt des
        Gesamtkorpus (repräsentativer Querschnitt auf Satzebene, randomisiertes Sampling)
        wird manuell auf POS getaggt und mit dem Output der Tagger verglichen. Liegt die
        Übereinstimmung größer oder gleich der Standard-Accuracy (ca. 97%), ist eine
        umfangreiche Fehleranalyse nicht notwendig. Sollte die Accuracy niedriger sein, wird
        in der Folge über ein manuell kodiertes Sample von ca. 40.000 Wörtern durch Machine Learning ein Algorithmus trainiert, der bessere Werte
        erreicht. Hierbei ist auch der Gebrauch von Taggern aus dem Conditional Random Field (CRF) Framework wie MarMoT vorgesehen, die eine
        größere Input-Spanne berücksichtigen. Der Workflow beinhaltet einen automatischen
        Vergleich des Tagger-Outputs durch eine eXist-Datenbank mit Annotationsinterface.
        Der Output wird in einem Stand-Off Format (TCF) gespeichert, wie es auch das DTA
        benutzt. Geplant ist zusätzlich eine Qualitätskontrolle des TEI-Markups, der
        Metadaten und der POS-Annotation durch einen bereits entwickelten Ansatz der Gamification (s. https://personae.gcdh.de/index.html). Das Kafka / Referenzkorpus soll im
        Rahmen der TextGrid Infrastruktur in SADE veröffentlicht und so der
        Forschungsgemeinschaft zur Verfügung gestellt werden. Gleichzeitig planen wir eine
        detaillierte Dokumentation der Arbeitsschritte zu veröffentlichen, die ähnlichen
        Projekten als Leitfaden zur Verfügung zu stehen soll. 
         Unser Projekt dokumentiert in seinem gegenwärtigen Status Entscheidungen auf
          verschiedenen konzeptionellen, analytischen und prozeduralen Ebenen. Es zeigt, dass
          der Aufbau eines digitalen Autor-Korpus, das den quantitativen Vergleich mit
          synchronen und diachronen Daten erlauben soll, bei Weitem keine triviale Aufgabe
          darstellt. So wird zum Beispiel deutlich, wie Forschungsfragen beziehungsweise
          Hypothesen zur Konstitution von Schreibweisen und Autorschaft die Korpuskompilation
          steuern – und deshalb auf einer möglichst präzisen Modellierung der
          zugrundeliegenden textwissenschaftlichen Theorien fußen sollten. Gleichzeitig sind
          Metadaten (u. a. Autor, Titel, Publikationsdatum, Publikationsort, Genre) und
          linguistische Parameter (wie POS) gerade die Ansatzpunkte, an denen philologische
          Fragestellungen in präzise und praktikable Kategorien umgewandelt werden können.
          Nicht zuletzt deshalb sollten literarische Daten in flexiblen Architekturen
          gespeichert werden, die zusätzliche Annotationsebenen zulassen – denn hermeneutische
          Erkenntnisprozesse stellen eine erwachsene Stärke der Geisteswissenschaften dar, die
          auch im digitalen Zeitalter einen explizit modellierten Platz einnehmen muss.
      
      
         
            
               Bibliographie
               
                  Biber, Douglas / Conrad, Susan (2009): Register, Genre, and Style. Cambridge: Cambridge
                University Press. 
               
                  Blank, Herbert (2001): In Kafkas
                  Bibliothek. Werke der Weltliteratur und Geschichte in der Edition,
                  wie sie Kafka besaß oder kannte; kommentiert mit Zitaten aus seinen Briefen
                  und Tagebüchern. Stuttgart: Blank. 
               
                  Born, Jürgen (1990): Kafkas
                    Bibliothek. Ein beschreibendes Verzeichnis; mit einem Index aller
                    in Kafkas Schriften erwähnten Bücher, Zeitschriften und
                    Zeitschriftenbeiträge. Frankfurt am Main: S. Fischer. 
               
                  Born, Jürgen / Koch, Elke (eds.) (1983): Franz Kafka: Kritik und Rezeption, 1924-1938.
                      Frankfurt am Main: S. Fischer. 
               
                  Born, Jürgen / Mühlfeit, Herbert (eds.) (1979): Franz Kafka: Kritik und Rezeption zu seinen Lebzeiten,
                        1912-1924. Frankfurt am Main: S. Fischer. 
               
                  Herrmann, J. Berenike / van Dalen-Oskam, Karina / Schöch,
                            Christof (2015): “Revisiting Style, a Key Concept in Literary
                            Studies”, in: Journal of Literary Theory 9, 1: 25-52. 
               
                  Nekula, Marek (2003): “Franz Kafkas Deutsch“, in: Linguistik online 13, 1 https://bop.unibe.ch/linguistik-online/article/view/879/1533
                              [letzter Zugriff 29. Dezember 2015]. 
            
         
      
   



      
         Verstehen wir unter einer digitalen Edition eine „erschließende Wiedergabe
        historischer Dokumente“, welche dem digitalen Paradigma folgt, indem sie die
        gegenwärtigen technischen Möglichkeiten berücksichtigt (cf. Sahle 2013: 138,
        148), dann stellt sich die Frage, welche
        technischen Möglichkeiten zu welchem Zweck eingesetzt werden sollen. In diesem
        Beitrag wird die Überzeugung vertreten, dass digitale Editionen als zentraler
        Bestandteil von Forschungsumgebungen der Textwissenschaft von weit größerem Nutzen
        sein können, wenn sie über standardisierte semantische Web-Schnittstellen verfügen.
        Digitale Editionen wären dann primär als Web-Services zu verstehen, die über ihre
        Web-Schnittstellen mit anderen Web-Services oder mit Web-Anwendungen kommunizieren.
        Es wäre erst die Web-Anwendung (welche im Browser ausgeführt wird), mit der der
        menschliche Nutzer interagiert, wogegen alle übrige Kommunikation von Maschine zu
        Maschine liefe. Herkömmliche digitale Editionen sind primär auf eine Nutzung durch
        den Menschen allein ausgerichtet. Die im Folgenden zu begründende These ist, dass
        Werkzeuge der Forschungsumgebungen mit diesen herkömmlichen digitalen Editionen
        deshalb nur unbefriedigend ineinandergreifen, weil sie programmatisch abgeschlossen
        sind. Dieser Zustand ist insofern unbefriedigend, als dadurch Textforschung weit
        weniger vernetzt und kollaborativ vonstatten geht als dies möglich wäre. 
         Eine Verbesserung dieses Zustands kann natürlich nicht allein von technischen
          Neuerungen digitaler Editionen erhofft werden. Es sind ebenso technische Neuerungen
          bei allen Komponenten bestehender Forschungsumgebungen nötig (und bei Initiativen
          wie TextGrid auch im Gange). Dabei besteht eine wechselseitige Abhängigkeit des
          Entwicklungsfortschritts: Nur wenn die eine Komponente das eine neue Feature
          anbietet, besteht bei der anderen Komponente die Chance eines Entwicklungssprungs.
          Mit Blick auf diese Koevolution müssen also diejenigen
          Komponenten einer Forschungsumgebung berücksichtigt werden, die mit einer digitalen
          Edition im Datenaustausch stehen oder stehen sollten. Dabei ist es zielführend, sich
          nicht ausschließlich von der Frage leiten zu lassen, wie man digitale Editionen
          möglichst interoperabel zu den am weitestverbreiteten Werkzeug der
          Textwissenschaftler (z. B. der dominierenden Textverarbeitungssoftware) machen kann.
          Vielmehr sollte die Aufmerksamkeit darauf gerichtet werden, welche nützlichen
          Werkzeuge man schaffen könnte, wenn man die digitalen Editionen mit bestimmten
          technischen Neuerungen ausstatten würde. 
         Im Folgenden wird daher das Umfeld digitaler Editionen innerhalb einer textwissenschaftlichen Forschungsumgebung in den Blick kommen und zwar in einer Weise, die auch noch nicht existierende Systeme mitdenkt. Dies ist möglich, wenn man eine solche Umgebung zu diesem Zweck nicht als eine Ansammlung bestehender Tools auffasst, sondern die textwissenschaftlichen Tätigkeiten identifiziert, für die man sich ohne Rücksicht auf bestehende Fertiglösungen technische Unterstützung überhaupt vorstellen kann.
         Die aus informationstechnischer Sicht relevanten Tätigkeiten lassen sich in diesem Kontext sinnvoll unterteilen in: das
            Lesen,
            Schreiben und
            Verwalten von Text. Während das Lesen und Schreiben von Text in diesem Rahmen keiner weiteren Erklärung bedarf, muss näher darauf eingegangen werden, was mit Textverwaltung alles gemeint sein kann. Eine positive Definition dieses Begriffs würde wahrscheinlich keine allgemeine Zustimmung finden, daher sollen ein paar paradigmatische Beispiele zur Begriffsklärung ausreichen: Exzerpieren, Organisieren von Textschnipseln in Zettelkästen, Anlegen von Literaturlisten, Zusammenstellen eines Semesterapparats, Sortierung von Büchern, Klassifikation von Texten, Erstellen von Registern und vieles mehr – für all diese und ähnliche Tätigkeiten soll der Begriff Textverwaltung hier stehen. Zwar wird in all diesen Fällen auch geschrieben und gelesen, aber das ist nicht das Wesentliche an der Textverwaltung, sondern die in diesen Tätigkeiten erzeugten Ordnungen oder Relationen.
          
         Fragen wir uns nun, zu welchen dieser drei Tätigkeitsfeldern (Lesen, Schreiben,
            Verwalten) eine digitale Edition eine unmittelbare und eine mittelbare Unterstützung
            liefern kann. Traditionell dienen digitale Editionen (wie ihre gedruckten Vorfahren)
            in erster Linie dazu gelesen zu werden. Zwar sind die in ihr enthaltenen Texte und
            ihre Metadaten natürlich auch Ergebnis einer Textverwaltung. Jedoch bieten sie dem
            Nutzer nur in seltenen Fällen und da auch nur rudimentär die Möglichkeit selbst Text
            zu verwalten (cf. z. B. Arbeitsmappen bei Jung 2015). Eine
            außergewöhnliche Ausnahme ist ein Editionsprojekt zu Pessoas „Buch der Unruhe“ (cf.
            Silva / Portela 2015). Hier ist das Lesen, Schreiben und Verwalten gleichermaßen
            möglich und ermöglicht den Nutzern aus dem vorhandenen Textmaterial und eigenen
            Kommentaren eine eigene virtuelle Edition kollaborativ zu erstellen. In diesem Sinne
            ist diese Plattform nicht mehr eine Edition im traditionellen Sinne, sondern selbst
            eine in sich abgeschlossene Forschungsumgebung – allerdings für eine ganz spezielle
            Aufgabe über ein abgegrenztes Textkorpus. 
         All diesen digitalen Editionen ist jedoch gemeinsam, dass, sofern sie eine Textverwaltung unterstützen, diese dann nur für die im System vorhandenen (oder darin erzeugten) Texte ermöglichen. Im Allgemeinen ist der Textwissenschaftler aber nicht mit einem einzelnen Textkorpus befasst, sondern mit mehreren. Eine Textverwaltung kann dann nur ihren Nutzen entfalten, wenn sie als eigenständiger Service auf mehrere digitale Editionen zugreifen kann.
         Nehmen wir als einfaches Beispiel die Zusammenstellung der Literatur zu einem Germanistikseminar, in dem Texte verschiedener Autoren behandelt werden. Von einer komfortablen Textverwaltung würde man jetzt nicht die URL der jeweiligen digitalen Editionen erwarten, sondern man möchte am besten die Texte selbst per Mausklick zur Verfügung gestellt bekommen ohne dabei auf die Web-Seiten der jeweiligen digitalen Editionen gehen zu müssen. Schon dieser einfache Fall zeigt den Nutzen, den eine programmatische Schnittstelle von digitalen Editionen haben könnte: Ein eigenständiger Service zur Aggregation von Semesterapparaten ließe sich mit geringem Aufwand implementieren.
         Tatsächlich bieten manche digitale Editionen (z. B. das Deutsche Textarchiv) ihre
              Texte (sogar in verschiedenen Formaten: TEI, HTML, plain text) zum Download an, so
              dass man die entsprechenden Links schon als Web-API auffassen könnte. Allerdings
              beschränkt sich diese Möglichkeit entweder auf den Download einer einzelnen Seite
              oder des gesamten Textdokuments. Für eine brauchbare Textverwaltung wäre es jedoch
              wesentlich praktischer, wenn man Texte nicht nach Paginierungsgrenzen sondern
              bezüglich semantischer Sinneinheiten beziehen könnte. Es fällt nicht schwer, sich
              entsprechende Szenarien vorzustellen: Für eine Anthologie möchte man etwa Balladen
              einer bestimmten Epoche zusammenstellen.; für eine Theaterprobe möchte jeder
              Schauspieler eine Zusammenstellung derjenigen Szenen, in der seine Rolle vorkommt;
              ein Übersetzungsforscher möchte alle deutschen Übersetzungen des Monolog der ersten
              Szene im dritten Aufzug von Shakespeares Hamlet. Die Zahl weiterer Szenarien ist
              unbegrenzt. Als entscheidende Anforderung an eine digitale Edition wäre
              festzuhalten: die Adressierbarkeit und Auffindbarkeit von Texten in allen üblichen
              Struktureinheiten (z. B. Kapitel, Absatz, Drama, Akt, Szene, Gedicht, Strophe, Vers,
              etc.). Da in den meisten digitalen Editionen die Texte im TEI-XML vorliegen, welche
              die Kodierung solcher Struktureinheiten erlauben, dürfte es prinzipiell nicht
              schwierig sein, diese auch über eine Web-API adressierbar zu machen. Was die
              Auffindbarkeit betrifft, wäre es wünschenswert, die Möglichkeitender in der
              Backend-Datenbank verwendeten Anfragesprachen weitgehend in der Web-API abzubilden.
              Das ganze Feld der Suchmöglichkeiten ist allerdings so umfangreich, dass es einen
              eigenen Beitrag rechtfertigen würde und daher hier nicht weiter vertieft werden
              soll. Allein die Adressierbarkeit aller textspezifischen Struktureinheiten (s. o.)
              mittels der Web-API von digitalen Editionen wäre eine große Chance zur Entwicklung
              nützlicher Textverwaltungsdienste. Allerdings sollten neben den vorgegebenen
              Struktureinheiten auch vom Nutzer frei definierte Textauswahlen von einer digitalen
              Edition adressierbar sein. Damit soll die verbreitete Praxis, Textausschnitte mit
              einem Textmarker zu markieren, im digitalen Medium nicht nur die Funktion erhalten,
              etwas farblich hervorzuheben, sondern die so ausgezeichneten Textpassagen sollen
              durch eine generierte Adresse permanent referenzierbar gemacht werden. Damit wäre
              beispielsweise eine Sammlung von Exzerpten referenzierbar, die ein Benutzer mit
              einem virtuellen Textmarker erzeugt hat.
         Bis hierin wurde die Adressierbarkeit von jeglichen Textausschnittenin den oben
                angeführten Szenarien ausschließlich für die Erstellung von Textsammlungen
                verwendet. Das ist aber nur eine einfache Form der Textverwaltung. Denn eine
                Textsammlung ist zunächst eine in sich unstrukturierte Menge von Texten. Ziel einer
                Textverwaltung ist es aber meist, in eine Textsammlung eine bestimmte Ordnung zu
                bringen. Das ist unter anderem der Fall, wenn man die gesammelten Texte nach
                forschungseigenen Kriterien klassifiziert; z. B. als Linguist nach grammatischen
                Eigenschaften, als Literaturwissenschaftler nach Motiven, als Übersetzer nach
                Idiomen, etc.
         Textklassifikation wäre eine Relation zwischen Texten und Sammelbegriffen. Darüber hinaus wäre es wichtig, in einer Textverwaltung die Beziehung der Texte untereinander explizit machen zu können. So könnte man beispielsweise explizit erfassen, dass eine bestimmte Textpassage eine Anspielung auf einen anderen Text ist; oder dass die eine Textfassung aus jener Skizze hervorgegangen ist, etc. Soweit würde man Textausschnitte aus digitalen Editionen in Beziehung zueinander setzen. Man würde aber in einer Textverwaltung insbesondere auch die Texte der digitalen Editionen in Beziehung zu selbstverfassten Texten setzten wollen. Auch würde man Texte zu nicht textartigen Gegenständen wie Personen, Orte oder Ereignissen in Beziehung setzen wollen; beispielsweise wenn man in historischen Romanen den Bezug zu historisch belegten Sachverhalten herstellen möchte.
         Eine Textverwaltung, die all die skizzierten Funktionalitäten bereitstellen würde, könnte einen Textwissenschaftler bei der Arbeit am Text bzw. der Organisation der eigenen Texte erheblich unterstützen. Sie würde darüber hinaus das kollaborative Arbeiten erleichtern, indem sie eine auf Austausch von Dokumenten basierte Arbeitsweise durch eine Praxis der direkten Vernetzung von Inhalten im Netz ersetzen würde. Sie könnte aber nur funktionieren, wenn die Texte digitaler Editionen in aller Granularität über Web-APIs adressierbar wären.
         Abschließend soll erwähnt werden, das eine ganze Reihe von Anstrengung von
                  verschiedenen Seiten schon unternommen wurden, die durch eine geeignete
                  Zusammenführung ein solides Fundament zur Umsetzung dieser Visionen bilden könnten.
                  Allgemeine technische Grundlage wären die Semantic-Web-Technologien. Darauf
                  aufbauend wären folgende theoretische und praktische Arbeiten hervorzuheben: Von
                  Silvio Peroni (2014) zu „Semantic Publishing“ , Fabio Ciottis und Francesca Tomasis
                  (2014) Entwurf zu „Formal ontologies, Linked Data and TEI semantics“, das semantic
                  annotation Tool Pundit (2013-*) und die Open Annotation Initiative: http://www.openannotation.org/. 
      
      
         
            
               Bibliographie
               
                  Ciotti Fabio, Tomasi Francesca (2014): Formal ontologies, Linked Data and TEI semantics. TEI
                        Conference and Members Meeting 2014. Evanston (IL), October 22-24, 2014.
                        http://tei.northwestern.edu/files/2014/10/Ciotti-Tomasi-22p2xtf.pdf
                          [letzter Zugriff 09. Januar 2016]. 
               
                  Jung, Joseph (ed.) (2015): Digitale
                            Briefedition Alfred Escher. Version: Juli 2015. Zürich: Alfred
                            Escher-Stiftung. http://www.briefedition.alfred-escher.ch/ [letzter Zugriff 09.
                            Januar 2016]. 
               
                  Peroni, Silvio (2014): Semantic Web
                              Technologies and Legal Scholarly Publishing. Switzerland: Springer
                              International Publishing http://www.springer.com/us/book/9783319047768 [letzter Zugriff
                              09. Januar 2016]. 
               
                  Pundit (2013-*): Pundit net7
                                http://thepund.it/ [letzter
                                Zugriff 09. Januar 2016]. 
               
                  Sahle, Patrick  (2013): Digitale
                                  Editionsformen. Zum Umgang mit der Überlieferung unter den
                                  Bedingungen des Medienwandels: Befunde, Theorie und Methodik (= Schriften
                                  des Instituts für Dokumentologie und Editorik 8). Norderstedt: Books on
                                  Demand. 
               
                  Silva, António Rito / Portela, Manuel (2015):
                                    "TEI4LdoD: Textual Encoding and Social Editing in Web 2.0 Environments", in:
                                    Journal of the Text Encoding Initiative 8 http://jtei.revues.org/1171
                                    [letzter Zugriff 09. Januar 2016]. 
            
         
      
   



      
         
            Introduction 
            
               Background: Large Knowledge Bases 
               A large portion of the material on which scholarly editing is based today is available electronically in large knowledge bases. Some of these emerge from the archive, library and museum communities, for example
            Kalliope. Such efforts require the use of standardized vocabularies and databases of entities such as persons and locations.
            Kalliope thus links to
            Gemeinsame Normdatei (GND), which provides more than 120 million facts about approximately 11 million entities. The prevailing technique to realize such linked knowledge bases is the Semantic Web, as advocated by the W3C, characterized by the use of ontologies to express standardized vocabularies, global identifiers (URIs) and the possibility to express knowledge in a machine understandable way as subject-predicate-object statements with RDF. Further large knowledge bases, such as
            Yago (Hoffart et al. 2013) and
            DBpedia (Lehmann et al. 2015), developed mainly in computer science with Semantic Web techniques, gather and combine machine processable knowledge from "crowd-maintained" sources like
            Wikipedia and centrally maintained sources like
            GND or
            GeoNames.
          
            
            
               Beyond
            TEI
               
               The seemingly best developed machine support for scholarly editing today is
            provided with the Text
                   Encoding Initiative (TEI) format, based on document
            markup. URIs as attribute values of markup elements can provide links to
            knowledge bases. Envisaged applications include in particular the rendering
            for different media and extraction of metadata. Some of the recent
            developments are actually orthogonal to the OCHCO text model and its
            representation through XML, core characteristics of the original TEI. Connecting TEI with
            Semantic Web techniques, data modeling and ontologies is, for example, an
            ongoing topic of discussion (e.g. Eide 2015). Recent versions of TEI provide support for names,
            dates, people, and places as well as linking,
            segmentation, and alignment (The TEI Consortium 2015: Chapters 13
            and 16). In a broad long-term perspective, important aspects that further go
            into these directions become apparent: 
               
                  Incorporation of advanced semantics related techniques such as named entity recognition or statistics-based text analysis. 
                  Relationships to external knowledge bases and to formal semantics.
                  Obtaining high-quality presentations without requiring expensive development of dedicated XML transformations and stylesheets. 
                  Loose coupling of object text and markup: Alternate markup by different authors or for different purposes should be supported. Markup generated by automated methods should not clutter up the document. Queries and transformations should remain applicable also after changes of the markup. Sustainability must not be compromised by dependency on short-lived technology and specifications.
               
               Addressing these issues, we approach the requirements of today's scholarly editing here from the view of computational logic: What can logics – as machine processable symbolic languages with formally specified semantics – contribute? A starting point is that with Semantic Web technology the large knowledge bases can already be considered as large sets of logic facts. Logic languages have various further potential roles in machine supported scholarly editing, such as specifying properties and values associated with texts, specifying pieces of text, specifying knowledge sources and their combination, and specifying inferences involved in automated computation of information associated with texts.
            
         
         
            Knowledge-Based Support for Scholarly Editing 
            
               High-Quality Support at all Phases 
               Three main phases of machine assisted scholarly editing can be identified, which all should be supported: (1) Creating the enhanced object text; (2) Generating intermediate representations for inspection by humans or machines; (3) Generating consumable presentations. Support for all three phases should be of high quality – for example entity recognition should precisely identify persons, or the print layout of a finally rendered document should be professional.
            
            
               Issues of Integrating Different Types of Knowledge 
               High-quality support is not possible without inclusion of specialized techniques and the combination of automated techniques with information and adjustments provided by humans. The adequate support of this combination is an important aspect where the considered scenario differs from conventional programming or query languages. Relevant techniques include non-monotonic reasoning, semantics-based knowledge partitioning (Wernhard 2004, Ghilardi et al. 2006, Cuenca Grau et al. 2008, Kontchakov et al. 2010) and the use of explanations for inferred information, as exemplified by proofs in mathematical knowledge bases (Urban et al. 2013). A further important integration requirement concerns the combination of statistics-based techniques, which are essential for natural language processing operations such as named entity recognition or keyphrase extraction, with a symbolic logic-based framework.
            
            
               External Annotations 
               The availability of powerful techniques to identify places in text – based on syntactic as well as semantic properties – suggests to prefer external annotations to in-place markup. Annotations are then maintained separated from the object text in annotation documents. An automated processor creates an annotated document by merging annotations and object text.
            
            
               Representation of Epistemic Status 
               Scholarly editing requires to associate various forms of epistemic status with facts, which is interesting to model formally from the viewpoint of artificial intelligence. Consider for example a creation date associated with written communication: it can be given by its author or can be inferred – by the editor or by a machine, it can be only partially specified by the author, it can be specified with different precision, considered as a point or range in time, etc. The current version of
              TEI offers some related elements to indicate certainty, precision and responsibility (The TEI Consortium 2015: Chapter 21), but these are not based on any formal semantic treatment and it is seems hardly possible to express the sketched date examples with them.
            
            
            
               Utilizing Inferred Access Patterns 
               Efficient access to large knowledge bases requires caching and preprocessing,
              which ideally should be performed automatically on the basis of the queries
              performed by the knowledge processing engine. Relevant techniques come from
              optimization in databases (Toman / Weddell 2011) and in first-order model
              computation systems (Pelzer / Wernhard 2007). It seems that recent
              techniques for view-based query processing (Calvanese et al. 2007) based on
              variants of Craig's interpolation and second-order quantifier elimination
              (Toman / Weddell 2011; Bárány et al. 2013; Wernhard 2014) where access
              patterns can be specifically considered in an abstract way (Bárány et al.
              2013) are particularly useful. Logic-based languages for programming as well
              as data access facilitate the application of such abstract techniques. For
              an overview on alternate ways to associate computational meaning with logics
              see (Kowalski 2014).
            
            
               The Role of Ontologies 
               Ontologies are an important ingredient for the Semantic Web because they provide agreed vocabularies. However, to evaluate queries arising in the text processing tasks of scholarly editing, ontology reasoning alone is not sufficient. Also, the basic ontologies relevant in the context of scholarly editing are – in contrast to the biomedical area (Horrocks 2013) – rather small and trivial. 
            
         
         
            A Prototype: The
              KBSET System
            
            Important issues of complex computer systems often become apparent only with applications. Thus, the authors developed the
              KBSET system, an experimental platform to clarify the precise requirements of machine support for scholarly editing and to experiment with advanced techniques. It follows the outlined approach, but, so far, only realizes some of the discussed aspects. A draft version of an edition of
              Max Stirner: Geschichte der Reaction, Band 1. Berlin, 1852 accompanies it as comprehensive example. The system is free software and available from http://cs.christophwernhard.com/kbset/.
            
            In a typical setting, the system takes as inputs:
            
               A source text file, possibly in
                LaTeX format. The system can parse
                LaTeX, where the set of recognized commands is configurable, including user defined commands as well as commands that establish some "ordered hierarchy of content objects". In this way plain or structured text is available within the system to modules that operate on such text models.
              
               
                  Annotation documents, that is, text files with annotations, possibly in
                LaTeX format. The associated places in the source text to which they are referring are specified abstractly.
              
               Large fact bases, currently in particular
                GND and
                 
                  GeoNames, as well as extracts from
                YAGO2 and
                DBpedia.
              
               A so-called
                assistance document, that is, a configuration file, where, among other things, the fact bases are specified and information is given to bias or override automated inferencing such that fully correct results are obtained.
              
            
            A user interface is provided that integrates the system into the
              Emacs editor, which is free software. The system includes a facility for named entity recognition, which – essentially based on
              GND and
              GeoNames as gazetteers – identifies persons, locations and dates. The system produces a variety of outputs, supporting all the phases of scholarly editing mentioned above:
            
            
               
                  LaTeX documents where annotations and inferred information are merged in. By passing unrestricted
                LaTeX access to the user, high-quality layouts can be achieved.
              
               Support during development by possibilities to highlight and inspect entities recognized by the system. 
               An export possibility to visualize detected locations mentioned in the source text with the
                Dariah geobrowser.
              
            
            A typical application would be the development of an annotated essay or book, where the source text is edited in
              LaTeX and the configuration evolves step-by-step until the inferred information is fully correct.
            
         
         
            Acknowledgments
            This work was supported by
              Alexander von Humboldt-Professur für neuzeitliche Schriftkultur
               und europäischen Wissenstransfer and by
              DFG grant WE 
               5641/1-1
               .
            
         
      
      
         
            
               Bibliographie
               
                  Bárány, Vince / Benedikt, Michael / ten Cate, Balder
                (2013): "Rewriting guarded negation queries", in: Mathematical Foundations of Computer Science 2013 (MFCS 2013),
                volume 8087 of LNCS. Berlin / Heidelberg / New York: Springer 89-110. 
               
                  Calvanese, Diego / De Giacomo, Giuseppe / Lenzerini,
                    Maurizio / Vardi, Moshe Y. (2007): "View-based query processing: On
                    the relationship between rewriting, answering and losslessness", in: Theoretical Computer Science 371, 3: 169-182. 
               
                  Cuenca Grau, Bernardo / Horrocks, Ian / Kazakov, Yevgeny /
                        Sattler, Ulrike (2008): "Modular reuse of ontologies: Theory and
                        practice", in: Journal of Artificial Intelligence
                        Research 31: 273-318. 
               
                  Eide, Øyvind (2015): "Ontologies, data modeling, and
                          TEI", in: Journal of the Text Encoding Initiative 8. 
               
                  Ghilardi, Silvio / Lutz, Carsten / Wolter, Frank
                            (2006): "Did I damage my ontology? A case for conservative extensions in
                            description logics", in: Doherty, Patrick / Mylopoulos, John / Welty,
                            Christopher A. (eds.): Proc. 10th Int. Conf. on Principles
                            of Knowledge Representation (KR'06). Cambridge, MA: AAAI Press
                            187-197. 
               
                  Hoffart, Johannes / Suchanek, Fabian M. / Berberich, Klaus /
                                Weikum, Gerhard (2013): "YAGO2: A spatially and temporally enhanced
                                knowledge base from Wikipedia", in: Artificial
                                Intelligence 194: 28-61. 
               
                  Horrocks, Ian (2013): "What are ontologies good for?",
                                  in: Kuppers, Bernd Olaf / Hahn, Udo / Artmann, Stefan (eds.): Evolution of Semantic Systems. Berlin / Heidelberg /
                                  New York: Springer 175-188. 
               
                  Kontchakov, Roman / Wolter, Frank / Zakharyaschev, Michael
                                    (2010): "Logic-based ontology comparison and module extraction, with an
                                    application to DL-Lite", in: Artificial Intelligence
                                    174, 15: 1093-1141. 
               
                  Kowalski, Robert A. (2014): "Logic Programming", in:
                                      Siekmann, Jörg (ed.): Computational Logic (= Handbook
                                      of the History of Logic 9). Amsterdam: Elsevier 523-569. 
               
                  Lehmann, Jens / Isele, Robert / Jakob, Max / Jentzsch, Anja
                                          / Kontokostas, Dimitris / Mendes N., Pablo / Hellmann, Sebastian /
                                          Morsey, Mohamed / van Kleef, Patrick / Auer, Sören / Bizer,
                                          Christian (2015): "DBpedia – A large-scale, multilingual knowledge
                                          base extracted from Wikipedia", in: Semantic Web 6,
                                          2: 167-195. 
               
                  Pelzer, Björn / Wernhard, Christoph (2007): "System
                                            description: E-KRHyper", in: Automated Deduction 
                                            (CADE-21), volume 4603 of LNCS (LNAI). Berlin / Heidelberg / New York:
                                            Springer 503-513. 
               
                  The TEI Consortium (2015): TEI P5:
                                              Guidelines for Electronic Text Encoding and Interchange, Version
                                              2.8.0 TEI Consortium http://www.tei-c.org/Guidelines/P5/ [letzter Zugriff 9. Oktober
                                              2015]. 
               
                  Toman, David / Weddell, Grant (2011): Fundamentals of Physical Design and Query Compilation San Rafael.
                                                CA: Morgan and Claypool. 
               
                  Urban, Josef / Rudnicki, Piotr / Sutcliffe, Geoff
                                                  (2013): "ATP and presentation service for Mizar formalizations", in:
                                                  Journal of Automated Reasoning 50 (2): 229-241. 
               
                  Wernhard, Christoph (2004): "Semantic knowledge
                                                    partitioning", in: Logics in Artificial Intelligence:
                                                    9th European Conf. (JELIA 04), volume 3229 of LNCS (LNAI). Berlin /
                                                    Heidelberg / New York: Springer 552-564. 
               
                  Wernhard, Christoph (2014): Expressing view-based query processing and related approaches with
                                                      second-order operators", Technical Report - Knowledge
                                                      Representation and Reasoning 14-02, TU Dresden, http://www.wv.inf.tu-dresden.de/Publications/2014/report-2014-02.pdf
                                                      [letzter Zugriff 9. Oktober 2015]. 
            
         
      
   



      
         ePoetics ist ein Forschungskooperationsprojekt der Universität Stuttgart und der
        Technischen Universität Darmstadt. Gefördert vom Bundesministerium für Bildung und
        Forschung zielt es gleichermaßen auf einen Erkenntnisgewinn für die Informatik sowie
        die Sprach- und Literaturwissenschaft dank einer wechselseitigen Anregung und
        Ergänzung im Sinne des ‚Algorithmic Criticism‘ nach Stephen Ramsay (Ramsay 2007).
        Dieser Ansatz ist explizit nicht darauf ausgerichtet, lediglich hermeneutische
        Hypothesen mit algorithmischen Verfahren zu überprüfen. Vielmehr zielt er darauf,
        durch den iterativen Einsatz analoger und digitaler Methoden verschiedene
        Perspektiven auf Texte einnehmen und abgleichen zu können. Darüber hinaus ist ein
        zentraler Aspekt dieses Forschungsparadigmas, Erschließungsentscheidungen und
        -verfahren sowie Analyseschritte transparent bzw. nachvollziehbar und nachnutzbar zu
        machen. Das Projekt ePoetics ist der Digitalisierung, Annotation, Analyse und
        Visualisierung eines für die Geisteswissenschaften zentralen Textkorpus‘ gewidmet:
        Poetiken und Ästhetiken von 1770 bis 1960. Diese Texte dokumentieren das Denken und
        Schreiben über Literatur und andere Künste in der zentralen Periode nach der Abkehr
        von der Normen- und Regelpoetik (vor 1770) und vor dem Übergang zur Literaturtheorie
        und damit dem Ende der Poetik als literaturwissenschaftlicher Textgattung (nach
        1960). Sie enthalten dabei grundlegendes Wissen über Sprache und Literatur
        (-wissenschaft), etwa die Erläuterungen zentraler Begriffe und deren Zusammenhänge.
        ePoetics betreibt die Entwicklung und Untersuchung eines Testkorpus‘ von zwanzig
        Poetiken, ausgewählt aus einem Gesamtkorpus von 1240 Texten (inkl. aller Auflagen),
        die Sandra Richter in ihrer Studie ‚A history of Poetics‘ (Richter 2010) als zur
        Gattung ‚Deutschsprachiger Poetik‘ zählbarer Werke bibliographiert hat. Die Auswahl
        des Testkorpus‘ enthält – historisch und systematisch betrachtet – die
        repräsentativsten Texte des Gesamtkorpus‘, d. h. die, die am häufigsten zitiert und
        in den meisten Auflagen herausgegeben wurden, und stellt dennoch auf den ersten
        Blick ein sehr heterogenes Korpus dar. Aus sprach- und literaturwissenschaftlicher
        Sich zeigen wir auf, wie sich diese Heterogenität im Einzelnen darstellt, aber auch,
        welche tiefergehenden Gemeinsamkeiten und Abhängigkeiten die Texte auf den zweiten
        Blick aufweisen und auf welche Ursprünge sich diese zurückführen lassen. Für
        ausführlichere Informationen zum ausgewählten Textkorpus und zum Projekt insgesamt
        besuchen Sie unsere Homepage (vgl. Ertl et al. 2013-2016).
         Im Zentrum unseres Interesses steht aktuell beispielsweise der Begriff der Metapher als ein zentrales sprach- und literaturwissenschaftliches Konzept, das in unserem Textkorpus verhandelt wird. Die mit diesem zusammenhängenden Fragen lauten: Wie wird der Begriff in einzelnen Poetiken verstanden und erklärt? Wie ändert sich dieses Verständnis innerhalb unseres Testkorpus‘? Welche literarischen oder theoretischen Werke werden im Zusammenhang damit genannt oder zitiert? Wie verändert sich der ‚Kanon‘ dieser Werke? Verändern sich die Zusammenhänge, in denen die Werke zitiert werden? Und schließlich: Wie verändert sich insgesamt der Umgang mit Zitaten und deren Nachweisen?
         Problemstellungen für die digitale Annotation mit dem Ziel der computergestützten
          Auswertbarkeit liegen bei solchen Texten und Anforderungen auf mehreren Ebenen vor:
          Das jeweilige Metaphernverständnis muss differenziert erschlossen und die
          Komponenten der Begriffsbestimmung müssen trennscharf kategorisiert werden können.
          Beispiele aus der Primärliteratur müssen eindeutig erkannt und den jeweiligen
          theoretischen Aspekten, für die sie stehen, zugeordnet werden. Und schließlich
          müssen die Textebenen und Referenzstrukturen der Poetik explizit gemacht werden –
          also wo der Autor selbst theoretisiert, wo zitiert oder paraphrasiert wird,
          inwiefern dies kenntlich gemacht wird oder nicht und sogar, wo bei Zitaten vom
          ursprünglichen Text abgewichen wird. Dies wird durch die Annotation nach einem
          komplexen Schema umgesetzt. Die Annotationen werden einerseits in TEI-konformen
          XML-Dateien publiziert, andererseits aber auch als Grundlage von computergestützten
          Analysen und Visualisierungen genutzt. Abbildung 1 veranschaulicht das Vorgehen im
          Projekt ePoetics im Sinne eines ‚Algorithmic Criticism‘ nach Stephen Ramsay
          (2007).
         
            
            
               Abb. 1: Intellektuelle Vorgehensweise und die algorithmischen Verfahren in ePoetics, in Anlehnung an Kuhn und Reiter (2015).
         
         Intellektuelle Vorgehensweise (Sprach- und Literaturwissenschaft)
         Die Texte des Testkorpus‘ stehen als Image-Digitalisate und als nach dem ‚Double Keying‘-Verfahren transkribierte und aufbereitete digitale Volltexte zur Verfügung. Die strukturellen (und auch die semantischen) Annotationen des Korpus‘ erfolgen nach den Konventionen der Text Encoding Initiative (TEI). Das Korpus wird in virtuelle Forschungs-Infrastrukturen wie TextGrid und das Deutsche Textarchiv (DTA) integriert und dort mit den vorhandenen Referenztexten verlinkt.
         Nach der Identifikation relevanter und interessanter Begriffe und Konzepte wurden zu einzelnen ausgesuchten Begriffen wie der Metapher mithilfe des UAM CorpusTool Annotationsschemata für manuelle Annotationen erstellt. Diese wurden unter ausführlicher Dokumentation von Annotationsguidelines durch mehrere Annotatoren getestet, kontinuierlich verbessert, ausgebaut und schließlich in den Poetiken durchgeführt. Abbildung 2 zeigt eine vereinfache Version des daraus hervorgegangenen Annotationsschemas, das sich in zwei Teilbereiche gliedern lässt, die teils direkt und teils mit leichten Veränderungen auch auf andere Begriffe übertragen werden können. Das Schema resultiert aus den oben genannten sprach- und literaturwissenschaftlichen Fragen, die sich in die Aspekte der Repräsentation und des Verständnisses bzw. der Anwendung des Metaphernbegriffs in den Poetiken aufteilen lassen.
         
            
            
               Abb. 2: Vereinfachte Darstellung des Annotationsschemas zur
            Metapher: Erkennbar ist der begriffsunspezifische (oben, orange) und -spezifische
            Bereich (unten, blau). Während sich der Teil des Schemas, der die Repräsentation des
            Begriffs im Text abbildet, sofort auf andere Begriffe anwenden lässt, ist der Teil,
            der sich dem Begriffsverständnis und der -anwendung widmet, begriffsspezifisch. D.
            h. die hier zu annotierenden Kategorien lassen sich nicht für andere Begriffe
            verwenden, aber leicht durch passende für den jeweiligen Begriff ersetzen.
         
         Das Annotationsschema stellt eine Systematisierung des Begriffs, d. h. seines Vorkommens und Verständnisses in den Poetiken dar. Die für den Begriff relevanten Textstellen werden zunächst dahingehend klassifiziert, ob es sich um Poetikentext handelt (also Text vom Autor der Poetik selbst), oder ob andere theoretische oder literarische Texte zitiert, paraphrasiert oder genannt werden. Neben den Verweisungsformen annotieren wir hierbei auch die Quellenangaben – beides im Übrigen nicht nur, wenn es explizit angegeben ist. So berücksichtigen wir auch die Möglichkeit von „versteckten“ Zitaten oder solchen, bei denen die Quelle nicht oder unvollständig benannt ist. Das Auffinden bestimmter Muster sowie zum Beispiel Titel und Personennamen oder Zitate wird dabei unterstützt durch computerlinguistische Methoden und Verfahren der interaktiven Visualisierung. Darüber hinaus systematisieren wir das vorliegende Begriffsverständnis, d. h. ob die Metapher z. B. als Übertragung erklärt wird, und grenzen sie von anderen Begriffen ab, z. B. im Unterschied zum Vergleich. Zusätzlich lassen sich auch Beobachtungen zu konkreten hermeneutischen Hypothesen annotieren, z. B. ob anhand des Metapherngebrauchs zwischen poetischer und prosaischer Sprache unterschieden wird. 
         Schon durch die Annotation von implizitem Wissen entsteht somit bereits bei den manuellen Annotationen eine Metaebene an Informationen, mit der der digitalisierte Poetikentext angereichert wird. Die Systematisierung erfordert eine andere Herangehensweise an den Gegenstand, als es bei einer rein hermeneutischen Analyse der Fall wäre. Ebenso führt diese zwangsläufig zur Problematisierung der Systematisierungs(un)möglichkeit eines per se komplexen, weil heterogenen Untersuchungsgegenstandes. Das Ziel der algorithmischen Weiterverarbeitung wird zum Paradigma für die systematisch-kategorisierende Ausdifferenzierung von theoretischen Begriffen, wobei diesbzgl. neue Erkenntnisse, aber auch Grenzen aufgezeigt werden können. Die Operationalisierung der Daten führt so bereits zu Erkenntnissen, bevor computertechnologische Auswertungen durchgeführt werden, womit sie sich über den Status bloßer Vorverarbeitung erheben und einen Eigenwert besitzen. 
         Algorithmische Verfahren (Computerlinguistik)
         Mit algorithmischen Verfahren können aus kleinen Mengen annotierter Daten (aus der manuellen und damit zeitaufwendigen Annotation) große Mengen gemacht werden, indem die annotierten Arten von Informationen automatisch auf größere Datenmengen übertragen werden.
         Im Folgenden wird anhand eines Beispiels in Anlehnung an den rechten Teil von
            Abbildung 1 beschrieben, wie die manuelle Annotation, das Training von
            Klassifikationsmodellen und die Analyse der Klassifikationsergebnisse ineinander
            greifen. Zur Klassifizierung von Text zwischen Anführungszeichen als eine der drei
            Klassen ‚Hervorhebung‘ (Wörter deren Bedeutung hervorgehoben wird), ‚Titel‘
            (Werktitel) und ‚Zitat‘ (Zitate aus anderen Werken) wurde manuell ein Korpus
            annotiert, in dem jeder Text zwischen Anführungszeichen einer dieser drei Klassen
            zugewiesen wurde (Manuelles Annotieren von Konzepten). Auf der Basis dieses Korpus
            wurden Klassifikationsmodelle zur automatischen Erkennung dieser drei Klassen
            trainiert (Modell trainieren / entwickeln). Die automatischen Modelle wiederum
            wurden benutzt, um in anderen Poetiken Text in Anführungszeichen automatisch in
            diese drei Klassen einzuteilen. Es wurde durch Stichproben und formale Evaluation
            auf einem für diesen Zweck annotierten separaten Korpus erkannt, dass die
            Klassifikation gut funktioniert (Evaluierung). Da so unter anderem direkte Zitate
            und Werktitel automatisch erkannt werden, ermöglicht dieser Schritt wiederum die
            automatische Verlinkung von Werktiteln und Zitaten mit ihren Einträgen (sofern
            vorhanden) im TextGridRepository-Korpus (Evaluierung). Durch diese Information kann
            vom Analysten manuell die Verteilung von Werken und Zitaten in den Poetiken
            untersucht und bedeutende Werke / Zitate erkannt werden. Diese Erkenntnisse können
            dann wiederum als Metadaten im Dokument annotiert werden (Manuelles Annotieren von
            Konzepten und Metadaten).
         Interaktive Visualisierung
         Interaktive Visualisierung spielt eine wesentliche Rolle in der Vorgehensweise von
              ePoetics, siehe Abbildung 1, da sie eine zusätzliche Interaktion zwischen Forschern
              und den Untersuchungsgegenständen ermöglicht. Zum einen können interaktive Systeme
              die hermeneutischen Vorgehensweise unterstützen, indem sie den
              Geisteswissenschaftlern die Möglichkeiten bieten, Annotationsschemata und
              -guidelines zu entwerfen, Konzepte und Metadaten in Texten manuell zu annotieren
              sowie diese Ergebnisse zu analysieren und darzustellen. Zum anderen kann die
              computerlinguistische Vorgehensweise unterstützt werden, so dass Forscher Einfluss
              auf komplexe Prozesse nehmen können wie beispielsweise dem Trainieren maschineller
              Lernmethoden durch visuelle Veränderungsparameter. Durch diese Art der Interaktion
              kann unterstützt werden, dass Modelle mit Hilfe des Experten entwickelt, angepasst,
              trainiert sowie die Ergebnisse evaluiert werden können. Um diese Herausforderungen
              umzusetzen, wurden zwei interaktive visuelle Analysewerkzeuge konzipiert und
              entwickelt. Der VarifocalReader (Ertl et al. 2014), der auf einem hierarchischen
              Navigationskonzept basiert (Wörner / Ertl 2013), ermöglicht den Anwendern einen
              direkten Zugang zu Details und Dokumentquellen, während sie auf unterschiedlichen
              Abstraktionsebenen mit Zusammenfassungen vorhandener Annotationen interagieren
              können. Des Weiteren bietet das System die Möglichkeit, computerlinguistische
              Modelle anzupassen bzw. zu trainieren sowie Metadaten zu analysieren, zu annotieren
              und zu korrigieren. Eine beispielhafte Analyse ist in Abbildung 3 dargestellt, in
              der der Forscher einen schnellen Überblick und Zugang zur ausgewählten Annotation
              „Wallenstein“ (in der 3. Word Cloud sichtbar) erhält.
         
            
            
               Abb. 3: Emil Staigers “Grundbegriffe der Poetik” unterteilt (von
                links nach rechts) in unterschiedliche Ebenen. Kapitel (mit Word Clouds),
                Unterkapitel (mit Balkendiagrammen und Piktogrammen), Seiten (mit Piktogrammen),
                Textzeilen und gescannte Digitalisate der aktuellen Seite.
         
         Der zweite Ansatz (Heimerl et al. 2014) wurde konzipiert, um eine textvergleichende
                Analyse zu ermöglichen (siehe Abbildung 4). Die Visualisierung bietet einen
                Vergleich von mehreren Dokumenten auf einer abstrakten Ebene in Bezug auf die
                Verteilung der Annotationen, während die Textfelder eine flexible Navigation durch
                die einzelnen Texte ermöglichen. Zusätzlich unterstützt dieser focus+context Ansatz
                einen reibungslosen Übergang zwischen close und distant reading.
         
            
            
               Abb. 4: In dieser Abbildung sind drei ausgewählte Texte
                  nebeneinander dargestellt. Jedes dieser Dokumente verfügt über Seitenangabenskala
                  (linke Seite) und jeweils zwei Bänder, die zum einen Annotation darstellen (grüne
                  Balken) und zu anderen Suchergebnisse (orangene Balken). Der Analyst kann durch die
                  einzelnen Dokumente navigieren (Textboxscrollleiste) und per Mausklick zwischen den
                  einzelnen Annotationen (Balken) springen.
         
         Conclusion
         Ergebnis des Projekts ePoetics ist ein digitalisiertes und annotiertes Korpus
                  poetologischer Texte (TEI-konform und nachnutzbar), in denen zentrale Konzepte der
                  Sprach- und Literaturtheorie durch XML-Auszeichnung explizit gemacht und
                  systematisiert werden. Durch Korpus-übergreifende Analysen dieser Auszeichnungen
                  können Gemeinsamkeiten und Unterschiede sowie diachrone Entwicklungen gezeigt
                  werden. Darüber hinaus werden die Referenz- und Diskursstrukturen erschlossen (auch
                  implizite, „versteckte“ Verweisungen), die auf verschiedenen Ebenen der Texte
                  bestehen – einerseits Verweisungen auf andere Poetiken sowie die Identifikation
                  bestimmter Denkschulen bzw. Theorielinien, die bis auf Ansätze aus der Antike
                  zurückgehen (z. B. Aristoteles, Quintilian), andererseits die Diskussion von
                  literarischen Beispielen, die Rückschlüsse auf die Entwicklungen des Literaturkanons
                  erlauben. Die manuellen Annotationen werden iterativ gestützt durch automatisierte
                  Methoden und Verfahren der interaktiven Visualisierung. Die dabei entwickelten
                  computerlinguistischen Anwendungen und Visualisierungssysteme (siehe Abbildungen 3
                  und 4) stellen ebenfalls Ergebnisse des Projekts dar.
      
      
         
            
               Bibliographie
               
                  Ertl, Thomas / Wörner, Michael (2013): “Smoothscroll. A
                        multi-scale, multi-layer slider”, in: Computer Vision,
                        Imaging and Computer Graphics - Theory and Applications 274:
                        142–154. 
               
                  Ertl, Thomas / John, Markus / Koch, Steffen / Wörner,
                            Michael (2014): “VarifocalReader – In-Depth Visual Analysis of
                            Large Text Documents”, in:  IEEE Transactions on
                            Visualization and Computer Graphics (TVCG) 20, 12: 1723-1732. 
               
                  Ertl, Thomas / Kuhn, Jonas / Richter, Sandra / Alscher,
                                Stefan / Rapp, Andrea (2013-2016): 
                  ePoetics.
                                Universität Stuttgart /www.epoetics.de
                                [letzter Zugriff 03. Februar 2016]. 
               
                  Heimerl, Florian / John, Markus / Koch, Steffen / Müller,
                                    Andreas (2014): “A Visual Focus+Context Approach for Text
                                    Comparison Tasks”, in:  VisLR Workshop, LREC 2014. 
               
                  Kuhn, Jonas / Reiter, Nils (2015): “A plea for a
                                      method-driven agenda in the Digital Humanities”, in: 
                                      Proceedings of the Digital Humanities Conference, Sydney, Australia
                                      2015. 
               
                  Ramsay, Stephen (2007): “Algorithmic Criticism”, in:
                                        Schreibman, Susan / Siemens, Ray (eds.): A Companion to
                                        Digital Literary Studies. Malden, MA: Blackwell 477-491. 
               
                  Richter, Sandra (2010): A History of
                                          Poetics. German Scholarly Aesthetics and Poetics in International
                                          Context, 1770-1960. With Bibliographies by Anja Zenk, Jasmin Azazmah, Eva
                                          Jost and Sandra Richter. Berlin / New York: de Gruyter.
            
         
      
   



      
         Die Chronik des Matthias von Edessa ist den meisten Wissenschaftlern des
        mittelalterlichen Nahen Ostens und des ersten Kreuzzuges bekannt für ihren Reichtum
        an Informationen, sowie der (mutmaßlichen) Ignoranz und Naivität ihres Autors.
        Matthias von Edessa war ein Armenischer Priester, der in der Kreuzfahrer-Grafschaft
        Edessa lebte und die Chronik zwischen den Jahren 1110 und 1132 verfasste. Gleichwohl
        der Text oft von Historikern verwendet wird, liegt er bis heute in keiner kritischen
        Edition vor und wurde zuletzt 1898 (Matthias von Edessa 1898) veröffentlicht. Die
        Chronik umfasst 35 (kopierte) Manuskripte, deren ältestes auf mindestens 450 Jahre
        nach dem Tod des Autors datiert werden kann. Diese werden zur Zeit für eine digitale
        Gesamtedition vorbereitet.
         Die Herausforderungen, die sich dabei stellen, beschränken sich nicht nur auf die
          Bearbeitung aus philologischer Sicht, sondern auch auf die Annotation und
          Präsentation als historisches Werk, mit dem Ziel, sie auch als Plattform für
          Historiker zur Verfügung zu stellen (z. B. mit Zeit- und Ortsangaben, etc.). Hierzu
          greifen wir zum Einen auf eine Reihe aktueller Methoden und Werkzeuge der digitalen
          Philologie zurück, wie zum Beispiel die Transkription aller Manuskripte,
          palaeographisches Markup unter der Benutzung des TEI-Vokabulars (TEI Consortium
          2015), automatische Manuskript-Kollation mit CollateX (Dekker et al. 2014),
          stemmatische Analyse mit Hilfe der Werkzeuge von Stemmaweb (Andrews / Macé 2013) und
          der Publikation aller Transkriptionen, sowie einer editorischen Rekonstruktion des
          Textes. Zum Anderen werden auch Textkommentare von der digitalen Plattform
          profitieren, da sie mit weiteren Informationen angereichert werden können. So können
          unter Anderem Ortsnamen nicht nur ausgezeichnet („getagged“) werden, sondern ihre
          mögliche Lokalisierung auch angegeben und soweit möglich geographisch angezeigt
          werden. Personen und ethnographische Bezeichnungen werden nicht nur in einem Index
          erfasst, sie werden, soweit möglich, mit prosopographischen Datenbanken oder
          relevanten Seiten auf Wikipedia verlinkt.
         Das derzeitige Projekt The Chronicle of Matthew of Edessa
            Online wird bis 2018 von dem Schweizer National Fond (SNF) finanziert und
            baut auf verschiedenen Projekten und gesammelten Erfahrungen der vergangenen fünf
            Jahre auf. 
         Ziel ist es, dem Forscher oder der Forscherin ein Werkzeug in die Hand zu geben, das
              es ihm / ihr erlaubt Bewegungen von Individuen und Gruppen über Raum und Zeit zu
              verfolgen, indem wir die Textedition selbst zu einer Plattform für mittelalterliche
              Geschichte machen.
         Im Folgenden werden wir zwei, aus technischer Sicht wesentliche Aspekte des Projektes vorstellen (Speichern und Wiedergabe von Informationen) und erste Ergebnisse präsentieren.
         
            Effizientes Speichern und Bearbeiten von Manuskripten und Annotationen 
            Aufbauend auf den Erkenntnissen, die wir aus dem Stemmaweb-Projekt gewonnen haben, werden die Manuskripte in Form von Graphen gespeichert
                  1. Daher ist für uns der naheliegenste Schritt, die gesamte Arbeit des Benutzers im Graph zu repräsentieren. Als mögliche Darstellungsformen für den Benutzer kommen hierfür Online-Präsentationen oder ein Export, zum Beispiel nach TEI, in Betracht.
                
            Während die Speicherung in Stemmaweb als Perl-Objekte in einer Relationalen-Datenbank
                  (MySQL (Oracle Cooperation 2015)) erfolgt, haben wir uns für dieses Projekt dafür
                  entschieden, Neo4J (Neo Technology Inc 2015), eine Graph-Datenbank zu verwenden. Zum
                  einen kommt dies unserer intern verwendeten Datenstruktur entgegen, die auf einer
                  Modellierung auf Graphen basiert. Damit verbunden hat dies auch den Vorteil, dass
                  viele, für uns wichtige Funktionen (z. B. Depth First Search, Breadth First Search,
                  etc.), nativ vom Datenbanksystem zur Verfügung gestellt werden und speziell
                  benötigte Operationen, wie z. B. Plausibilitätsprüfungen, einfacher implementiert
                  und effizienter ausgeführt werden können. 
            Des Weiteren migrieren wir derzeit große Teile, des aus Stemmaweb vorliegenden
                    Quellcodes, von Perl nach Java. Gleichzeitig überarbeiten wir die API, mit dem Ziel,
                    den Service auch über eine Web-API nutzen zu können. Die Hauptgründe hierfür sind,
                    die bestehende Stemmaweb Backend-Engine zu modernisieren und effizienter zu
                    gestalten. Dieser Schritt soll bis Ende 2015 abgeschlossen sein.
            Parallel zu der Migration, haben wir bereits einen ersten Prototypen des Editions-Interfaces erstellt. Dieser Prototyp basiert bereits auf Neo4J und implementiert ein Textmodel das interoperabel mit Stemmaweb ist und wir zu Testzwecken bereits mit zusätzlichen Annotationen versehen haben.
            In Abbildung 1 ist ein typischer Ausschnitt unserer Neo4J Datenbankstruktur zu sehen. Man erkennt, dass sich eine „TRADITION“ aus ein oder mehreren „SECTION“-Knoten zusammensetzt, die wiederum mittels gerichteter „NEXT“-Kanten miteinander verbunden sind und somit ihre Reihenfolge gewährleistet bleibt
                      2. Jeder „SECTION“-Knoten verweist wiederum auf eine Folge von untereinander mit „LEMMA_TEXT“ verbundenen „READING“-Knoten. Darüber hinaus existieren noch sogenannte „TRANSLATION“-Knoten, die die jeweilige Übersetzung in einer Sequenz aus READING-Knoten speichern. Hierbei bleibt es dem Bearbeiter freigestellt, die Granularität der Übersetzung festzulegen. So sind Einheiten, wie zum Beispiel Sigle-Wörter, Sätze oder ganze Paragraphen, etc. vorstellbar.
                    
            
               
               
                  
                   
                  
                  Abb. 1 : Beispiel für die
                        derzeitige Datenbankstruktur in Neo4J. Gut zu Erkennen ist die Gliederung in
                        Tradition (blau), Sections (orange), Readings (pink) und Translations
                        (rosa). 
            
         
         
            Eine adäquate Wiedergabe der gespeicherten Informationen
            Wie bereits erwähnt, liegt ein weiterer Schwerpunkt des Projektes auf einer angemessenen Wiedergabe der im System vorhandenen Informationen. Aus diesem Grund haben wir, parallel zu den Arbeiten am Backend, bereits damit begonnen, den Prototypen einer Webseite zur Darstellung der Informationen zu implementieren.
            Dieser Prototyp stellt zur Zeit vier Abschnitte (Sections) aus der Chronik bereit,
                        zwischen denen der Benutzer auswählen kann (Abbildung 2). Zu einem ausgewählten
                        Abschnitt werden dessen Transkription, sowie (englische) Übersetzung angezeigt. Eine
                        Karte, in Form einer eingebetteten Google-Map (Google Inc. 2012), zeigt zusätzlich
                        alle im Abschnitt vorkommenden Örtlichkeiten an, sofern sie lokalisierbar sind. In
                        der Transkription werden Textteile, zu denen Zusatzinformationen vorliegen, dem
                        Benutzer farblich kodiert angezeigt (Ortsangaben (blau), Personenangaben (rot) und
                        Zeitangaben (gelb)).
            
               
               
                  
                  Abb. 2 : Screenshot der
                          Webseite. Neben der Transkription (linke Spalte), befinden sich die
                          englische Übersetzung (rechte Spalte), sowie ein Karte mit
                          Positionsmarkierungen der im Text vorkommenden Ortsbeschreibungen. 
            
            Fährt nun der Benutzer mit dem Mauszeiger über einen solchen farblich hervorgehobenen Textteil, werden zu diesem in einem Popup-Fenster weitere Informationen oder Verlinkungen angezeigt. So ist zum Beispiel für Ortsangaben, wie in Abbildung 3 zu sehen, dies üblicherweise der Ortsname, sowie ein vergrößerter Ausschnitt der Karte mit einer Markierung der genauen Position, soweit diese bekannt ist. 
            
               
               
                  Abb. 3: Beispiel für Zusatzinformationen bezüglich
                          Constantinople (Faith).
            
            Bei Personenangaben, siehe Abbildung 4, wird die Übersetzung des Namens, der volle
                          Name und weitere Informationen, sowie Links, die zu Einträgen bezüglich der Person
                          auf externen Seiten verweisen, hier Links zu Wikipedia
                          (Wikimedia Foundation 2015) und Prosopography of the Byzantine
                          World (Jeffreys et al. 2011), in einem Popup-Fenster angezeigt. 
            
            
               
               
                  Abb. 4 : Beispiel für Zusatzinformationen bezüglich
                          Ioannes 1 (John I Tzimiskes).
            
            Darüber hinaus arbeiten wir daran, sowohl individuelle Textzeugen als auch verschiedene Variationen eines Textes anzeigen zu können.
            Hervorzuheben ist, wie oben bereits angedeutet, das die Plattform auch die
                          Möglichkeit bietet, die Edition zu exportieren. Hierzu werden relevante Standards
                          wie zum Beispiel TEI oder CIDOC-CRM (International Council of Museums 2014)
                          angeboten. 
         
         
            Zusammenfassung und Ausblick
            Wir haben einen kurzen Einblick in das Projekt The Chronicle
                            of Matthew of Edessa Online gegeben. Dieses hat als Ziel eine digitale
                            Plattform zur Verfügung zu stellen, welche das Untersuchen und Bearbeiten von
                            Manuskripten aus philologischer, als auch historischer Sichtweise unterstützt.
                            Hierzu haben wir einen Einblick in technische Aspekte und dem derzeitigen Stand der
                            Implementierung gegeben. Zusammendfassend kann man sagen, dass wir schon eine gute
                            Strecke zurückgelegt haben und dass das Projekt noch einige interessante Aufgaben
                            für uns bereit hält 3.
         
      
      
         
            Stemmaweb bietet seinen Benutzern die Möglichkeit mit Text-Kollationen und Varianten zur stemmatischen Analyse zu arbeiten und diese zu modifizieren. Des Weiteren können Texte basierend auf Kollationen und Stemmata rekonstruiert und erstellt werden. Hierfür hat es sich bewährt, Texte in einem Graph abzubilden.
            Dieser Ansatz wurde gewählt, da eine automatische Kollation für längere Texte am besten mit einer Einteilung in diskrete Abschnitte (Sections) gelingt. Desweiteren können wir somit Textzeugen, deren Abschnitte abweichend angeordnet sind, ohne Schwierigkeiten darstellen
            Interessante Aufgaben wären zum Beispiel die Gestaltung der Web-Oberfläche, die Interaktion mit dem Benutzer, Optimierung der Graph-Datenbank, Web-API oder die Bereitstellung verschiedener Exportformate, um nur einige zu nennen.
         
         
            
               Bibliographie
               
                  Andrews, Tara L. / Macé, Caroline (2013): „Beyond the
                                  tree of texts: Building an empirical model of scribal variation through
                                  graph analysis of texts and stemmata“, in: Literary and
                                  Linguistic Computing 28, 4: 504-21 http://dx.doi.org/10.1093/llc/fqu072 [letzter Zugriff 09. Februar
                                  2016]. 
               
                  Dekker, Ronald Haentjens / Hulle, Dirk van / Middell, Gregor
                                      / Neyt, Vincent / Zundert, Joris van (2014): „Computer-supported
                                      collation of modern manuscripts: CollateX and the Beckett Digital Manuscript
                                      Project“, in Literary and Linguistic Computing 25:
                                      1-19 http://dx.doi.org/10.1093/llc/fqu007 [letzter Zugriff 09. Februar
                                      2016]. 
               
                  Google Inc (2012): Google Maps 
                  http://www.google.com/maps
                                        [letzter Zugriff 15. Oktober 2015]. 
               
                  International Council of Museums (2014): The CIDOC Conceptual Reference Model
                  http://www.cidoc-crm.org
                                          [letzter Zugriff 15. Oktober 2015]. 
               
                  Jeffreys, Michael (2011): Prosopography of the Byzantine World
                  www.pbw.kcl.ac.uk [letzter
                                            Zugriff 15. Oktober 2015]. 
               
                  Matthias von Edessa (Mattʿēos Uṙhayecʿi) (1898): Die Croniken des Matthias von Edessa
                                              Žamanakagrutʿiwn. Vałaršapat. 
               
                  Neo Technology Inc. (2015): Neo4j
                  http://www.neo4j.com [letzter
                                                Zugriff 15. Oktober 2015]. 
               
                  Oracle Cooperation (2015): MySQL 
                  https://www.mysql.com [letzter
                                                  Zugriff 15. Oktober 2015]. 
               
                  TEI Consortium (2015): Guidelines for
                                                    Electronic Text Encoding and Interchange. Version 2.8.0 
                  http://www.tei-c.org/Vault/P5/2.8.0/doc/tei-p5-doc/en/html/
                                                      [letzter Zugriff 15. Oktober 2015]. 
               
                  Wikimedia Foundation (2015): Wikipedia
                  http://www.wikipedia.org
                                                        [letzter Zugriff 15. Oktober 2015]. 
            
         
      
   



      
         
            Implizite und explizite Semantik TEI-basierter Fachdatenrepositorien
            Zahlreiche geisteswissenschaftliche Fachdatenrepositorien setzen zur Modellierung
          ihrer Forschungsdaten auf die Richtlinien der Text Encoding Initiative (TEI) und
          somit auf XML als primäres Datenformat. XML eignet sich sehr gut zur Lösung
          editorisch-philologischer Aufgabenstellungen und entspricht den geforderten
          Kriterien der Interoperabilität und Nachhaltigkeit von Forschungsdaten. Durch
          die standardkonforme Auszeichnung der Forschungsgegenstände in TEI werden diese
          formal und inhaltlich erschlossen. TEI-kodierte Daten beinhalten in jeder
          Hinsicht semantische Bezüge (bspw. Raumbezüge, Personenbezüge, begriffliche und
          konzeptuelle Bezüge etc.). Aus der Perspektive des Semantic
          Web sind diese Bezüge jedoch zunächst nur implizit und nicht explizit
          in den Daten vorhanden (Abbildung 1). Im Gegenzug gründen sich Semantic Web-Technologien auf das Resource
          Description Framework (RDF) zur Formulierung semantischer Aussagen (statements) in Form von Subjekt – Prädikat –
          Objektbeziehungen (triples). Die besondere Stärke von RDF
          liegt in der automatisiert möglichen Vernetzung (interlinking), Zusammenführung (merging) und
          Analyse (reasoning) eigentlich separater Datenbestände.
          RDF ist modellierungstechnisch auf einer höheren Abstraktionsebene anzusiedeln
          als TEI-kodierte XML-Daten (Abbildung 2; vgl. auch Polleres u. a. 2009). 
            
               
               
                  Abb. 1
               
            
            
               
               
                  Abb. 2
               
            
            
            Während die formale Erschließung geisteswissenschaftlicher
            Forschungsgegenstände mittels XML-basierter Annotationsmethoden mittlerweile als
            weit fortgeschritten gelten kann, bleibt die semantische Erschließung häufig
            noch weit zurück. Zwar wird in den Daten oft das Auftreten bestimmter Ortsnamen,
            Personennamen, Werktitel etc. annotiert. Dennoch gehen diese Annotationen meist
            nicht darüber hinaus, anzuzeigen, dass eine bestimmte Entität an einer
            spezifischen Stelle erwähnt ist. Damit bleibt die Semantik der Fachdaten weit
            hinter den Möglichkeiten zurück, die aktuelle Technologien – insbesondere die
            des Semantic Web und des Linked Open
            Data (LOD) – bieten könnten. Der durch LOD mögliche Zugang auf
            vernetzte Forschungsdaten eröffnet neuartige Perspektiven der Nutzung bisher
            isoliert stehender Fachdatenrepositorien. Wesentlich ist dabei, dass LOD und RDF
            bestehende Standards der Digital Humanities (wie bspw.
            TEI / XML) um die Verwendung gemeinsamer Terminologien und Metadatenschemata
            erweitern (vgl. Iglesia et al. 2015). Dadurch wird es möglich, auch Bestände
            verteilter Provenienz und unterschiedlicher Struktur gemeinsam inhaltlich zu
            beschreiben und zu analysieren. 
            Insgesamt existiert momentan also noch eine Kluft: Auf der einen Seite die
              zahlreichen geisteswissenschaftlichen Fachdatenrepositorien mit implizit
              semantischem Potential, auf der anderen Seite die Technologien und Datenmodelle
              des Semantic Web, die neue Sichten und Analysemethoden
              auf die Daten eröffnen könnten. Zwar existieren einige Sprachkonzepte, Methoden
              und Tools zur Übersetzung zwischen TEI / XML und RDF. Diese sind jedoch
              ausnahmslos komplex, teilweise technisch veraltet, verfügen nur über
              prototypische Implementierungen oder sind hochgradig spezialisiert auf einen
              bestimmten Datenbestand. 1 Während die dem Semantic Web zugrunde liegenden Technologien aus informatischer Sicht
              als erschlossen und anwendbar angesehen werden können (vgl. Lanthaler 2014:
              11–35), besteht zum jetzigen Zeitpunkt also ein Bedarf an exemplarischen
              Bearbeitungen repräsentativer Forschungsdatenbestände aus den
              Geisteswissenschaften, um die Tragfähigkeit dieser Technologien auch für die
              geisteswissenschaftliche Forschung zu demonstrieren. 
         
         
            
               Semantische Aussagen aus XML mit Hilfe des
                XTriples-Webservices
              
            An dieser Stelle setzt der 
                  XTriples-Webservice der 
                  Digitalen Akademie
               
                der Mainzer Akademie der Wissenschaften und der Literatur an. Grundgedanke des
                generischen Dienstes ist das Crawling beliebiger XML-Datenbestände und die
                anschließende Generierung semantischer Aussagen aus den XML-Daten auf Basis
                definierter Aussagemuster. Das Prinzip der Explizierung semantischer Aussagen
                aus XML ist dabei nicht sonderlich komplex: Wird die URI einer XML-Ressource
                oder eine Dateneinheit in dieser Ressource als das Subjekt einer semantischen
                Aussage begriffen, können diesem Subjekt über Prädikate aus kontrollierten
                Vokabularen weitere Werte aus den XML-Daten bzw. URIs zu weiteren
                Datenressourcen als Objekte zugeordnet werden. Im Übersetzungsvorgang zwischen
                XML und RDF geht es also vor allem um die Bestimmung semantischer Aussagemuster,
                die sich gesamthaft auf alle Ressourcen eines XML-Datenbestandes anwenden
                lassen. 
            Die Aussagemuster werden in Form einer einfachen, XPATH-basierten Konfiguration
                  an den Dienst übermittelt. Dabei ist es auch möglich, über die Bestände eines
                  spezifischen XML-Repositoriums hinauszugehen und externe Ressourcen oder
                  Dateneinheiten in die Transformation mit einzubeziehen (bspw. aus der GND, der
                  Dbpedia, aus Geonames u. a ). Die technische Realisierung als Webservice hat den
                  Vorteil, dass AnwenderInnen keine weitere Software zur semantischen Übersetzung
                  von Forschungsdaten benötigen. Gleichzeitig kann der Webservice auch als eine
                  Art „externe“ RDF-Schnittstelle (im Sinne eines Proxy) für ein oder mehrere
                  XML-Repositorien eingesetzt werden. Grundvoraussetzung hierfür ist lediglich,
                  dass die jeweiligen Repositorien über HTTP erreichbar sein müssen. 
            
               
               
                  Abb. 3
               
            
            Das Ergebnis einer XTriples-Extraktion steht in einer
                  Vielzahl gängiger RDF-Serialisierungen zur Verfügung (Abbildung 3). Neben rein
                  RDF-basierten Formaten ist es auch möglich, die semantischen Bezüge eines
                  Repositoriums mittels SVG darzustellen oder das Extraktionsergebnis zur weiteren
                  Analyse und Visualisierung an Semantic Web-Tools
                  weiterzureichen. 2
            
         
         
            
               Anwendungsbeispiele
                
            
               XTriples wurde vom Autor im Kontext des Akademievorhabens
                  
                  Deutsche
                    Inschriften Online
                in Verbindung mit dem BMBF-Projekt 
                  Inschriften im Bezugssystem des Raumes
                entwickelt und steht der
                    DH-Community in einer stabilen Version unter Open Source Lizenz (MIT) zur
                    Verfügung. Das zugrunde liegende Softwarepaket ist vollständig dokumentiert und auf GitHub
                    veröffentlicht. 
            Neben den Deutschen Inschriften wird XTriples aktuell auch in den Akademievorhaben 
                  Regesta Imperii
                und
                    Die Schule von Salamanca verwendet. Das Salomon Ludwig Steinheim-Institut
                    für deutsch-jüdische Geschichte nutzt XTriples
                    für eine CIDOC-CRM basierte, semantischen Modellierung von EpiDoc-Daten im
                    Rahmen des BMBF-Projektes Relationen im Raum. Gemeinsam
                    mit der Berlin-Brandenburgischen Akademie der
                    Wissenschaften wird gerade eine Schnittstelle zwischen XTriples und 
                  correspSearch
               , dem Webservice der BBAW zur dezentralen
                    Aggregation digitaler Briefeditionen, implementiert. 
            Folgende Beispiele geben einen ersten Überblick über die unterschiedlichen Anwendungsgebiete von
                      XTriples:
                    
            
               Semantische Extraktion und nachfolgende Visualisierung von Familienbeziehungen aus dem
                        Epidat Grabstein Corpus für den jüdischen Friedhof in Hamburg-Altona (Ausgangsdaten EpiDoc/TEI):
                        
                  http://xtriples.spatialhumanities.de/examples/dh/epidat/index.html
               
               Semantische Extraktion und SVG-Visualisierung eines Briefnetzwerks (Teilbestand der Korrespondenz Goethes aus den in
                        correspSearch aggregierten CMI-Daten bei gleichzeitiger
                        on-the-fly Einbeziehung der RDF-Schnittstellen von
                        GND und
                        Geonames):
                        
                  http://bit.ly/1LKK1dv
               
               Beispielhafte semantische Extraktion und Visualisierung europäischer
                        Kommunikationsnetzwerke auf Basis der correspSearch
                        TEI / CMI-Daten: 
                  http://metacontext.github.io/presentation-correspsearch-xtriples/viz/map.html
               
            
            Weitere Beispiele zu den einzelnen Funktionalitäten finden sich auf der
                        XTriples-Website unter
                        http://xtriples.spatialhumanities.de/examples.html. Einen schnellen Überblick über die Funktionsweise des Dienstes gibt folgende Präsentation:
                        http://metacontext.github.io/presentation-correspsearch-xtriples.
                      
            Ziel des Vortrags ist eine Veranschaulichung der Methoden und Potentiale, die sich aus der semantischen Extraktion, Modellierung, Vernetzung und Visualisierung XML-basierter, geisteswissenschaftlicher Fachdaten ergeben. Neben einer Darstellung der technischen Hintergründe des
                        XTriples-Webservices werden auch Fragen der semantischen Modellierung geisteswissenschaftlicher Fachdaten mittels bestimmter Ontologien (bspw. FOAF, CIDOC-CRM u.a.) in den Blick genommen. Weiterhin werden auch beispielhafte Analyse- und Visualisierungsmöglichkeiten für semantisch modellierte geisteswissenschaftliche Fachdaten vorgestellt.
                      
         
      
      
         
            Projekte wie
                        bspw. SPQR oder
                        das Textual Encoding
                        Framework sind veraltet oder technisch nicht generalisiert. Einen
                        interessanten Ansatz bietet die XSPARQL Language Specification des DERI, die 2009 in Form einer
                        W3C Member Submission niedergelegt wurde. Hier fehlen jedoch praktische
                        Implementierungen. Die Benutzung von RDFa innerhalb von XML-Daten stellt
                        eine weitere Möglichkeit dar, doch verfolgen die wenigsten
                        geisteswissenschaftlichen Fachdatenrepositorien eine so ausgerichtete
                        semantische Markup-Strategie. Auch das bereits 2007 in Form einer W3C
                        Recommendation grundgelegte GRDDL-Framework (Gleaning Resource Descriptions from Dialects of
                        Languages) ist bis heute eine theoretische Spezifikation geblieben. Der OxGarage Transformations-Webservice der 
                  Text Encoding
                        Initiative
                bietet zwar eine Routine für die Konvertierung
                        von TEI kodierten Daten nach RDF an, legt sich für die Transformation aber
                        auf das CIDOC-CRM als Ontologie fest. Mit OxGarage
                        können out-of-the-box also keine anderen Ontologien
                        für eine semantische Modellierung benutzt werden. Zudem ist der Webservice
                        nicht darauf ausgelegt, auch weitere, externe Datenrepositorien in eine
                        Transformation mit einzubeziehen oder andere RDF-Serialisierungen jenseits
                        von RDF/XML zurückzugeben.
                      
             Beispielsweise an den
                        RDF zu SVG Transformations-Webservice oder an die RDF
                        Visualisierungsbibliothek d3sparql.
                      
         
         
            
               Bibliographie
               
                  Akademie der Wissenschaften und der Literatur Mainz
                        (o. J.a): Digitale Akademie
                  http://www.digitale-akademie.de [letzter Zugriff 16. Februar
                          2016].
               
                  Akademie der Wissenschaften und der Literatur Mainz
                          (o. J.b): Regesta Imperii
                  http://www.regesta-imperii.de/startseite.html [letzter Zugriff
                            16. Februar 2016].
               
                  BBAW (o. J.): Berlin-Brandenburgische Akademie der Wissenschaften
                  http://www.bbaw.de/ [letzter Zugriff
                            16. Februar 2016].
               
                  correspSearch (o. J.): correspSearch. Search diverse letter editions http://correspsearch.bbaw.de/index.xql [letzter Zugriff 16.
                            Februar 2016].
               
                  Deutsche Inschriften Online (o. J.): http://www.inschriften.net
                            [letzter Zugriff 16. Februar 2016]. 
               
                  Haft, Michael (2013): "RDF als Verknüpfungsmethode
                              zwischen geisteswissenschaftlichen Forschungsdaten und Geometrien am
                              Beispiel des Projektes 'Inschriften im Bezugssystem des Raumes'",in: Skriptum 2,3 http://nbn-resolving.de/urn:nbn:de:0289-2013120622 [letzter
                              Zugriff 14. Oktober 2015]. 
               
                  i3Mainz / Akademie Mainz (o. J.): Inschriften im Bezugssystem des Raumes
                  http://www.spatialhumanities.de/ibr/startseite.html [letzter
                                Zugriff 16. Februar 2016].
               
                  IBR (Inscriptions in their spatial context) / Academy of
                                  Sciences and Literature, Mainz / Institute for Spatial Information and
                                  Surveying Technology i3Mainz (o. J.): XTriples
                  http://xtriples.spatialhumanities.de/index.html [letzter Zugriff
                                    16. Februar 2016].
               
                  Iglesia, Martin de la / Moretto, Nicolas / Brodhun,
                                        Maximilian  (2015): "Metadaten, LOD und der Mehrwert
                                        standardisierter und vernetzter Daten." In: Neuroth, Heike / Rapp, Andrea /
                                        Söring, Sibylle (eds.): TextGrid: Von der Community – für
                                        die Community. Eine Virtuelle Forschungsumgebung für die
                                        Geisteswissenschaften. Göttingen: Universitätsverlag Göttingen 91–102 http://dx.doi.org/10.3249/webdoc-3947 [letzter Zugriff 14.
                                        Oktober 2015]. 
               
                  Lange, Felix, Martin Unold (2015): Semantisch
                                          angereicherte 3D-Messdaten von Kirchenräumen als Quellen für die
                                          geschichtswissenschaftliche Forschung, in: Zeitschrift für
                                          digitale Geisteswissenschaften 1 http://dx.doi.org/10.17175/sb001_015 [letzter Zugriff 14. Oktober
                                          2015]. 
               
                  
                  Lanthaler, Markus (2014): Third
                                            Generation Web APIs. Bridging the Gap between REST and Linked Data.
                                            Diss. Institute of Information Systems and Computer Media. Technische
                                            Universität Graz http://www.markus-lanthaler.com/research/third-generation-web-apis-bridging-the-gap-between-rest-and-linked-data.pdf
                                            [letzter Zugriff 14. Oktober 2015]. 
               
                  Polleres, Axel u.a. (2009): XSPARQL
                                              Language Specification
                  http://www.w3.org/Submission/xsparql-language-specification
                                                letzter Zugriff 14. Oktober 2015]. 
               
                  Salomon Ludwig Steinheim-Institut für deutsch-jüdische
                                                  Geschichte (o. J.): http://www.steinheim-institut.de [letzter Zugriff 16. Februar
                                                  2016].
               
                  Schrade, Torsten  (2013): "Datenstrukturierung", in:
                                                    Über die Praxis des kulturwissenschaftlichen
                                                      Arbeitens. Ein Handwörterbuch. Bielefeld: transcript 91–97. 
            
         
      
   



      
         
            Einleitung
            Der Beitrag möchte zeigen, wie die Berücksichtigung detaillierter,
          gattungsbezogener Metadaten auf produktive Weise mit dem Verfahren des Topic
          Modeling verbunden werden kann, um bisher nicht bekannte thematische Strukturen
          im Textverlauf in einer Sammlung spanischer und hispanoamerikanischer Romane zu
          entdecken. Ausgangshypothese ist, dass die Wichtigkeit bestimmter Topics nicht
          nur im Textverlauf variiert, sondern dies auch in verschiedenen Untergattungen
          auf unterschiedliche Weise tut. Eine Pilotstudie wurde im März 2015 beim
          Workshop zu Computational Narratology bei der DHd-Tagung in Graz vorgestellt. Im
          Rahmen der interdisziplinären Würzburger eHumanities-Nachwuchsgruppe
          "Computergestützte literarische Gattungsstilistik (CLiGS)
          wurde dieser Fragestellung nun mit weiter entwickelten Methodik sowie einer neu
          erstellten Sammlung spanischsprachiger Romane aus Spanien und Hispanoamerika
          nachgegangen. 
         
         
            Stand der Forschung und Fragestellung
            Die Frage nach dem Text- oder Handlungsverlauf in narrativen literarischen Texten hat jüngst zunehmende Aufmerksamkeit in der digitalen Literaturwissenschaft erhalten. Matthew Jockers kam durch Sentiment Analysis im Verlauf zahlreicher Romane zu dem (kontrovers diskutierten) Ergebnis, es gäbe sechs oder sieben grundlegende Plotstrukturen (Jockers 2015). Ben Schmidt hat unter anderem den Verlauf von Topic-Wahrscheinlichkeiten in der "screen time" amerikanischer Fernsehserien verfolgt (Schmidt 2014). Der vorliegende Beitrag verbindet die Frage nach dem Textverlauf mit der nach den Untergattungen, seine zentrale Fragestellung lautet: Können wir nach Untergattung unterschiedliche Verlaufsmuster für bestimmte Topics über den Textverlauf hinweg feststellen?
         
         
            Daten
            Die Textsammlung enthält 150 spanische und hispanoamerikanische Romantexte aus
            der Zeit von 1880 bis 1930 (für den spanischen Roman: Altisent 2008; de Nora
            1963, für den hispanoamerikanischen Roman: Gallo 1981; Williams 2009). Die Texte
            sind in TEI aufbereitet und mit detaillierten Metadaten versehen worden. Es
            wurden vier weit gefasste Untergattungen gewählt, um die Romane miteinander
            vergleichen zu können: novela sentimental, novela histórica, novela
            político-social und novela de tendencia
            subjetiva. Die Auswahl der Texte ist auch von der Verfügbarkeit als
            digitaler Volltext beeinflusst und daher nicht unbedingt repräsentativ.
            Abbildung 1 zeigt die Verteilung der Romane nach ausgewählten Metadaten. 
            
               
               
                  Abb. 1: Verteilung der Romane nach Metadaten 
            
         
         
            Methode
            Topic Modeling ist eine unüberwachte, nicht-deterministische Methode aus dem
              Bereich des Natural Language Processing, die auf Annahmen
              aus der distributionellen Semantik basiert und verborgene semantische Strukturen
              in großen Textsammlungen aufdeckt (einführend Blei 2011, grundlegend Blei 2003).
              Gruppen semantisch verwandter Wörter werden insbesondere aufgrund ihres häufigen
              gemeinsamen Auftretens in den untersuchten Dokumenten entdeckt. Ein Topic ist
              eine Wahrscheinlichkeitsverteilung von Wörtern; ein Dokument wird als
              Wahrscheinlichkeitsverteilung von Topics beschrieben. Topic Modeling ist eine in
              den DH äußerst beliebte Methode (Anwendungsbeispiele: Blevins 2010; Rhody 2012;
              Jockers 2013; Schöch 2015). 
            Hier wurde Topic Modeling als Teil eines umfassenden, weitgehend automatischen Arbeitsablaufes als Serie von Python-Skripten implementiert: Präprozessieren der Texte (Segmentierung, Binning, Lemmatisierung, POS-Tagging), das eigentliche Topic Modeling (mit Mallet, siehe McCallum 2002), Aufbereitung des Mallet-Outputs, zahlreiche Visualisierungen als Perspektiven auf die Ergebnisse. Die wichtigsten Parameter: Berücksichtigung ausschließlich der Substantive, Weglassung der 70 häufigsten Substantive, Romansegmente von ca. 600 Wörtern (unter Berücksichtigung von Absatzgrenzen), Anzahl von 70 Topics. Die Python-Skripte sind frei verfügbar und ausführlich dokumentiert, Begleitmaterialien (Skripte, Parameterdatei, Metadaten, Abbildungen) sind unter
                https://github.com/cligs/projects/tree/master/2016/dhd einsehbar.
              
         
         
            Ergebnisse und Diskussion
            Es werden zunächst die Topics selbst dargestellt, dann Unterschiede in den Topic-Verteilungen nach Untergattungen, über den Textverlauf hinweg und schließlich über den Textverlauf in Abhängigkeit der Untergattung.
            
               Topics
               Die Mehrheit der erhobenen Topics beinhaltet konkrete typische Themen und
                  Motive des spanischsprachigen Romans der Epoche. Man erkennt eine klare
                  semantische Beziehung der Wörter: ein konkreter Bereich menschlicher
                  Tätigkeiten, wie in Topic 19 (maestro-colegi o-escuela, dt.
                  "Lehrer-Schule-Schule") oder Topic 23 (sangre-golpe-arma, dt.
                  "Blut-Schlag-Waffe"); oder abstrakte Begriffe und Gefühle, wie bei Topic 69
                  (conciencia-honor-crimen, dt. "Gewissen-Ehre-Verbrechen"). Weniger kohärent
                  ist Topic 45 (marido-rato-chico, dt. "Ehegatte-Weile-Junge"). Die folgenden
                  Wordclouds (Abbildung 2) veranschaulichen die erwähnten Topics.
               
                  
                  
                     Abb. 2: Wordclouds für ausgewählte Topics.
               
            
            
               Untergattungen und Topics
               Die folgende Heatmap (Abbildung 3) zeigt die Verteilung der
                    durchschnittlichen Topic-Wahrscheinlichkeiten in den vier Untergattungen für
                    diejenigen 20 Topics, deren Werte zwischen den Untergattungen besonders
                    stark schwanken (nach Standardabweichung). Besonders distinktive Topics
                    existieren für die novela de tendencia subjetiva
                    (Topic 11: mirada-huerto-silencio, dt. "Blick-Garten-Stille") und die novela sentimental (Topic 45). Wenig überraschend
                    auch, dass die novela histórica als distinktiven
                    Topic unter anderem Topic 57 hat (rey-caballero-príncipe, dt.
                    "König-Ritter-Prinz"). Für die novela
                    histórico-social, für die aufgrund der großen Zahl von Beispielen
                    eine größere Bandbreite an Topic-Verteilungen zu erwarten ist, gibt es
                    keinen vergleichbar stark distinktiven Topic. Dennoch sind die
                    Untergattungen ein wichtiger Faktor für die Verteilung der Topics in der
                    Sammlung und die thematische Komponente spielt für die Definition der
                    Untergattungen tatsächlich eine wesentliche Rolle. 
               
                  
                  
                     Abb. 3: Verteilung von Topic-Scores nach Untergattungen.
               
            
            
               Topics im Textverlauf
               Die Ausprägung der Topics variiert nicht nur hinsichtlich der Untergattungen,
                      sondern auch über den Textverlauf hinweg. So gibt es einige Topics, deren
                      Vorkommen am Anfang der Romane besonders wahrscheinlich ist (Abbildung 4a).
                      Dazu zählen Topic 10 (vino-plato-pan, dt. "Wein-Teller-Brot"), Topic 17
                      (sombrero-ropa-bota, dt. "Hut-Kleidung-Stiefel") und Topic 19, welche auf
                      die Beschreibung von Ambiente, Situation und Personen hindeuten. Gegen Ende
                      der Romane sind andere Topics wahrscheinlicher (Abbildung 4b), z. B. Topic 2
                      (pecado-caridad-conciencia, dt. "Sünde-Wohltätigkeit-Gewissen"), Topic 23
                      und Topic 69, also abstraktere Themen oder solche, die sich auf
                      Wertvorstellungen beziehen. Dies deutet darauf hin, dass in den Romanen am
                      Ende Bilanz gezogen wird, die Handlung einen drastischen Ausgang nimmt oder
                      das im Textverlauf Behandelte in gesellschaftliche oder religiöse Diskurse
                      eingebunden wird.
               
                  
                  
                     Abb. 4a: Verteilung von Topics im Textverlauf (fallend).
               
               
                  
                  
                     Abb. 4b: Verteilung von Topics im Textverlauf (steigend).
               
            
            
               Textverlauf abhängig von den Untergattungen
               Für einige der genannten Topics, die in bestimmten Bereichen des Textverlaufs
                        wahrscheinlicher sind, kann die Tendenz über alle Untergattungen hinweg
                        bestätigt werden (bspw. bei Topic 10 und 17, siehe oben). Es gibt aber auch
                        Themen, bei denen sich durch die Betrachtung des Verlaufs in den einzelnen
                        Untergattungen ein differenzierteres Bild ergibt. Die Wahrscheinlichkeit von
                        Topic 23 beispielsweise nimmt nur für die novela
                        político-social zum Ende hin zu (Abbidung 5a): 
               
                  
                  
                     Abb. 5a: Topic 23 nach Textverlauf und Untergattung.
               
               Das kann so interpretiert werden, dass die novela
                          político-social im Gegensatz zu den anderen Untergattungen dazu
                          tendiert, am Ende des Textes mit einer gewalttätigen Szene und einem Umbruch
                          zu schließen. Topic 19 ist nicht in allen Untergattungen zu Beginn des
                          Textverlaufes stark ausgeprägt, sondern nur bei der novela
                          de tendencia subjetiva. Dies erklärt sich, weil bei diesen Romanen
                          das Schulthema als Teil einer autofiktionalen Erzählung zu Beginn erscheint
                          (Abbildung 5b): 
               
                  
                  
                     Abb. 5b: Topic 19 nach Textverlauf und Untergattung 
               
               Allgemein gilt, dass die Untergattungen sich in ihrer Topicverteilung im Textverlauf auch dann deutlich unterscheiden können, wenn dies für alle Untergattungen zusammengenommen nicht der Fall ist und so leicht übersehen werden könnte.
               Für die Berechnung wurden die Romansegmente von 600 Wörtern bezüglich des Textverlaufs auf 15 Romanabschnitte (Bins) verteilt, um die unterschiedliche Romanlänge zu berücksichtigen. Diese Bins wurden hinsichtlich der Untergattung gruppiert und jeweils das arithmetische Mittel bestimmt. Die in den Plots eingezeichneten Kurven entsprechen der linearen Interpolation dieser gemittelten Werte. Zusätzlich wurde der Standardfehler vertikal um den jeweiligen Kurvenpunkt eingezeichnet, der deutlich macht, wie sehr die jeweiligen dem Mittelwert zugrunde liegenden Werte streuen, also wie gut der Mittelwert die Gesamtheit der Segmentwerte repräsentiert.
            
         
         
            Die Ergebnisse im literaturgeschichtlichen Kontext
            Insgesamt zeigen sich verschiedene Zusammenhänge: Zwischen bestimmten Topics und einzelnen Roman-Untergattungen, zwischen Topics und dem Textverlauf, und dies zum Teil dann auch wieder in Abhängigkeit von den Untergattungen. Aus literaturgeschichtlicher Perspektive betrachtet erweisen sich die in die Untersuchung einbezogenen Metadaten für eine Einordnung der Topic-Resultate als nützlich. Topics sind für die Romangattungen im vorliegenden Korpus ein wichtiger Faktor, ähnlich wie dies für Gattungen wie die klassische Komödie und Tragödie bereits gezeigt werden konnte (Schöch 2015).
            Ein detaillierterer Blick zeigt beispielsweise Folgendes: Topic 11, welches typisch für die
                          novela de tendencia subjetiva ist, ist vor allem in den 1910er- und 1920er-Jahren wichtig sowie für bestimmte Autoren. Interessanterweise ist dieses bei spanischen und hispanoamerikanischen Modernisten vorkommende Thema auch bei der früher wirkenden Schriftstellerin Juana Manuela Gorriti schon wichtig, die offenbar thematische Präferenzen späterer Autoren vorweggenommen hat. Außerdem kommt Topic 11 bei Larreta in einem (modernistischen) historischen Roman vor, obwohl es ansonsten vor allem für die Romane subjektiver Tendenz typisch ist. Es ist anzunehmen, dass für dieses spezielle Thema eher die literarische Strömung bestimmend ist als die Untergattung. Der Topic enthält einige für die modernistische Strömung typische Wörter, etwa zu Sinneseindrücken (azul, dt. "blau", olor, dt. "Geruch") und Zurückgezogenheit (huerto, silencio, campo, soledad, dt. "Garten, Ruhe, Land, Einsamkeit").
                        
         
         
            Fazit und Ausblick
            Die Nutzung von Topic Modeling als Methode kann für die digitale Literaturwissenschaft verbessert werden, wenn spezifisch literaturwissenschaftliche Metadaten in die Betrachtungen einbezogen werden und die Textstruktur - hier als Sequenz von Textverlaufseinheiten - berücksichtigt wird. Verschiedene Visualisierungsstrategien erweisen sich als entscheidende "Interfaces" zu den Daten (im Sinne von Doueihi 2012), die Muster sichtbar machen und den Blick lenken. Die Ergebnisse des Topic Modelings können differenzierter und aus verschiedenen Perspektiven betrachtet und mit literaturhistorischem Wissen in Verbindung gebracht werden. Die Ergebnisse ergänzen und erweitern etablierte hermeneutische Lektürestrategien, insofern sie einen synthetisierenden Blick auf sehr umfangreiche Textsammlungen erlauben.
            Nächste Schritte betreffen insbesondere die weitere Auseinandersetzung mit der
                          Signifikanz von Unterschieden in den Topic-Wahrscheinlichkeiten im Textverlauf,
                          deren Berechnung u. a. durch die mangelnde Normalverteilung der Werte nicht
                          trivial ist. Zusätzlich zu den Untergattungen sollen auch Kategorien wie das
                          Setting modelliert werden. Zudem sollen die Textverlaufs-Daten für die
                          automatische Klassifikation von Romanen nach Untergattungen genutzt werden.
                          Schließlich wird bereits an der Erweiterung der Textsammlung gearbeitet,
                          insbesondere mit Blick auf den Umfang und ein ausgeglicheneres Verhältnis der
                          Untergattungen.
         
      
      
         
            
               Bibliography
               
                  Altisent, Marta E. (2008): A
                            Companion to the Twentieth-Century Spanish Novel. Woodbridge:
                            Tamesis. 
               
                  Blei, David M. (2011): “Introduction to Probabilistic
                            Topic Models,” in:  Communication of the ACM. 
               
                  Blei, David M. / Ng, Andrew Y. / Jordan, Michael I.
                            (2003): “Latent Dirichlet Allocation,” in:  Journal of
                            Machine Learning Research 3: 993–1022. 
               
                  Blevins, Cameron (2010): “Topic Modeling Martha
                              Ballard’s Diary,” in:  Historying
                  http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/
                                [letzter Zugriff 16. Februar 2016]. 
               
                  Doueihi, Milad (2012):  Pour un
                                humanisme numérique (2011). Paris: Seuil. 
               
                  Gallo, Marta (1981):  La Novela
                                Hispanoamericana En El Siglo XIX. Madrid: La Muralla. 
               
                  García de Nora, Eugenio (1963):  La
                                Novela Española Contemporánea. Madrid: Gredos. 
               
                  Jockers, Matthew L. (2013):  Macroanalysis - Digital Methods and Literary History. Champaign,
                                IL: University of Illinois Press. 
               
                  Jockers, Matthew L. (2015): “Revealing Sentiment and
                                Plot Arcs with the Syuzhet Package” in:  Matthew. L.
                                Jockers
                  http://www.matthewjockers.net/2015/02/02/syuzhet/ [letzter
                                  Zugriff 09. Februar 2016]. 
               
                  McCallum, Andrew K. (2002): MALLET:
                                  A Machine Learning for Language Toolkit
                  http://mallet.cs.umass.edu
                                  [letzter Zugriff 09. Februar 2016]. 
               
                  Nachwuchsgruppe CLiGS (o.J.): Computergestützte literarische Gattungsstilistik
                  http://cligs.hypotheses.org/ [letzter Zugriff 16. Februar
                                    2016].
               
                  Rhody, Lisa M. (2012): “Topic Modeling and Figurative
                                    Language,” in:  Journal of Digital Humanities 2  http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody
                                    [letzter Zugriff 09. Februar 2016]. 
               
                  Schmidt, Benjamin M. (2014): “Typical TV Episodes:
                                    Visualizing Topics in Screen Time,” in:  Sapping
                                    Attention
                  http://sappingattention.blogspot.de/2014/12/typical-tv-episodes-visualizing-topics.html
                                      [letzter Zugriff 09. Februar 2016]. 
               
                  Schöch, Christof (2015): “Topic Modeling Genre: An
                                      Exploration of French Classical and Enlightenment Drama [submitted]”, in: 
                                      Digital Humanities Quarterly. 
               
                  Williams, Raymond L. (2009):  The
                                      Twentieth-Century Spanish American Novel. Austin, Texas: University
                                      of Texas Press. 
            
         
      
   



      
         
            Projekthintergrund: A. v. Humboldts „Kosmos-Vorträge“ (1827/28)
            Im Projekt Hidden Kosmos: Reconstructing A. v. Humboldt’s »Kosmos-Lectures«, das an der Berliner Humboldt-Universität (HU) aus Mitteln der Exzellenzinitiative über den Zeitraum von zwei Jahren (Juni 2014–Mai 2016) gefördert wird, werden sämtliche derzeit bekannte Nachschriften von Besuchern der weltberühmten Vorlesungen über physikalische Geographie Alexander von Humboldts erschlossen und als vernetztes Forschungskorpus ediert.
      
            
               
               
                  Abb. 1: „Alles ist Wechselwirkung.“ Humboldt [1803/04]: 27r,
        http://resolver.staatsbibliothek-berlin.de/SBB0001527C00000000
                  1
               
            
            Humboldts heute so genannten Kosmos-Vorträge fanden im Wintersemester 1827/28
        in zwei unabhängig voneinander verlaufenden Zyklen statt: Er absolvierte
        insgesamt 62 Vortragsstunden vor etwa 400 Studierenden und Lehrbefugten in der
        Berliner Universität und parallel dazu 16 Vorträge vor einer bis dahin
        unerreichten Zahl von etwa 1000 Zuhörern im großen Saal der Berliner
        Singakademie. Humboldt verband in diesen Vorträgen seine eigenen Forschungen,
        dabei aus dem reichen Erfahrungsschatz seiner fünfjährigen Amerikareise
        schöpfend, mit dem damals aktuellen Erkenntnisstand auf faktisch jedem Gebiet
        der aufstrebenden Naturwissenschaften. Beide Zyklen unterscheiden sich
        wesentlich hinsichtlich der Abfolge der besprochenen Themen und – aufgrund ihres
        sehr ungleichen Umfangs – auch im Hinblick auf die dabei jeweils erreichte Tiefe
        und Ausführlichkeit der Darstellung. Gemeinsam ist beiden Vortragsreihen jedoch
        der Anspruch, einen in sich abgeschlossenen Überblick zu geben, d. h. die
        astronomischen und tellurischen Phänomene, die Gestalt der Erdoberfläche und das
        organische Leben auf ihr, die kulturelle Entwicklung der Menschheit in den für
        Humboldt allgegenwärtigen „Wechselwirkungen“ (Abbildung 1) darzustellen.
            
               Forschungsstand zu den Kosmos-Vorträgen und Ziele des Projekts Hidden Kosmos
               Die Kosmos-Vorträge können nach wie vor als ‚blinder Fleck‘ der
            Humboldt-Forschung gelten (vgl. Erdmann/Thomas 2014: 35f.). Zum einen wohl
            deshalb, weil aufgrund einer (nachweislich falschen) Behauptung Humboldts im
            Kosmos (Humboldt 1845: X) die Manuskripte des Vortragenden als nicht
            existent galten, zum anderen weil auch der größte Teil der Nachschriften
            seiner HörerInnen de facto unbekannt blieb. Bis zur laufenden
            Veröffentlichung mehrerer Manuskripte durch das Hidden Kosmos-Projekt waren
            nur zwei solcher Nachschriften publiziert worden – beide jedoch in kaum
            wissenschaftstauglichen Editionen. Die übrigen zehn bisher bekannten
            Nachschriften lagerten unberührt in verschiedenen Bibliotheken in
            Deutschland und Polen bzw. in Privatbesitz.
               Die digitale Edition dieser unikalen Manuskripte schafft überhaupt erst eine
              solide Materialbasis, um die intensive Erforschung der Vortragsreihen zu
              ermöglichen.2 Das Projekt Hidden Kosmos arbeitet dabei eng mit dem Deutschen
              Textarchiv (DTA) der
              BBAW zusammen, wo die Nachschriften im Kontext des derzeit umfangreichsten
              digitalen ‚Alexander-von-Humboldt-Korpus‘ (s. Thomas 2015) veröffentlicht
              werden. Die weitere Dissemination und langfristige Bereitstellung der Daten
              erfolgt über das web- und zentrenbasierte Infrastrukturprojekt CLARIN-D.3 Derzeit (15.10.2015) stehen fünf Nachschriften mit mehr als 2 100
              handschriftlichen Seiten im DTA bzw. über CLARIN-D bereit; bis zur DHd 2016 werden zehn
              Bände mit ca. 3 760 Seiten zur Verfügung stehen. 
               
                  
                  
                     Abb. 2: DTA-Präsentation der Nachschrift
                eines anonym gebliebenen Zuhörers der Kosmos-Vorträge an der Berliner
                Universität: [N. N.]: Die physikalische Geographie von Herrn Alexander v.
                Humboldt, vorgetragen im Semestre 1827/28. [Berlin], [1827/28]. 
               
            
         
         
            Gliederung des Vortrags
            
               Überblick über die edierten Materialien und erste Forschungsergebnisse
               Auf der DHd 2016 wird zunächst ein konzentrierter Überblick über die bis
                dahin publizierten Materialien gegeben und werden erste, auf dieser
                Grundlage gewonnene Forschungsergebnisse vorgestellt. Der Fokus wird dabei
                im Sinne des Konferenzthemas auf der Modellierung der
                Daten und deren Annotation in TEI-XML, der Vernetzung
                der Dokumente sowie verschiedenen Visualisierungen der
                annotierten Forschungsdaten liegen. Im Vergleich der tief strukturierten und
                annotierten Online-Volltexte mit früheren Printeditionen zweier
                Nachschriften werden die Vorzüge digitaler Editionen sichtbar: Auf der
                Makroebene, d. h. hier: mit Blick auf die Gesamtheit der vielstimmigen
                Überlieferung, liegen diese Vorzüge zum einen in deren Perfektibilität und
                permanenten Erweiterbarkeit, zum anderen in der Vernetzung aller bekannten Nachschriften untereinander und mit weiteren
                elektronischen Ressourcen. In der Mikroperspektive, d. h. hier: mit Blick
                auf die einzelne, jeweils unikale Quelle ermöglicht die digitale Edition
                eine überlieferungsadäquate Repräsentation jedes einzelnen Manuskripts und
                der Besonderheiten seiner handschriftlichen Verfasstheit. Die digitale
                Edition erreicht dabei eine tiefere Granularität und eine größere
                Flexibilität der Nutzungsmöglichkeiten als die Print-Edition. 
               Anschließend sollen die Auswirkungen diskutiert werden, die der Einsatz von Methoden und Verfahren aus dem Bereich der ‚Digital Humanities‘ sowohl auf die Produktions- als auch auf die Rezeptionsseite einer (Manuskript-)Edition haben. Ganz im oben zitierten Sinne Humboldts sollen diese als ‚Wechselwirkungen‘ zwischen Projektdesign und Produzent bzw. Rezipient erfasst werden.
            
            
               Wechselwirkung zwischen Projektdesign und Nutzerperspektive
               Anschließend an die Präsentation im vergangenen Jahr auf der DHd 2015 in Graz
                  (Thomas 2014/15) kamen zwei grundsätzliche Fragen aus dem Publikum, die
                  vor dem Hintergrund des zum Zeitpunkt der DHd 2016 fast abgeschlossenen
                  Projekts beantwortet werden sollen. Die erste Frage stellte sich aus der
                  Perspektive der/s Rezipientin/en, der/die – so die an den Herausgeber der
                  Edition herangetragene Forderung – an die Quellen ‚herangeführt‘ werden
                  müsse. Denn im Unterschied zum überkommenen Prinzip der ‚Leithandschrift‘
                  bzw. der Konstruktion eines ‚idealen‘ Textes durch den Editor, das den
                  Beschränkungen gedruckter Editionen im ‚typographischen Paradigma‘ (Sahle
                  2013: 88) geschuldet ist, stehen im DH-Projekt Hidden Kosmos zehn parallele,
                  teils einander ergänzenden, teils miteinander konkurrierende Nachschriften
                  prinzipiell gleichwertig nebeneinander. Die komplexe und vielschichtige
                  Überlieferungslage soll für den/die Nutzer/in der edierten Texte
                  transparent werden, anstatt sie wie im typographischen Paradigma einzuebnen.
                  Der/die Nutzer/in soll ermutigt und befähigt werden, sich diese Komplexität
                  und Vielschichtigkeit durch eine parallele Lektüre verschiedener Quellen und
                  durch die angebotenen, explorativen Zugänge zu erschließen.4
               
               Um dieses Ziel zu erreichen, wurden im Hidden Kosmos-Projekt mehrere Dokument-übergreifende Zugänge geschaffen. In der vergleichsweise kurzen zweijährigen Projektlaufzeit mit sehr begrenzten personellen Ressourcen konnte diese Aufgabe überhaupt nur durch den konsequenten Einsatz digitaler Methoden bewältigt werden. Da mir das Problem begrenzter Zeit- und Personalressourcen typisch für den heutigen Forschungsalltag erscheint, soll auf die dabei verwendeten Ansätze näher eingegangen werden.
               
                  Automatisch erstellte, korpusübergreifende Zugänge als Orientierungshilfen
                  Anhand der -Ebenen der TEI-strukturierten Volltexte wurden je
                    eine thematische Gliederung für den Universitätszyklus und für die
                    Singakademie-Vorträge extrahiert und diese wiederum mit der
                    chronologischen Gliederung nach Vortragsstunden des jeweiligen Zyklus
                    kombiniert. Diese abstrahierte, alle Nachschriften verbindende
                    Orientierungshilfe konnte in dieser Vollständigkeit nur durch eine
                    Kombination aller vorliegenden Quellen erstellt werden, da die
                    jeweiligen Schreiber ihre Hefte entweder nur grob thematisch
                    strukturierten oder nur die Daten der jeweiligen Vortragsstunde
                    notierten. Das Ergebnis ist eine sehr viel detaillierte Übersicht über
                    die Kurse als die von Humboldt selbst überlieferte (Abbildung 3).
                  
                     
                     
                        Abb. 3: A. v. Humboldt:
                      Vorlesungsverzeichnis für die Berliner Universität, Staatsbibliothek zu
                      Berlin – Preußischer Kulturbesitz, Nachl. Alexander von Humboldt, gr.
                      Kasten 8, Nr. 5a, Bl. 1r (http://resolver.staatsbibliothek-berlin.de/SBB0001676C00000000)
                  
                  Nutzer/innen der Edition können sich anhand der extrapolierten Gliederung
                      leicht einen Überblick über die beeindruckende Themenfülle der
                      Vortragsreihen verschaffen und zwischen dem Einstieg in ein beliebiges
                      Thema oder eine Vortragsstunde wählen. Zudem wird die unterschiedliche
                      Abfolge der Themen in beiden Vortragszyklen anschaulich und ein
                      gezielter Einstieg in deren vergleichende Lektüre ermöglicht. Zugleich
                      bilden die Gliederungen einen Anknüpfungspunkt für die weitere
                      Kontextualisierung der Kosmos-Vorträge im übrigen Humboldt’schen Œuvre.
                      Beispielseise übernahm Humboldt für seinen monumentalen Kosmos (1845–62) im Wesentlichen die Anordnung
                      der Singakademie-Vorträge (vgl. Erdmann/Thomas 2014: 37), wodurch sich
                      die Vorträge nun auf Grundlage der aus den Nachschriften extrapolierten
                      Gliederung mit den entsprechenden -Ebenen der im DTA
                      verfügbaren XML-Volltexte des Kosmos verknüpfen
                      lassen. 
                  Ebenso automatisch wurde aus den annotierten Daten, die mit dem
                        TEI-Element  und einem @ref-Attribut mit Link auf
                        verfügbare Normdaten versehen wurden, ein übergreifendes
                        Personenverzeichnis extrahiert, das derzeit mehr als 2000 Einträge
                        enthält (siehe http://deutschestextarchiv.de/kosmos/person.
                        Durch die Verknüpfung mit Normdaten aus der GND, VIAF o.Ä.
                        werden die zwischen den Nachschriften teilweise erheblich voneinander
                        abweichenden Vorlageformen der von Humboldt erwähnten Personen
                        vereinheitlicht. Jeder Eintrag im Gesamtregister führt per Klick zum
                        Kontext derjenigen Nachschrift, in der der gewählte Personenname getaggt
                        wurde. Eine Verbindung des Personenregisters mit weiterführenden
                        Informationsangeboten z. B. über eine BEACON-Datei bietet NutzerInnen – ohne nennenswerten
                        Mehraufwand für das Projekt – direkten Zugang zu weiteren Informationen.
                        Neben seiner Funktion als übergreifende Orientierungshilfe eignet sich
                        das Personenregister auch als Impulsgeber für die Beforschung der
                        Nachschriften: Fehlt beispielsweise ein Personenname in einem Dokument,
                        der in einem anderen an der entsprechenden Stelle referenziert wurde,
                        erlaubt dies schon erste Rückschlüsse auf die Zuverlässigkeit und
                        Vollständigkeit der Nachschrift. 
                  Weitere dokumentübergreifende Ordnungshilfen listen beispielsweise die
                          von Humboldt im Zusammenhang mit naturwissenschaftlichen Untersuchungen
                          eingesetzten Instrumente (http://www.deutschestextarchiv.de/kosmos/instrument),
                          die von ihm erwähnten Himmelskörper sowie die
                          mineralischen und chemischen Elemente auf, die im Laufe der Vorträge
                          eine Rolle spielten. Diese und weitere ‚Inventarlisten‘ konnten direkt
                          aus den Volltexten extrahiert werden, dank der Verbindung mit der
                          computerlinguistischen Erschließung der Volltexte im Deutschen
                          Textarchiv. Die im DTA implementierte Kombination der linguistischen
                          Suchmaschine DDC mit dem Wortnetz GermaNet (Henrich/Hinrichs 2010;
                          Hamp/Feldweg 1997) ermöglicht es beispielsweise, alle Nachschriften
                          gezielt nach denjenigen Begriffen zu durchsuchen, die in GermaNet als
                          „Element“ bzw. übergeordnet als „Grundstoff; Urstoff“ klassifiziert
                          wurden (Abbildung 4). Eine umfassende Liste mit knapp 3000 Treffern und
                          mehr als 300 verschiedenen Mineralien, Gesteinen und Substanzen ist das
                          Ergebnis, das dem/der Nutzer/in unmittelbar zur zielgerichteten
                          Navigation angeboten werden kann.
                  
                     
                     
                        Abb. 4: DDC-Suche im Deutschen
                            Textarchiv nach Begriffen aus dem GermaNet-Synset „Grundstoff;
                            Urstoff“
                  
               
            
            
               Wechselwirkung zwischen Projektdesign und Produzentenperspektive
               Ebenfalls im Anschluss an die DHd-2015-Präsentation in Graz – und seitdem des
                          Öfteren wiederholt – wurde die manche/n Geisteswissenschaftler/in offenbar
                          beunruhigende Frage gestellt, worin denn noch die Aufgaben des Herausgebers
                          einer wissenschaftlichen (Manuskript-)Edition bestünden, wenn wie im Projekt
                          Hidden Kosmos grundständige Arbeiten wie das Transkribieren und Annotieren
                          (fast) aller Textzeugen an einen Dienstleister ausgelagert werden, wenn
                          bestehende Datenmodelle und Infrastrukturen einfach mit- oder nachgenutzt
                          und wenn immer mehr Arbeitsschritte automatisiert werden können.
               Auf der DHd 2016 möchte ich mit Bezug auf die oben skizzierten
                            Arbeitsschritte einige aus meiner Sicht notwendige Änderungen in der ‚Job
                            Description‘ des Editors bzw. allgemeiner: der/des
                            Geisteswissenschaftlerin/s in einem DH-Projekt anregen. Diese liegen m. E.
                            nach wie vor im traditionellen Bereich der hermeneutisch-interpretierenden
                            Quellenkritik, aber wesentlich mehr noch in den Bereichen der Datenkuration,
                            tieferen Annotation, der Analyse und ständigen Wiederausführung und
                            Optimierung automatisierter Prozesse. Denn selbstverständlich bleibt bei
                            jedem computergestützten Prozess der Datenanalyse ein
                            nicht-automatisierbarer ‚Rest‘, der interpretiert werden muss und der durch
                            weitere, wiederum computergestützte Optimierung der Datenbasis oder durch
                            eine Rekonfiguration des automatisierten Prozesses verringert oder zumindest
                            verändert werden kann. Dabei wird dieser Rest wohl auch durch die
                            Optimierung der Daten und die fortschreitende Verbesserung der Tools und
                            Services nicht verschwinden, möglicherweise aber immer interessanter
                            werden.
            
         
      
      
         
            Alle URLs in diesem Text abgerufen am 15. Oktober 2015.
            Humboldts eigenhändige Manuskripte liegen,
                            soweit sie sich überhaupt erhalten haben, verstreut und z. T. nicht
                            sicher identifizierbar in dessen Nachlass in der Staatsbibliothek zu
                            Berlin und der Jagiellonen-Bibliothek in Krakau. Bis voraussichtlich
                            Ende 2016 werden beide Nachlassteile komplett digitalisiert sein. In
                            einem die Hidden Kosmos-Idee weiterführenden Anschlussprojekt sollen
                            dann die ursprünglichen Vortragsmanuskripte mit Hilfe der
                            Hörernachschriften identifiziert und in die laufende Edition
                            integriert werden.
                          
            Für eine detailliertere Darstellung der
                            Kooperation zwischen Hidden Kosmos, DTA und CLARIN-D s. Thomas
                            2014/15.
                          
            Siehe auch dazu Sahle (2013: 107), der zu den
                            oben skizzierten Auswirkungen digitaler Editionsformen festhält:
                            „Die Visualität und Materialität der Überlieferung kann besser
                            sichtbar gemacht werden, die Aufforderung zur Konstruktion der einen autoritativen editorischen Fassung, die
                            alle anderen Fassungen nahezu unsichtbar macht, wird schwächer.“
                            (Hervorhebung CT)
                          
         
         
            
               Bibliographie
               
                  Berlin-Brandenburgische Akademie der Wissenschaften
                              (BBAW) (2007–16): DTA. Deutsches Textarchiv
                              http://www.deutschestextarchiv.de/ [letzter Zugriff 15. Oktober
                              2015].
               
                  CLARIN-D
                  http://clarin-d.net [letzter
                              Zugriff 15. Oktober 2015].
               
                  Erdmann, Dominik/Thomas, Christian (2014): „‚… zu
                              den wunderlichsten Schlangen der Gelehrsamkeit zusammengegliedert‛. Neue
                              Materialien zu den ‚Kosmos-Vorträgen‛ Alexander von Humboldts, nebst
                              Vorüberlegungen zu deren digitaler Edition". In: HiN –
                              Humboldt im Netz. Internationale Zeitschrift für Humboldt-Studien
                              (Potsdam – Berlin) XV, 28: 34–45. http://dx.doi.org/10.18443/189 [letzter Zugriff
                              27. Mai 2016]. 
               
                  Hamp, Birgit/Feldweg, Helmut (1997): “GermaNet – a
                              Lexical-Semantic Net for German.” In: Proceedings of the
                              ACL workshop Automatic Information Extraction and Building of Lexical
                              Semantic Resources for NLP Applications. Madrid: 9–15.
               
                  Henrich, Verena/Hinrichs, Erhard (2010): “GernEdiT
                              – The GermaNet Editing Tool”. In: Proceedings of the
                              Seventh Conference on International Language Resources and Evaluation
                              (LREC 2010). Valletta, Malta: 2228–2235. http://www.lrec-conf.org/proceedings/lrec2010/pdf/264_Paper.pdf
                              [letzter Zugriff 15. Oktober 2015]. 
               
                  Humboldt, Alexander von (1845): Kosmos. Entwurf einer physischen Weltbeschreibung, 1. Stuttgart u.
                              a.: Deutsches Textarchiv. http://www.deutschestextarchiv.de/humboldt_kosmos01_1845 [letzter
                              Zugriff 15. Oktober 2015]. 
               
                  Humboldt, Alexander von ([1803/04]): Tagebücher der Amerikanischen Reise: IX: Varia. Obs.
                              Astron. de Mexico a Guanaxuato, Torullo, Tiluca, Veracruz, Cuba. Voy. De la
                              Havana à Philadelphia. Geologie de Guanaxato, Volcans de Torullo et de
                              Toluca. Voyage de Veracruz à la Havana et de la Havana à Philadelphia.
                              Torulla. Berlin: Staatsbibliothek zu Berlin – Preußischer Kulturbesitz. http://resolver.staatsbibliothek-berlin.de/SBB0001527C00000000
                              [letzter Zugriff 15. Oktober 2015]. 
               
                  Lehrstuhl für Kulturtechniken und Wissensgeschichte
                              (2014–16): Hidden Kosmos. Reconstructing Alexander von
                              Humboldt‛s »Kosmos-Lectures«. https://www.culture.hu-berlin.de/de/forschung/projekte/hidden-kosmos/
                              [letzter Zugriff 15. Oktober 2015].
               
                  N.N. (1827): Die physikalische
                              Geographie von Herrn Alexander v. Humboldt, vorgetragen im Semestre
                              1827/28. Berlin: Deutsches Textarchiv. http://www.deutschestextarchiv.de/nn_oktavgfeo79_1828/7 [letzter
                              Zugriff 15. Oktober 2015]. 
               
                  Sahle, Patrick (2013): Digitale
                              Editionsformen. Zum Umgang mit der Überlieferung unter den
                              Bedingungen des Medienwandels. 2: Befunde, Theorie und Methodik.
                              Norderstedt: Books on Demand.
               
                  Thomas, Christian (2014/15): „Hidden Kosmos –
                              Humboldts ‚Kosmos-Vorträge‘ als Probe der Digital Humanities", in:
                              DHd-Jahrestagung 2015 „Von Daten zu Erkenntnissen: Digitale
                              Geisteswissenschaften als Mittler zwischen Information und Interpretation“,
                              23.-27.2.2015, Zentrum für Informationsmodellierung – Austrian Centre for
                              Digital Humanities an der Universität Graz. https://www.culture.hu-berlin.de/de/forschung/projekte/hidden-kosmos/media/c-thomas-dhd-graz-paper-hidden-kosmos-20150126.pdf
                              [letzter Zugriff 15. Oktober 2015]. 
               
                  Thomas, Christian (2015): „99 unselbständige
                              Schriften Humboldts als Volltext im Deutschen Textarchiv verfügbar", in: avhumboldt.de. Alexander von Humboldt Informationen
                              online. http://www.avhumboldt.de/?p=10922 [letzter Zugriff 15. Oktober
                              2015]. 
            
         
      
   



      
         
            Ansatz 
            Neben dem ›klassischen‹ strukturalistischen Paradigma, das sich wesentlich an
          Theoremen der Linguistik orientiert (u. a. Lotman 1972; Titzmann 1977), gibt es
          in der Literaturwissenschaft bereits seit Jahrzehnten Ansätze zu einer
          Strukturanalyse, die sich auf die empirische Soziologie – insbesondere auf die
          Social Network Analysis – bezieht und Struktur
          entsprechend nicht über basale semantische Relationen (etwa als Opposition oder
          Äquivalenz) definiert, sondern über soziale Interaktionen (Marcus 1973; Stiller
          et al. 2003; de Nooy 2006; Stiller / Hudson 2005; Elson et al. 2010; Agarwal et
          al. 2012). Im Kontext der Digital Humanities haben diese Ansätze zu einer
          literaturwissenschaftlichen Netzwerkanalyse (Trilcke 2013) in den letzten Jahren
          eine neue Dynamik gewonnen (Moretti 2011; Rydberg-Cox 2011; Park et al. 2013).
          Aus literaturwissenschaftlicher Sicht versprechen diese Analyseverfahren dabei
          auf umfangreichen Korpora basierende, von quantitativen Daten gestützte
          Erkenntnisse über die Literaturgeschichte wie auch über die generischen
          Eigenarten literarischer Texte. Im Projekt dlina. Digital
          Literary Network Analysis haben wir einen Workflow zur Extraktion,
          Analyse und Visualisierung von Netzwerkdaten aus dramatischen Texten mit
          rudimentärer TEI-Auszeichnung entwickelt (Fischer et al. 2015). Der hier
          projektierte Vortrag wird Ergebnisse der netzwerkanalytischen Auswertung dieser
          Daten präsentieren und vor dem Hintergrund etablierter fachwissenschaftlicher
          Fragestellungen diskutieren. 
         
         
            Datenerhebung und -analyse
            Unser derzeitiges Korpus umfasst 465 deutschsprachige Dramen (Zeitraum 1730 bis
            1930), die aus dem Textgrid
            Repository extrahiert wurden. Die für die Netzwerkanalyse relevanten
            Strukturdaten dieser Dramen (Segmentierung, Figurenidentifikation) wurden in
            einem regelbasierten Prozess händisch ediert, um OCR- und TEI-Tagging-Fehler zu
            beheben sowie solchen ›Eigenarten‹ der literarischen Texte zu begegnen, die die
            Analyseergebnisse verfälschen würden (u. a. unterschiedliche Bezeichnungen
            identischer Figuren; Bezeichnung von Figurengruppen mit unbestimmten Numeralien
            wie ›beide‹ oder ›alle‹; etc.). Die edierten Strukturdaten liegen in einem
            eigens entwickelten Datenformat, dem dlina-Format, in Form von XML-Dateien vor.
            Die Visualisierung der Netzwerke und die Berechnung netzwerkanalytischer Werte
            erfolgt – mittels Python- und D3-Skripten – automatisiert auf Basis der in den
            dlina-Dateien gespeicherten Strukturdaten. Neben Graphen und basalen Werten, die
            die Netzwerke global beschreiben (Network Size, Density, Average Degree, Average
            Path Length), werden dabei auch Zentralitätswerte für sämtliche Figuren eines
            Dramas erhoben (u. a. Degree, Average Distance, Closeness Centrality,
            Betweenness Centrality). Die Implemtierung weiterer Berechnungsrountinen (u. a.
            Clustering Coefficient, logarithmierte Degree Distribution-Tabellen) ist für den
            Winter 2015/16 vorgesehen. Sämtliche Daten und Visualisierungen werden frei
            verfügbar im Netz publiziert (https://github.com/dlina und https://dlina.github.io/linas/). 
         
         
            Literaturwissenschaftliche Auswertung 1: Dramengeschichte 
            Die diachrone Erstreckung unseres Dramenkorpus über ca. 200 Jahre deutscher
              Literaturgeschichte macht es möglich, größere Entwicklungen im Bereich der
              strukturellen Komposition von dramatischen Texten zu beobachten (erste
              Überlegungen dazu haben wir in einem Blogpost skizziert: https://dlina.github.io/200-Years-of-Literary-Network-Data/). Neben
              Werten, die sich auf die Gesamtnetzwerke der einzelnen Dramen beziehen (u. a.
              Network Size, Density, Average Degree; s. exemplarisch zur Average Path Length,
              Abbildung 1), werden dabei auch figurenbezogene Werte, v.a. Zentralitätsmaße,
              einbezogen, die Aufschluss etwa über die Streuung des Personals eines Dramas
              bzw. dessen Zusammensetzung aus ›zentralen‹ und weniger ›zentralen‹ Figuren
              gibt. Auf Grundlage dieser Werte sollen im Vortrag einige globale Thesen der
              Literaturgeschichte diskutiert werden. So werden wir erstens diskutieren, inwieweit sich anhand der netzwerkanalytischen
              Werte eine Ausdifferenzierung der strukturellen Komposition von dramatischen
              Texten am Ende des 18. Jahrhunderts beobachten lässt: Eine solche
              Ausdifferenzierung wäre angesichts des Nebeinanders von ›geschlossenen‹, in der
              Tradition der Französischen Klassik stehenden Dramen und ›offenen‹ Dramen, die
              sich u. a. an der Drramatik Shakespeares orientieren, zu erwarten. Zweitens werden wir einige geläufige
              literaturwissenschaftliche Periodisierungshypothesen testen (u. a. aus dem
              Strukturalismus und der Sozialgeschichte); gefragt werden soll hier, inwieweit
              die Entwicklung der netzwerkanalytischen Werte mit den von der Forschung
              vorgeschlagenen Periodisierungen korreliert. 
            
               
               
                  Abb. 1: Average Path Length (Mean; nach Dekaden) 
            
         
         
            Literaturwissenschaftliche Auswertung 2: Dramentypen
            Die von uns bisher erhobenen Werte zeigen, dass Dramen in dem untersuchten
                  Zeitraum auf sehr unterschiedliche Weise strukturiert wurden. In der
                  ›traditionellen‹ Literaturwissenschaft wurden für solche unterschiedlichen
                  ›Bauformen‹ diverse Typologien entwickelt, in der Germanistik am bekanntesten
                  ist Volker Klotz’ Unterscheidung in eine ›offene‹ und eine ›geschlossen‹
                  Dramenform (Klotz 1960). Diesen typologischen Impuls wollen wir aufgreifen und
                  einen Vorschlag unterbreiten, wie sich mittels netzwerkanalytischer Daten
                  bestimmte Typen der strukturellen Komposition von Dramen unterscheiden (und dann
                  wiederum historisch verorten) lassen. Unser Vorschlag greift dabei Überlegungen
                  aus der Forschung zu sog. Small-world-Netzwerken auf. Diese Forschungen setzen
                  bei der Beobachtung an, dass die Werte von empirisch erhobenen Netzwerken nicht
                  selten signifikant von entsprechenden Random-Netzwerken (also z. B. nach dem
                  Erdős-Rényi-Modell erstellten Graphen) abweichen. Abweichungen sind dabei
                  insbesondere beim Clustering Coefficient, bei der Averge Path Length sowie bei
                  der Degree Distribution zu beobachten (Albert / Barabási 2002). Für den hier
                  projektierten Vortrag werden wir diese Werte – sowie die Werte für die
                  entsprechenden Random-Netzwerke – für unser Gesamtkorpus erheben (sowie einen
                  Workflow für die automatisierte Erhebung entwickeln) und diskutieren. Erste
                  Testläufe deuten dabei darauf hin, dass sich auf diese Weise tatsächlich
                  unterschiedliche Typen der strukturellen Komposition von Dramen beschreiben
                  lassen könnten. So zeigen sich z. B. auffällige Unterschiede bei der Degree
                  Distribution (s. exemplarisch die Tabellen für vier Dramen in Abbildung 2); und
                  mit Blick auf den Clustering Coefficient zeigt sich, dass im Vergleich zu
                  Random-Netzwerken signifikant höhere Werte, wie sie bei Small-world-Netzwerken
                  zu erwarten sind, zwar in mehreren Fällen vorkommen, jedoch keineswegs für alle
                  Dramennetzwerke charakteristisch sind (siehe exemplarisch die Werte in Abbildung
                  3). Im Vortrag werden wir diese Werte für alle Dramen unseres Korpus
                  präsentieren; wir werden diskutieren, inwieweit sich hier – aufbauend auf dem
                  Small-world-Konzept – netzwerkanalytisch basierte Typen der strukturellen
                  Komposition von Dramen unterscheiden lassen und wir werden literarhistorisch
                  fundiert erörtern, welche Eigenschaften der Dramen für die unterschiedlichen
                  Werte verantwortlich sind.
            
               
            
            
               
            
            
               
            
            
               
               
                  Abb. 2.1 bis 2.4: Node Degree Distribution für »Der
                    sterbende Cato« (1731), »Emilia Galotti« (1772), »Götz von Berlichingen« (1773)
                    und »Die Räuber« (1781) 
            
            
               
               
                  Abb. 3: Vergleich des Clustering Coefficent des
                      Dramen-Netzwerks mit dem eines jeweils entsprechenden Random-Netzwerks 
            
         
      
      
         
            
               Bibliographie
               
                  Albert, Réka / Barabási, Albert-László (2002):
                      "Statistical mechanics of complex networks", in: Reviews
                      of Modern Physics 74: 47–97. 
               
                  Agarwal, Apoorv / Corvalan, Augusto / Jensen, Jacob /
                        Rambow, Owen (2012): "Social Network Analysis of Alice in Wonderland" in: Proceedings of the
                        Workshop on Computational Linguistics for Literature. Montréal
                        88–96. 
               
                  de Nooy, Wouter (2006): "Stories, Scripts, Roles, and
                        Networks" in: Structure and Dynamics 1, 2 http://escholarship.org/uc/item/8508h946#page-1 [letzter Zugriff
                        12. Oktober 2015]. 
               
                  Elson, David K. / Dames, Nicholas / McKeown, Kathleen R.
                        (2010): "Extracting Social Networks from Literary Fiction", in: Proceedings of the 48th Annual Meeting of the Association
                        for Computational Linguistics. Uppsala 138–147. 
               
                  Fischer, Frank / Kampkaspar, Dario / Göbel, Mathias /
                          Trilcke, Peer (2015): "Digital Network Analysis of Dramatic Texts",
                          in: DH 2015, Sydney, 2. Juli 2015
                  https://dlina.github.io/Our-Talk-at-DH2015/ [Skript] und https://dlina.github.io/presentations/2015-sydney/sydney.html#/
                            [Slides] [letzter Zugriff 12. Oktober 2015]. 
               
                  Klotz, Volker (1960): Geschlossene
                            und offene Form im Drama. München: Hanser. 
               
                  Lotman, Jurij M. (1972): Die
                            Struktur literarischer Texte. München: Wilhelm Fink. 
               
                  Marcus, Solomon (1973): Mathematische Poetik. Frankfurt am Main: Editura Academiei. 
               
                  Moretti, Franco (2011): "Network Theory, Plot
                            Analysis" in: Stanford Literary Lab Pamphlets 2 http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf [letzter
                            Zugriff 12. Oktober 2015]. 
               
                  Park, Gyeong-Mi / Kim, Sung-Hwan / Cho, Hwan-Gue
                              (2013): "Structural Analysis on Social Network Constructed from Characters
                              in Literature Texts", in: Journal of Computers 8, 9:
                              2442-2447 http://ojs.academypublisher.com/index.php/jcp/article/view/jcp080924422447/7672
                              [letzter Zugriff 12. Oktober 2015]. 
               
                  Rydberg-Cox, Jeff (2011): "Social Networks and the
                              Language of Greek Tragedy", in: Journal of the Chicago
                              Colloquium on Digital Humanities and Computer Science 1, 3 https://letterpress.uchicago.edu/index.php/jdhcs/article/view/86/91
                              [letzter Zugriff 12. Oktober 2015]. 
               
                  Stiller, James / Nettle, Daniel / Dunbar, Robin I. M.
                              (2003): "The Small World of Shakespeare's Plays", in: Human Nature 14: 397–408. 
               
                  Stiller, James / Hudson, Matthew (2005): "Weak Links
                              and Scene Cliques Within the Small World of Shakespeare", in: Journal of Cultural and Evolutionary Psychology 3:
                              57–73. 
               
                  TextGrid: TextGrid Repository
                  https://textgridrep.de [letzter
                              Zugriff 10. Februar 2016].
               
                  Titzmann, Michael (1977): Strukturale Textanalyse. Theorie und Praxis der Interpretation.
                              München: Wilhelm Fink. 
               
                  Trilcke, Peer (2013): "Social Network Analysis (SNA)
                              als Methode einer textempirischen Literaturwissenschaft", in: Ajouri, Philip
                              / Mellmann, Katja / Rauen, Christoph (eds.): Empirie in
                              der Literaturwissenschaft. Münster: mentis 201–247. 
            
         
      
   



      
         
            Fragen/Probleme für die Zukunft von Datenbanken musikalischer Quellen
            Die Entwicklung digitaler Medien und die daraus resultierenden Chancen für eine Weiterentwicklung computergestützter Verfahren zeitigt weitreichende Folgen auch für die musikwissenschaftliche Grundlagenforschung. Die philologisch arbeitenden Disziplinen profitieren ungemein von den Digitalisierungsvorhaben in Bibliotheken oder haben selbst an solchen Vorhaben ihren Anteil. Und sie haben starke gemeinsame Interessen: Neben Tools zur digitalen Edition sind dies vor allem Schreiber-Erkennung, Papier- und Wasserzeichenforschung sowie Provenienzrecherchen. Das Interesse, über Standards zu diskutieren und die Grundlagen für gemeinsame digitale Standards weiterzuentwickeln ist bei Editionsvorhaben genauso vorhanden wie bei Bibliotheken. Derzeit bereits gegebene Vernetzungsmöglichkeiten werden genutzt und sollten weiter ausgebaut werden; ein Beispiel dafür ist eine übergeordnete Forschungsinfrastruktur, wie sie etwa in Bezug auf Papierforschung das Wasserzeichen-Informationssystem und die Papierhistorischen Sammlungen der DNB zur Verfügung stellen. 
            Darüber hinaus werden in vielen musikwissenschaftlichen Forschungseinrichtungen seit Jahrzehnten Daten zu Komponisten und ihren Werken zusammengetragen, seit etwa 2000 erfolgt dies im deutschsprachigen Raum auch per Datenbanken. Vernetzungen dieser Daten sind dabei aber bislang die Ausnahme. Ein Austausch könnte also auch auf dieser Ebene intensiviert werden. 
            Ein weiterer Aspekt betrifft die in den letzten Jahren entwickelten Methoden der Auswertung strukturierter Daten. Auch wenn sie im Bereich der Musikwissenschaft quantitativ wohl noch nicht unter den Begriff „Big Data“ fallen, so stellen diese Daten einen Fundus dar, welcher mit Hilfe vieler, in verschiedenen Projekten unter dem Label Digital Humanities laufender Methoden einer Auswertung harrt. Voraussetzung dafür wäre allerdings eine stärkere Vernetzung.
            In der Bach-Forschung sind engmaschige Untersuchungen zur Überlieferung jedes einzelnen Musikwerks seit langem ein essentieller Bestandteil, denn viele Werke J. S. Bachs sind weder autograph überliefert noch genau zu datieren. Dies hat zur Folge, dass ein großer Teil der Untersuchungen von Bach-Quellen Handschriften des gesamten 18. und frühen 19. Jahrhunderts betreffen müssen. Sie stammen von oft unbekannten Schreibern mit unklarer Provenienz. Ihren Bezug zu verschollenen originalen Quellen zu ermitteln, ist damit seit den 1950er Jahren – angestoßen und betrieben durch die Arbeit an der Neuen Bach-Ausgabe – ein essentieller Bestandteil der Bach-Forschung. Diese hat sich so auf einigen Feldern zu einem Vorreiter in der paläographisch und philologisch orientierten Quellenforschung entwickelt. Die entsprechenden Erkenntnisse wurden in den Kritischen Berichten dieser Gesamtausgabe ausgewertet; mit Blick auf die gesamte Bach-Familie darüber hinaus in gedruckten Katalogen über einzelne Quellenbestände, vor allem in den Leipziger Beiträgen zur Bach-Forschung: Brüsseler Bibliotheken (1997), Singakademie zu Berlin (2005), Wien und ‚Alt-Österreich‘ (2011). Sowohl das Wissen als auch die Methoden wurden im Laufe der vergangenen Jahrzehnte ebenfalls für Forschungen zu anderen Komponisten nachgenutzt. Durch diese Impulse konnten wiederum auch für die Bach-Überlieferung Erkenntnisse gewonnen werden. So haben beispielsweise durch die Recherchen zu Berliner Überlieferungskreisen, namentlich der Singakademie, gerade auch die Gesamtausgaben zu den Söhnen Bachs sehr profitiert.
            Um die Fülle der auf viele Kritische Berichte und andere Publikationen verteilten
          Forschungsergebnisse strukturiert recherchierbar zu machen, wurde 1999 in
          Göttingen am dortigen Johann Sebastian Bach-Institut die Bach-Quellen-Datenbank
          erstellt (seit 2001 als bach.gwdg.de
          online, Blanken 2002), die 2010 in das Portal Bach digital integriert wurde, das
          nunmehr nicht allein Informationen zu den Werken und ihren Quellen bietet,
          sondern auch hochauflösende Digitalisate der Handschriften selbst. Seit einigen
          Jahren werden sukzessive auch Daten / Digitalisate zu den Werken weiterer
          Komponisten der Bach-Familie berücksichtigt (Alt-Bachisches Archiv, Carl Philipp
          Emanuel, Wilhelm Friedemann und Johann Christoph Friedrich Bach), so dass
          bach-digital.de mittlerweile eine Datenbank zur gesamten Bach-Familie ist, mit
          derzeit knapp 7800 Quellen-, 3500 Werk-Datensätzen sowie 1750 Digitalisaten.
          Durch die Zusammenschau von Quellen und die Möglichkeit des strukturierten
          Zugriffs auf die hierzu gehörenden Informationen wird eine immer neue
          Beschäftigung mit den Werken der Bach-Familie herausgefordert. 
            Bach digital versteht sich dabei als ein Work in Progress, das es täglich
            weiterzuentwickeln und mit neuen Inhalten zu befüllen gilt. Dafür werden
            Anregungen von Nutzern und auch die aktive Mitarbeit einzelner externer
            registrierter Nutzer in Anspruch genommen. Die Zugriffsstatistik zeigt, dass
            Bach digital auch international sehr gut angenommen wird. Derzeit wird daher
            mittels mehrsprachiger Datenvorhaltung (Teilübersetzungen in Englisch, Japanisch
            und Französisch) gerade die internationale Ausrichtung gestärkt. Über die
            Bestände der drei derzeitigen Kooperationspartner Bach-Archiv Leipzig,
            Staatsbibliothek zu Berlin – Preußischer Kulturbesitz, Sächsische
            Landesbibliothek – Staats- und Universitätsbibliothek Dresden und das
            Rechenzentrum der Universität Leipzig hinaus ist es nun das Ziel, die Zahl der
            Bibliothekspartner zu erhöhen, um den Zugang zu den weltweit verstreuten Quellen
            zu erleichtern. Neben etlichen kleineren Sammlungen in Deutschland sind dies u.
            a. auch die British Library und die Library of Congress, die zugesagt haben,
            digitalisierte Bach-Quellen über Bach digital zur allgemeinen Verfügung zu
            stellen. 
            Die seit 16 Jahren ununterbrochene und tägliche Arbeit an einer open
              source-basierten Datenbank (Blanken et al. 2015) und ihre technische wie
              inhaltliche Weiterentwicklung sind nun an einem Punkt, da richtungweisende
              Entscheidungen zum Ausbau, aber auch zur Vernetzung mit anderen Projekten
              anstehen. Der Grundbestand der Daten von bach-digital.de ist jederzeit für
              andere Projekte nachnutzbar. Diese ganz oder teilweise erfolgende Überführung
              von Daten in andere Datenbanken hat Konsequenzen, über die grundsätzlich zu
              sprechen ist. 
            Hier nun sollen Erfahrungen, Perspektiven und Wünsche anderer und eventuell vergleichbarer Datenbanken oder Digitalisierungsprojekte einbezogen werden.
            Fragen / Probleme für die Zukunft von Datenbanken musikalischer
                Quellen 
         
         
            Inhaltliche Fragestellungen
            
               Potential von Provenienz-Recherchen
               Ausschöpfung des Potenzials der Gemeinsamen Normdatei (GND) für die
               Provenienz-Forschung (Dokumentation historischer Musiksammlungen, Digitalisierung von Besitz- oder Auktionskatalogen, Geo-Referenzierung etc.
               Vernetzung mit anderen Projekten, externe Nutzung dieser Daten)
            
            
               Schreiberforschung
               
                  Entwicklung von Standards für Schreiber-Nomenklaturen (Leitfragen: Wie sollten Beispiel-Sammlungen von Schriftproben strukturiert sein? Wie lassen sich gemeinsame Schreiber-Portale aufbauen?)
                  Vernetzung von bereits vorhandenen Schriftproben-Datenbanken
                  Automatische Schreiberhanderkennung (Leitfragen: Wo stehen wir in der Musikpaläographie? Was lässt sich von außermusikalischen Projekten lernen? Gibt es überhaupt Bedarf, wissenschaftliche Anstrengungen zu einer automatischen Schreibererkennung mithilfe der Informatik zu unternehmen?)
               
            
            
               Wasserzeichen/Papier-Forschung
               
                  Gemeinsamer sukzessiver Ausbau von Wasserzeichen-Recherche-Portalen:
                      Wasserzeichen-Informationssystem (WZIS), Bernstein / Memory of Paper
                      (WZ-Pause versus Aufnahmen mit moderner Kameratechnik, z. B. mittels
                      Thermographie)
               
            
         
         
            Technische Fragestellungen
            
               Vernetzung
               
                  Nutzerfreundliche Anbindung externer Angebote mit zusätzlichen Informationen (Crosslinking) durch den breiten Einsatz von Normdaten und dem BEACON-Format
               
            
            
               Austauschformate 
               
                  Bereitstellung von Forschungsdaten in standardisierten und etablierten Formaten zur einfachen Weiterverwendung und automatisierten Auswertung
                  Verwendung freier Lizenzen für wissenschaftskonforme Nachnutzung (Prinzipien guter wissenschaftlicher Praxis)
               
            
            
               Notenincipits und Libretti
               
                  Nutzen und Potenzial von TEI und MEI in Musiker-Datenbanken 
               
            
            
               Tabellen oder Ontologien? 
               
                  Datenmodelierung zwischen Standardisierung und individuellen sowie praktischen Bedürfnissen
               
            
            
               Dokumentation und Lizenzen
               
                  Offenlegung von Struktur und Inhalt zur langfristigen Verfügbarmachung
               
            
         
         
            Wissenschaftskommunikation
            
               Stringenter Ausbau einer Common Science-Plattform; Installierung eines Redaktionsteams
               Nutzerfreundliche Weiterentwicklung von bisher primär wissenschaftlich
                      orientierten Plattformen, Öffentlichkeitswirksamkeit (unter Einbeziehung
                      weiterer digitaler Medien: Audio / Video / Editionen)
            
         
         
            Datenqualität 
            
               RISM-OPAC (Chancen und Probleme bei Datenübernahmen aus Komponisten-basierten Datenbanken)
               Konsistenz von Daten (innerhalb eines Projekts und projektübergreifend) 
               Identifizierung und Auffindbarkeit durch Verwendung von Normdaten
            
         
      
      
         
            
               Bibliographie
               
                  Betz, Florian (2016): "Papiermacher und
                      Papiermühlen in der Gemeinsamen Normdatei (GND). Das Normdaten-Projekt
                      'Papiermacherkatalog' des Deutschen Buch- und Schriftmuseums der Deutschen
                      Nationalbibliothek", in: Eckhardt, Wolfgang / Neumann, Julia / Schwinger,
                      Tobias / Staub, Alexander (eds.): Wasserzeichen –
                      Schreiber – Provenienzen. Neue Methoden der Erforschung und
                      Erschließung von Kulturgut im digitalen Zeitalter: Zwischen
                      wissenschaftlicher Spezialdisziplin und 'Catalog Enrichment' (= Zeitschrift
                      für Bibliothekswesen und Bibliographie Sonderband 118). Frankfurt am Main:
                      Vittorio Klostermann 243-254
               
                  Blanken, Christine (2002): Göttinger Bach-Katalog
                  http://www.bach.gwdg.de/
               
               
                  Blanken, Christine (2016): "Die
                      Komponisten-Datenbank 'Bach digital'. Erfahrungen und Perspektiven abseits
                      einer Präsentation von Digitalisaten", in: Eckhardt, Wolfgang / Neumann,
                      Julia / Schwinger, Tobias / Staub, Alexander (eds.): Wasserzeichen – Schreiber – Provenienzen. Neue Methoden der
                      Erforschung und Erschließung von Kulturgut im digitalen Zeitalter: Zwischen
                      wissenschaftlicher Spezialdisziplin und 'Catalog Enrichment' (= Zeitschrift
                      für Bibliothekswesen und Bibliographie Sonderband 118). Frankfurt am Main:
                      Vittorio Klostermann 135-148.
               
                  Blanken, Christine / Rettinghaus, Klaus /
                        Hausmann, Christiane / Kupferschmidt, Jens / Freitag, Stefan
                        (2015): Bach digital. Dokumentation: Umsetzung des
                        Projektes auf Basis der Content Management Anwendung des
                        MyCoRe-Arbeitskreises. http://www.bach-digital.de/docs/BachDigital_Doku.pdf?XSL.lastPage.SESSION=/docs/BachDigital_Doku.pdf.
               
                  Eckhardt, Wolfgang (2016): "Digitale
                        Dokumentation von Wasserzeichen in Musikhandschriften im Rahmen des Projekts
                        KoFIM", in: Eckhardt, Wolfgang / Neumann, Julia / Schwinger, Tobias / Staub,
                        Alexander (eds.): Wasserzeichen – Schreiber –
                        Provenienzen. Neue Methoden der Erforschung und Erschließung von
                        Kulturgut im digitalen Zeitalter: Zwischen wissenschaftlicher
                        Spezialdisziplin und 'Catalog Enrichment' (= Zeitschrift für
                        Bibliothekswesen und Bibliographie Sonderband 118). Frankfurt am Main:
                        Vittorio Klostermann 167-196
               
                  Mühlberger, Günter (o. J.): Die
                        automatisierte Volltexterkennung historischer Handschriften als gemeinsame
                        Aufgabe von Archiven, Geistes- und Computerwissenschaftlern. Das Modell
                        einer zentralen Transkriptionsplattform als virtuelle Forschungsumgebung
                        https://www.academia.edu/7451967/Die_automatisierte_Volltexterkennung _historischer_Handschriften_als_gemeinsame _Aufgabe_ von_Archiven_Geistes-_und _Computerwissenschaftlern._Das_Modell_einer _zentralen_Transkriptionsplattform_als_ virtuelle_ Forschungsumgebung
               
               
                  Rettinghaus, Klaus (2014): "Bringing
                          together Bach and MEI – Future prospects for Bach digital", Vortrag bei der
                          “Music Encoding Conference” 2014, Charlottesville, Virginia / USA:
                          University og Virginia.
               
                  Stadler, Peter (2016): "Zum Einsatz von
                          Normdaten bei der Carl-Maria-von-Weber-Gesamtausgabe", in: Eckhardt,
                          Wolfgang / Neumann, Julia / Schwinger, Tobias / Staub, Alexander (eds.): Wasserzeichen – Schreiber – Provenienzen. Neue
                          Methoden der Erforschung und Erschließung von Kulturgut im digitalen
                          Zeitalter: Zwischen wissenschaftlicher Spezialdisziplin und 'Catalog
                          Enrichment' (= Zeitschrift für Bibliothekswesen und Bibliographie Sonderband
                          118). Frankfurt am Main: Vittorio Klostermann 19-26.
               
                  Transkribus. Universität Innsbruck https://transkribus.eu/.
               
                  Wenger, Emanuel (2016): "Metasuche in
                          Wasserzeichendatenbanken (Bernstein-Projekt): Herausforderungen für die
                          Zusammenführung heterogener Wasserzeichen-Metadaten", in: Eckhardt, Wolfgang
                          / Neumann, Julia / Schwinger, Tobias / Staub, Alexander (eds.): Wasserzeichen – Schreiber – Provenienzen. Neue
                          Methoden der Erforschung und Erschließung von Kulturgut im digitalen
                          Zeitalter: Zwischen wissenschaftlicher Spezialdisziplin und 'Catalog
                          Enrichment' (= Zeitschrift für Bibliothekswesen und Bibliographie Sonderband
                          118). Frankfurt am Main: Vittorio Klostermann 289-295
            
         
      
   



      
         
            Ziele des Workshops
            Der Workshop setzt sich das Ziel, die Möglichkeiten, Aufgaben und
          Herausforderungen bei der Wiederverwendung von historischen Korpora zu
          identifizieren und zu diskutieren. Insbesondere sollen dabei deren Architektur,
          Dokumentation, Veröffentlichung und Speicherung betrachtet werden. So wollen wir
          versuchen, Methoden und Strategien für das interdisziplinäre Forschungsparadigma
          der Digital Humanities zu entwickeln und diese in den Fragestellungen der
          Konferenz der DHd 2016 zu verorten. Der Fokus wird auf die spezifischen und
          fächerübergreifenden Anforderungen historischer Texte in Bezug auf deren
          Aufbereitung und Speicherung in Repositorien zum Zweck der Wiederverwendung
          gelegt. Damit richtet sich der Workshop gleichermaßen an Korpuserstellende,
          Entwickelnde und an Betreiber_innen von Repositorien und deren Nutzer_innen.
         
         
            Forschungsfragen und Kontextualisierung des Workshops
            Historische Texte bilden den Forschungsgegenstand verschiedener
            geisteswissenschaftlicher Fächer wie der Linguistik, der Geschichtswissenschaft,
            der Literaturwissenschaft und vieler weiterer. Jede Disziplin hat dabei ihre
            eigenen Forschungsfragen und Arbeitsweisen, die sich in beispielsweise den
            genutzten Formaten und Annotationsweisen zeigen, wie beispielsweise die TEI
            Guidelines und deren TEI-XML-Format (TEI Consortium 2015) für digitale Editionen
            oder das Stand-Off-Format PAULA (Dipper 2005) für linguistische Korpora. Dennoch
            gibt es Ähnlichkeiten bei der Textauswahl und -aufbereitung, die eine gemeinsame
            Nutzung der vorhandenen Daten sinnvoll erscheinen lassen. In vielen Fällen wird
            zwischen den digitalisierten historischen Texten – den Primärdaten – und den
            hinzugefügten Interpretationen in Form von Metadaten und Annotationen
            unterschieden. Diese Begriffe – Primärdatum, Annotation, Metadatum – werden
            sowohl fachübergreifend aber auch innerhalb einer Disziplin oft sehr
            unterschiedlich genutzt. 1
           Dadurch entsteht der Eindruck, dass die Daten der Disziplinen
          grundverschieden sind. Da sich die oft unterschiedlichen Korpusarchitekturen
          jedoch auf den gleichen Forschungsgegenstand, nämlich den gleichen Text,
          beziehen können, gibt es durchaus große Schnittmengen zwischen den verschiedenen
          Disziplinen. 
            Ein Beispiel für eine ähnliche Textauswahl sind historische Zeitungen, auf deren
            Grundlage ganz unterschiedliche Fragestellungen adressiert werden (für einen
            kleinen Überblick siehe bspw. Burr et al. 2015). Die korpusbasierten
            Aufbereitungsarten reichen bei diesem Register beispielsweise von digitalen
            Editionen (z. B. „Korpus romanischer Zeitungssprachen“, Burr 1994-2007), über
            registerspezifische Referenzkorpora (z. B. „German
            Manchester Corpus“, Bennett et al. 2008) bis hin zu syntaktisch tief
            annotierten Korpora (z. B. Mercurius
            Baumbank, Demske 2003-2005). Ein weiteres Beispiel sind historische
            Briefe, die als digitale Edition literaturwissenschaftlich untersucht werden (z.
            B. „Briefe und Texte aus dem intellektuellen Berlin um 1800“, Baillot /
            Seiffert 2013b), oder die als linguistisch annotierte Korpora zur
            Untersuchung von historischen Sprachständen dienen (z. B. „Kasseler
            Junktionskorpus“, Ágel / Hennig 2007-2009).
            Die Beantwortung der jeweiligen Forschungsfrage stützt sich dann häufig auf
              Interpretationen in Form von Annotationen in einem Korpus, deren Formen sich
              disziplinübergreifend ähneln können. Dennoch existieren vielfältige manuell zu
              erstellende oder automatisch generierbare Annotationsarten wie zum Beispiel
              Named-Entity-Recognition, Referenzierung auf Personendatenbanken (wie z. B. die
              Gemeinsame Normdatei 2
            ), GEO Tagging (vgl. z. B. Elliot / Gillies 2009), Lemmatisierung (z. B.
            Schmid 1994) oder syntaktisches Parsing (z. B. Malt Parser
               3
          ), die die interpretatorischen Analysegrundlagen stellen. Da die
          Digitalisierung der historischen Texte (auf Grundlage von Handschriften, Drucken
          oder Editionen) und deren Annotation aufwändig und vielschichtig sind (vgl. u.
          a. Rissanen 2008, Kytö / Pahta 2012) kann die Wiederverwendung von historischen
          Korpora von Vorteil sein. Die Vorstellung der verschieden disziplinspezifischen
          Sicht- und Zugriffweisen auf solche textbasierten Daten (vgl. Pitti 2004) und
          deren Wiederverwendung ist ein zentraler Themenbereich des Workshops. 
            Damit diese heterogenen historischen Korpora von unterschiedlichen Disziplinen
            genutzt und wiederverwendet werden können, müssen sie über eine gemeinsame
            Plattform zugreifbar sein. Diese Plattform muss das Durchsuchen der Daten sowie
            der Metadaten, ggf. das Evaluieren sowie das Anreichern mit weiteren
            Annotationen und erneute Hochladen der Daten ermöglichen. Idealerweise können
            Repositorien diese Funktion übernehmen. Sie funktionieren dann wie eine Art
            Marktplatz, auf dem historische Korpora fachübergreifend ausgetauscht und mit
            Informationen angereichert werden können.
            Der Workshop nimmt diesen Startpunkt, dessen Voraussetzungen und Konsequenzen zum Thema und begreift ihn als einen Teilbeitrag zu einem Fragenkomplex der DHd-Konferenz:
            
               
                  „Was sind die Daten der Geisteswissenschaften? Wie
                  müssen die Daten der Geisteswis­senschaften (digitalisierte bzw.
                  digitale Texte, Bilder, Musik, Audio, Filme / Videos etc.) aufgearbeitet
                  und vorgehalten werden, um sie über die Fächer hinweg nicht nur für
                  unterschiedliche, sondern auch derzeit noch unbekannte Fragestellungen
                  nutzen zu können?“
                  4
               
            
            Der Workshop versucht für historische Korpora zu ergründen, wie und welche
                Wiederverwendungsszenarien unter welchen Voraussetzungen möglich sind und was
                der aktuelle Stand der Forschung ist. Dabei ist es enorm wichtig, dieses Thema
                vielschichtig und aus mehreren Perspektiven zu beleuchten. Fallstudien für die
                Wiederverwendung historischer Daten können exemplarisch Erfahrungen,
                Herausforderungen und Aufgaben thematisieren. Anhand von Korpusarchitekturen,
                die die Wiederverwendung unterstützen, können wichtige Konzepte und Modelle
                diskutiert und verglichen werden. Die Beschreibung von konkreten Technologien
                für die Umsetzung eines Repositoriums erlaubt es, die theoretischen Datenmodelle
                auf ihre Praxistauglichkeit zu untersuchen. Die Nutzer dieser Technologien
                tragen durch ihre Erfahrungen über die potentiellen Vorteile der Wiedernutzung
                und die Bereiche, in denen sie Sinn machen, maßgeblich zur Diskussion bei.
            Damit stellen sich folgende Fragen in Bezug auf die historischen Korpora, deren Aufbereitung, die Repositorien bzw. Technologien und deren Nutzung:
            
               Können dieselben Primärdaten unter verschiedenen Forschungsfragen unterschiedlich genutzt werden?
               Welche Gemeinsamkeiten, welche Unterschiede weisen die Korpora hinsichtlich ihrer umfangreichen Aufbereitung historischer Texte auf.
               In wie weit fördern / erschweren die Annotationen als theoretische Konzepte und
                  Interpretationen eine Wiederverwendung?
               Welche Arten von Annotationen und Analysen können wie wiederwendet werden?
               Welche Arten der Wiederverwendung können sich ergeben?
               Wie unterschiedlich bewerten Disziplinen die Qualität eines Korpus?
               Welche interdisziplinären Nutzer- und Nutzungsszenarien ergeben sich?
               Welche Anforderungen ergeben sich hinsichtlich der Korpusarchitektur inklusive Annotationsarten und Format?
               Welche Speicherformate eignen sich für die Wiederverwendung von Forschungsdaten?
               Wie können Lizenzen den Austausch und die Wiederverwendung fördern?
               Was sind die relevanten Metadaten über ein Korpus?
               Welche Art von Zugriff auf die Korpora ist notwendig, um eine Wiederverwendung zu erleichtern? Wie müssen Repositorien beschaffen sein?
               Welche Vor- und Nachteile besitzen disziplinspezifische / interdisziplinäre
                    oder / und formatabhängige oder -unabhängige Repositorien?
               Eine Diskussion und mögliche Beantwortung dieser Fragen wollen wir durch einen fächerübergreifenden Austausch von Entwicklern, Korpuserstellern und Nutzern im Rahmen des Workshops ermöglichen.
            
         
         
            Form des Workshops
            Der Workshop soll bestehend aus zwei impulsgebenden Keynotes und sechs Vorträgen
                      an einem Tag vor der DHd-Konferenz stattfinden. Eine Keynote wird das Thema des
                      interdisziplinären Zugangs und der Wiederverwendung zu historischen Daten
                      allgemein thematisieren und problematisieren (Lüdeling und Dreyer, Projektleiter
                      des LAUDATIO-Repositoriums für historische Texte. 
            Eine zweite Keynote wird die Frage nach einem Qualitätsmanagement im
                        Rahmen von Wiederverwendungsszenarien, dessen Umfang und Zweck aufwerfen und
                        diskutieren (Laurent Romary, DARIAH Director. 
            Die Vorträge, die aus dem offenen Call des Workshops hervorgehen,
                          sollen Korpuserstellende, Repositorienentwickler_innen und -nutzer_innen aus
                          ganz verschiedenen Fachbereichen die Gelegenheit geben, die oben aufgeworfenen
                          Fragen aufzunehmen und aus einer notwendigerweise interdisziplinären Sicht die
                          Möglichkeiten, Herausforderungen und Lösungen für die Wiederverwendung von
                          historischen Korpora zu diskutieren. Die Keynotes erhalten je 30 Minuten
                          Redezeit sowie 15 Minuten Diskussion und die Vorträge je 20 Minuten und 10
                          Minuten Diskussion. Eine Diskussion wird den Workshop abschließen. Die primäre
                          Sprache des Workshops ist Deutsch. 
         
      
      
         
            Zur Diskussion über Primärdatum, Transkriptionen, Normalisierungen siehe
                            bspw. Claridge 2008, Himmelmann 2012, Kramer 2014; über Metadaten siehe
                            bspw. Odebrecht 2015, Haynes 2004; über Annotationen siehe bspw.
                            Lüdeling 2011, Kübler / Zinsmeister 2015).
            „Die Gemeinsame Normdatei (GND) ist eine Normdatei für Personen,
                            Körperschaften, Konferenzen, Geografika, Sachschlagwörter und Werktitel,
                            die vor allem zur Katalogisierung von Literatur in Bibliotheken dient,
                            zunehmend aber auch von Archiven, Museen, Projekten und in
                            Webanwendungen genutzt wird.“ (DNB 2015). 
            Tool zum automatischen Annotieren von syntaktischen Dependenzen (Hall et
                              al. 2014).
            Siehe den Call for Papers DHd 2016 http://www.dhd2016.de/node/9 [letzter Zugriff:
                                12.September 2015]. 
         
         
            
               Bibliographie
               
                  Ágel, Vilmos / Hennig, Mathilde (2007-2009): KAJUK (Version 1.1). Justus-Liebig-Universität Gießen
                                  http://www.uni-giessen.de/kajuk/index.htm, http://hdl.handle.net/11022/0000-0000-2102-8 [letzter Zugriff 10.
                                    September 2015].
               
                  Baillot, Anne / Seifert, Sabine (2013a): „The Project
                                      "Berlin Intellectuals 1800–1830" between Research and Teaching“ in:  Journal of the Text Encoding Initiative 4. DOI:
                                      10.4000/jtei.707.
               
                  Baillot, Anne / Seifert, Sabine (2013b): Briefe und Texte aus dem intellektuellen Berlin um
                                      1800
                  http://tei.ibi.hu-berlin.de/berliner-intellektuelle/ [letzter
                                        Zugriff: 22.Dezember 2015].
               
                  Bennett, Paul / Durrell, Martin / Ensslin, Astrid /
                                            Scheible, Silke / Whitt, Richard (2008): GerManC
                                            Project (Version 1.0), University of Manchester http://www.llc.manchester.ac.uk/research/projects/germanc/, http://hdl.handle.net/11022/0000-0000-2D1B-1 [letzter Zugriff
                                            10.September 2015].
               
                  Burr, Elisabeth (1994-2007): Korpus
                                            Romanischer Zeitungssprachen. Duisburg - Bremen - Leipzig http://www.uni-leipzig.de/~burr/CorpusLing/ [letzter Zugriff 10.
                                            Sepzember 2015].
               
                  Burr, Elisabeth / Burkhardt, Julia / Potapenko, Elena /
                                                Sierig, Rebecca / Concepción Durán, Arámis (2015): „Das
                                                Duisburg-Leipzig Korpus romanischer Zeitungssprachen und sein Textmodell“,
                                                in: Von Daten zu Erkenntnissen. 2. Jahrestagung des
                                                Verbandes der Digital Humanities im deutschsprachigen Raum, DHd
                                                2015, Graz. https://www.conftool.pro/dhd2015/index.php/Burr-Das_Duisburg-Leipzig_Korpus_ romanischer_Zeitungssprachen-1771016.pdf?page=downloadPaper&filename=Burr-Das_Duisburg-Leipzig_Korpus_romanischer_Zeitungssprachen -1771016. pdf&form_id=177
                                                [letzter Zugriff: 11.September 2015]. 
               
                  Claridge, Claudia (2008): “Historical Corpora” in:
                                                  Lüdeling, Anke / Kytö, Merja (eds.): Corpus Linguistics.
                                                  An International Handbook. Volume 1. Berlin: De Gruyter 242–259. 
               
                  Demske, Ulrike (2003-2005): Mercurius (Version 1.1). Universität Potsdam http://www.uni-potsdam.de/guvdds/projekte/abgproj.html, http://hdl.handle.net/11022/0000-0000-467D-6 [letzter Zugriff:
                                                    10.September 2015]. 
               
                  Dipper, Stefanie (2005): “XML-Based Stand-Off
                                                      Representation and Exploitation of Multi-Level Linguistic Annotation”, in:
                                                      Proceedings of Berliner XML Tage Berlin 39-50. 
               
                  DNB = Deutsche Nationalbiliothek (2015): Gemeinsame Normdatei (GND)
                  http://www.dnb.de/gnd [letzter
                                                      Zugriff 10.September 2015].
               
                  Elliott, Tom / Gillies, Sean (2009): “Digital Geography
                                                        and Classics. Changing the Center of Gravity”, in: Digital
                                                        Humanities Quarterly 3, 1  
                  
                     http://www.digitalhumanities.org/dhq/vol/3/1/000031/000031.html
                   [letzter Zugriff 10. September 2015]. 
               
                  Hall, Johan / Nilsson, Jens / Nivre, Joakim (2014):
                                                          MaltParser
                  http://www.maltparser.org/
                                                          [letzter Zugriff 10. September 2015]. 
               
                  Haynes, David (2004): Metadata for
                                                            information management and retrieval. London: Facet publishing. 
               
                  Himmelmann, Nikolaus. P. (2012): “Linguistic Data Types
                                                              and the Interface between Language Documentation and Description”, in: Language Documentation and Conservation 6: 187-207. 
               
                  Kramer, Michael J. (2014): “Defining Data for
                                                                Humanists: Text, Artifact, Information or Evidence?”, in: Journal for Digital Humanities 3, 2  
                  
                     http://journalofdigitalhumanities.org/1-1/defining-data-for-humanists-by-trevor-owens/
                   [letzter Zugriff: 10. September 2015]. 
               
                  Kübler, Sandra / Zinsmeister, Heike (2015): Corpus Linguistics and Linguistically Annotated
                                                                    Corpora. London / New York: Bloomsbury. 
               
                  Kytö, Merja / Pahta, Päivi (2012): “Evidence from
                                                                      historical corpora up to the twentieth century” in: Nevalainen, Terttu /
                                                                      Traugott, Elizabeth C. (eds.): The Oxford Handbook of the
                                                                      History of English. Oxford o.a.: Oxford University Press 123-133. 
               
                  Lüdeling, Anke (2011): “Corpora in Linguistics:
                                                                        Sampling and Annotation” in: Grandin, Karl (ed.): Going
                                                                        Digital. Evolutionary and Revolutionary Aspects of Digitization.
                                                                        Nobel Symposium 147. New York: Science History Publications 220-243. 
               
                  Odebrecht, Carolin (2015): „Interdisziplinäre Nutzung
                                                                          von Forschungsdaten mithilfe einer technisch-abstrakten Modellierung“, in:
                                                                          Von Daten zu Erkenntnissen. 2. Jahrestagung des
                                                                            Verbandes der Digital Humanities im deutschsprachigen Raum DHd
                                                                            2015, Graz. https://www.conftool.pro/dhd2015/index.php/Odebrecht-Interdisziplin%C3%A4re_Nutzung_von_Forschungsdaten _mithilfe_einer_technisch-abstrakten_Modellierung-63110.pdf?page=downloadPaper&filename=Odebrecht-Interdisziplin%C3%A4re_Nutzung_von_Forschungsdaten _mithilfe_einer_technisch-abstrakten_Modellierung-63110.pdf &form_id=63
                                                                            [letzter Zugriff 12.September 2015]. 
               
                  Pitti, Daniel V. (2004): “Designing Sustainable
                                                                              Projects and Publications” in: Schreibman, Susan / Siemens, Ray / Unsworth,
                                                                              John (eds.): A Companion to Digital Humanities.
                                                                              Oxford: Blackwell 471–487. 
               
                  Rissanen, Matti (2008): “Corpus Linguistics and
                                                                                Historical Linguistics” in: Lüdeling, Anke / Kytö, Merja (eds.): Corpus Linguistics. An International Handbook. Volume
                                                                                1. Berlin: Mouton de Gruyter 53-68. 
               
                  Schmid, Helmut (1994): “Probabilistic Part-of-Speech
                                                                                  Tagging Using Decision Trees“, in: Proceedings of
                                                                                  International Conference on New Methods in Language Processing,
                                                                                  1994. Manchester. 
               
                  TEI Consortium (eds.) (2015): TEI P5:
                                                                                    Guidelines for Electronic Text Encoding and Interchange. Version 2.8.0.
                                                                                    2015-04-06. TEI Consortium. http://www.tei-c.org/Guidelines/P5/ [letzter Zugriff 11. August
                                                                                    2015].
            
         
      
   



      
         
            Beschreibung
             Dieser Workshop widmet sich der Textannotation und der Textanalyse mit der
          web-basierten Annotationsplattform CATMA (Computer Aided Text Markup and
          Analysis) (Meister et al. 2015), welche seit 2008 an der Universität
          Hamburg entwickelt wird. Die Bedarfe der Modellierung und Operationalisierung
          geisteswissenschaftlicher Konzepte und die Anwendung dieser Modelle auf
          Textdaten stand bei der Entwicklung von CATMA im Fokus. CATMA ist Open Source
          und außerdem XML / TEI-kompatibel, dadurch ist die Nachnutzbarkeit der mit CATMA
          erstellten Annotationen und Analyseergebnisse sichergestellt. Der Workshop wird
          neben einer Einführung in die Nutzung der Plattform für manuelles Annotieren auf
          zwei weitere – gerade bei der Umsetzung größerer Annotationsprojekte wesentliche
          – Aspekte genauer eingehen: Kollaboration und Automatisierung. 
            Im Workshop wird gezeigt werden, welche Funktionalitäten in CATMA für
            kollaboratives Arbeiten zur Verfügung stehen und wie das kollaborative Arbeiten
            unter den erschwerten Bedingungen der literaturwissenschaftlichen Praxis, wie z.
            B. der Polyvalenz literarischer Texte, möglich ist. Beim automatischen Erstellen
            von Annotationen hingegen spielen die kürzlich im Rahmen des heureCLÉA-Projektes
            1 in CATMA integrierten
            Möglichkeiten eine zentrale Rolle. 
            Ziel von heureCLÉA ist die Bereitstellung einer digitalen Heuristik zur
              Annotation von einfachen bis komplexen Konzepten der Narratologie. Unter einer
              digitalen Heuristik verstehen wir ein Werkzeug zur automatischen und
              semiautomatischen Annotation. Das heureCLÉA-Projekt konzentriert sich hierfür
              auf die Analyse temporaler Phänomene. Auf einer einfachen, an der Textoberfläche
              orientierten Ebene handelt es sich hierbei unter anderem um die Erkennung von
              Zeitausdrücken in literarischen Texten, auf einer komplexeren, metatextuellen
              Ebene beispielsweise um die Erkennung von Phänomenen der zeitlichen Ordnung wie
              Prolepse und Analepse (Gius / Jacke 2014). Dafür entwickelten wir einen auf
              manuellen und automatischen Annotationen basierten Ansatz, in dem die
              regelbasierte Extraktion und Normalisierung von Zeitausdrücken als Ausgangspunkt
              für Machine Learning Verfahren verwendet wurde (Bögel et al. 2015). Eine ganz
              wesentliche Komponente dieses Ansatzes ist das an der Universität Heidelberg
              entwickelte System HeidelTime (Strötgen / Gertz 2013). Sowohl die automatische
              Annotation von Zeitausdrücken, als auch linguistischer Oberflächenphänomene
              (Wortarten und Satzgrenzen), sowie Tempusannotationen sind bereits in CATMA
              integriert und können mit manuellen Annotationen kombiniert werden.
            Im Workshop werden zunächst wesentliche Aspekte wie Modellierung, Annotation,
                Analyse, Kollaboration und Automatisierung mit CATMA anhand der Erfahrungen des
                heureCLÉA-Projektes vorgestellt. Anschließend haben die Teilnehmer_innen die
                Gelegenheit in einer praktischen hands-on-Session CATMA sowie HeidelTime und
                andere Komponenten der in CATMA integrierten NLP-Pipeline auszuprobieren.
                Automatische Annotationen können evaluiert werden und mit manuellen Annotationen
                kombiniert in die Analyse einfließen. Es kann entweder mit eigenen oder
                kollaborativ mit von uns zur Verfügung gestellten Texten gearbeitet werden. 
            Wir erhoffen uns außerdem durch den Workshop kritisches Feedback zur weiteren Verbesserung von CATMA und eine Diskussion über die Anforderungen für Textanalyse-Plattformen in verschiedenen Bereichen der Digital Humanities.
         
         
            Beitragende
            Alle Veranstalter_innen sind Mitglieder des heureCLÉA-Projektes. Wir haben auf
                  zahlreichen nationalen und internationalen Tagungen und Konferenzen unsere
                  Arbeiten zu CATMA, HeidelTime und heureCLÉA vorgestellt. Dieser Workshop baut
                  auf Erfahrungen aus anderen Workshops zum selben Thema sowie der Einbettung von
                  CATMA in die Lehre auf. Auch das sehr positive Feedback vergangener Workshops,
                  z. B. zu unserem Tutorial im Rahmen der DH 2014, hat uns dazu motiviert, erneut
                  einen Workshop anzubieten bzw. einen Antrag dafür einzureichen.
            
               Thomas Bögel, Institut für Informatik, Universität Heidelberg,
                  
            Nach seinem Computerlinguistikstudium begann Thomas Bögel sein Promotionsstudium am Institut für Informatik an der Universität Heidelberg, wo er auch wissenschaftlicher Mitarbeiter ist. Seine Forschung beschäftigt sich vor allem mit "event extraction" und "timeline generation" sowie mit der Entwicklung von Machine Learning Systemen für die Extraktion von temporalen Relationen in narratologischen Texten.
            
               Evelyn Gius, Institut für Germanistik, Universität Hamburg,
                  
            Evelyn Gius forscht und lehrt im Bereich der Digital Humanities mit einem Fokus auf computergestützter Textanalyse und der Hermeneutik digitaler Zugänge zu Texten. In ihrer Promotion hat sie mithilfe von CATMA an einem Korpus von Erzählungen über Arbeitssituationen untersucht, inwiefern narratologische Kategorien aus der Literaturwissenschaft für die Analyse der Konflikthaftigkeit von Alltagserzählungen genutzt werden können.
            
               Marco Petris, Institut für Germanistik, Universität Hamburg,
                  
            Marco Petris ist Informatiker mit starker Affinität für die Geisteswissenschaften und hat von Beginn an CATMA federführend aufgebaut. Als Software Entwickler ist er in zahlreiche Projekte für die Digital Humanities involviert, wobei er sich dabei vor allem um die Konzeption und Implementierung kümmert.
            
               Jannik Strötgen, Max-Planck-Institut für Informatik, Saarbrücken,
                  
            Bevor Jannik Strötgen als Postdoc zum MPI wechselte, studierte er in Heidelberg
                    Computerlinguistik und promovierte und arbeitete am Institut für Informatik der
                    Universität Heidelberg. Im Rahmen seiner Dissertation beschäftigte er sich vor
                    allem mit Informationsextraktion sowie Information Retrieval und begann die
                    Entwicklung von HeidelTime, einem frei verfügbaren Temporal Tagger, der für
                    verschiedene Domänen und Sprachen geeignet ist. 
         
         
            Kapazität und Ausstattung
            Die Teilnehmerzahl ist auf 20 Personen begrenzt. Jede_r Teilnehmer_in braucht
                      einen Laptop (ein Tablet PC reicht nicht aus!) und ein Google Mail Konto für den
                      CATMA Login.
         
      
      
         
            heureCLEA ist ein BMBF-gefördertes
                        ehumanities Projekt zwischen der Universität Hamburg und der Universität
                        Heidelberg (Gertz / Meister 2016).
         
         
            
               Bibliographie
               
                  Bögel,Thomas / Strötgen, Jannik / Gertz, Michael
                            (2015): „A Hybrid Approach to Extract Temporal Signals from Narratives“,
                            accepted at: International Conference of the German
                            Society for Computational Linguistics and Language Technology
                            (GSCL’15), Duisburg-Essen, Germany. 
               
                  Gertz, Michael / Meister, Jan Christioph (2016): heureCLÉA. Collaborative Literature exploration &
                            annotation. Hamburg: Universität Hamburg http://heureclea.de/ [letzter Zugriff 08. Oktober 2015].
               
                  Gius, Evelyn / Jacke, Janina (2014): „Zur Annotation narratologischer
                              Kategorien der Zeit. Guidelines zur Nutzung des CATMA-Tagsets“. Hamburg 2014
                              [letzter
                              Zugriff 08. Oktober 2015]. 
               
                  Meister, Jan Christoph / Gius, Evelyn / Petris, Marco / Meister, Malte
                                / Jacke, Janina (2015): CATMA. Computer
                                Aided Textual Markup Computer Aided Textual Markup & AnalysisAnalysis.
                                Hamburg: Universität Hamburg http://www.catma.de/ [letzter Zugriff 08. Oktober 2015].
               
                  Strötgen, Jannik / Gertz, Michael (2013): „Multilingual and cross-domain temporal
                                  tagging“ in: Language Resources and Evaluation 47, 2:
                                  269-298. 
            
         
      
   



      
         
            Kurzbeschreibung
            Im Verlauf der letzten 10 Jahre hat die Menge an digital verfügbaren,
          fachwissenschaftlich annotierten Volltexten für die historische Forschung stark
          zugenommen. Damit einher geht auch eine Veränderung sowohl der Nutzungsformen
          digitaler Quellen als auch der Möglichkeiten der historischen Arbeitsweise.
          Bestand um die Jahrtausendwende noch enger Kontakt zwischen Historiker_innen und
          Quellen, nimmt dies mit zunehmender Digitalisierung perspektivisch ab. Hat der /
          die Forscher_in früher die für seine Forschungsfragen relevanten Quellen in der
          Regel alle mindestens einmal gelesen, scheint dies bei den heute
          recherchierbaren Mengen an digitalen Quellen kaum noch möglich. Ein Hauptproblem
          ergibt sich hier aus der Schnittstelle zwischen Forscher_innen und den im Netz
          erreichbaren Quellendatenbanken. Die Suchinterfaces der Datenbanken sind oft für
          die Nutzung durch Expert_innen des jeweiligen Materials optimiert. Dies ist auf
          der einen Seite zu begrüßen, da sie den Fachwissenschaftler_innen damit besten
          Zugriff auf das Material gewähren. Daneben sollten aber weitere
          Zugriffsmöglichkeiten für übergreifende Text-Mining- oder Big-Data-Recherchen
          bereitgestellt werden, mit denen verschiedene Quellenkorpora parallel im
          Hinblick auf übergreifende Fragestellungen untersucht werden können.
            Neben Such- bzw. sammlungsorientierten Zugriffen auf die Daten wird die Fähigkeit, mittels bestimmter Visualisierungsmethoden und -instrumente neue Zusammenhänge in den Daten zu erkennen und diese dann für die historische Analyse zu nutzen immer wichtiger. Insbesondere im Bereich der graphorientierten Visualisierungsmethoden ist im Moment ein regelrechter Boom an Softwarebibliotheken und Online-Tools zu beobachten. Auch in den einschlägigen Forschungsumgebungen bzw. Forschungsinfrastrukturen für die Geisteswissenschaften wie TextGrid und DARIAH werden zunehmend Visualisierungsinstrumente für annotierte Fachdaten eingebunden.
            Im Workshop zweier Partnerinstitutionen aus dem DARIAH-DE Cluster
            "Fachwissenschaftliche Annotationen" (Salomon Ludwig Steinheim-Institut für
            deutsch-jüdische Geschichte / Akademie der Wissenschaften und der Literatur
            Mainz, Digitale Akademie) soll es mit konkretem Praxisbezug um die Potentiale,
            Methoden und Instrumente zur Visualisierung von historischen Fachdaten gehen.
            Als Anwendungsbeispiel dienen die historischen Fachdatenrepositorien, die beide
            Partner in den Workshop mit einbringen (bspw. Epidat –
            epigraphische Datenbank; die Deutschen
            Inschriften Online; die Regesta Imperii Online). 
            Visualisiert werden zunächst klassische Fragestellungen, wie beispielsweise die
              nach bestimmten Familienbeziehungen, nach Orts- oder Zeitbezügen in den Daten.
              Genutzt werden Instrumente wie beispielsweise die Graphdatenbank neo4j, das
              Netzwerkvisualisierungsinstrument Gephi, aber auch JavaScript-Tools auf Basis
              von sigma.js,
              dracula oder auch  d3 . Der Workshop wird
              schrittweise vorgehen. Ein grundlegendes Verständnis für TEI/XML, JSON, RDF
              sowie JavaScript-Webtechnologien ist für die Teilnahme am Workshop hilfreich,
              aber nicht zwingend. Nach einer allgemeinen Einführung in den Bereich des
              fachwissenschaftlichen Annotierens wird es um praktische Beispiele gehen, die in
              den Daten vorhandenen Annotationen für verschiedene Visualisierungsinstrumente
              aufzubereiten und dann die jeweiligen Visualisierungen zu erzeugen. In kurzen
              Impulsreferaten werden sich die Workshop-Teilnehmer_innen die gemeinsam
              erarbeiteten Visualisierungen und die Instrumente, mit denen diese
              Visualisierungen erzeugt worden sind, gegenseitig vorstellen. 
            Im Fazit soll der Workshop ein Bewusstsein und auch schon erste Fähigkeiten
                entwickeln, sich mit Hilfe von Visualisierungsinstrumenten neue Sichten auf das
                Quellenmaterial und somit neue Forschungsperspektiven zu eröffnen. Deutlich
                werden wird aber auch, dass dieser Ansatz nicht automatisch zu einer
                „Antwort-Maschine“ führt, die dem / der Wissenschaftler_in die interpretative
                Arbeit abnimmt. Vielmehr können sich durch Visualisierungen neue
                Interpretationsmöglichkeiten des Quellenmaterials bieten, die vorher einfach auf
                Grund der Datenmasse nicht sichtbar gemacht werden konnten.
            Weiterführende Literatur:
            Kuczera, Andreas (2015): Graphdatenbanken für Historiker.
                Netzwerke in den Registern der Regesten Kaiser Friedrichs III. mit neo4j und
                Gephi http://mittelalter.hypotheses.org/5995.
              
            Schrade, Torsten (2013): "Datenstrukturierung", in: Frietsch, Ute / Rogge, Jörg
                (eds.): Über die Praxis des kulturwissenschaftlichen
                Arbeitens. Ein Handwörterbuch. Bielefeld: transcript 91–97.
         
         
            Teilnehmerzahl
            10 - 15 Personen
         
         
            Benötigte Ausstattung
            
               WLAN-Zugang
               Beamer
               Workshop-Teilnehmer sollten ihre eigenen Laptops mitbringen
            
         
      
   



      
         
            Einleitung
            Die Diskussion rund um digitale Editionen als Ergänzung oder sogar Ersatz für die
          klassische Buchausgabe ist in vollem Gange. Grund dafür ist auch der Siegeszug
          der plattform- und implementationsunabhängigen Metasprache XML in den „Digital
          Humanities“. Insbesondere der TEI-Standard (Burnard / Bauman 2007) hat dafür
          gesorgt, dass Informationen aller Art in einem Sammelformat vorliegen, das von
          vielen neu entwickelten Werkzeugen als Ausgabe- und Austauschformat verwendet
          wird.
            Der weitere Bearbeitungsprozess bis hin zur Fertigstellung einer Edition fußt daher heutzutage stark auf XML. Gleichwohl bleibt das Buch als notwendiges Ergebnis eines Editionsprojekts weiterhin die Regel. Somit stehen viele Wissenschaftler zu Beginn eines solchen Projekts vor dem Problem, einen Workflow auf XML-Basis zu definieren, der am Ende möglichst komfortabel – mit oder ohne Zutun eines Verlags – auch einen hochwertigen Buchdruck erlaubt.
            Projekte wie
            XML-Print oder
            Apache FOP setzen hier an und wollen das Textsatzproblem innerhalb der „X-Technologien“
            1  lösen. Es ist in den vergangenen Jahren jedoch deutlich geworden, dass der wissenschaftliche Textsatz von diesen Werkzeugen (noch) nicht in all seinen Facetten erfasst werden kann. Daher greifen aktuelle Editionsprojekte in der Regel auf etablierte Werkzeuge wie
            TUSTEP oder andere nicht notwendigerweise XML-basierte Ansätze zurück, indem der XML-Eingabetext mittels XSLT in die entsprechenden Zwischenformate überführt wird.
          
            Genau hier möchte der Workshop ansetzen und die Möglichkeiten des Textsatzsystems
            TeX 2 im Bezug auf die Erstellung
            einer historisch-kritischen Ausgabe (kritische Edition) vorstellen. Leider wird
            dieser Weg aus Unwissenheit bzw. wegen falscher oder schlicht veralteter
            Informationen bzgl. des Funktionsumfangs viel zu selten beschritten. Umso
            erfreulicher sind Forschungs- und Arbeitsumgebungen wie 
                  FuD
                oder
            
                  ediarum
               , die am Ende des editorischen XML-basierten
              Workflows eine mit TeX erzeugte PDF-Datei zur Kontrolle bzw. als Vorstufe des
              Druckergebnisses ausgeben. 
         
         
            Funktionsweise von LaTeX
            
               Allgemein
               Das quelloffene Textsatzsystem TeX und die gleichnamige Programmiersprache wurden Ende der Siebziger Jahre vom amerikanischen Mathematikprofessor Donald E. Knuth für den Druck seiner eigener Bücher entwickelt. Das Problem des Textsatzes – „Wie bringe ich unter Beachtung verschiedener Regeln möglichst schön Zeichen aller Art aufs Papier?“ – wurde von ihm als mathematisches Optimierungsproblem definiert und mit neuartigen Algorithmen gelöst. Die
                  subjektive Schönheit wurde dadurch von Knuth auf Basis typographischer Traditionen und Methoden
                  objektiviert.
                
               Die so entstandenen Algorithmen, z. B. derjenige für den Zeilenumbruch (Knuth
                  / Plass 1981) waren bahnbrechend und sind bis heute „State of the Art“.
                  Entsprechend werden sie auch in jüngerer Software wie Adobe InDesign oder Apache FOP nahezu
                  unverändert verwendet. Für den Autor eines Texts bedeutet dies, dass er sich
                  vollständig auf die inhaltliche bzw. strukturelle Gestaltung konzentrieren
                  kann. Dies entspricht der Arbeit mit XML-Quelldaten, die in der Regel
                  keinerlei typographische Anweisungen enthalten. 
               Somit lebt die klassische Trennung zwischen Autor und Setzer wieder auf, die
                    durch Textverarbeitungsprogramme in den vergangenen
                    Jahrzehnten stückweise aufgeweicht worden ist – mit negativen Folgen für die
                    Druckqualität. In heutigen digitalen Arbeitsumgebungen entspricht der Setzer
                    einem Satzprogramm, das eine Druckvorlage – heutzutage oft „hinter den
                    Kulissen“ einer virtuellen Arbeitsumgebung – auf die Quelldokumente eines
                    Autors anwendet. 
            
            
               Anwendungsfall Kritische Edition
               Eine kritische Edition stellt besondere Anforderungen an ein
                      Textsatzwerkzeug. Daher eignet sich dieser Dokumenttyp besonders gut, um die
                      Qualität von LaTeX zu demonstrieren. Plachta definiert für eine kritische
                      Edition zehn elementare Bestandteile (Plachta 2006: 14-15). Diese lassen
                      sich zu den folgenden drei Themenkomplexen zusammenfassen:
               
                  Edierter Text,
                  Apparate,
                  Verzeichnisse.
               
               Zu all diesen Bereichen liefert LaTeX entweder direkt oder über Erweiterungen (Pakete) Möglichkeiten, hochwertige Ergebnisse zu erzielen. Sie stehen über das zentrale Paketarchiv CTAN
                        3 allen Nutzern kostenfrei zur Verfügung.
                      
            
         
         
            Inhalte des Workshops
            Entlang der im vorherigen Abschnitt genannten Bestandteile einer kritischen Edition wird der Workshop den Teilnehmern die Gelegenheit geben, LaTeX als Satzprogramm kennenzulernen bzw. vorhandene Kenntnisse auszubauen. Im Detail werden die folgenden Inhalte vermittelt:
            
               Edierter Text: Neben den grundlegenden Satzalgorithmen werden
                        mikrotypographische Fragen thematisiert. Dazu gehören neben der Nutzung
                        typographisch korrekter Zeichen (z. B. bei Anführungszeichen, Gedankenstrich
                        oder Ellipse) auch der automatische optische Randausgleich oder die
                        Laufweitenänderung, die beide durch das Paket microtype bereitgestellt werden. Mehr zum Thema Mikrotypographie
                        findet man bei Beinert (2015). 
               Apparate: Die Anforderungen an den Satz von Apparaten gehen weit über
                          diejenigen „normaler“ Fußnoten hinaus. Es wird das Paket reledmac vorgestellt und neben verschiedenen Anpassungen auch
                          Lösungen für Probleme wie z. B. überlappende Lemmata erarbeitet. 
               Verzeichnisse: Eine kritische Edition enthält neben einem Quellenverzeichnis verschiedene Register. Diese sollen im Idealfall direkt aus den Druckdaten dynamisch erzeugt werden. Dazu werden die Erweiterungen
                            biblatex sowie
                            imakeidx vorgestellt, die genau dies bewerkstelligen.
                          
            
         
         
            Teilnehmerkreis / Technische Ausstattung
            Der Workshop richtet sich an alle interessierten Wissenschaftler_innen, die Wert
                          auf einen hochwertigen Druck ihrer Edition legen. Auch Entscheidungsträger, die
                          ein Textsatzsystem für ihr Editionsprojekt suchen, sind ausdrücklich
                          angesprochen. Für die praktischen Beispiele werden grundlegende Kenntnisse der
                          Textsatzsprache LaTeX vorausgesetzt.
            Da es sich um eine „Hands-on“-Sitzung handelt, sollte die Teilnehmerzahl 15 nicht
                            übersteigen, wobei ich eine Warteliste begrüßen würde. Die Teilnehmerinnen und
                            Teilnehmer benötigen einen Laptop mit einer (möglichst aktuellen)
                            TeX-Distribution (MiKTeX oder TeX Live). Für die Präsentation selbst wird ein
                            Beamer benötigt.
         
      
      
         
            Unter den X-Technologien versteht man die W3C-Standards XML, XSL und XPath sowie je nach Kontext weitere im XML-Umfeld entstandene Sprachen und Formate wie XQuery oder XLink.
            TeX wird seit langem schon nur noch
                              selten direkt angewendet, sondern in der Regel über eine Makrosprache wie
                              LaTeX (siehe z. B. http://www.latex-project.org) oder ConTeXt (siehe z. B. http://wiki.contextgarden.net/What_is_ConTeXt) angesprochen. Der
                              Workshop konzentriert sich auf LaTeX.
            Das
                                Comprehensive TeX Archive Network stellt über zwei zentrale Server und mehr als hundert Spiegelserver (Mirrors) weltweit unter
                                http://www.ctan.org insgesamt über 4500 Erweiterungen bereit.
         
         
            
               Bibliographie
               
                  Beinert, Wolfgang (2015): Typolexikon.de. Das Lexikon der europäische Typographie http://www.typolexikon.de/m/mikrotypographie.html [letzter
                                  Zugriff 05. Februar 2016].
               
                  Berlin-Brandenburgische Akademie der Wissenschaften
                                  (2010): ediarum. Eine digitale Arbeitsumgebung für
                                  Editionsvorhaben http://www.bbaw.de/telota/software/ediarum [letzter Zugriff 05.
                                  Februar 2016].
               
                  Burnard, Lou / Bauman, Syd (2007): TEI P5. Guidelines for Electronic Text Encoding and Interchange.
                                    Text Encoding Initiative http://www.tei-c.org/release/doc/tei-p5-doc/en/html/ [letzter
                                    Zugriff 05. Februar 2016].
               
                  Knuth, Donald E. / Plass, Michael F. (1981): “Breaking
                                      paragraphs into lines”, in: Journal Software: Practice and
                                      Experience 1 0.1002/spe.4380111102.
               
                  Plachta, Bodo (22006): Editionswissenschaft: eine Einführung in Methode und
                                        Praxis der Edition neuerer Texte (= Reclams Universal-Bibliothek
                                        17603). Stuttgart: Philipp Reclam jun. 
               
                  Trier Center for Digital Humanities (TCDH) /
                                          Forschungszentrum Europa (FZE) (2014): FuD.
                                          Eine virtuelle Forschungsumgebung für die Geisteswissenschaften. Universität
                                          Trier http://fud.uni-trier.de/de/ [letzter Zugriff 05. Februar
                                          2016].
            
         
      
   



      
         Das Panel wird durch vier Kurzvorträge in Probleme der Nutzung digitaler Werkzeuge für nicht-indoeuropäische Sprachen einführen. Die Vorträge basieren auf Erfahrungen aus langfristig angelegten Projekten und haben jeweils individuelle Lösungen für die spezifischen Anforderungen gefunden, die hauptsächlich durch die Sprache formuliert werden.
         Allen Projekten gemein sind Probleme der Nutzung von digitalen Werkzeugen, die insbesondere bei der Verwendung von Quellen historischer oder wenig erforschter Sprachen auftreten. 
         Aktuelle Content Management Systeme und Annotationstools wurden selten im Hinblick auf Anforderungen aus Orchideenfächern entwickelt. Dies betrifft beispielsweise einige Sprachen mit nichtkonkatenativer Morphologie oder komplexen Schriftsysteme. Daher müssen für erwähnte Sprachen entweder existierende Anwendungen angepasst oder erschaffen werden.
         Bei der Adaption können während der Modellierung wichtige Eigenschaften nicht berücksichtigt werden oder bleiben nur als Kommentar erhalten, was eine weitere maschinelle Bearbeitung erschwert. Bezüglich der Datenkodierung ergibt sich das Problem der Ineffizienz. So wurden morphologische Tagsets primär für die indo-europäische Sprachfamilie entwickelt. Für eine tiefe linguistische Annotation müssen aber diese Standards beispielweise für einige semitische Sprachen angepasst werden. 
         Nicht selten ist die Alternative die Eigenentwicklung projektbezogener Lösungen, die aber aufgrund der Anforderungen mit eigenen Datenformaten arbeiten, und so nicht mehr den geltenden Standards folgen und den Austausch erschweren. Hinzu kommt der immense Zeit- und Ressourcenaufwand bei der Implementierung.
         Allerdings sind gerade im deutschsprachigen Raum viele langfristige Projekte auf digitale Tools angewiesen.
         Durch eine Vernetzung solcher Projekte können gemeinsame Anforderungen an, und Begrenzungen von aktuellen Lösungen besprochen und Initiativen zur Entwicklung digitaler Tools und Ressourcen koordiniert werden. Daher ist das Ziel dieses Panels eine erste Zusammenführung langfristig ausgerichteter Projekte im deutschen Sprachraum, die mit historischen nicht-indo-europäischen Sprachen im digitalen Kontext arbeiten. Dabei sollen die Probleme der Nachhaltigkeit entwickelter Werkzeuge und Ressourcen, sowie der bearbeiteten Daten besprochen werden. Anschließend werden die vielfältigen Herangehensweisen mit einem Fokus auf drei große Punkte diskutiert:
         
            Nachhaltigkeit von Repositorien
         
         
            Welche Frameworks werden für welche Datentypen benötigt?
            Wie können Informationen über unpräzise Daten gespeichert werden? 
            Wie gehen verfügbare Systeme mit Multilingualität um?
         
         
            Nachhaltigkeit von (Annotations-)Werkzeugen
         
         
            Analyse historischer Daten impliziert die Annotation von Textmaterialien in Sprachen, die aus verschiedenen Gründen zu Problemen führen.
            Welche Annotationstools können genutzt werden? Mit welchen Limitierungen?
            Was bedeutet es, ein neues Tool zu entwerfen?
            Häufige Anforderungen durch strukturell komplexe Sprachen: Multilevel-Annotation, Textkorrektur während der Annotationsphase, Multilevel-Segmentierung
         
         
            Nachhaltigkeit des annotierten Materials (Standards)
         
         
            Während der Standard TEI-XML als Schnittstellenformat sehr nützlich ist, ergeben sich dennoch Probleme wie:
                    
                  für interne Verarbeitung kann dessen Verwendung hinderlich sein. Daher müssen projekt-spezifische Lösungen mit standardisiertem Export entwickelt werden.
                  Können diese Daten von Dritten in TEI-XML verarbeitet werden?
                  Welche anderen Formate können genutzt werden (z.B. JSON)?
                  Sind existierende Tagset-Formate ausreichend spezifiziert, um auch nicht-europäische Sprachen taggen zu können?
               
            
         
         
            Herausforderungen in der Nutzung vorhandener Tools für arabische Daten
            Alicia González, Tillmann Feige
            Universität Hamburg
            ERC- Projekt COBHUNI (
                    )
                
            Email: 
                    alicia.gonzalez@uni-hamburg.de; 
                    tillmann.feige@uni-hamburg.de
            
            Wir beschreiben den Ansatz, einen Korpus der neben modernem auch klassisches Arabisch (siehe Romanov, 2016) enthält, mit computerlinguistischen und semantischen Verfahren analysierbar zu machen. Wir setzen auf bereits vorhandene Software für die Hauptpunkte Annotation und Analyse. Dazu wurde ein Pflichtenheft erstellt, dass mit vorhandenen Tools abgeglichen wurde.
            Da wir mit arabischen Daten arbeiten, ist eine große Herausforderung die Schrift. Es ist eine linksläufige verbundene Schrift, die durch Konsonanten und lange Vokale repräsentiert wird. Kurze Vokale sind Diakritika, die optional gesetzt werden und gerade bei Referenzen auf religiöse Quellen im Textkorpus vorkommen. Dabei ist vollständige UTF-8 Unterstützung und die saubere Darstellung der Schrift unabdingbar. Dies reduziert die Auswahl erheblich. Hinzu kommt, dass wir auf flexible Import- und Exportmöglichkeiten angewiesen sind. Ähnliche Probleme führen Peralta und Verkinderen auf (Peralta / Verkinderen 2016). Durch unsere Herangehensweise gibt es weitere Einschränkungen wie Mehrebenen-, Multitoken- aber auch Subtoken-Annotation.
            Die Auswahl für die semantische Annotation fiel auf WebAnno, dass durch sein spezielles Datenmodell die erforderliche Datenaufbereitung und Kontrolle gestattet.
            Als Visualisierungstool haben wir ANNIS ausgewählt, dass ebenfalls Arabisch unterstützt, einen konfigurierbaren Converter mitbringt und Mehrebenenkorpora erlaubt, so dass auch hier die Hauptkriterien erfüllt wurden. Zusätzlich lassen sich potentielle Probleme in der Darstellung durch eine anpassbare HTML-Visualisierung umgehen. Durch Zusammenarbeit mit den Entwicklern beider Programme wurde die Unterstützung für Arabisch stetig ausgebaut.
            Im Beitrag werden wir die einzelnen Punkte erläutern und darstellen, warum wir uns für die angeführten Programme und gegen eine Eigenentwicklung entschieden haben, sowie welche Implikationen diese Entscheidung für die Nachhaltigkeit des Projekts, der Daten und der genutzten Tools hat.
         
         
            Tiefe Mehrebenen-Annotation für semitische Sprachen: der Fall von Ge'ez
            
               Cristina Vertan
            
            Universität Hamburg
            ERC-Projekt TraCES (
                    )
                
            Email: 
                    cristina.vertan@uni-hamburg.de
            
            Das südsemitische Gәʿәz ist die Sprache des Königreichs Aksum in der heutigen nordäthiopischen Provinz Tigray, von wo aus die im 4. Jahrhundert beginnende Christianisierung Äthiopiens ihren Anfang nahm. Die in der Folge entstehende reiche Literatur ist in großem Umfang geprägt von Übersetzungen, was durch grammatische Interferenzphänomene reflektiert wird. Das Altäthiopische hat aus einer südsemitischen Schrift ein eigenes Silbenalphabet entwickelt, das bis heute in mehreren modernen Sprachen Äthiopiens und Eritreas Verwendung findet. Innerhalb der semitischen Sprachen fällt es durch die verwendete Rechtsläufigkeit auf; außerdem werden die Vokale vollständig geschrieben. Beides unterscheidet das Gәʿәz von verwandten Sprachen wie Altsüdarabisch, Arabisch, Hebräisch und Syro-Aramäisch Mit den genannten eng verwandten semitischen Sprachen teilt das Altäthiopische die nichtkonkatenative Morphologie. Durch das äthiopische Silbenalphabet sind Morphemgrenzen in der Schrift nicht darstellbar, so dass beispielsweise ein einzelner Vokal als Bestandteil einer Silbe eine eigenständige Wortart darstellt und tokenisiert werden muss.
            Die Komplexität des Annotationstools wird sehr vielfältige linguistische Anfragen und detaillierte Analysen der Sprache ermöglichen, aber auch eine vollautomatische Annotation verhindern. Ein alle morphologischen Merkmale abdeckendes Vektorraum-Modell (das für maschinelle Lernverfahren benutzt werden muss) wäre zu groß. Vorstellbar ist lediglich eine flache automatische Annotation (z. B. der Wortarten); jedoch wird auch für eine solche zunächst eine relativ große Menge an Trainingsdaten benötigt. Daher ist die Entwicklung eines Werkzeugs für die manuelle Annotation ein obligatorischer Schritt.
            Die Besonderheit der entwickelten Lösung (Vertan/Ellwardt/Hummel 2016) sind:
            
               automatische Transkription
               manuelle Korrektur der Transkription während des Annotationsprozesses
               semi-automatische Verfahren: automatische Verläufe werden farbig markiert und sind automatisch zur manuellen Korrektur hinterlegt
               Mehrebenenannotation: Linguistik, Edition, Textstruktur
               Anpassungen an unterschiedliche Schriftsysteme und Transkriptionsregeln
            
         
         
            Nutzungs- und Nachhaltigkeitsstategien im Projekt "Textdatenbank und Wörterbuch des Klassischen Maya"
            Christian M. Prager
            NRW Akademie der Wissenschaften und der Künste
            
               
            
            Email: 
                    
            
            Die Mayaschrift ist das einzig lesbare Schriftsystem der vorspanischen Amerikas. Die über 10.000 Texte sind in einer logographisch-syllabischen Hieroglyphenschrift verfasst und von den rund 800 Zeichen sind erst 60% sicher entziffert. Die Texte enthalten taggenaue Kalenderangaben, die es uns ermöglichen die rund 2000jährige Sprach- und Schriftgeschichte genau zu dokumentieren. Das Projekt (Prager 2015) wird sämtliche Inschriften einschließlich Metadaten in einer Datenbank einzupflegen und darauf basierend ein digitales Wörterbuch des Klassischen Maya zu kompilieren. Herausforderung dabei ist, dass die Schrift noch nicht vollständig entziffert ist und bei der Modellierung zu berücksichtigen ist. Unser Projekt verfolgt den Ansatz, wonach die Bedeutung von Wörtern ihre Verwendung ist - Texte nehmen Bezug auf den Textträger und den Verwendungskontext und nur die exakte Dokumentation sogenannter nicht-textueller Informationen erlaubt es, textuelle und nicht-textuelle Informationsbereiche zueinander in Beziehung zu setzen und bei der Entzifferung von Zeichen und Textstellen zu berücksichtigen. Zum Zweck der Nachhaltigkeit und Nachnutzung greift das Projekt bei der Beschreibung der Artefakte und der relevanten objektgeschichtlichen Ereignisse auf CIDOC CRM zurück, das eine erweiterbare Ontologie für Begriffe und Informationen im Bereich des kulturellen Erbes anbietet. Das entstandene Anwendungsprofil wird durch Elemente aus weiteren Standards und Schemata angereichert und wird damit auch für vergleichbare Projekt nachnutzbar. Die Schemata und erstellten Metadaten werden in einer Linked (Open) Data-Struktur (LOD) abgebildet. Durch die Repräsentation im XML-Format, sowie die Nutzung von HTTP-URIs wird eine einfache Austauschbarkeit und Zitierbarkeit der Daten ermöglicht. Durch diese Umsetzung können Objektmetadaten getrennt vom erfassten Text gespeichert werden und durch die Verwendung der HTTP-URI verlinkt werden. Die Nachnutzung bereits bestehender und fachlich anerkannter Terme trägt darüberhinaus auch zu einer hohen Interoperabilität mit anderen Datenbeständen und Informationssystemen bei. Das ausgestaltete Schema hat eine ontologisch-vernetzte Struktur, die komplexe Beziehungen und Zusammenhänge abbildet.
         
         
            Interdisziplinäre Digitale Zusammenarbeit für seltene Sprachen und Kulturen
         
         
            - Eine Fallstudie über jiddische Texte aus der frühen Neuzeit -
            
               Walther v. Hahn (Universität Hamburg), Berndt Strobach (Wolffenbüttel)
            
            Email: 
                    vhahn@informatik.uni-hamburg.
               de, berndt.strobach@freenet.de>
                
            In den Geisteswissenschaften werden häufig die fachlichen Interpretationen und die sprachlichen Erklärungen von verschiedenen Gruppen mit unterschiedlicher Kompetenz bearbeitet. Gute Beispiele sind Studien zu Texten aus semitischen Sprachen, wobei, speziell bei historischen Dokumenten die historische oder geistes- und sozialgeschichtliche Würdigung von Forschern verfasst werden muss, die des Hebräischen, Arabischen, Aramäischen etc. nicht mächtig sind, die sprachwissenschaftlichen Forscher dagegen bei der Interpretation gelegentlich weniger engagiert bleiben. Extremfälle wie Studien über das Sephardische in Spanien (Ladino, Djudezmo) machen etwa solide Kenntnisse zumindest des Spanischen, Hebräischen, Türkischen, Griechischen und Italienischen zur Voraussetzung für seriöse hermeneutische Forschungsergebnisse.Wir berichten über Studien zu jiddischen Texten aus dem Wolffenbüttel des 18. Jahrhunderts, in denen die Rolle der "Hofjuden" und ihres kultur- und sozialgeschichtlichen Hintergrundes diskutiert wird. 
            Die Herausforderung einer interdisziplinären Zusammenarbeit zwischen Historikern, Sprachwissenschaftlern und Informatikern besteht darin,
            1. die Lesbarkeit der Originalquellen für alle Gruppenmitglieder sicher zu stellen (Invertierte Transkriptionen, Vokalisierung, Visualisierung), sowie 
            2. in der Gruppe eine gemeinsame Behandlung von Vagheit, Unsicherheit und Unbekanntem zu definieren, so dass die Unklarheiten in den einzelnen Forschungsstufen erhalten und im Endergebis sichtbar bleiben (Vagheits-Annotationen und vage Inferenzen). Heute werden derartige Unsicherheiten meist bereits in den Annotationen unterschlagen (von Hahn, 2016).
         
      
      
         
            
               Bibliographie
               
                  Hahn, Walther von (2016):
                        „Humanities meet Computer Science – Digital Humanities between Expectations and Reality“,
                        zu erscheinen in: von Hahn, Walter / Papadima, Liviu / Vertan, Cristina (eds.):
                        Humanities2020, New Trends in Education and Research. 
                        Bukarest: University of Bucharest Publishing House.
                    
               
                  Peralta, José Haro / Verkinderen, Peter (2016): 
                        „‚Find for me!‘: Building a Context-Based Search Tool Using Python“,
                        in: Muhanna, Elias (ed.):
                        The Digital Humanities and Islamic & Middle East Studies. 
                        Berlin: Walter de Gruyter GmbH 199–231. 
                    
               
                  Prager, Christian M. (2015): 
                        „Das Textdatenbank- und Wörterbuchprojekt des Klassischen Maya: Möglichkeiten und Herausforderungen digitaler Epigraphik“,
                        in: Neuroth, Heike / Rapp, Andrea / Söring, Sibylle (eds.):
                        TextGrid: Von der Community - für die Community: Eine Virtuelle Forschungsumgebung für die Geisteswissenschaften.
                        Glückstadt: Werner Holsbusch 105–124 
                        https://www.academia.edu/17957108/Das_Textdatenbank-_und_W%C3%B6rterbuchprojekt_des_Klassischen_Maya_M%C3%B6glichkeiten_und_Herausforderungen_ digitaler_Epigraphik.
                    
               
                  Romanov, Maxim (2016): 
                        Creating Frequency-Based Readers for Classical Arabic
                  http://maximromanov.github.io/2016/05-30.html [letzter Zugriff 1. Dezember 2016].
                    
               
                  Vertan, Cristina / Ellwardt, Andreas / Hummerl, Susanne (2016): 
                        „Ein Mehrebenen-Tagging-Modell für die Annotation altäthiopischer Texte“, 
                        in:
                        DHd 2016: Modellierung - Vernetzung - Visualisierung
                  http://www.dhd2016.de/abstracts/vorträge-061.html.
                    
            
         
      
   



      
         
            Eine zentrale Forderung zur Unterstützung digitaler Editionen ist das Anbieten virtueller Umgebungen (Interfaces, Software) zur Produktion, aber auch zum Management digitaler Daten (BMBF 2013). In den letzten Jahren wurden aufgrund dieser durch FachwissenschaftlerInnen getragenen Nachfrage mehrere Plattformen und Softwareangebote/Infrastrukturen geschaffen, die Prozesse der digitalen Datenerstellung von der Aufnahme von Informationen (Metadaten, Transkriptionen) über die Auswertung und Anreicherung bis zur Publikation unterstützen (DARIAH-DE (Hg.), 2015) und nachhaltig betrieben werden sollen. Unterschiedliche Konzepte und angebotene Abläufe sowie integrierte Hilfsmittel stehen für eine je eigene Profilierung der Plattformen. Merkmale der Angebote, insbesondere Leistungsfähigkeit, unterstützte Prozesse und Ausrichtungen unterscheiden sich zwangsläufig. Im Panel werden aus diesem Grund wichtige und häufig eingesetzte Plattformen in ihrem Leistungsumfang verglichen und einander gegenübergestellt. Im Sinne geisteswissenschaftlicher software studies (Andrews, 2016) müssen die Plattformen nicht nur aus pragmatischen Gründen gegeneinander abgewogen werden sondern auch, um in den angebotenen Prozessen angelegte Praktiken auf ihre Logik und dadurch entstehende Folgen zu untersuchen (Drucker, 2013). Anhand eines klar umrissenen Fragebogens präsentieren Monk, Textgrid, Transcribo und Transkribus Arbeitsabläufe, Services und Vernetzungsmöglichkeiten. Damit wird Interessierten in einem Panel aus erster Hand ein Vergleich wichtiger, produktiv nutzbarer Angebote geliefert.
            Das Panel wird moderiert von Michael Piotrowski (IEG Mainz).
            Folgende Frage- und Themenschwerpunkte werden schriftlich und in kurzen Präsentationen dargeboten:
            
               Idealtypischer/Schematisierter Ablauf für den Gebrauch der Plattform
               Zeitliche Anforderungen, um ein Projekt aufzusetzen/ein Dokument zu verarbeiten; zu exportieren
               Herstellung von Transkriptionen
               Bild-Text-Verknüpfung
               Text-Markup
               Ausgabemöglichkeiten (für Edition und/oder Transkription)
               Vernetzungsmöglichkeiten (Wörterbücher, externe Ressourcen, Ontologien)
               Datei-/Bildverwaltung
               Projektverwaltung
               Auswertungs-/Abfrageoptionen
               Automatisierungen
               Crowdsourcing/Optionen zum Einbezug von Laien oder Externen
               Nachhaltigkeit der Plattform/der enthaltenen Daten
               Updates bis 2018
            
            Monk (presented by Lambert Schomaker, Rijksuniversiteit Groningen)
            The Monk system is a trainable search engine for handwritten material. For the humanities, it may serve as a method for getting keyword access to scanned pages at the earliest stages after a document digitisation. For pattern recognition research, it is an observatory for complicated visual material and its human-provided labels (e.g., word or character labels). The system act as an e-Science service that is continuously available.
            An internal image and metadata format is used, which can be exported to, e.g., PAGE xml. Provisional transcriptions can be retrieved as flat text. Indices can be exported upon request.
            The system makes a distinction between four different forms of annotation: page (scan) descriptors, typically page titles, page regions of interest (tags for visual objects), transcription of segmented lines, and finally, word labeling. The system could export in TEI, however, within the OCR community, there is a preference for layout-centric description languages, as opposed to editorial descriptions. In practice, both TEI and PAGE are used, as well as other formalisms that allow to provide metadata to polygonal image sections.
            In order to proceed data in Monk, scans are uploaded via sftp or mailed hard disks. The collection is then judged on the required preprocessing steps (multicolumn, contrast enhancement, line segmentations), and ‘ingested’. Within one or two days users can start to label words. The system performs data mining on the collection and presents hit lists for words which can be labeled further, and so on. Static indices and provisional transcriptions are updated nightly.
            At the moment 400 documents from different periods and handwriting styles are being processed. The Monk system is one of the first 24/7 machine learning systems. The system detects where compute resources should be directed, on the basis of observed user activities and interests.
            The Monk system is part of the large multi-petabyte Target platform of the university of Groningen, in collaboration with astronomy, genomics and the IBM company.
            TextGrid (präsentiert durch Wolfram Horstmann, Niedersächsische Staats- und Universitätsbibliothek Göttingen)
            Hintergrund
            Die Entwicklung von TextGrid, einer Virtuellen Forschungsumgebung für die Geistes- und Kulturwissenschaften, wurde durch die zunehmende Nachfrage aus den Fachwissenschaften nach digitalen Werkzeugen v.a. des philologischen Edierens und kollaborativen Arbeitens angestoßen. Das Bundesministerium für Bildung und Forschung (BMBF) hat TextGrid als Verbundprojekt mit über zehn institutionellen und universitären Partnern zwischen 2006 und 2015 gefördert.
            Die Software steht mittlerweile in einer stabilen Version 3.0 zum kostenfreien Download bereit. Software, Archiv und damit das gesamte Angebot werden in Zusammenarbeit mit AnwenderInnen, FachwissenschaftlerInnen und Fachgesellschaften und in Kooperation mit DARIAH-DE - Digital Research Infrastructure for the Arts and Humanities weiter entwickelt und dauerhaft betrieben.
            Zielpublikum
            
               FachwissenschaftlerInnen, die mit TextGrid Forschungsprojekte wie z.B. digitale Editionen erarbeiten
            
            
               EntwicklerInnen, die TextGrid-Tools und Services für eigene Vorhaben anpassen oder externe Services und Tools in TextGrid integrieren
            
            
               Forschungsprojekte und -institutionen, die Daten in TextGrid archivieren und für Dritte zugänglich und nutzbar machen (Repository)
            
            Form des Einsatzes
            Die virtuelle Forschungsumgebung (VFU) TextGrid unterstützt digital arbeitende GeisteswissenschaftlerInnen im gesamten Forschungsprozess – insbesondere beim Erstellen digitaler Editionen.
            Sie besteht aus drei Kernbereichen:
            -       Die Software 
                TextGrid Laboratory stellt den Einstiegspunkt in die VFU dar und bietet unterschiedliche Open-Source-Werkzeuge und -Services für den gesamten Forschungsprozess zur Verfügung, z. B. einen Text-Bild-Link Editor für die Verknüpfung von Digitalisaten und Transkriptionen
            
            -       Im 
                TextGrid Repository, einem Langzeitarchiv für geisteswissenschaftliche Forschungsdaten, können XML / TEI-kodierte Texte, Bilder und Datenbanken sicher gespeichert, publiziert und durchsucht werden.
            
            -       Die beständig wachsende 
                TextGrid Community trifft sich bei regelmäßigen Nutzertreffen zu themen- bzw. anwendungsspezifischen Workshops, die nicht zuletzt auch den Austausch zwischen digitalen Forschungs- vorhaben aus den Geisteswissenschaften befördern.
            
            Eine Stärke
            TextGrid unterstützt den gesamten wissenschaftlichen Arbeitsprozess im Rahmen der Erstellung digitaler Editionen vom Ingest des Ausgangsmaterials (Text- und/oder Bilddatei- en / Faksimiles) über die Anreicherung und Auszeichnung der Daten (Annotationen, Verknüpfungen) bis zur Veröffentlichung (Portal, Print) und nachhaltigen Archivierung (Repository) und wird stetig basierend auf konkreten fachwissenschaftlichen Anforderungen weiterentwickelt.
            Eine Schwäche
            Technisch setzt TextGrid auf dem Eclipse-Framework auf, aus heutiger Sicht, wären webbasierte Tools wünschenswerter. Zugleich verdeutlicht dies, dass Softwareentwicklungen permanente Weiterentwicklung benötigen, um sich neuen technologischen aber auch sich wandelnden User-Requirements stellen zu können. 
            Transcribo (präsentiert durch Thomas Burch, Universität Trier)
            Transcribo wird in enger Zusammenarbeit von Philologen und Informatikern der Kooperationspartner entwickelt. Die grafische Nutzeroberfläche ist um das digitale Faksimile, also in der Regel den gescannten Überlieferungsträger, zentriert. Beliebig große Einheiten (z.B. Wörter, Zeilen oder Absätze) können mittels eines Rechteck- oder Polygonwerkzeugs markiert, transkribiert und annotiert werden. Dabei wird jede Bilddatei doppelt dargeboten: links liegt das Original zur Ansicht, die rechte Version dient als Arbeitsunterlage, hier wird der transkribierte Text topografisch exakt über das leicht ausgegraute Faksimile gelegt. Wo die räumliche Anordnung nicht der textuellen Wortreihenfolge entspricht, können Wörter in der grafischen Oberfläche zu Sequenzen zusammengefasst und so die semantischen Zusammenhänge im Transkript protokolliert werden. Ein zentrales Merkmal des Programms liegt außerdem in der Möglichkeit, in jeder erfassten Einheit textgenetische und editionsphilologisch relevante Phänomene zu kennzeichnen und mit Annotationen zu versehen. Dabei kommt ein Kontextmenü mit einer projektspezifischen Auswahl zum Einsatz. Diese umfasst bisher unterschiedliche Varianten von Korrekturen (wie etwa Sofortkorrekturen, Spätkorrekturen mit ein-, zwei- oder mehrfacher Durchstreichung und Überschreibung), die Kennzeichnung von Hervorhebungen sowie von unsicheren Lesungen oder nicht identifizierten Graphen. Diese Auswahl ist jedoch beliebig erweiterbar und wird über den gesamten Projektverlauf hinweg an die Erfordernisse der Textgrundlage angepasst.
            Transkribus (präsentiert durch Tobias Hodel, Staatsarchiv Zürich)
            Hintergrund
            Transkribus ist eine Plattform, die zur automatisierten Erkennung und Annotierung von Texten dient. Sie leistet einerseits eine Verlinkung zwischen Text und Bild (auf Block, Zeilen und Wortebene), produziert andererseits standardisierte Exportformate (XML nach TEI-Standard, PDF, aber auch METS für die Integration in Repositorien). Damit steht eine vollausgerüstete Softwaresuite zur Verfügung, die von der Segmentierung über die Erkennung, Transkription und Edition bis zur Ausgabe alle Schritte in der Herstellung hochwertiger Daten unterstützt.
            Die im Projekt READ weiterentwickelte Software vereint somit praxisnah die Bedürfnisse von GeisteswissenschaftlerInnen und Aufbewahrungsinstitutionen mit den technischen Möglichkeiten und Angeboten, die momentan im Bereich der Informatik und Computerlinguistik ermöglicht werden.
            Die Software steht in einer stabilen Version zum kostenfreien Download bereit. Das Projekt READ wird unterstützt durch das Horizon 2020 Forschungs- und Innovationsprogramm der Europäischen Union.
            Zielpublikum
            
               Aufbewahrungsinstitutionen, die eigene Bestände und Dokumente aufbereiten und zur Verfügung stellen wollen 
            
            
               Geisteswissenschaftlerinnen, die eigene Transkriptionen und Editionen in Transkribus erstellen wollen oder mit darin aufbereiteten Daten arbeiten
            
            
               Interessierte Laien, die sich an Crowdsourcing-Initiativen beteiligen wollen
            
            
               ComputerwissenschaftlerInnen, die mit den gewonnenen Daten arbeiten und eigene Algorithmen entwickeln oder verbessern wollen
            
            Form des Einsatzes
            Auf Transkribus werden Bilddateien hochgeladen, mit Layoutverlinkungen und Transkriptionen sowie Annotationen versehen. Unterstützt werden die Vorgänge durch Automatisierungsvorgänge im Bereich der Layouterkennung und der Transkription. Der Export der gewonnen Daten ist in unterschiedlichen Formaten möglich. Zusätzlich werden Module zum Crowdsourcing und zukünftig für e-Learning und Analyse mit Smartphone bereitgestellt. 
            Eine Stärke
            Transkribus nutzt neueste Automatisierungsprozesse (u.a. mit rekursiven neuronalen Netzen) somit werden bestmögliche Resultate in Aussicht gestellt.
            Eine Schwäche
            Transkribus ist eine Expertensoftware und benötigt entsprechende Einarbeitungszeit, um die Dokumente effizient und zielgerichtet zu bearbeiten.
         
      
      
         
            
               Bibliographie
               
                  Andrews, Tara (2015):
                        „Software and Scholarship – Editorial“,
                        in: 
                        Interdisciplinary Science Reviews 40: 342–348  10.1080/03080188.2016.1165456.
                    
               
                  BMBF (Bundesministerium für Bildung und Forschung) (eds.) (2013): 
                        Forschungsinfrastrukturen für die Geistes- und Sozialwissenschaften
                  https://www.bmbf.de/pub/forschungsinfrastrukturen_geistes_und_sozialwissenschaften.pdf.
                    
               
                  DARIAH-DE (ed.) (2015): 
                        Handbuch Digital Humanities: Anwendungen, Forschungsdaten und Projekte
                  http://handbuch.io/w/DH-Handbuch.
                    
               
                  Drucker, Johanna (2013):
                        „Performative Materiality and Theoretical Approaches to Interface“, in: 
                        DHQ: Digital Humanities Quarterly 7 (1) 
                        http://digitalhumanities.org:8081/dhq/vol/7/1/000143/000143.html.
                    
               
                  Schomaker, Lambert (2016):
                        „Design considerations for a large-scale image-based text search engine in historical manuscript collections“,
                        in: 
                        Information Technology 58 (2): 80–88 10.1515/itit-2015-0049.
                    
            
         
      
   



      
         
            Das vom österreichischen Wissenschaftsfonds FWF geförderte Projekt hat sich zum Ziel gesetzt, die illuminierten Urkunden des Mittelalters zu sammeln, auf der Plattform 
                    
                  monasterium.net
                zur Verfügung zu stellen und umfassend zu untersuchen. Die ExpertInnen aus den Bereichen der Diplomatik (Zajic Andreas, Gneiß Markus), der Kunstgeschichte (Roland Martin, Bartz Gabriele) und den Digitalen Geisteswissenschaften (Vogeler Georg, Bürgermeister Martina) arbeiten bewusst interdisziplinär zusammen und achten dabei vor allem auch auf Nachhaltigkeit. Das Poster wird zeigen, wie bei Materialerfassung, Erschließung und wissenschaftlicher Auswertung zukünftige Nutzerszenarien bedacht werden – also die soziale Dimension von Nachhaltigkeit konsequent berücksichtigt wird. Es reicht nicht, die Daten von Festplatte zu Festplatte zu kopieren (Langzeitarchivierung), sondern sie müssen auch nutzbar bleiben. Die im Projekt eingesetzten Mittel dafür sind 1. Datenkuratierung über eine etablierte Online-Plattform mit projektübergreifendem institutionellem Interesse, 2. Datenmanagement durch die Verwendung von gut dokumentierten und öffentlichen Datenstandards und 3. kontrollierte Vokabularien für die inhaltliche Erschließung. 
                
            
               Datenkuratierung
               Schon in der Planungsphase des Projektes war klar, dass alle Projektdaten im weltweit größten Onlineangebot von Urkunden 
                        monasterium.net verarbeitet werden sollen. Damit wird ein sozialer Aspekt von Nachhaltigkeit berücksichtigt: Monasterium ist ein seit 2002 existierendes Großprojekt zur Zurverfügungstellung und (kollaborativen) Erschließung von Beschreibungen und Faksimiles von Urkunden des Mittelalters und der Frühen Neuzeit. Das Portal wird überwiegend von Archiven gespeist, es sind aber auch retrodigitalisierte Urkundenbücher und von ForscherInnen erstellte Sammlungen enthalten. Hinter dem virtuellen Archiv Monasterium steht 
                        ICARUS, ein Konsortium von Archiven und wissenschaftlichen Institutionen, das sein Wissen und seine Erfahrungen ständig austauscht und erweitert. Die große Datenmenge, die Etabliertheit des Angebots in der Fachcommunity und der institutionelle Hintergrund haben Monasterium aus einem kleinen DH-Projekt zu einem nachhaltigen Host nicht nur für unser Projekt gemacht: 1. Die projektübergreifende Infrastruktur erlaubt, dass die projektspezifischen Forschungsdaten über die Projektdauer hinaus zur Verfügung stehen. 2. Durch die Integration der Forschungsdaten in Monasterium bekommt jeder Datensatz auch einen persistenten Identifikator. D.h. alle Datensätze sind eindeutig adressierbar und zitierbar. 3. Das Interesse am Erhalt des Angebots ist groß, sodass selbst unter widrigen finanziellen Bedingungen aktiv nach Lösungen für den Erhalt der auf 
                        monasterium.net verfügbaren Daten gesucht werden wird.
                    
            
            
                Datenmanagement
               Die im Projekt 
                        Illuminierte Urkunden entstehenden Forschungsdaten werden als strukturierte Datensätze in einer XML-Datenbank verwaltet und archiviert. Die einzelnen Urkunden-Datensätze werden nach dem Standard der CEI annotiert, die sich als TEI-P4-Dialekt in andere Datenstrukturen integriert und öffentlich dokumentiert ist. In der Datenbank ist ein für 
                        monasterium.net spezialisiertes Schema (XSD 1.1) im Einsatz, das einerseits die Verwendung der zulässigen Beschreibungselemente dokumentiert und andererseits die Konsistenz und Validität der zu importierenden Daten prüft. Schon in der Projektplanungsphase haben die Projektbeteiligten über Mittel und Möglichkeiten der Datenmodellierung gemeinsam diskutiert. Da der CEI-Standard zur wissenschaftlichen Bearbeitung von Urkunden initiiert wurde, brauchte die Überführung der aus dem Projekt 
                        Illuminierte Urkunden stammenden Forschungsdaten aus dem Bereich der Diplomatik keine Anpassungen an das Datenmodell. Um aber die Beschreibungsdaten zu Dekor und Buchschmuck aufnehmen zu können, musste das Datenmodell um die Möglichkeit einer kunsthistorischen Beschreibung erweitert werden. Dafür konnten Strukturen aus der TEI direkt übernommen werden. Die Daten werden also sozial nachhaltig, indem sie öffentlich dokumentierte und in der Fachcommunity geläufige Datenbeschreibungsstandards verwenden, Standards, die jede und jeder nachlesen kann.
                    
            
            
               Kontrollierte Vokabularien
               Im Projekt 
                        Illuminierte Urkunden ist die Vergleichbarkeit von Datensätzen für die Weiternutzung ein wichtiger Faktor. Kulturelle Kontexte und Fragen der Mehrsprachigkeit spielen seit Projektstart eine Schlüsselrolle, da von Beginn an mit Forschungspartnern aus West-, Süd- und Südosteuropa zusammengearbeitet wird.
                    
               Bisher werden noch keine vollständigen Beschreibungen in mehreren Sprachen auf 
                        monasterium.net angeboten, aber im Rahmen des Projekts wurde die Möglichkeit entwickelt, Metadaten in mehrsprachigen kontrollierten Vokabularien zu erfassen. Sie werden im W3C SKOS als RDF/XML ausgedrückt. Bisher wurden ein viersprachiges kontrolliertes Vokabular zur Klassifikation des Dekors von Urkunden (vgl. Roland 2014) und ein Glossar erstellt. Diese Neuerung steigert die Qualität einerseits des Information Retrieval und führt zu einer umfassenden Kontextualisierung des Forschungsgegenstandes. Damit sind also auch die Inhalte der Daten für eine breitere Community besser nachvollziehbar, ein Konzept, das wir vorläufig als „Langzeitverständlichkeit“ bezeichnen möchten.
                    
            
         
         
            Zusammenfassung
            Die kurze Laufzeit jedes Drittmittelprojektes – und damit auch des Projektes 
                    Illuminierte Urkunden macht Nachhaltigkeit als soziales Phänomen zu einer zentralen Frage: Die Forschungsdaten sollen einer sekundären Nutzung zur Verfügung gestellt werden und zu neuen Forschungsfragen führen. Deshalb werden im Projekt 
                    Illuminierte Urkunden drei „soziale“ Nachhaltigkeitsstrategien angewandt. Integration in eine in der Forschercommunity und bei Institutionen etablierte Plattform (monasterium.net), Verwendung von verbreiteten und facheinschlägigen Metadatenstandards und Erschließung von Inhalten mit kontrollierten Vokabularien.
                
         
      
      
         
            
               Bibliographie
               
                  CEI, Charter Encoding Initiative: 
                        http://www.cei.lmu.de [letzter Zugriff 23. August 2016].
                    
               
                  Heinz, Karl (2010):
                        „Monasterium.net. Auf dem Weg zu einem europäischen Urkundeportal“,
                        in: Kölzer, Theo (ed.):
                        Regionale Urkundenbücher. Die Vorträge der 12. Tagung der Commission Internationale de Diplomatique, St. Pölten 2010 (Mitteilungen aus dem Niederösterreichischen Landesarchiv 14) 
                        139–145.
                    
               
                  ICARUS, International Centre for Archival Research: 
                        http://icar-us.eu/ [letzter Zugriff 23. August 2016].
                    
               
                  Krah, Adelheid (2009):
                        „Monasterium.net - das virtuelle Urkundenarchiv Europas: Möglichkeiten der Bereitstellung und Erschließung von Urkundenbeständen“,
                        in:
                        AZ 91: 221–246.
                    
               
                  Roland, Martin (2013):
                        „Illuminierte Urkunden im digitalen Zeitalter – Maßregeln und Chancen“,
                        in: Ambrosio, Antonella / Barret, Sébastien / Vogeler, Georg (eds.):
                        Digital diplomatics. The computer as a tool for the diplomatist?, Archiv für Diplomatik, Beiheft 14.
                        Köln / Weimar /Wien 245–269.
                    
               
                  Roland, Martin / Zajic, Andreas (2013):
                        „Illuminierte Urkunden des Mittelalters in Mitteleuropa“,
                        in:
                        Archiv für Diplomatik 58: 237-428.
                    
               
                  SKOS, Simple Knowledge Organization System: 
                        https://www.w3.org/2004/02/skos/ [letzter Zugriff 23. August 2016]. 
                    
            
         
      
   



      
         Ziel von 
                PaLaFra
            1 („Le passage du latin au français“) ist der Aufbau eines digitalen Korpus spätlateinischer und altfranzösischer Texte, das durch die Kombination von Lemmatisierung, syntaktischer und morphologischer Annotation sowie diskurspragmatischen und texttypologischen Deskriptoren komplexe Abfragestrategien ermöglicht und so eine qualitativ neuartige Nutzung der Texte bei der Rekonstruktion des lateinisch-romanischen Sprachwandels erreichen soll. Daran arbeitet ein deutsch-französisches Team der Universität Regensburg, der Universität Tübingen, der 
                École Normale Supérieure in Lyon und der Universität Lille, das seit Sommer 2015 von der Deutschen Forschungsgemeinschaft (DFG) und der 
                Agence Nationale de Recherche (ANR) gefördert wird. Das Projektteam ist interdisziplinär ausgerichtet und besteht aus romanischen Sprachwissenschaftlern, Computerlinguisten und Medieninformatikern. Während für den Bereich des Altfranzösischen auf das bestehende 
                Base de Français Médiéval-Korpus
                2 zurückgegriffen werden kann, so ist die Erstellung eines — was die Annotation angeht — kompatiblen Korpus spätlateinischer Texte ein wichtiges Teilziel des 
                PaLaFra-Projekts.
            
         In diesem Posterbeitrag berichten wir über Herausforderungen und Lösungsansätze bei der Erstellung einer Annotationsumgebung und eines diachronen Tagsets, das gleichermaßen in der Lage ist, die Idiosynkrasien der beiden Sprachstufen adäquat abzubilden, aber auch die diachronen Elemente im Sprachwandel einheitlich zu markieren.
         Bereits für das spätlateinische Teilkorpus zeigt sich, dass es an einem standardisierten Tagset fehlt. Mindestens drei Varianten wurden in der Vergangenheit für die Annotation (spät-)lateinischer Texte entwickelt: 
                CoLaMer (Selig et al. 2015), 
                CompHistSem (Eger et al. 2015) und 
                LASLA (Denooz 1978). Diese unterscheiden sich sowohl in den zugrunde liegenden linguistischen Konzepten als auch in ihrer Granularität. Demzufolge existiert auch kein einfaches Mapping zwischen ihnen. Für die Entwicklung eines sprachübergreifenden Tagsets in 
                PaLaFra kommt erschwerend hinzu, dass die beiden Zielsprachen — Spätlatein und Altfranzösisch — trotz ihrer Verwandtschaft klare strukturellen Unterschiede aufweisen.
            
         Zumindest für die Ebene der Wortarten (PoS, Part-of-Speech) liefert beispielsweise das Projekt 
                Universal Dependencies
            3 wichtige Anhaltspunkte für ein sprachübergreifendes Tagset. Dieses Projekt hat sich die Entwicklung sprachübergreifend-kompatibler Baumbanken als Ziel gesetzt hat, die auf universellen Wortartkategorien basieren. Trotzdem bedingt die Entwicklung eines übergreifenden Tagsets oft den manuellen Vergleich von Annotationen, z.B. durch visuelle Gegenüberstellung annotierter Parallelkorpora. Unsere Recherche ergab, dass es an einem adäquaten Werkzeug für diese Aufgabe mangelt. Einerseits gibt es unzählige Annotationswerkzeuge, welche auf die Darstellung nur eines Textes samt Annotationen fokussieren (Burghardt 2014, Neves and Leser 2014). Auf der anderen Seite gibt es Alignierwerkzeuge, die auf die parallele Darstellung von Texten spezialisiert sind, aber dabei Annotation meist ignorieren, z.B. 
                LF Aligner
            4, 
                Moses
            5 oder 
                ParaVoz
            6. Um diese Lücke zu schließen, haben wir auf der Basis von 
                InterText
            7 — einem im Webbrowser zu bedienenden Alignierwerkzeug (Vondricka 2014) — ein Vergleichswerkzeug für annotierte Parallelkorpora entwickelt. Unsere Erweiterung unterstützt sowohl die Hervorhebung zueinander kompatibler (PoS-)Tags als auch die flexible Darstellung von Lemmata und morpho-syntaktischen Annotationen (
                Abbildung 1). Die dafür nötigen Informationen werden beim Import aus den TEI-XML-Daten extrahiert und mit Hilfe von JavaScript dynamisch visualisiert.
            
         
            
            
               Abbildung 
               : Das Bildschirmfoto zeigt die modifizierte InterText-Ansicht, erkennbar oben an der zusätzlichen Schalterleiste. Links ist die lateinische „Vita Benedicti“ (Vogüe und Antin 1979) zu sehen, annotiert mit dem LASLA Tagset (Denooz 1978), rechts das französische Gegenstück „Vie de saint Benoit“ (Foerster 1876), annotiert mit dem Cattex Tagset (Guillot et al. 2010). Aktuell ist sowohl die Hervorhebung kompatibler PoS-Tags („Toogle color“) als auch die Anzeige der vollständigen PoS-Annotation („Toogle POS (full)“) aktiviert.
            
         
         Neben der eigentlichen Datenaufbereitung ist auch die Optimierung des Annotationsworkflows mit geeigneten Werkzeugen im Sinne verbesserter 
                User Experience ein wesentliches Projektziel (
                tool science, Wolff 2015). 
            
         Die Entwicklung des spätlateinisch-altfranzösischen Tagsets wird im Projekt — auch mit Hilfe unseres modifizierten 
                InterText-Tools — vorangetrieben. In unserem Posterbeitrag erläutern wir das Vorgehen und präsentieren erste Ergebnisse.
            
      
      
         
            
               http://www.palafra.org/
            
            
               http://bfm.ens-lyon.fr/
            
            
               http://universaldependencies.org/
            
            
               http://sourceforge.net/projects/aligner/
            
            
               http://www.statmt.org/moses/
            
            
               https://bitbucket.org/rvwfels/paravoz2
            
            
               http://wanthalf.saga.cz/intertext
            
         
         
            
               Bibliographie
               
                  Burghardt, Manuel (2014):
                        „Engineering annotation usability - Toward usability patterns for linguistic annotation tools“.
                        Diss. Phil., Universität Regensburg, Institut für Information und Medien, Sprache und Kultur, urn:nbn:de:bvb:355-epub-307682.
                    
               
                  Denooz, Joseph (1978):
                        „L’ordinateur et le latin, techniques et methods“,
                        in:
                        Revue de l’Organisation Internationale pour l’Etude des Langues Anciennes par Ordinateur 4.
                    
               
                  Eger, Steffen / vor der Brück, Tim / Mehler, Alexander (2015):
                        „Lexicon-assisted tagging and lemmatization in latin: A comparison of six taggers and two lemmatization methods“,
                        in: 
                        LaTeCH 2015 105.
                    
               
                  Guillot, Céline / Prévost, Sophie / Lavrentiev, Alexei (2010): 
                        Manuel de référence du jeu cattex09. technical manual, UMR ICAR, CNRS/ENS-LSH. 
                        http://bfm.ens-lyon.fr/IMG/pdf/Cattex2009_manuel_2.0.pdf
               
               
                  Neves, Mariana / Leser, Ulf (2014):
                        „A survey on annotation tools for the biomedical literature“,
                        in:
                        Briefings in bioinformatics 15 (2): 327–340.
                    
               
                  Selig, Maria / Eufe, Rembert / Linzmeier, Laura (2015): 
                        CoLaMer (corpus du latin mérovingien). (im Erscheinen).
                    
               
                  Vondricka, Pavel (2014):
                        „Aligning parallel texts with intertext“,
                        in:
                        Proceedings of LREC 2014.
                    
               
                  Vogüé, Adalbert de / Antin, Paul (1979): 
                        GREGOIRE LE GRAND, Dialogues II. 
                        Cambridge University Press.
                    
               
                  Von Foerster, Wendelin (1876): 
                        Li Dialoge Gregoire lo Pape. Altfranzösische Uebersetzung des XII. Jahrhunderts der Dialogen des Papstes Gregor, mit dem lateinischen Original, einem Anhang: Sermo de Sapientia und Moralium in Iob Fragmenta, einer grammatischen Einleitung, erklärenden Anmerkungen und einem Glossar, première partie: Textes. 
                        Paris: Champion.
                    
               
                  Wolff, Christian (2015):
                        „The case for teaching ‚tool science‘. Taking software engineering and software engineering education beyond the confinements of traditional software development contexts“,
                        in: 
                        Global Engineering Education Conference (EDUCON), 2015 IEEE 932–938 10.1109/EDUCON.2015.7096085
                    
            
         
      
   



      
         Das Poster „Interoperabilität bei der Erstellung eines deutschsprachigen Blogkorpus für die Repräsentation der Diskursstruktur“ informiert über das Vorgehen sowie die ersten Forschungszwischenergebnisse und die weiteren Ziele der Kompilierung und Annotation eines deutschsprachigen Blogkorpus. Gegenwärtig gibt es lediglich eine geringe Anzahl an öffentlich zugänglichen, umfangreichen Blogkorpora, wie zum Beispiel das englischsprachige Birmingham Blog Corpus der Birmingham Universität (vgl. WebCorp 2013) oder das bilinguale (deutsch-französische) Korpus d’apprentissage INFRAL (Interculturel Franco-Allemand en Ligne) (vgl. Abendroth-Timmer et al. 2014). Betrachtet man den großen Einfluss von Blogs für die Geschichte der Kommunikation im Internet, erscheint die geringe Anzahl überraschend.
         Bislang existiert kein Standard für das Repräsentieren von sogenannten Computer-Mediated Communication-Daten (kurz CMC), allerdings arbeitet die Text Encoding Initiative CMC Special Interest Group (TEI SIG) (vgl. Beißwenger 2016) seit 2013 an einem Schema für die Repräsentation von CMC-Genres. Eine Standardisierung, wie sie die Text Encoding Initiative für CMC anstrebt, ist ein wichtiger Punkt, wenn es um ‚Digitale Nachhaltigkeit‘ geht. Unser Forschungsvorhaben leistet hierfür einen Beitrag.
         Das Hauptziel des Vorhabens umfasst die semi-automatische Kompilation sowie die Repräsentation der Blogdiskursstruktur. Dabei sollen die Relationen zwischen den textuellen und multimodalen Elementen (Blogbeiträge, Kommentare, Hyperlinks, Bilder und Töne) und den verschiedenen Textproduzenten (Blogger, Kommentatoren) abgebildet werden. Das annotierte Blogkorpus soll am Ende als eine nachhaltige Ressource verfügbar gemacht werden. Hierfür stehen wir momentan in Kontakt mit der Redaktion von Spektrum der Wissenschaft Verlagsgesellschaft mbH, um die Form der Bereitstellung zu klären.
         Die Grundlage des Korpus bildet das Wissenschaftsblogportal SciLogs – Tagebücher der Wissenschaft (SciLogs 2016) und deckt den Inhalt des Jahres 2015 vollständig ab. Die Daten wurden aus den vier SciLogs-Blogbereichen „WissensLogs“, „BrainLogs“, „KosmoLogs“ und „ChronoLogs“ erhoben. Das Korpus soll mit drei Informationstypen annotiert werden, die einerseits direkt, andererseits indirekt in den Blogdaten vorhanden sind oder anhand von statistischen Analysen und computerlinguistischen Tools sichtbar gemacht werden. Zum jetzigen Zeitpunkt beschränken sich die Annotationen des Korpus auf die direkt auslesbaren Informationen des Blogs wie beispielsweise der Titel des Blogbeitrags, der Name des Bloggers und das Einstelldatum des Blogeintrags. Ferner wird darauf geachtet, dass sämtliche Informationen, die die Inhalte der Scilogs-Website in Bezug auf die Bloginhalte liefern, ebenfalls annotiert werden. Wir sind der Meinung, dass auch auf den ersten Blick nicht für die Diskursstruktur relevante Informationen ausgezeichnet werden sollten. Im Fokus steht der Ansatz, dass das Blogkorpus aus CMC-Daten später für die Erforschung unterschiedlicher linguistischer Fragestellungen verwendet werden kann. 
         Zusammenfassend soll das Poster nicht nur unser Vorhaben vorstellen, sondern auch einen Einblick in unser grundsätzliches Vorgehen bei der Erstellung eines CMC-Korpus geben. Der Fokus für die DHd 2017 liegt unter anderem auf der Darstellung der Entscheidungsfindung innerhalb der Auszeichnungssprachen. Es soll erläutert werden, warum wir uns beispielsweise für die deskriptive Auszeichnungssprache TEI und nicht XML (Extensible Markup Language) entschieden haben. Des Weiteren möchten wir Einblicke in die semi-automatische TEI-Annotation geben und unsere Erkenntnisse mit dem vorläufigen, von der TEI SIG bereitgestellten, TEI-Schema teilen. Letztlich wollen wir auch das bisherige Korpus selbst vorstellen, das aus ca. 3.000.000 Tokens (ca. 1.200 Blogposts von 80 Bloggern und 15.000 Kommentaren von 1500 Kommentatoren) besteht.
      
      
         
            
               Bibliographie
               
                  Abendroth-Timmer, Dagmar / Bechtel, Mark / Chanier Thierry / Ciekanski, Maud (2014): 
                        Corpus d’apprentissage INFRAL (Interculturel Franco-Allemand en Ligne). 
                        Banque de corpus CoMeRe. 
                        Nancy: Ortolang.fr 
                        https://hdl.handle.net/11403/comere/cmr-infral [letzter Zugriff 1. Juli 2016].
                    
               
                  Beißwenger, Michael (2016):
                        SIG: Computer-Mediated Communication
                  http://wiki.tei-c.org/index.php/SIG:Computer-Mediated_Communication [letzter Zugriff 1. Juli 2016].
                    
               
                  SciLogs (2016):
                        SciLogs: Tagebücher der Wissenschaft. 
                        Spektrum der Wissenschaft Verlagsgesellschaft mbH 
                        http://www.scilogs.de/impressum/ [letzter Zugriff 1. Juli 2016].
                    
               
                  WebCorp (2013):
                        Birmingham Blog Corpus. WebCorp: Linguist’s Search Engine. 
                        Birmingham City University
                        http://wse1.webcorp.org.uk/cgi-bin/BLOG/index.cgi[letzter Zugriff 1. Juli 2016].
                    
            
         
      
   



      
         
            Zielstellung
            Digitale Nachhaltigkeit verstehen wir als eine komplexe Anforderung, die aus besonders vielen Blickwinkeln betrachtet werden kann und sollte. Wir stellen mit dem LAUDATIO-Projekt
                    1 eine Möglichkeit vor, digitale Nachhaltigkeit herzustellen, in dem wir einen unabhängigen und freien Zugriff auf historische Korpora zum Zweck der Wiederverwendung für Forschungszwecke, die über diejenigen hinausgehen, für die die Daten ursprünglich gesammelt wurden, ermöglichen. Eine so definierte digitale Nachhaltigkeit realisieren wir, in dem wir eine Vielzahl an verschiedenen, teils sehr unterschiedlichen historischen Korpusdaten inklusive einer umfangreichen aber einheitlichen Dokumentation, die ihre Erschließbarkeit in Bezug auf konkrete Nutzungsszenarien gewährleistet, bereitstellen. Aus dieser Arbeit zeigt sich, dass eine interdisziplinäre und insbesondere lokale institutionelle Zusammenarbeit mit den Korpuserstellern notwendig ist, die einen engen kommunikativen Austausch von Zielen und Anforderungen diesbezüglich sowie eine Identifikation von möglichen Kooperationen erst ermöglicht. Digitale Nachhaltigkeit definiert sich dementsprechend aus der jeweiligen Perspektive der Daten, der Dokumentation und der Institutionen ein wenig anders. Diese drei Perspektiven und deren Zusammenspiel in Bezug auf die digitale Nachhaltigkeit wollen wir anhand von historischen Korpora (vgl. Claridge 2008; Kytö 2011; Gippert und Gehrke 2015) diskutieren und an einem Best-Practice-Beispiel einer interdisziplinären Zusammenarbeit der Philosophischen Fakultät und dem Rechenzentrum der Humboldt-Universität zu Berlin (HU) erklären. 
                
         
         
            Heterogenität von Forschungsdaten 
            Die vielfältige Datenlandschaft in den Geisteswissenschaften stellt eine große Herausforderung in Bezug auf die digitale Nachhaltigkeit dar, da es unterschiedliche Datenmodelle und -formate und verschiedene Aufbereitungs- und Analyseverfahren gibt, die alle aus immer neuen Nutzungsszenarien und ihren Forschungsfragen, resultieren. In diesem Sinne verstehen wir die Arbeit mit Korpora als eine innovative, fortlaufende, wissenschaftliche Arbeit, die sich nicht ausschließlich auf existierende Standards stützen kann (auch wenn solche Standards natürlich immer beachtet werden müssen). Es werden daher zusätzlich andere, neue Forschungsdatenmodelle und -formate sowie Ressourcentypen entwickelt und auf die unterschiedlichsten Weisen weiter- und wiedergenutzt. Beispielsweise werden in den Projekten, die LAUDATIO unterstützt, eine Vielzahl an allein XML-basierten Formaten (nach z.B. Dipper 2005; Schmidt und Wörner 2009; Romary et al. 2015; TEI Consortium 2015) sowie CSV-basierte Formate (nach z.B. Nivre, Hall und Nilsson 2004; Krause und Zeldes 2016) für die Erstellung von historischen Korpora genutzt. Daneben finden auch und graphbasierten Lösungen (nach z.B. Ide und Suderman 2014) sowie proprietäre Formate, wie bei Ágel und Hennig (2007)
                    2, und immer häufiger JSON-basierte Formate wie bei Vertan et al. (2016) Anwendung. Zusätzlich dazu können Korpora in mehreren Formaten vorliegen, vor allem wenn Korpora in einer Mehrebenenarchitektur (vgl. Romary und Ide 2004; Lüdeling 2012) entworfen und verschiedene Formate für unterschiedliche Abschnitte des Forschungsdatenzyklus (vgl. Rümpel 2011) genutzt werden.
                
            Der innovative Charakter der Korpuserstellung zeigt sich neben den verwendeten Formaten auch in den Annotationsrichtlinien: Einige Formate wie TIGER-XML (Romary et al. 2015) und tcf (Heid et al. 2010) legen die Anzahl und die Bedeutung der Annotationen inklusive ihrer Tagsets fest, andere wie TEI-XML (TEI Consortium 2015) lassen Spielraum in der Serialisierung und in der Anwendung, in dem es beispielsweise mehrere valide Möglichkeiten gibt, Autoren in einem Dokument auszuweisen.
                    3 Wieder andere Formate wie EXMARaLDA-XML (Schmid und Wörner 2009) oder PAULA-XML (Dipper 2005) geben keinerlei solcher Beschränkungen vor.
                
            Insbesondere bei solchen Formaten mit größtmöglichem Spielraum für Interpretationen in Form von Annotationen zeigt sich die Vielfältigkeit der korpusbasierten Forschung. Ein typisches Annotationsbeispiel für historische Korpora sind Normalisierungen. Viele Korpora besitzen eigene Richtlinien für die Normalisierung, vgl. zum Beispiel für historisches Deutsch Jurish (2010), Bollmann et al. (2012); Odebrecht et al., (eingereicht). Auch für verschiedene Forschungsfragen wesentliche Kategorisierungen wie Wortarten gibt es annähernd für jedes historische Korpus eine eigene Lösung. So werden beispielsweise de-facto-Standards wie das STTS (Schiller et al. 1999) jeweils für ein Korpus angepasst, z.B. Dipper et al. (2013)
                    4 oder Fürstinnenkorresondenzkorpus
                    5. 
                
            Diese Beispiele zeigen, dass sich Korpora desselben Formats und ein Korpus, das in verschiedenen Formaten vorliegt, in Bezug auf ihre Annotationen stark unterscheiden können.
            Ausgehend von dieser Datenlage erscheint die Nachvollziehbarkeit des Lebenszyklus der Korpora als ein weiterer wesentlicher Faktor für die digitale Nachhaltigkeit. Welche der bisher genutzten Formate und Annotationskonzepte sich langfristig durchsetzen, welche Formate wie technisch unterstützt und welche neuen Lösungen entwickelt werden, hängt dann im Wesentlichen von der Entwicklung der korpusbasierten Forschung ab. Daher setzen wir eine Archivierung und Dokumentation aller verwendeten Formate und Annotationen eines Korpus in LAUDATIO um. 
         
         
            Metadaten
            Über einheitliche extensive Metadaten dieser heterogenen Korpusdaten und deren Lebenszyklus kann eine umfangreiche Korpusdokumentation sowie eine einheitliche Suche und ein gezielter Zugriff über eine Plattform auf die Forschungsdaten erstellerunabhängig gewährleistet werden (Bird und Simons 2001; Broeder et al. 2010; Burnard 2013; Hedges et al. 2013; Odebrecht et al. 2015). Die relevanten Kriterien für die Dokumentation und die Suche leiten sich aus den Wiederverwendungsszenarien ab (Odebrecht 2014). Um eine überfachliche Suche zu ermöglichen, wird in LAUDATIO eine technisch-abstrakte Modellierung der Metadaten eingesetzt, um die fachspezifischen Konzepte von Korpora überfachlich abzubilden (Odebrecht 2015). Neben den deskriptiven Metadaten sind für eine nachhaltige Vorhaltung von Forschungsdaten auch administrative, strukturelle, technische und Archivierungsmetadaten relevant (vgl. Xie und Matusiak 2016; Solodovnik 2011; NISO 2004), die für eine technische Infrastruktur berücksichtigt werden müssen, um die Nachvollziehbarkeit über längere Zeiträume und wechselnde Anwendergruppen hinweg gewährleisten zu können. So ermöglicht das einheitliche Metadatenmodell eine extensive und transparente Informationsarchitektur für die unterschiedlichen Ressourcen, was wiederum ein Baustein für deren digitale Nachhaltigkeit darstellt.
         
         
            Institutionelle (Zusammen-)Arbeit
            Bedingt durch die unterschiedlichen Sichtweisen und Anforderungen zur digitalen Nachhaltigkeit aus den Fachdisziplinen und durch die technische Infrastruktur sind Lösungen nur im Team und in Zusammenarbeit unterschiedlicher Kompetenzen plan- und erstellbar. Im Beispiel ist für den erstellerunabhängigen Zugang zu historischen Korpora die enge Zusammenarbeit zwischen den FachwissenschaftlerInnen und LAUDATIO erforderlich. Der Computer- und Medienservice (CMS) und die Arbeitsgruppe Korpuslinguistik am Institut für deutsche Sprache und Linguistik der HU setzen mit dem LAUDATIO-Projekt einen Schwerpunkt auf eine enge institutionelle Zusammenarbeit mit den verschiedenen Arbeitsgruppen der philosophischen Fakultät, in dem es die Anforderungen für die digitale Nachhaltigkeit von Forschungsdaten der Fakultät mit den ForscherInnen erarbeitet und umsetzt und in Bezug auf die Hochschule als Ganzes in einen Entwicklungsrahmen einbettet (Dreyer und Vollmer 2016). Der Betrieb des LAUDATIO-Repositoriums wird nach Ende des Projektes, in dem Entwicklungen und Anpassungen am System vorgenommen werden, weiterhin durch das CMS sowie durch die Arbeitsgruppe Korpuslinguistik am Institut für deutsche Sprache und Linguistik gewährleistet. Um die heterogene Datenlandschaft zu verstehen, die umfassende Dokumentation zu erstellen und die neuen Entwicklungen aufzunehmen, erweist sich eine lokale Zusammenarbeit über die Fakultäten hinweg als sehr vorteilhaft. So bestehen enge Kooperationen unter anderem mit Projekten
            
               der Sprachgeschichte: DDD-AD
                        6, HIPKON
                        7, DDB
                        8, Fürstinnenkorrespondenzkorpus
                        9
               
               der Germanistik: Märchenkorpus
                        10 und RIDGES
                        11
               
               der Slawistik: “Korpuslinguistik und diachrone Syntax: Subjektkasus, Finitheit und Kongruenz in slavischen Sprachen”
                        12
               
            
            die jeweils sehr unterschiedliche Arbeitsweisen und Zielrichtungen haben. Durch diese Synergiebildung können Projekte ohne gesonderte Finanzierung oder Ressourcen nach ihrem Projektende durch eine genaue Kenntnis der Forschungsdatenlandschaft in einer Institution identifiziert und unterstützt werden. Die durch die Förderer (z.B. Deutsche Forschungsgemeinschaft 2015) oder Universitäten (z.B. Humboldt-Universität zu Berlin 2014) vorgegeben Richtlinien zur Veröffentlichung und Archivierung von Projektergebnissen können so ebenfalls berücksichtigt werden. 
            Mit diesem Ansatz können gleichzeitig zwei Ziele erreicht werden: Die Projekte müssen sich keinen umfangreichen semantischen Anforderungen unterwerfen und können sich frei ausdrücken. Gleichzeitig können sie ihre fachspezifischen Anforderungen direkt an die Arbeitsgruppe Korpuslinguistik bzw. an den CMS richten. Andersherum können die Korpusprojekte in Umfragen zur gewünschten Anforderungen und Softwarelösungen direkt befragt und in die Entwicklung mit einbezogen werden.
                    13 Auf diese Art findet ein Community-Aufbau statt, der sich nicht nur über gemeinsame technische Plattformen definiert, sondern rein über die inhaltliche Gemeinsamkeit der Arbeit mit historischen Korpora und somit auch über disziplinäre Grenzen hinweg. 
                
         
         
            Einordnung und Schlussfolgerungen
            Der hier vorgestellte Weg, digitale Nachhaltigkeit von Forschungsdaten zu ermöglichen, stützt sich auf eine Spezialisierung auf einen bestimmten Typ von Forschungsdaten - historisches Korpus - und grenzt sich so von Ansätzen wie Zenodo
                    14 und dem Virtual Language Observatory (Van Uytvanck 2012) ab, die keine deutliche Eingrenzung hinsichtlich der Daten und deren Nutzungsszenarien machen. 
                
            Weiterhin zielt unsere Strategie auf die Unterstützung der Diversität der genutzten Formate und Konzepte, die sich von den TEI spezialisierten Ansätzen wie dem Deutschen Textarchiv (Geyken 2013) und Textgrid (Hedges et al. 2013) unterscheiden. 
            Die Korpusersteller selbst nutzen LAUDATIO auch mehr und mehr, um ihre eigenen neuen Versionen der historischen Korpora und damit ihren wissenschaftlichen Fortschritt zu veröffentlichen. Dass unser Ansatz, sich auf die erstellerunabhängige Wiederverwendung von Korpora als eine Strategie für digitale Nachhaltigkeit zu fokussieren, auch außerhalb der Institution funktioniert, zeigt Dumont (2016).
            Um die unterschiedlichen Entwicklungen und Innovationen bei der Korpuserstellung zu identifizieren, kennenzulernen und zu dokumentieren, ist eine enge, auf eine rein fachliche Ebene bezogene Zusammenarbeit mit der Community notwendig, die wir angefangen haben, im Rahmen der Philosophischen Fakultät der HU aufzubauen. 
         
      
      
         
             LAUDATIO steht für 
                            Long-term 
                            Access and 
                            Usage of 
                            Deeply 
                            Annotated Informa
                            tion. 
                            
                  www.laudatio-repository.org
               . Zugriff am 16.08.2016.
                        
             Ágel, Vilmos; Hennig, Mathilde; KAJUK (Version 1.1), Justus-Liebig-Universität Gießen. http://www.uni-giessen.de/kajuk/index.htm.
             Wie es zum Beispiel mit den Element  möglich ist, vgl. 
                            http://www.tei-c.org/release/doc/tei-p5-doc/de/html/ref-author.html Zugriff am 19.08.2016. 
                        
             Donhauser, Karin; Gippert, Jost; Lühr, Rosemarie; ddd-ad (Version 0.1), Humboldt-Universität zu Berlin. https://referenzkorpusaltdeutsch.wordpress.com/.
                            
               
                  http://hdl.handle.net/11022/0000-0000-7FC2-7
               
            
             Lühr, Rosemarie; Faßhauer, Vera; Prutscher, Daniela; Seidel, Henry; Fuerstinnenkorrespondenz (Version 1.1), Universität Jena, DFG. http://www.indogermanistik.uni-jena.de/Web/Projekte/Fuerstinnenkorr.htm.
            
               
                  http://www.deutschdiachrondigital.de/
                Zugriff am 16.08.2016.
                            
            
               
                  https://www.linguistik.hu-berlin.de/de/institut/professuren/sprachgeschichte/forschung/sfb632-informationsstruktur
                Zugriff am 16.08.2016.
                            
            
               
                  http://korpling.german.hu-berlin.de/ddb-doku/index.htm
                Zugriff am 16.08.2016.
                            
            
               
                  http://dwee.eu/Rosemarie_Luehr/?Projekte___DFG-Projekte___Fruehneuzeitliche_Fuerstinnen korrespondenz_im_mitteldeutschen_Raum
                Zugriff am 16.08.2016.
                            
             Die Erstellung des Korpus erfolgte im Rahmen von universitärer Lehre 
                                http://www.textbewegung.de/lehre.html Zugriff am 16.08.2016.
                            
            
               
                  https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/forschung/ridges-projekt/
                Zugriff am 16.08.2016.
                            
            
               
                  https://www.slawistik.hu-berlin.de/de/member/meyerrol/subjekte/corpora
                Zugriff am 16.08.2016.
                            
             Zusätzlich helfen uns auch Evaluationen mit Kooperationspartner durch Dritte, die angebotenen Lösungen zu verbessern (z.B. Stiller et al. 2016).
            
               http://zenodo.org Zugriff am 19.08.2016.
                        
         
         
            
               Bibliographie
               
                  Ágel, Vilmos / Hennig, Mathilde (eds.) (2007):
                        Zugänge zur Grammatik der gesprochenen Sprache. Germanistische Linguistik 269.
                        Tübingen: Niemeyer.
                    
               
                  Bird, Steven / Simons, Gary (2001): 
                        „The OLAC Metadata Set and Controlled Vocabularies“, 
                        in: 
                        Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics 7–18 
                        arXiv:cs/0105030v1.
                    
               
                  Bollmann, Marcel / Dipper, Stefanie / Krasselt, Julia / Petran, Florian (2012): 
                        „Manual and semi-automatic normalization of historical spelling - case studies from Early New High German“, in: 
                        Proceedings of KONVENS 2012 342–350 
                        http://www.oegai.at/konvens2012/proceedings/51_bollmann12w/ [letzter Zugriff 22 August 2016].
                    
               
                  Broeder, Daan / Kemps-Snijders, Marc / Van Uytvanck, Dieter / Windhouwer, Menzo / Withers, Peter / Wittenburg, Peter / Zinn, Claus (2010): 
                        „A data category registry- and component-based metadata framework“, 
                        in: 
                        Proceedings of LREC 2010 43–47.
                    
               
                  Burnard, Lou (2013): 
                        „The Evolution of the Text Encoding Initiative: From Research Project to Research Infrastructure“, 
                        in: 
                        Journal of the Text Encoding Initiative 5: 1–13 10.4000/jtei.811.
                    
               
                  Claridge, Claudia (2008): 
                        „Historical Corpora“, 
                        in: Lüdeling, Anke / Kytö, Merja (eds): 
                        Corpus Linguistics. An International Handbook 1. 
                        Berlin: De Gruyter 242–259.
                    
               
                  Dipper, Stefanie (2005): 
                        „XML-based Stand-off Representation and Exploitation of Multi-Level Linguistic Annotation“, in: 
                        Proceedings of Berliner XML Tage 39–50.
                    
               
                  Dipper, Stefanie / Donhauser, Karin / Klein, Thomas / Linde, Sonja / Müller, Stefan / Wegera, Klaus-Peter (2013): 
                        „HiTS. Ein Tagset für historische Sprachstufen des Deutschen“, 
                        in: Zinsmeister, Heike / Heid, Ulrich / Beck, Kathrin (eds.): 
                        Das Stuttgart-Tübingen Wortarten-Tagset: Stand und Perspektiven. Journal for Language Technology and Computational Linguistics 28(1) 85–137.
                    
               
                  Deutsche Forschungsgemeinschaft (2015): 
                        Leitlinien zum Umgang mit Forschungsdaten. 
                        Bonn: DFG. 
                        http://www.dfg.de/download/pdf/foerderung/antragstellung/forschungsdaten/richtlinien_forschungsdaten.pdf [letzter Zugriff 22. August 2016].
                    
               
                  Dreyer, Malte / Vollmer, Andreas (2016): 
                        „An Integral Approach to Support Research Data Management at the Humboldt-Universität zu Berlin“, 
                        in: 
                        Proceedings of the European University Information Systems organisation 320–327.
                    
               
                  Dumont, Stefan (2016): 
                        „Fürstinnenkorrespondenzen. Experiment einer Nachnutzung“, 
                        in: 
                        Entwicklung und Nutzung interdisziplinärer Repositorien für historische textbasierte Korpora. DHd 2016 Workshop
                  http://www.laudatio-repository.org/laudatio/workshop-dhd2016/ [letzter Zugriff 22. August 2016].
                    
               
                  Gippert, Jost / Gehrke, Ralf (eds.) (2015): 
                        Historical Corpora. Korpuslinguistik und interdisziplinäre Perspektiven auf Sprache 5. 
                        Tübingen: Narr.
                    
               
                  Geyken, Alexander (2013): 
                        „Wege zu einem historischen Referenzkorpus des Deutschen. das Projekt Deutsches Textarchiv“, 
                        in: Hafemann, Ingelore (ed.): 
                        Perspektiven einer corpusbasierten historischen Linguistik und Philologie. Internationale Tagung des Akademienvorhabens "Altägyptisches Wörterbuch“ an der BBAW. Thesaurus Linguae Aegyptiae, 4: 221–234.
                    
               
                  Hedges, Mark / Neuroth, Heike / Smith, Kathleen M. / Blanke, Thomas / Romary, Laurent / Küster, Marc / Illingworth, Malcom (2013): 
                        „TextGrid, TEXTvre, and DARIAH. Sustainability of Infrastructure for Textual Scholarship“, in: 
                        Journal of the Text Encoding Initiative 5: 1–13.
                    
               
                  Heid, Ulrich / Schmid, Helmut / Eckart, Kerstin / Hinrichs, Erhard W. (2010): 
                        „A Corpus Representation Format for Linguistic Web Services. The D-SPIN Text Corpus Format and its Relationship with ISO Standards“, 
                        in: 
                        Proceedings of the Seventh International Conference on Language Resources and Evaluation 494–499.
                    
               
                  Humboldt-Universität zu Berlin (2014): 
                        Grundsätze zum Umgang mit Forschungsdaten an der Humboldt-Universität zu Berlin. Unter Mitarbeit von Elena Simukovic 
                        https://www.cms.hu-berlin.de/de/ueberblick/projekte/dataman/hu-fdt-policy/view [letzter Zugriff 6. Juni 2016].
                    
               
                  Ide, Nancy / Sudermann, Keith (2014): 
                        „The Linguistic Annotation Framework. a standard for annotation interchange and merging“,
                        in: 
                        Language Resources and Evaluation 48 (3): 395–418.
                    
               
                  Jurish, Bryan (2010): 
                        „More than Words: Using Token Context to Improve Canonicalization of Historical German“,
                        in: 
                        Journal for Language Technology and Computational Linguistics 25 (1): 23–40.
                    
               
                  Krause, Thomas / Zeldes, Amir (2016): 
                        „ANNIS3. A new architecture for generic corpus query and visualization“, 
                        in: 
                        Digital Scholarship in the Humanities 31 (1): 118–139 10.1093/llc/fqu057.
                    
               
                  Kytö, Merja (2011): 
                        „Corpora and historical linguistics“, 
                        in: 
                        Revista Brasileira de Linguistica Aplicada 11: 417–457 10.1590/S1984-63982011000200007.
                    
               
                  Lüdeling, Anke (2012): 
                        „A corpus-linguistics perspective on language documentation, data, and the challenge of small corpora“, 
                        in: Seifart, Frank / Haig, Geoffrey / Himmelmann, Nikolaus P. / Jung, Dagmar / Margetts Anna / Trilsbeek, Paul (eds.): 
                        Potentials of Language Documentation. Methods, Analyses, and Utilization 4. Language Documentation & Conservation Special Publication 3. 
                        Hawaii: University of Hawai‘i Press 32–38.
                    
               
                  NISO (2004): 
                        Understanding Metadatada. 
                        Bethesda: NISO Press 
                        http://www.niso.org/publications/press/UnderstandingMetadata.pdf [letzter Zugriff 13. Februar 2015].
                    
               
                  Nivre, Joakim / Hall, Johan / Nilsson, Jens (2004): 
                        „Memory-Based Dependency Parsing“,
                        in: 
                        Proceedings of the Eighth Conference on Computational Natural Language Learning 49–56. 
                    
               
                  Odebrecht, Carolin / Belz, Malte / Zeldes, Amir / Lüdeling, Anke / Krause, Thomas (eingereicht):
                        RIDGES Herbology - Designing a Diachronic Multi-Layer Corpus. 
                        Vorversion
                        https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/forschung/ridges-projekt/download-files/pubs/odebrechtetaleingereicht_ridgesherbology.pdf/at_download/file [letzter Zugriff 22. August 2016].
                    
               
                  Odebrecht, Carolin (2014): 
                        „Modeling Linguistic Research Data for a Repository for Historical Corpora“,
                        in: 
                        DH2016: Book of Abstracts 284–285.
                    
               
                  Odebrecht, Carolin (2015): 
                        „Interdisziplinäre Nutzung von Forschungsdaten mithilfe einer technisch-abstrakten Modellierung“, 
                        in: 
                        DHd 2015: Von Daten zu Erkenntnissen
                  https://dh2014.files.wordpress.com/2014/07/dh2014_abstracts_proceedings_07-11.pdf [letzter Zugriff 22. August 2016].
                    
               
                  Odebrecht, Carolin / Krause, Thomas / Lüdeling, Anke (2015):
                        „Austausch von historischen Texten verschiedener Sprachen über das LAUDATIO-Repository“,
                        in: 
                        DGfS-CL Poster Session. 37. Jahrestagung der Deutschen Gesellschaft für Sprachwissenschaft
                  http://asvdoku.informatik.uni-leipzig.de/dgfs2015cl-ps/index.html [letzter Zugriff 22. August 2016].
                    
               
                  Romary, Laurent / Ide, Nancy (2004): 
                        „International standard for a linguistic annotation framework“,
                        in: 
                        Natural Language Engineering 10 (3-4): 211–225.
                    
               
                   Romary, Laurent / Zeldes, Amir / Zipser, Florian (2015):
                        „: serialising the ISO SynAF syntactic object model“, 
                        in: 
                        Language Resources and Evaluation 49 (1): 1–18.
                    
               
                  Rümpel, Stefanie (2011): 
                        „Der Lebenszyklus von Forschungsdaten“, 
                        in: Büttner, Stephan / Hobohm, Hans-Christoph / Müller, Lars (eds.): 
                        Handbuch Forschungsdatenmanagement. 
                        Bad Honnef: Bock + Herchen 25–34.
                    
               
                  Schmidt, Thomas / Wörner, Kai (2009): 
                        „EXMARaLDA. Creating, analysing and sharing spoken language corpora for pragmatic research”, 
                        in: 
                        Pragmatics 19 (4): 565–582.
                    
               
                  Schiller, Anne / Teufel, Simone / Stöckert, Christine / Thielen, Christine (1999): 
                        „Guidelines für das Tagging deutscher Textkorpora mit STTS“, 
                        in: Universität Tübingen (ed.): Seminar für Sprachwissenschaft. Technischer Report. 
                        http://www.sfs.uni-tuebingen.de/resources/stts-1999.pdf [letzter Zugriff 22. August 2016]. 
                    
               
                  Solodovnik, Iryna (2011): 
                        „Metadata issues in Digital Libraries. key concepts and perspectives“,
                        in: 
                        Italian Journal of Library, Archives and Information Science 2 (2): 4663-1–4663-27. 10.4403/jlis.it-4663.
                    
               
                  Stiller, Juliane / Thoden, Klaus / Zielke, Dennis (2016): 
                        „Usability in den Digital Humanities am Beispiel des LAUDATIO-Repositoriums“,
                        in: 
                        DHd 2016: Modellierung - Vernetzung - Visualisierung 244–247.
                    
               
                  TEI Consortium (2015): 
                        TEI P5: Guidelines for Electronic Text Encoding and Interchange (2.9.1). 
                        http://www.tei-c.org/Guidelines/P5/ [letzter Zugriff 15. November 2015].
                    
               
                  Van Uytvanck, Dieter / Stehouwer, Herman / Lempen, Lari (2012):
                        ”Semantic metadata mapping in practice: the Virtual Language Observatory“, 
                        in: 
                        Proceedings of LREC 2012 1029–1033.
                    
               
                  Vertan, Cristina / Ellwardt, Andreas / Hummerl, Susanne (2016):
                        „Ein Mehrebenen-Tagging-Modell für die Annotation altäthiopischer Texte“, 
                        in: 
                        DHd 2016: Modellierung - Vernetzung - Visualisierung 258–261.
                    
               
                  Xie, Iris / Matusiak, Krystyna (2016): 
                        Discover Digital Libraries. Theory and Practice. 
                        Oxford: Elsevier.
                    
            
         
      
   



      
         Mit „Ulysses: A Critical and Synoptic Edition“ erschien 1984 eine der ersten Forschungseditionen, die auf Basis der systematischen Verwendung von Kollationierungssoftware digital erzeugt wurde. Das Münchner Team um Hans Walter Gabler verwendete hierzu TUSTEP sowohl zur Validierung der Transkripte einzelner Zeugen als auch zur Erschließung der zeugenübergreifenden Synopse. Für die gedruckte Edition wurden die halbautomatisch erzeugten Kollationsergebnisse mit einem eigens entwickelten System komplexer Diakritika ausgezeichnet, die es dem geübten Leser ermöglichen sollten, die Textentstehung über stellenweise mehr als zwanzig inter- und intradokumentarische Textstufen hinweg in einer synoptisch integrierten Textfassung nachzuvollziehen. Während die Konzeption und Umsetzung dieser Arbeit bis heute als bahnbrechend im Bereich der Computerphilologie zu bezeichnen ist, konnte das Potenzial der resultierenden Druckausgabe für die Joyce-Forschung nicht annähernd ausgeschöpft werden. Zu komplex war das Markup, dem es gelingen sollte, zu verknüpfen, was zuvor getrennt war und zu hoch war der Aufwand, sich in diese Systematik einzuarbeiten.
         Im Digitalen hingegen führten die Daten jene Odyssee fort, die die Druckedition beenden sollte. Auf der Suche nach einem Markup-Standard, der es vermag, die Inhalte der Druckedition digital zu repräsentieren, wurden die TUSTEP Ergebnisse zunächst von Tobias Rischer im Rahmen seiner Diplomarbeit (1997) in SGML/TEI transformiert und anschließend in mehreren Überarbeitungen über TEI P4 bis hin zur aktuellen Version der TEI P5v3 (2016) migriert. Dieser Beitrag vollzieht die Evolution dieser “Legacy Data” nach, bis hin zu ihrer jüngsten Station - der noch andauernden Bemühung einer Migration nach TEI P5v3, welche im Rahmen des DFG- und NEH-geförderten Kooperationsprojektes “Diachronic Markup and Presentation Practices for Text Editions in Digital Research Environments” am Lehrstuhl für Digital Humanities der Universität Passau durchgeführt wird. 
         Erstmals seit der zweiten, überarbeiteten Ausgabe der synoptisch-kritischen Gabler Edition 1986 gelang es, aus den TEI-Daten die synoptische Visualisierung der Druckedition zu rekonstruieren und somit eine Konsistenzprüfung gegen die ursprünglichen Daten zu ermöglichen. Erst durch diese visuelle Rückführung offenbarten sich migrationsbedingte Fehler und Provisorien, welche zuvor, wenn überhaupt, nur in Fußnoten und privaten Aufzeichnungen vergangener Beteiligter dokumentiert wurden. Neben dem allgemeinen Versuch, die vollzogenen Änderungen aus den Aufzeichnungen und Migrationsergebnissen früherer Projekte zu rekonstruieren, hat es sich das Passauer Team zur Aufgabe gemacht, Strategien zur Entdeckung, Typisierung und Korrektur derartiger „Migrationsverluste“ zu entwickeln. Ein wesentlicher Bestandteil dieser Arbeit ist die Abschätzung der Leistungsfähigkeit und Wirtschaftlichkeit von automatisierten Batch-Konvertierungen mittels XSLT und Python im Vergleich zur manuellen Intervention und Korrektur der Kodierung. 
         Neben der Identifikation und Korrektur von „Migrationsfehlern“, steht die Rekonstruktion der textgenetischen Perspektive, durch welche sich die Druckedition auszeichnete, im Vordergrund. Während Gabler die textuelle Entwicklung, welche er mittels der Kollation chronologisch aufeinander folgender Textzeugen erschlossen hatte, im Druck synoptisch darstellen konnte, beinhalteten die TEI Guidelines bis zur Version P5v2 kein Modell zur Auszeichnung textgenetischer Prozesse. Es fehlte schlicht die Möglichkeit zur formalisierten Dokumentation einer stufenweisen, zeugenübergreifenden Chronologie der Textentwicklung. In der Druckedition wurde jeder auktorialen Textänderung 
                genau eine Textstufe aus der heuristisch erschlossenen Chronologie zugeordnet. Diese lineare Textentwicklung über intra- und interdokumentarische Textstufen, in Gablers Terminologie auch Overlay und Level genannt, musste im Digitalen in eine Auszeichnung überführt werden, welche die Genese in den Hintergrund rückt und zu jeder auktorialen Modifikation anstelle einer Textstufe eine Liste sämtlicher Zeugen verzeichnet, auf welcher die spezifische Änderung Bestand hat. Diese Art der dokumentenorientierten Kodierung von Textgenese entspricht zwar bis heute der gängigen Auszeichnungspraxis historisch-kritischer Editionsprojekte, repräsentierte aber zu keinem Zeitpunkt die textgenetische Intension der 84er 
                Ulysses Edition. Erst mit der Integration eines textgenetischen Modells in die TEI Guidelines, kann die ursprüngliche Intension erstmals auch in TEI kodiert werden. Hierzu bedarf es einer weiteren Episode der Datenmigration auf der Odyssee zum richtigen Standard. 
            
      
      
         
            
               Bibliographie
               
                  Brüning, Gerrit / Henzel, Katrin / Pravida, Dietmar (2014): 
                        „Multiple Encoding in Genetic Editions: The Case of Faust“,
                        in: 
                        Journal of the Text Encoding Initiative 4. Available from: jtei.revues.org.
                    
               
                  Burnard, Lou / O’Brien O’Keeffe, Katherine / Unsworth, John (2006): 
                        Electronic Textual Editing.
                        New York: Modern Language Association of America.
                    
               
                  Burnard, Lou / Jannidis, Fotis / Pierazzo, Elena / Midell, Gregor / Rehbein, Malte (2010): 
                        „An Encoding Model for Genetic Editions“,
                        in:
                        TEI: Text Encoding Initiative. 
                        Retrieved from www.tei-c.org/ Activities/Council/Working/tcw19.html/. 
                    
               
                  Joyce, James / Gabler, Hans Walter (eds.) (1984): 
                        Ulysses: A Critical and Synoptic Edition.
                        New York: Garland.
                    
               
                  Joyce, James (1922): 
                        Ulysses. 
                        Paris: Shakespeare and Company.
                    
               
                  Fordham, Finn (2010): 
                        I do, I undo, I redo: The Textual Genesis of Modernist Selves in Hopkins, Yeats, Conrad, Forster, Joyce, and Woolf. 
                        Oxford / New York: Oxford University Press.
                    
               
                  Rischer, Tobias (1997): 
                        Eine TEI/SGML-Edition der textkritischen Ausgabe von James Joyces Ulysses. 
                        Diplomarbeit, LMU München.
                    
               
                  TEI Consortium (eds.) (2016): 
                        TEI P5: Guidelines for Electronic Text Encoding and Interchange. P5v3.
                        Available from: http://www.tei-c.org/Guidelines/P5/.
                    
            
         
      
   



      
         Bei der Planung und Durchführung von langfristigen (Editions-)Projekten stellen sich hinsichtlich der Nachhaltigkeit besondere Herausforderungen, da sich die technischen Entwicklungen der nächsten Jahre und Jahrzehnte eben nur bedingt vorausahnen lassen. Gerade in der Anfangsphase müssen aber bereits zahlreiche, oftmals richtungsweisende Entscheidungen getroffen werden, die den zukünftigen Erfolg oder Misserfolg, potenziell auftretende Probleme und Lösungsmöglichkeiten determinieren. Allerdings scheint es auch überhaupt nur in Projekten mit langer Laufzeit möglich, jenseits von reinen Willensbekundungen umfassende Strategien zur Nachhaltigkeit zu entwickeln und entsprechende Maßnahmen zu ergreifen. Damit leitet sich gleichzeitig die Pflicht ab, dies auch zu tun.
         Am Beispiel des Projektes 
                Capitularia, einem Langzeitprojekt (2014-2029), welches mit einer Hybrid-Edition frühmittelalterlicher Herrschererlasse befasst ist, sollen die verschiedenen Ebenen digitaler Nachhaltigkeit, damit verbundene Herausforderungen sowie erste Lösungsansätze präsentiert werden. Der von uns vorgeschlagene und in Teilen bereits umgesetzte Maßnahmenkatalog betrifft die Ebenen
            
         
            Datenmodellierung und Textauszeichnung,
            Datenhaltung und Dokumentation,
            Infrastrukturen (technisch und institutionell),
            Webseite und verwendete (Web-)Technologien, 
            Präsentation, Zugänglichkeit und Nachnutzbarkeit der Forschungsergebnisse.
         
         Bei der 
                Textauszeichnung sollte auf etablierte Standards zurückgegriffen werden, um die programm- und plattformunabhängige Weiterverarbeitung der Daten langfristig zu gewährleisten. Bei 
                Capitularia werden Transkriptionen, Handschriftenbeschreibungen, Register und bibliographische Daten gemäß der im Projekt erarbeiteten Richtlinien in 
                TEI-XML codiert und durch ein den projektspezifischen Anforderungen angepasstes, restriktives Schema sowie den Einsatz von 
                Schematron (ISO/IEC 19757-3) kontrolliert, um damit über die gesamte Projektlaufzeit hinweg und auch bei wechselndem Personal die Konsistenz und Einheitlichkeit und damit die Qualität der erstellten Daten zu sichern. (Hedler u.a. 2011: 11-12)
            
         Dies bedingt von Beginn an eine 
                umfassende Dokumentation, die nicht nur z.B. in Form eines Wikis vor allem für interne Zwecke verwendet wird, sondern auch möglichst viele Informationen öffentlich zugänglich macht, so dass andere Projekte von den Erfahrungen profitieren können. Die Entwicklung und Rationalisierung von Arbeitsprozessen sowie deren genaue Darlegung gewährleisten dabei neuen Mitarbeitern einen möglichst einfachen Einstieg. Für die öffentliche Dokumentation bieten sich neben der eigenen Projektwebseite beispielsweise Dienste wie 
                GitHub an, auf denen gleichzeitig eine 
                transparente Datenhaltung mit Versionierung möglich ist, und auch eigene technische Entwicklungen einfach zur Nachnutzung bereitgestellt werden können. Dies alles setzt natürlich die Verwendung entsprechender 
                Open Access Lizenzen voraus.
            
         Die 
                Sicherung der Langzeitverfügbarkeit einer Ressource lässt sich generell nur durch eine entsprechende 
                technische Infrastruktur (Daten-, Kompetenz-, Rechenzentren) in Kombination mit 
                institutioneller Anbindung (Universitäten, Forschungsbibliotheken, kulturbewahrende Institutionen) gewährleisten. (Wissenschaftsrat 2011) Zu unterscheiden ist dabei zwischen der langfristigen Archivierung und Vorhaltung der Forschungsdaten und dem möglichst langen Erhalt der Webressource insgesamt, die ja ebenfalls mit allen Funktionalitäten über die Projektlaufzeit hinaus verfügbar sein soll. (Oßwald u.a. 2012: 13-15) Die Voraussetzungen in Köln erscheinen ideal, da dem Projekt mit dem 
                CCeH ein im Bereich der DH ausgewiesener technischer Partner zur Verfügung steht, der durch enge Kooperation mit weiteren universitären Einrichtungen wie dem 
                Data Center for the Humanities und dem Kölner Rechenzentrum, ideale strukturelle Voraussetzungen für Aufbau und langfristige Erhaltung digitaler Ressourcen schaffen konnte. Zusätzlich nimmt das 
                Capitularia-Projekt auch am Webarchivierungsprogramm der Bayerischen Staatsbibliothek teil.
            
         Für die Webpräsentation wurden in enger Zusammenarbeit mit dem technischen Partner 
                geeignete (open source) Technologien ausgewählt, deren zukünftiger Erhalt sowie deren Weiterentwicklung von einer breiten Community getragen werden, und die sich in anderen Projekten bereits bewährt haben. Im Fall von 
                Capitularia wird aktuell das PHP-basierte Content Management System 
                WordPress zur Verwaltung der Webpräsenz verwendet. Dabei werden die Standardfeatures (Blog, Suche, Mehrsprachigkeit etc.) möglichst breit genutzt und vereinzelt um weitere Eigenentwicklungen ergänzt, die insbesondere den Bereich des XSLT-Pipelining betreffen. Diese im Kontext des Projektes entwickelten 
                Plugins sollen auch der Nachnutzung durch andere Projekte zur Verfügung stehen. WordPress erfüllt durch die freie Zugänglichkeit des Quellcodes, der aktiven und heterogenen Entwickler-Gemeinschaft, des Ökosystems, welches sich um diese Software etabliert hat, sowie der verwendeten Lizenzmodells wichtige Voraussetzungen digitaler Nachhaltigkeit. (Stürmer 2015, S. 36-37) Die bewusst gewählten 
                low-tech-Lösungen gewährleisten weiterhin langfristig die leichte Wartbarkeit des Systems sowie plattform- und auch personelle Unabhängigkeit. Bei Bedarf könnte somit ohne größere Probleme auf ein anderes Präsentationsframework umgezogen werden.
            
         Die 
                Präsentation der Forschungsergebnisse findet auf mehreren Ebenen statt. Die angefertigten Transkriptionen werden zusammen mit weiteren Materialien projektbegleitend auf der Webseite veröffentlicht. Die Inhalte sind dort über Permalinks adressierbar und die zugrunde liegenden Daten (
                XML) stehen zum Download bereit. Um die Nachnutzung der Daten weiter zu erleichtern, ist die Implementierung von Schnittstellen (z.B. REST) vorgesehen.
                1 Die kritische Edition erscheint in Druckfassung in der 
                Leges-Reihe der 
                Monumenta Germaniae Historica und wird somit auch langfristig über deren Online-Angebot (dMGH) verfügbar sein. Vorabversionen der kritischen Editionstexte werden aber bereits zeitnah in digital aufbereiteter Form auf der Webpräsenz zur Verfügung gestellt. 
            
         Wenn auch finanzielle Ausstattung und Laufzeit von Projekten und damit auch deren Handlungsmöglichkeiten hinsichtlich Nachhaltigkeitsstrategien durchaus unterschiedlich sind, so erscheinen doch die Bereiche, in denen Maßnahmen getroffen werden können, allgemeingültig zu sein. Mit den hier vorgestellten Ansätzen soll ein aktiver und vor allem praxisorientierter Beitrag zur Diskussion um digitale Nachhaltigkeit geleistet werden.
      
      
         
             Die freie Bereitstellung von im Rahmen von Digitalen Editionen entstandenen (XML-)Daten lässt aktuell noch zu wünschen übrig. Nur durch diese wird aber Nachnutzung und damit integrative und innovative Forschung erst ermöglicht. (Turska u.a. 2016: 1) Technische Schnittstellen können die Nachnutzung befördern und stellen daher ein Kriterium einer aktuellen Ansprüchen genügenden digitalen Edition dar. (Sahle u.a. 2014). 
         
         
            
               Bibliographie
               
                  Hedler, Marko / Montero Pineda, Manuel / Kutscherauer, Nico (2011):
                        Schematron. Effiziente Business Rules für XML-Dokumente.
                        Heidelberg: dpunkt-Verlag.
                    
               
                  Oßwald, Achim / Scheffel, Regine / Neuroth, Heike (2012):
                        „Langzeitarchivierung von Forschungsdaten. Einführende Überlegungen“, 
                        in: Neuroth, Heike / Strathmann, Stefan / Oßwald, Achim / Scheffel, Regine / Klump, Jens / Ludwig, Jens (eds.): 
                        Langzeitarchivierung von Forschungsdaten: Eine Bestandsaufnahme.
                        Boizenburg: Verlag Werner Hülsbusch 13–23 
                        http://www.nestor.sub.uni-goettingen.de/bestandsaufnahme/nestor_lza_forschungsdaten_bestandsaufnahme.pdf [letzter Zugriff 9. November 2016].
                    
               
                  Sahle, Patrick / Vogeler, Georg / IDE (eds.) (2014):
                        Kriterien für die Besprechung digitaler Editionen. Version 1.1 Juni 2014 
                        http://www.i-d-e.de/publikationen/weitereschriften/kriterien-version-1-1/ [letzter Zugriff 12. November 2016].
                    
               
                  Stürmer, Matthias (2015):
                        „Wann sind Open Source Projekte digital nachhaltig?“,
                        in: swissICT und Swiss Open Systems User Group (eds.):
                        Open Source Studie: Schweiz 2015 36–37 
                        http://www.swissict.ch/fileadmin/customer/Publikationen/OSS-Studie2015.pdf [letzter Zugriff 9. November 2016].
                    
               
                  Turska, Magdalena / Cummings, James / Rahtz, Sebastian (2016):
                        „Challenging the Myth of Presentation in Digital Editions“,
                        in: 
                        Journal of the Text Encoding Initiative 9
                        http://jtei.revues.org/1453 [letzter Zugriff 12. November 2016].
                    
               
                  Wissenschaftsrat (ed.) (2012); 
                        Empfehlungen zur Weiterentwicklung der wissenschaftlichen Informationsstrukturen in Deutschland bis 2020. 
                        Drs. 2359-12. 
                        Berlin 
                        http://www.wissenschaftsrat.de/download/archiv/2359-12.pdf [letzter Zugriff 09. November 2016].
                    
            
         
      
   



      
         
            Wo die Wörter sind: eine visuell-interaktive Erforschung von Pflanzennamen
         
          In den Digital Humanities werden häufig Visualisierungsmethoden eingesetzt, um bestimmte Trends, Beziehungen oder Inhalte innerhalb oder zwischen verschiedenen Datensätzen hervorzuheben. Oft werden gut etablierte und weit verbreitete Arten graphischer Darstellung von Daten herangezogen, wie Verbert (2015) gezeigt hat. Der Einsatz innovativer Visualisierungsmethoden für die Datenerforschung und den Datenzugriff ist jedoch bei Humanities-Projekten, die sich mit nicht-numerischen Daten beschäftigen, noch relativ selten. In diesem Beitrag stellen wir ein Visualisierungstool vor, das im Rahmen des DH-Projekts 
                exploreAT! – exploring Austria’s culture through the language glass entwickelt wird, und erläutern dessen Anwendung am Beispiel der Pflanzennamen-Sammlung für das Wörterbuch der bairischen Dialekte in Österreich.
            
         
            exploreAT! (vgl. Wandl-Vogt et al, 2015) bietet unterschiedliche Einblicke in die vielfältige Beschaffenheit der deutschen Sprache in Österreich, durch exploratives Erforschen mittels einer Synthese von digitalen Infrastrukturen, Lexikographie, visueller Analyse und Citizen Science. Das Projekt basiert auf einer Sammlung von Daten zu den  bairischen Dialekten in Österreich aus dem frühen 20. Jahrhundert aus der Region der ehemaligen österreichisch-ungarischen Monarchie. Die Datenerhebung erfolgte ursprünglich mittels Fragebögen, die eine Vielzahl von Themen aus dem Alltag abdecken. Die gesammelten Daten bestehen aus rund 200.000 Stichwörtern in geschätzten 4 Millionen Datensätzen. Teile davon wurden als fünfbändiges Wörterbuch mit etwa 50.000 Stichwörtern (WBÖ), und Teile als Datenbank (DBÖ) ausgegeben. Innerhalb des Projekts gibt es vier spezifische, aber miteinander verbundene Arbeitsbereiche: kulturelle Lexikographie, semantisch-technologieorientierte Forschungsinfrastrukturen, visuelle Analyse und Bürgerwissenschaften. Des Weiteren werden use-cases für spezifische Themen wie Pflanzennamen, Farben oder Lebensmitteln entwickelt. TEI / XML Schnittstellen werden eingesetzt, um die Organisation von Metadaten, Konzepten und linguistischen Daten zu verbessern. Darüber hinaus ist vorgesehen, weitere Zugangspunkte zur Arbeit mit LOD zu schaffen, ontologische Ressourcen zu nutzen und damit die Visualisierung von konzeptionellen und semantischen Informationen zu gewährleisten.
            
         Mit Hilfe des vorgestellten visuellen Analysetools werden weitere Einblicke in die komplexe Struktur dieser Dialektdaten gegeben, wobei ein intuitiver und leicht zugänglicher Ansatz vorgesehen ist. In diesem Beitrag nehmen wir Pflanzennamen als exemplarischen Fall für die visuelle Exploration, Analyse und Darstellung von Datenstrukturen.
         Der Prototyp dieses Tools basiert auf einer Treemap-Visualisierungsmethode (vgl. Shneiderman, 1992), da diese eine kompakte Art und Weise für die Übertragung von Hierarchien ermöglicht. Der Zweck des Tools besteht darin, ein Mittel zur interaktiven Erforschung der verfügbaren Daten bereitzustellen, so dass der Benutzer Verständnis dafür gewinnt, wie das Wissen, das sich auf ein bestimmtes Wort (oder eine Zeichenkette) bezieht, in der Datenbank "gespeichert" ist, wobei die jeweiligen Lemmata mit der Benutzerabfrage zusammenhängen. Abgesehen von der Darstellung des resultierenden Sets von Lemmata, bauen wir eine Hierarchie je nach Kontext der Lemmata (in diesem Fall sind wir daran interessiert, die Lemmata in Bezug auf verschiedene Pflanzenarten zu gruppieren). Deshalb verwenden wir die beiden wichtigsten visuellen Merkmale von Treemaps: a) das Treemap-Layout (basierend auf einem Satz von verschachtelten Rechtecken, wobei jedes Rechteck einen Zweig der Hierarchie darstellt, der dann mit kleineren Rechtecken, welche Unterzweige darstellen, gekachelt wird) und b) die Fläche jedes Rechtecks ​​(die proportional zur Größe der Daten ist).
         Da in diesem Prototyp Pflanzennamen von größter Bedeutung sind, aber dem Benutzer, der eventuell mit den wissenschaftlichen Namen der Pflanzen nicht vertraut ist, wichtige Information verborgen bleiben könnte, entschieden wir uns für eine visuelle Art den Kontext (Pflanzen) der Lemmata, die in Zusammenhang mit der Abfrage stehen, zu vermitteln: wir verwenden den Flickr-Webdienst, um Fotos abzurufen, die mit dem wissenschaftlichen Namen der Pflanze versehen sind (siehe Abbildung 1).
         
            
         
         Abbildung 1: Beispiel für eine visuelle Darstellung von Pflanzennamen mit verschachtelten Rechtecken. 
         Als Ergebnis unseres visuellen Ansatzes kann der Benutzer die Verteilung der Lemmata in Abhängigkeit von den Pflanzen, auf die sie sich bezieht, verstehen (jedes Rechteck enthält das abgerufene Foto einer bestimmten Pflanze mit einem Bereich entsprechender Größe, die davon abhängt, wie viele Lemmata in Zusammenhang damit stehen). Der Benutzer kann dann auf das Rechteck seiner Wahl klicken, um tiefer zu gehen und alle relevanten Informationen für die mit dieser Pflanze zusammenhängenden Lemmata zu erhalten (siehe Abbildung 2).
         
            
         
         Abbildung 2: Beispiel für die Exploration von Pflanzennamen-Lemmata in einem bestimmten Rechteck, in diesem Fall Vaccinium myrtillus; Heidelbeere, Schwarzbeere, Blaubeere (siehe Abbildung 1).
         Schließlich öffnen sich künftige Arbeitsfelder dank der Tatsache, dass dieser visuelle Ansatz auch noch gültig ist, wenn wir die Lemmata nach anderen Kriterien (d.h. nach einer mehrstufigen Hierarchie) gruppieren. Zum Beispiel könnte man zuerst die Lemmata nach Pflanze gruppieren; dann könnte man für eine bestimmte Pflanze die dazugehörigen Lemmata nach Zeit gruppieren, die wiederum nach Regionen gruppiert werden. Mit diesen verschiedenen Arten der Gruppierung können diverse andere Daten mit einer ähnlichen strukturellen Beschaffenheit in derselben Weise visualisiert und analysiert werden. Dies würde unser Tool vielseitig und auch offen für andere Daten, nicht nur Pflanzennamen, machen.
      
      
         
            
               Bibliographie
               
                  Verbert, Karen (2015):
                        „On the Use of Visualization for the Digital Humanities“
                        in:
                        DH2015: Global Digital Humanities.
                    
               
                  Wandl-Vogt, Eveline / Kieslinger, Barbara / O’Connor, Alexander / Theron, Roberto
                        (2015):
                        „exploreAT! Perspektiven einer Transformation am Beispiel eines lexikographischen Jahrhundertprojekts“,
                        in:
                        DHd 2015: Von Daten zu Erkenntnissen.
                    
               
                  Shneiderman, Ben (1992):
                        „Tree visualization with tree-maps: 2-d space-filling approach“,
                        in:
                        ACM Transaction on Graphics (TOG) 11 (1): 92-99.
                    
            
         
      
   



      
         Die Rolle digitaler Ressourcen in den Geisteswissenschaften und die zunehmende Bedeutung von Algorithmen werden noch immer unterschätzt. In den letzten Jahren bzw. Jahrzenten wurden zahlreiche Software-Werkzeuge, virtuelle Forschungsumgebungen und interaktive Publikationen, wie Datenbanken oder Digitale Editionen, für die geisteswissenschaftliche Forschung entwickelt. Diese werden innerhalb der Forschungscommunity mit zunehmender Tendenz akzeptiert und inzwischen breit eingesetzt. Jedoch stehen wir heute vor der Herausforderung diese verschiedenen Ressourcen, die zu einem großen Teil auf unterschiedlichen technischen Grundlagen basieren, weiter zu pflegen und verfügbar zu halten. Transparenz und Reproduzierbarkeit von Forschungsergebnissen, die mit einer digitalen Ressource oder Software erstellt wurden leiden darunter, dass diese Software oft nach wenigen Jahren nicht mehr gepflegt wird und damit nicht mehr lauffähig ist. 
         Der Vortrag beleuchtet die hier skizzierte Problematik am Beispiel Digitaler Editionen. Es werden drei Punkte vorgeschlagen, wie durch einen Bottom-Up-Ansatz die Nachhaltigkeit digitaler Ressourcen gefördert werden kann. Der Fokus liegt dabei auf den Erfahrungen, die im Laufe der letzten 10 Jahre mit der Entwicklung von XML-basierten Digitalen Editionen und dem Einsatz ausgewählter Software-Werkzeuge, wie der nativen XML-Datenbank eXistdb (
                http://exist-db.org), gewonnen wurden. 
            
         Bei digitalen Ressourcen handelt es sich um dynamische Objekte. Das bedeutet einerseits, dass die Inhalte jederzeit korrigiert, erweitert oder verändert werden können. Andererseits muss die technologische Basis fortlaufend aktuell, sicher und verfügbar gehalten werden. Diese beiden Prozesse sind Teilaufgaben eines größeren Aufgabenbereichs, der unter dem Begriff „
                data curation“ zusammengefasst werden kann.
            
         Eine Digitale Edition ist mehr als nur ihre Forschungsdaten. Letztere, in vielen Fällen XML-Dokumente, werden oft erst durch eine adäquate Darstellung, durch Visualisierungen, wie Text-Bild-Verlinkungen, verschiedene Ansichten auf den Text, Netzwerke oder Timelines oder auch Verknüpfungen mit anderen externen Ressourcen „zum Leben erweckt“. Dieses Leben in Form programmierter Funktionalität ist ein genuiner Forschungsbereich der Digital Humanities. Allerdings führt die Funktionalitätsschicht (siehe Abbildung 1) einer Digitalen Edition auch dazu, dass 
                data curation eine sehr komplexe Aufgabe werden kann, da hier die Flexibilität der Implementierung erheblich höher ist als auf der Ebene der Datenschicht. 
            
         
            
            Abbildung 1: Schichtenmodell Digitale Edition
         
         Digitale Editionen sind Software-Werkzeuge für die Analyse von Forschungsdaten. Damit sind sie ein Teil des Forschungsprozesses, der erhalten werden muss, um die Reproduzierbarkeit der Forschungsergebnisse zu gewährleisten.
         Eine digitale geisteswissenschaftliche Ressource durchläuft üblicher Weise einen typischen Lebenszyklus. Dieser beginnt mit der Analyse der analogen Quellen, geht über die Datenmodellierung, die Auswahl bzw. Anpassung oder Neuentwicklung von Bearbeitungswerkzeugen sowie der digitalen Publikation der Forschungsergebnisse bis hin zu Fragen der Langzeitverfügbarkeit und Langzeitarchivierung. Bei jedem dieser Schritte sind verschiedenen Kompetenzen involviert. Das bedeutet, der Aufbau einer Digitalen Edition ist immer Teamwork. Dieses Team setzt sich in den meisten Fällen aus Personen zusammen, die einerseits das inhaltliche Fachwissen mitbringen und andererseits aus Personen mit einer Vielzahl unterschiedlicher technischer Kompetenzen:
         
            Analyse der Quellen (Geisteswissenschaftler)
            Anforderungsanalyse der digitalen Ressource, 
                    Requirement Engineering (alle Projekbeteiligten)
                
            Entwurf des Daten- / Dokumentenmodells, Auswahl von Standards (Geisteswissenschaftler, Datenbankspezialisten, Markupspezialisten, Metadatenspezialisten)
            Auswahl, Anpassung bzw. Entwicklung von Tools (Programmierer, Geisteswissenschaftler)
            Aufsetzen und Betreuen der Server (Systemadministratoren)
            Konzept, Design und Umsetzung der Web-Publikation (Webdesigner, Webentwickler, Geisteswissenschaftler)
            Vorbereitung für Langzeitverfügbarkeit / -archivierung (Metadatenspezialisten, Dokumentationsspezialisten)
            Betreuung und Wartung nach Projektende („
                    data curators“)
                
         
         An jeder Stelle in diesem Lebenszyklus einer digitalen Ressource werden Entscheidungen getroffen, die Auswirkungen auf den nachfolgenden Schritt haben. So bilden die eigentliche Analyse der Inhalte, die z.B. in einer Digitalen Edition publiziert werden sollen, und die Anforderungsanalyse das Fundament, auf dem alles aufbaut, vom Daten- oder Dokumentenmodell, bis hin zur Publikation und der 
                data curation. 
            
         Aus methodischer Sicht, mit besonderem Augenmerk auf das zugrundeliegende Text- bzw. Dokumentenmodell, wurden Digitale Editionen bereits ausführlich beschrieben (siehe Pierazzo 2015 und Sahle 2013). Eine Analyse aus technischer Sicht steht noch aus. Um die Entwicklung, Betreuung und Nachhaltigkeit Digitaler Editionen zu gewährleisten bedarf es eines technologischen Publikationskonzepts, das aus möglichst standardisierten Komponenten besteht. 
         Bisher existieren sehr erfolgreiche Standardisierungen auf dem Gebiet der Metadaten und der Textauszeichnungen, z.B. mit den Richtlinien der 
                Text Encoding Initiative (TEI), aber wenig bis gar nichts bei der technischen Umsetzung und der Dokumentation. Dies würde helfen anschlussfähigere, stabilere und nachhaltigere digitale Ressourcen aufzubauen und damit auch die Arbeit eines Datenkurators, der sich um die Pflege dieser Ressourcen nach Projektende kümmert, deutlich vereinfachen.
            
         Für eine aussichtsreiche Nachhaltigkeit kann die Lösung nicht allumfassend sein, sondern nur für klar definierte Anwendungsfälle gelten. In dem hier vorgestellten Fall ist dies eine XML-basierte Digitale Edition, die mit Technologien aus der X-Familie (XSLT, XQuery, XML-Schema, eXistdb) entwickelt wird. Das Grundprinzip ist jedoch auf andere Anwendungsszenarien übertragbar:
         
            Eine ausführliche Dokumentation
            Ein klar definierter Werkzeugkasten
            Die Paketierung aller Projektressourcen
         
         Dokumentation
         Eine nachhaltige digitale Forschungsressource bzw. –software ist langfristig verfügbar, gut dokumentiert, lizenziert und versioniert, um die Reproduzierbarkeit der Forschungsprozesse zu garantieren. Die wichtigste Komponente ist eine ausführliche, formalisierte Dokumentation, die mindestens die folgenden Informationen enthalten sollte:
         
            Den Namen des Projekts und aller beteiligten Institutionen und Personen.
            Den Projektstatus: geplant, in Arbeit, veröffentlicht, beendet.
            Die eingesetzten Technologien und Standards inklusive Versionsangabe.
            Lizenzangaben zu Forschungsdaten, Quellcode, und anderen Komponenten, wie Schriftarten, Audio- oder Videodokumenten.
            Informationen darüber, wo der Quellcode und die Forschungsdaten zu finden sind.
            Informationen über die bereitgestellten APIs und andere Schnittstellen, um die Forschungsdaten in verschiedenen Formaten abzurufen (XML, HTML, PDF, JSON usw.) und in anderen Kontexten weiterzuverarbeiten. 
            Details über die Forschungsmethode und den Hintergrund des Projekts. (Mehr dazu siehe Faniel 2015)
            Zitations- und Referenzierungsanweisungen für die persistente Adressierung aktueller und älterer Versionen der Forschungsdaten, Metadaten und Software.
            Eine standardisierte Historie der Projektentwicklung.
         
         Selbstverständlich kann diese Liste nur ein erster Vorschlag sein. Sie enthält keinesfalls alle möglichen Informationen, die zu einer Digitalen Edition angegeben werden können. Die Dokumentation sollte maschinenlesbar (um z.B. als XML oder JSON weiter verarbeitet werden zu können) und über eine standardisierte Adresse bzw. einen klar definierten Zugriffspunkt (z.B. http://home.of.project/api/projectdescription) abrufbar sein. Dadurch wäre es möglich eine Digitale Edition bei einem zentralen Verzeichnis anzumelden, in dem alle Informationen und Updates über Digitale Editionen, die demselben Publikationsmodell folgen, gesammelt werden. Ein solches Verzeichnis existiert noch nicht. 
         Definierter Werkzeugkasten
         Wie oben beschrieben kann Nachhaltigkeit nur in einem definierten Rahmen hergestellt werden, indem man klare Anwendungsfälle beschreibt. Selbst in diesen sind die Möglichkeiten der Umsetzung nahezu unbegrenzt. Daher ist es wichtig, genau zu definieren, welche Technologien, Standards und Software-Werkzeuge zum Einsatzkommen und welche Abhängigkeiten bestehen. Es ist ratsam die Zahl der eingesetzten Tools überschaubar zu halten und sich auf etablierte und gut dokumentierte Technologien zu konzentrieren.
         Paketierung 
         Alle zusammengehörenden Komponenten einer Digitalen Ressource (Daten, Metadaten, Quellcode, Binärdatein, Dokumentation) müssen immer zusammen abrufbar sein. Diese Pakete tragen deutlich zu einer nachhaltigeren Entwicklung bei. Es ist immer klar, wo sich alle relevanten Informationen und Daten befinden. Zudem könnte ein Paket einer zentralen Kurationsstelle, z.B. einem Digital Huamnities Data Center, übergeben werden, die sich um die Betreuung der digitalen Ressourcen abgeschlossener Projekte kümmert. Diese Anlaufstelle existiert ebenfalls noch nicht.
         Für den hier vorgestellten Anwendungsfall bietet das von eXistdb verwendete EXPath-Format ( 
                
               http://expath.org/ 
            ) einen guten Ausgangspunkt für die Paketierung. Dieses Format beschreibt ein Packaging-System ( 
                
               http://expath.org/modules/pkg/ 
            ), das es erlaubt XML-basierte Dokumente zusammen mit Abfrage- und Transformationsskripten sowie verschiedenen anderen Ressourcen auf eine standardisierte Art und Weise zu paketieren, das dieses Paket von allen Softwaresystemen, die diesem Standard folgen verstanden und ausgewertet werden kann. Damit kann dieses Packagesystem ähnlich fungieren, wie ein App-Store für Smartphones. Auch der Anpassungsaufwand für Software, die während der Projektlaufzeit eingesetzt wird, würde sich so verringern. 
            
         Andere Anwendungsszenarien lassen sich nicht auf Softwareebene paketieren. In diesen Fällen kann man auf Anwendungsvirtualisierung zurückgreifen, wie sie zum Beispiel mittels Docker (
                https://www.docker.com/) möglich ist.
            
         Um eine solche Standardisierung auf der technischen Ebene durchzuführen benötigt es eine aktive Digital-Humanities-Enwicklercommunity. Die Rolle des DH-Entwicklers ist im allgemeinen Diskurs noch deutlich unterrepräsentiert. Um wirklich erfolgreich Softwaretools für die geisteswissenschaftliche Forschung programmieren zu können, benötigt ein Entwickler mehr als nur ein grundlegendes Verständnis der typischen Problematiken in den einzelnen Fachdisziplinen. Umgekehrt erlauben Programmierkenntnisse ein wesentlich besseres Verständnis über die Funktionsweise der Software und damit bessere Einsatzmöglichkeiten sowie das Potential zu eigenen Verbesserungen. Um hier Fortschritte zu erzielen, müssen sich die DH-Entwickler besser organisieren. Die DH-Entwicklercommunity wäre der Kreis an Personen, die die Einführung und Anwendung von Standards, wie dem EXPath-System diskutieren und tragen. Mittelfristig wäre das Ziel für jeden Schritt im Lebenszyklus einer digitalen Ressource einen oder mehrere Vorschläge einer standardisierten Herangehensweise in Form eines 
                best-practice-Leitfadens zu haben, der sich als 
                technical reader zur Erstellung digitaler geisteswissenschaftlicher Ressourcen eignet und Vorschläge zu Schnittstellen, Standards, Lizenzen, Dokumentation, Zitationshinweise usw. anbietet.
            
         Ein Ansatzpunkt dafür bietet die Arbeit des 2010 gegründeten Software Sustainability Institute (
                https://www.software.ac.uk/), Hong 2010 sowie Hettrick 2016, die verschiedene Ansätze zur Forschungs-Software-Nachhaltigkeit untersucht haben.
            
      
      
         
            
               Bibliographie
               
                  Faniel, Ixchel (2015): 
                        Data Management and Curation in 21st Century Archives, 21. September 2015, 
                        http://hangingtogether.org/?p=5375.
                    
               
                  Hettrick, Simon (2016): 
                        Research Software Sustainability. Report on a Knowledge Exchange Workshop.
                    
               
                  Hong, N. Chue et al. (2010): 
                        Software Preservation Benefits Framework. Software Sustainability Institute Technical Report.
                    
               
                  Pierazzo, Elena (2015): 
                        Digital Scholarly Editing, Theories, Models and Methods. 
                        Ashgate.
                    
               
                  Sahle, Patrick (2013):
                        Digitale Editionsformen, Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. 3 Bände.
                        Norderstedt: Books on Demand.
                    
            
         
      
   



      
         
            Einleitung
            Dreht sich ein Roman vorab um das Roman-Schreiben, den Zustand der zeitgenössischen Literatur und die Kritik an ihr, dann ist es vielleicht gar nicht so abwegig, dass sein cleverster Witz in seiner eigenen Nicht-Veröffentlichung steckt. Der schweizerische Schriftsteller Hermann Burger griff auf den letzten Seiten seines Romans 
                    Lokalbericht (1970) zu genau dieser selbstreflexiven und auto-dekonstruktiven Volte, als er den Mentor des jungen Protagonisten und angehenden Schriftstellers urteilen lässt, so könne man heute nicht mehr schreiben, das Manuskript solle liegen bleiben, “ein Jahr, zwei Jahre, zehn Jahre lang” – und Burger sich offenbar selbst an den Ratschlag hält und den Roman tatsächlich zeit seines Lebens nicht veröffentlicht.
                
            Ausschlaggebend für die Nicht-Veröffentlichung war freilich nicht diese Pointe, sondern ein biographischer Umstand (vgl. dazu den Kommentar in Zumsteg 2016a: 257-304 bzw. http://www.lokalbericht.ch/kommentar). Aus heutiger Warte erscheint die Veröffentlichung jedoch zweifellos geboten. Der 
                    Lokalbericht nimmt in Burgers Lebenswerk und Werkleben eine Scharnierfunktion ein, indem sich Burger in diesem ‘Rohdiamanten’ erstmals ungestüm an jene unverwechselbare Poetik herantastet, die ab seinem Roman 
                    Schilten: Schulbericht zuhanden der Inspektorenkonferenz von 1976 zu seinem Markenzeichen wird. Die lang währende Beschäftigung Burgers mit dem Romantext und seinen Vorstufen – schreibend, vorlesend, auszugsweise publizierend – legt darüber hinaus Zeugnis ab für die Bedeutung, die er dem Text beimaß. Burgers Bonmot “Literatur ist, wenn man trotzdem druckt”, muss also zur Rechtfertigung dieser postumen Veröffentlichung gar nicht erst bemüht werden.
                
         
         
            Der Lokalbericht als Hybrid-Edition
            Die erstmalige Herausgabe des Romans aus dem Nachlass im Schweizerischen Literaturarchiv, Bern (SLA), erforderte eine andere editorische Handhabe als die vor zwei Jahren erschienene Leseausgabe von Burgers Werken in acht Bänden (Zumsteg 2014), die sich auf den Wiederabdruck seiner bereits zuvor publizierten Texte beschränkte. Eine Hybrid-Edition, bestehend aus einer umfassenden digitalen Edition und einem schlichten Lesebändchen als deren Spin-off (Zumsteg 2016a, 2016b), wird dem Archivfund besser gerecht, zumal sich die Entstehung des Romans dadurch auszeichnet, dass einzelne Textbausteine eine lange Vorgeschichte haben. Die Änderungen und Unterschiede werden dabei viel weniger auf den einzelnen Typoskript-Seiten manifest als zwischen den Textstufen, sie werden erst im Vergleich der betreffenden Dokumente erkennbar. Die Möglichkeiten einer digitalen Edition, die nicht durch das Format der Buchseite und die lineare Folge bestimmt und begrenzt sind, boten sich für diese Situation besonders an. Das digitale Format gab überdies Gelegenheit, die vielfältigen allographen Materialien aus dem Nachlass und aus anderer Provenienz miteinzubeziehen, die für die Kontextualisierung und das Verständnis der Autographen sowie für Burgers mosaikartige Arbeitsweise erhellend sind. Das Projekt bot also eine gute Ausgangslage, den vielgerühmten Mehrwert einer digitalen Edition gegenüber den hergebrachten Publikationsformen zu realisieren. Die digitale Edition (beta) ist seit dem 22. Oktober 2016 unter http://www.lokalbericht.ch verfügbar.
         
         
            Grundlage: hochqualitative Digitalisate, dokument-orientierte TEI-Encodings
            Neben den 179 beschriebenen Typoskriptseiten des Romans wurden von den knapp 900 weiteren Textträgern, die im Rahmen der digitalen Edition präsentiert werden, hochauflösende Digitalisate im TIFF-Format und/oder mit OCR hinterlegte PDF-Dateien angefertigt. Aufgrund der fotografisch hervorragenden Qualität und der ohnehin guten Leserlichkeit der (zumeist nur punktuell korrigierten) Typoskripte und handschriftlichen Dokumente nehmen diese Digitalisate den Rang der primären digitalen Repräsentation ein.
            Die Digitalisate aller Dokumente, die Bestandteil des eigentlichen dossier génétique sind, wurden durch dokument-orientierte, auf der Basis von OCR-Daten erstellte, TEI-Encodings (sourceDoc) ergänzt. Diese Encodings bilden die Grundlage der diplomatischen Transkription, d.h. der Umschrift aller Texte inklusive mikrogenetischer Varianz. Die minutiöse Aufzeichnung dieser Phänomene erwies sich als sehr zeitaufwändig. Retrospektiv wäre zu erwägen, die Tiefe des Encodings im Sinne des Vermeidens eines “Over-taggings” (Bernhart/Hahn 2014, Hanrahan 2015) zu reduzieren, zumal die derart codierten Phänomene aus der Perspektive der naheliegenden Forschungsfragen nur einen begrenzten Mehrwert gegenüber dem durchsuchbaren Volltext und den Digitalisaten schaffen.
            Durch Extraktion der Text-Nodes inklusive der regularisierten Varianten und basierend auf Milestone-Elementen (Absätze, Seitenumbrüche) ließen sich ab dem dokument-orientierten TEI-Encoding sowohl der für die Druckausgabe verwendete Lesetext als auch text-orientierte TEI-Encodings gewinnen, welche die Grundlage für die Lesefassung der digitalen Edition bilden (Umschrift aller Texte ohne Berücksichtigung mikrogenetischer Varianz).
            Die TEI-Encodings, die unter den Bedingungen der CC BY 2.0-Lizenz nachnutzbar sind, enthalten selbstredend auch die Metadaten des jeweiligen Typoskript-Konvoluts oder Briefes.
         
         
            Präsentation und Funktionalität
            Als digitale Edition mit ausgeprägter textgenetischer Komponente lehnt sich der Aufbau der 
                    Lokalbericht-Edition an vergleichbare aktuelle Projekte wie die textgenetische Edition Wofgang Koeppens 
                    Jugend (Krüger, Mengaldo, Schumacher 2016) und die historisch-kritische Edition von Goethes 
                    Faust an (Bohnenkamp, Henke, Jannidis 2016). Die angebotenen Interaktionsmechanismen lassen sich mit der Edition Ch. G. Heynes' Vorlesungen über die Archäologie vergleichen (Graepler o. J.). Die Benutzerin oder der Benutzer der Edition hat in jeder Dokumentansicht die Möglichkeit, zwischen den vier Präsentationsformen Digitalisat, Transkription, Lesefassung und Metadaten hin und her zu wechseln. Dabei gibt es drei Ansichtsmodi, die für unterschiedliche Lese- bzw. Benutzungspraktiken stehen:
                
            In der 
                    Grundansicht wird eine Oberfläche eines Textträgers dargestellt. Schaltflächen erlauben das Navigieren innerhalb des Texts, offerieren aber auch Verknüpfungen zu textgenetisch verwandten Seiten anderer Konvolute. Die verlinkten Ziele werden standardmäßig an Stelle des aktuellen Dokumentes geladen. Die Grundansicht eignet sich daher besonders für eine (zumeist) lineare Lektüre. Sie lässt sich im Gegensatz zu den beiden Doppelansichten auch auf kleinen Anzeigegeräten verwenden.
                
            Die 
                    Parallelansicht dient dazu, die gleiche Textstelle in zwei Ansichten zu vergleichen. Sie erlaubt z.B. die Gegenüberstellung von Digitalisat und Transkription. Vor- und Zurückblättern wirkt sich dabei jeweils auf beide Ansichten aus. Auch in der Parallelansicht lassen sich Seiten anderer Konvolute laden, die einen textgenetischen Bezug zur aktuell dargestellten Seite haben.
                
            Die flexibelste Benutzungsmöglichkeit bietet schließlich die 
                    Synopsis. Sie ähnelt beim ersten Aufruf der Parallelansicht, lässt im Unterschied zu ihr aber den Vergleich zwischen textgenetisch verwandten Textträgern Seite an Seite zu. In dieser Ansicht wird typischerweise zweimal die gleiche Präsentationsform gewählt, etwa Faksimile gegen Faksimile oder diplomatische Transkription gegen diplomatische Transkription.
                
            
               
                  
                  Digitale Edition 
                            Lokalbericht: Synoptische Darstellung
                        
               Alle drei Ansichtsmodi binden unter der Textansicht auch eine visuelle Navigation in der Form eines skizzenbasierten Graphs ein, der eine Vogelperspektive auf das gesamte Materialkorpus bietet. Indem in Benutzerinteraktion die textgenetischen Bezüge als Verbindungslinien (Links) zwischen den als Kreissymbolen (Nodes) dargestellten Einzelblättern ein- und ausgeblendet werden können, lässt sich die Textentwicklung auf der Makroebene anschaulich verfolgen. Die einzelnen Verbindungslinien dienen dabei zugleich als Links, über die sich die beiden ausgewählten Blätter wiederum in der synoptischen Ansicht laden lassen (Dängeli, Theisen, Wieland, Zumsteg 2016).
                
            Editoriale Expertise zur Textgenese wird zusätzlich durch einen erläuternden Überblickskommentar und Stellenkommentare zum Romantext sowie durch Verweise auf editionsinterne und -externe Ressourcen befördert.
            Durch die Auszeichnung von Personen (421), Orten (378) und Werken (191), die im Roman bzw. im Korpus vorkommen oder einen wichtigen Bezug dazu haben, ist das Korpus überdies auch semantisch erschlossen. Diese wo immer möglich auf Normdatensätze (VIAF, GND, GeoNames) referierenden Einheiten sind über Register zugänglich, sie lassen sich aber auch in der Transkription und im Lesetext hervorheben.
            Die Volltextsuche mit flexibel kombinierbaren Filtern (z.B. nach Textkategorie) bietet dem Benutzer einen weiteren Einstiegspunkt in die vielfältigen und spannenden Materialien der digitalen Edition.
         
         
            Aspekte digitaler Nachhaltigkeit
            Weil von Anfang an die Perspektive bestand, die digitale Edition nach der Entwicklung am Cologne Center for eHumanities an das Schweizerische Literaturarchiv bzw. die Schweizerische Nationalbibliothek zu übergeben –, galt ab Projektbeginn die Prämisse, eine bewusst einfache und leichtgewichtige technische Lösung anzustreben, die nur einen geringen Anteil an serverseitiger Progammierung erfordert und die sich mittel- und langfristig leicht warten lässt. Diese Kriterien legten es nahe, aus den TEI-Encodings statisches HTML zu erzeugen, das nur punktuell durch dynamisch erzeugten Code ergänzt werden muss (z.B. freie Volltextsuche). Dass eine solche datenbanklose Lösung durchaus zeitgemäß sein kann, zeigen die zahlreichen Generatoren für statische Webseiten, die in jüngerer Zeit entwickelt wurden und die mitunter große Akzeptanz fanden.
                    1
            
            Für die digitale Lokalbericht-Edition fiel die Wahl mit Apache Cocoon
                    2 auf eine vom Ansatz her in gewissem Grad vergleichbare, jedoch besser zu den vorliegenden XML-Daten passende und insbesondere auch sehr ausgereifte Anwendung. Dabei profitierten wir von der soliden Grundlage des im Produktiveinsatz bewährten Werkzeugs 
                    Kiln (vormals 
                    xMod) von Monteiro Viera und Norrish (2012), das Cocoon mit SOLR und Sesame bündelt und dank guter Dokumentation schnell einsatzbereit ist.
                    3 Ergänzt um eigene Pipeline-Definitionen und XSL-Transformationen ließ sich auf der Grundlage von Kiln eine monolithische Webanwendung erstellen, die abgesehen vom Bildserver alle Funktionalitäten umschließt und die mit einem einzigen Befehl auf einem Standardwebserver lauffähig ist. Im Bedarfsfall lässt sich die Seite auch als komplett statisches HTML abspeichern, wodurch ein wesentlicher Schritt zur langfristigen Konservierung und Verfügbarmachung erfüllt sein sollte.
                
            Um die Hürde der Applikations-Präservierung auch hinsichtlich der permanenten Referenzierung tief zu halten, verweisen die Permalinks lediglich auf – irgendwie geartete – digitale Repräsentationen real existierender Dokumente. Bestimmte Ansichten oder Funktionalitätszustände sind damit explizit nicht permanent referenzierbar, was die Verantwortung der übernehmenden Institution reduziert und ihr mehr Flexibilität für künftige konzeptuelle oder technische Veränderungen zubilligt. Im Bedarfsfall sind die Benutzer gehalten, bestimmte Ansichten selbständig zu persistieren, beispielsweise durch Übergabe der URL an die Wayback Machine.
                    4
            
            Ein weiterer Mosaikstein zur Sicherung der Nachhaltigkeit betrifft die (derzeit laufende) Aufnahme der Ressource durch das Data Center for the Humanities (DCH)
                    5 der Universität zu Köln, in deren Rahmen neben der Klärung technischer und rechtlicher Fragen auch die periodische Prüfung der Ressource nach festgelegten Kriterien geregelt wird. Möge dies gewährleisten, dass der zu Lebzeiten unpubliziert gebliebene Lokalbericht nach seiner Erstveröffentlichung nicht abermals ins archivalische Dunkel versinkt.
                
         
      
      
         
            Die ihrerseits mit DocPad erstellte Liste unter https://staticsitegenerators.net führt per November 2016 445 derartige Tools auf. Zur Beliebtheit vgl. auch https://www.staticgen.com. Die Vorteile dieses Ansatzes liegen auf der Inputseite vorab in den einfachen Quellformaten, die zumeist zur Verwendung kommen (z.B. Markdown, Textile, YAML), auf der Outputseite in der durch die direkte HTML-Auslieferung bedingten hervorragenden Performanz (Biilmann Christensen 2015, Kraetke/Imsiek 2016, Rinaldi 2015).
            Vgl. https://cocoon.apache.org/2.1/.
            Vgl. zu Kiln auch Turska 2014.
            Die Umsetzung einer technischen Adressierbarkeit granulärer Dateneinheiten war nicht Bestandteil des Projekts, sie könnte auf der Bestehenden Grundlage aber nachgerüstet werden.
            Vgl. http://dch.phil-fak.uni-koeln.de.
         
         
            
               Bibliographie
               
                  Bernhart, Toni / Hahn, Carolin (2014): 
                        „Datenmodellierung in digitalen Briefeditionen und ihre interpretatorische Leistung. Ontologien, Textgenetik und Visualisierungsstrategien. Workshop im Jacob-und-Wilhelm-Grimm-Zentrum der Humboldt-Universität zu Berlin, 15./16. Mai 2014“,
                        in: 
                        editio 28: 225-229.
                    
               
                  Biilmann Christensen, Mathias (2015): 
                        „Why Static Website Generators Are The Next Big Thing“,
                        in:
                        Smashing Magazine 2. November 2015
                        https://www.smashingmagazine.com/2015/11/modern-static-website-generators-next-big-thing/ [letzter Zugriff 24. August 2016].
                    
               
                  Bohnenkamp, Anne / Henke, Silke / Jannidis, Fotis (2016): 
                        Historisch-kritische Faustedition. Unter Mitarbeit von Gerrit Brüning, Katrin Henzel, Christoph Leijser, Gregor Middell, Dietmar Pravida, Thorsten Vitt und Moritz Wissenbach. Beta-Version 2. http://beta.faustedition.net [letzter Zugriff 28. Oktober 2016].
                    
               
                  Daengeli, Peter / Theisen, Christian / Wieland, Magnus / Zumsteg, Simon (2016): 
                        „Visualizing the Gradual Production of a Text“, in: 
                        DH2016: Conference Abstracts 767–769.
                    
               
                  Graepler, Daniel (o. J.):
                        Christian Gottlob Heyne – Vorlesungen über die Archäologie. 
                        http://heyne-digital.de [letzter Zugriff 20. November 2016].
                    
               
                  Hanrahan, Elise (2015): 
                        „‚Over-tagging‘ with XML in Digital Scholarly Editions“, 
                        in: 
                        DHd 2015: Von Daten zu Erkenntnissen 193–196.
                    
               
                  Kraetke, Martin / Imsieke, Gerrit (2016): 
                        „XSLT as a powerful static website generator. Hogrefe's Clinical Handbook of Psychotropic Drugs“, 
                        in: 
                        Proceedings of XML In, Web Out: International Symposium on sub rosa XML. Balisage Series on Markup Technologies 18 
                        http://www.balisage.net/Proceedings/vol18/html/Kraetke02/BalisageVol18-Kraetke02.html [letzter Zugriff 24. August 2016].
                    
               
                  Krüger, Katharina / Mengaldo, Elisabetta / Schumacher, Eckhard (2016): 
                        Wolfgang Koeppen. Jugend. 
                        http://www.koeppen-jugend.de/ [letzter Zugriff 24. August 2016].
                    
               
                  Monteiro Viera, Jose Miguel / Norrish, Jamie (2012): 
                        Kiln. 
                        https://github.com/kcl-ddh/kiln [letzter Zugriff 24. August 2016].
                    
               
                  Rinaldi, Brian (2015): 
                        Static Site Generators. Modern Tools for Static Website Development. 
                        Sebastopol: O’Reilly.
                    
               
                  Turska, Magdalena (2014): 
                        „What prevents people from firing their own Kiln?“,
                        in:
                         Nesting Instinct. Build in progress
                  http://blogs.it.ox.ac.uk/mturska/2014/07/30/what-prevents-people-from-firing-their-own-kiln [letzter Zugriff 24. August 2016].
                    
               
                  Zumsteg, Simon (2014): 
                        Hermann Burger: Werke in acht Bänden. 
                        Zürich: Nagel und Kimche.
                    
               
                  Zumsteg, Simon (2016a): 
                        Hermann Burger – Lokalbericht: Roman. Herausgegeben aus dem Nachlass. 
                        Zürich: Voldemeer.
                    
               
                  Zumsteg, Simon / Dängeli, Peter / Wieland, Magnus / Wirtz, Irmgard (2016b): 
                        Hermann Burger – Lokalbericht: Digitale Edition. 
                        http://www.lokalbericht.ch [Beta-Version vom 22. Oktober 2016].
                    
            
         
      
   



      
         Angesichts stetig wachsender Kapazitäten zur Speicherung großer Datenmengen nutzen Bibliotheken und Archive zunehmend die Möglichkeit, ihre Sammlungen zu digitalisieren und die Faksimiles online bereitzustellen. Rein konservatorischen Erwägungen folgend, belassen sie es dabei häufig bei der Erfassung der Metadaten und verzichten auf die weiterreichende inhaltliche Erschließung des Materials. So bleibt es oftmals allein dem Nutzer überlassen, sich einen Zugang zu den Inhalten der Sammlungen zu verschaffen. 
         Sofern es sich dabei um Druckwerke handelt, ist dieses Vorgehen durchaus hinreichend, zumal die Fähigkeit zur Lektüre von Antiqua- und Frakturdrucken zumindest im deutschsprachigen Raum allgemein vorausgesetzt werden kann. Da mit Hilfe der OCR-Technologie inzwischen selbst bei der automatischen Erkennung der Frakturschrift sehr gute Ergebnisse erzielt werden können, werden digitalisierte Druckwerke auch jenseits der genauen inhaltlichen Erfassung nutzbar, indem sie durch 
                distant reading und statistische Zugänge erschlossen werden können.
            
         Anders verhält es sich bei historischen Handschriften: Da heutzutage nur sehr wenige Personen über hinreichende paläographische Kenntnisse verfügen, stellen digitale Reproduktionen handgeschriebener Dokumente für den größten Teil des Publikums nicht viel mehr als bloße Abbildungen historischer Artefakte dar, die in ihrer Materialität zwar eine ganz bestimmte Oberflächenstruktur aufweisen, aber die darin transportierten Inhalte nur wenigen erfahrenen Lesern preisgeben. Zusätzlich erschwert wird die Lektüre im Fall von Tagebuchaufzeichnungen oder Notizbüchern, die nur selten auch für fremde Augen bestimmt waren. 
         Die Gewährleistung eines unbeschränkten und langfristigen Zugangs zu digitalisierten historischen Handschriftenarchiven ist also nicht 
                per se gleichbedeutend mit einer unbegrenzten Zugänglichkeit, Nutzbarkeit und Weiterverwertbarkeit ihrer Inhalte. Eine wichtige Aufgabe der 
                Digital Humanities bei der nachhaltigen Pflege des kulturellen Erbes ist deshalb eine über die bloß konservierende Ablichtung hinausgehende Erschließung der in diesen Textbeständen enthaltenen Informationen. Die Fragestellung ist also: Wie lassen sich diese Daten erfassen und für 
                close- wie auch für 
                distant reading-Prozesse aufbereiten? Lässt sich ein Zugang schaffen, ohne den gesamten Bestand manuell zu bearbeiten? Und inwieweit kann die klassische paläographische Hand- und Kopfarbeit durch automatisierte Prozesse ersetzt werden? Der Beitrag stellt diese Problemlage zunächst am Fallbeispiel der Senckenberg-Tagebücher exemplarisch dar und zeigt anschließend eine Lösungsstrategie auf, bei der manuelle und digitale Methoden kombiniert zum Einsatz kommen und bereits vorhandene, frei zugängliche Software verwendet wird. 
            
         Der Frankfurter Arzt Johann Christian Senckenberg (1707–1772) hinterließ handschriftliche Aufzeichnungen im Umfang von 53 Quartbänden mit je etwa 700 Seiten. Während die späteren Bände einesteils in ausführlichen ärztlichen Fallstudien und anderenteils in kritischen Bemerkungen über die sittlichen Missstände der Reichsstadt bestehen, befassen sich die mit 
                Observationes in me ipso factae übertitelten ersten dreizehn Jahrgänge hauptsächlich mit dem Schreiber selbst. Da Senckenberg dem radikalen Pietismus nahestand und sich ganz aus dem kirchlichen Gemeindeleben zurückgezogen hatte, erfüllten die frühen Tagebücher hauptsächlich die Funktion eines religiösen Gewissensspiegels. Darüber hinaus notierte er über Jahrzehnte hinweg täglich seinen Speiseplan, sein Bewegungspensum und seine Stoffwechselaktivität ebenso detailliert wie die jeweilige Wetterlage, die Umgebungstemperatur und den Luftdruck, die er mit den wechselnden Zustände seines Gemüts und mit äußeren Umwelteinflüssen in Beziehung setzte. Der Zweck dieser akribischen Beobachtungen war seine diätetische und moralische Selbstoptimierung, welche sowohl eine untadelige Lebensführung im Diesseits als auch seine Erlösung im Jenseits gewährleisten sollte. Zugleich dienten sie der Erfassung und Deutung von Korrelationen zwischen Vorgängen in Leib, Seele, Natur und Kosmos. 
            
         Diese Aufzeichnungen stellen sich nicht nur dem heutigen Publikum als Big Data dar, sondern wurden bereits von ihrem Autor als riesiger Datenpool konzipiert: Zeitweise brachte er täglich bis zu 5000 Wörter in deutscher und lateinischer Sprache zu Papier, so dass er in manchen der insgesamt 43 Jahrgänge ca. 2600 Seiten sehr eng mit jeweils etwa 900 Wörtern beschrieb. Zugleich pietistisches Selbstzeugnis und wissenschaftliche Aufzeichnungsform, ist dieser schriftlich fixierte und weltweit einzigartige Erfahrungsschatz eine Fundgrube für die Erforschung der frühneuzeitlichen Religions- und Wissenschaftsgeschichte. Darüber hinaus wirft er neue historische Schlaglichter auf die aktuell diskutierten Möglichkeiten und Grenzen der Nutzung großer Datensammlungen und ihr Verhältnis zur Theorie (vgl. Anderson 2008; boyd et al. 2012, Rosenberg 2014).
         Mit Förderung durch die Dr. Senckenbergische Stiftung wurden die insgesamt ca. 40.000 Quartseiten in hochaufgelöster Form digitalisiert und von der Universitätsbibliothek Frankfurt unter Open Access-Bedingungen online zur Verfügung gestellt (UB Frankfurt 2013–2016). Am Frankfurter Institut für Deutsche Literatur und ihre Didaktik entsteht derzeit eine TEI/XML-basierte Online-Edition der Aufzeichnungen, welche gleichfalls von der durch den Autor selbst begründeten Stiftung finanziert wird. In Anbetracht ihres riesigen Umfangs und der schwer entzifferbaren Handschrift Senckenbergs ist eine zeitnah fertigstellbare Volltextedition des Gesamtbestandes schwerlich möglich und wäre aufgrund der bei dieser Aufzeichnungspraxis naturgemäß häufig auftretenden inhaltlichen Redundanzen auch nicht sinnvoll. Aus diesem Grund wurde im Vorfeld eine repräsentative Bandauswahl getroffen, welche nach den Maßgaben der historischen Signifikanz, der thematischen Vielfalt und der größtmöglichen Vermeidung von Redundanzen erfolgte. Die inhaltliche Komplexität und die auf schnelle Erfassung großer Datenmengen ausgerichtete Schreibroutine des Autors machen zudem eine Transkriptionsweise erforderlich, die weit über die diplomatisch-zeichengetreue Textwiedergabe hinausgeht: Abgesehen von der Tatsache, dass es sich um einen halb frühneuhochdeutschen und halb lateinischen Text handelt und der Schreiber oftmals mehrfach in einem Satz zwischen beiden Sprachen hin- und herwechselt, sind viele der Sätze so komplex, dass der Leser zum Verständnis auf alle verfügbaren grammatischen Merkmale angewiesen ist. Vor allem die morphologischen Merkmale sind aber im Deutschen wie auch im Lateinischen hauptsächlich in eben jenen Wortendungen enthalten, welche häufig durch Abkürzung entfallen. Ein ähnliches Problem besteht auch hinsichtlich der Symbole, die größtenteils dem alchemistischen Kontext entstammen: Sie können ein ganzes Wort oder auch nur einen Teil davon ersetzen, bis zu vier verschiedene Wortbedeutungen und noch viel mehr grammatische Formen repräsentieren und in völlig verschiedenen semantischen Umgebungen erscheinen. Um dem Leser einen hinreichenden Zugang zum Sprachgebrauch des Autors zu bieten und ein Textverständnis überhaupt erst zu ermöglichen, müssen Abkürzungen und Symbole ihrem kontextspezifischen Zusammenhang entsprechend aufgelöst und sowohl semantisch als auch grammatikalisch in den Text eingepasst werden. 
         Auf den ersten Blick scheinen digitale Methoden hier kaum weiterzuhelfen: Zu wenig deutlich ist die Schrift, zu komplex die Inhalte, zu spezifisch das Vokabular und zu mehrdeutig die einzelnen Zeichen. Hinzu kommt noch, dass sich sowohl Senckenbergs Handschrift als auch die Inhalte seiner Aufzeichnungen im Verlauf von vier Jahrzehnten stark veränderten und mithin ganz neue graphische Muster hervorbrachten. Wenngleich die Transkription der Texte nur händisch erfolgen kann, wird dadurch doch ihre Maschinenlesbarkeit überhaupt erst gewährleistet und damit die grundlegende Voraussetzung für automatisierte Prozesse sowie die Anwendung, Weiterentwicklung und Schulung der sie ermöglichenden Technologien geschaffen. So erfordert das Training des Tools Transkribus (Universität Innsbruck o.J.) zunächst einmal eine ausreichende Menge an manuell erzeugten und präzisen Texttranskriptionen und die anschließende händische Überarbeitung des Outputs (vgl. Transkribus Wiki o.J.). Aufgrund der wachsenden Nachlässigkeit der Handschrift und der inhaltlichen Heterogenität der drei Unterbestände muss der Lernprozess für jeden Teilbestand separat erfolgen. Nach Abschluss dieses Lernprozesses ist jedoch zumindest eine halbautomatische Texterfassung möglich. Der erkannte Text kann anschließend elektronisch durchsucht und wissenschaftlich ausgewertet werden. 
         Ein ähnliches Verhältnis zwischen manuellen und automatisierten Prozessen besteht hinsichtlich der inhaltlichen Erschließung der Texte. Da sie von einem einzigen Schreiber mit umfassender grammatischer Bildung stammen, liegt nur eine geringe orthographische Varianz bei der Schreibung ein- und desselben Wortes vor. Anders als in heterogenen Korpora, die Texte mehrerer Schreiber mit unterschiedlichem Bildungshintergrund und sprachgeografischer Herkunft versammeln, ist deshalb eine vorherige händische Normierung der Grafie nicht notwendig (vgl. demgegenüber Faßhauer et al. 2013, Faßhauer et al. 2014). Mit Hilfe der vorliegenden Transkriptionen kann deshalb ein effizientes Training des Tools TreeTagger (Schmid 1994-) für das Frühneuhochdeutsche und Neulateinische vorgenommen werden. Die halbautomatisch generierten Lemmata und Part-of-Speech-Tags, welche sowohl für die manuellen Transkriptionen als auch für die automatisch erfassten Texte erstellt wurden, werden anschließend in den Partitur-Editor der Software EXMARaLDA (Hedeland et al. o.J.) eingespielt. Mit dem zugehörigen Analysetool EXAKT werden per RegEx-Suche auf der Lemmaspur zunächst alle Nomina herausgefiltert und in einem manuellen Prozess Schlagwörter ausgewählt (ähnlich auch Biehl et al. 2015). Aus der Untermenge der großgeschriebenen Substantive, die sich mittels der automatischen Sortierfunktion der Trefferliste leicht ermitteln lassen, werden alle Personen- und Ortsnamen entnommen. Anhand dieser Recherchezugänge kann nun das gesamte Korpus systematisch recherchiert werden. Die von EXAKT angebotenen Anfragen über RegEx und Levenshtein-Distanzen ermöglichen dabei eine schreibweisentolerante Begriffsermittlung, wodurch mancher HTR-Lesefehler überwunden werden kann.
      
      
         
            
               Bibliographie
               
                  Anderson, Chris (2008): 
                        „The End of Theory: The Data Deluge Makes the Scientific Method Obsolete“, 
                        in: 
                        Wired Magazine
                  http://www.wired.com/2008/06/pb-theory/
               
               
                  Biehl, Theresia / Lorenz, Anne / Osierenski, Dirk (2015): 
                        „Exilnetz33. Ein Forschungsportal als Such- und Visualisierungsinstrument“,
                        in: Baum, Constanze / Stäcker, Thomas (eds.): 
                        Grenzen und Möglichkeiten der Digital Humanities (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 1).
                    
               
                  Boyd, Danah / Crawford, Kate (2012): 
                        „Critical Questions for Big Data“,
                        in: 
                        Information, Communication & Society 15 (5): 662–679
                        10.1080/1369118X.2012.678878.
                    
               
                  Fasshauer, Vera / Lühr, Rosemarie / Prutscher, Daniela /Seidel, Henry (2013): 
                        Dokumentation der Annotationsrichtlinien für das Korpus Frühneuzeitliche Fürstinnenkorrespondenzen im mitteldeutschen Raum.
                        dwee.eu/Rosemarie_Luehr/userfiles/downloads/Projekte/Dokumentation.pdf.
                    
               
                  Fasshauer, Vera / Lühr, Rosemarie / Prutscher, Daniela /Seidel, Henry (2014): 
                        Fürstinnenkorrespondenz (version 1.1), Universität Jena, DFG. LAUDATIO Repository.
                        http://www.indogermanistik.uni-jena.de/Web/Projekte/Fuerstinnenkorr.htm.
                    
               
                  Hedeland, Hanna / Lehmberg, Timm / Schmidt, Thomas / Wörner, Kai (o.J.): 
                        EXMARaLDA. Werkzeuge für mündliche Korpora
                  http://www.exmaralda.org/ [letzter Zugriff 21. März 2016].
                    
               
                  Rosenberg, Daniel (2014): 
                        „Daten vor Fakten“,
                        in: Reichert, Ramón (ed.): 
                        Big Data: Analysen zum digitalen Wandel von Wissen, Macht und Ökonomie. 
                        Bielefeld: transcript Verlag 133–156.
                    
               
                  Schmid, Helmut (1994-): 
                        TreeTagger. A part-of-speech tagger for many languages. 
                        http://www.cis.unimuenchen.de/~schmid/tools/TreeTagger/ [letzter Zugriff 20. August 2016].
                    
               
                  Transkribus Wiki (o.J.): 
                        Transkribus-Benutzeranleitung. 
                        https://transkribus.eu/wikiDe/index.php/Hauptseite [letzter Zugriff 20. August 2016].
                    
               
                  UB Frankfurt =Universitätsbibliothek Frankfurt am Main (2013-2016): 
                        Nachlass Johann Christian Senckenberg. 
                        http://sammlungen.ub.uni-frankfurt.de/senckenberg/nav/index/all [letzter Zugriff 20. August 2016].
                    
               
                  Universität Innsbruck (o.J.): 
                        Transkribus. 
                        https://transkribus.eu/Transkribus/ [letzter Zugriff 20. August 2016].
                    
            
         
      
   



      
         
            
            Der hier vorgestellte Workflow für die Digitalisierung und Integration bibliographischer Informationen ist Teil des Forschungsprojektes 
            Niklas Luhmann - Theorie als Passion. Wissenschaftliche Erschließung und Edition des Nachlasses
            . Das Langzeitvorhaben (2015-2030) an der Fakultät für Soziologie der Universität Bielefeld in Kooperation mit dem Cologne Center for eHumanities (CCeH) wird im Akademienprogramm durch die Nordrhein-Westfälische Akademie der Wissenschaft und der Künste gefördert. Weitere Kooperationspartner sind das Archiv und die Bibliothek der Universität Bielefeld.
            
               1
            
         
         Ziel des Gesamtprojektes ist die Sicherung, Digitalisierung, Erschließung, werkgenetische Erforschung und teilweise Edition des wissenschaftlichen Nachlasses Niklas Luhmanns. Zu diesem Zweck werden die bewahrenswerten Teile des Nachlasses (Manuskripte, Zettelkasten, Korrespondenz, Bibliothek etc.) zunächst archivarisch gesichert und in den Teilen, die wissenschaftlich erschlossen werden sollen, digitalisiert, sowie für die weitere Bearbeitung bereitgestellt. Die daran anschließende Edition will den Luhmannschen Nachlass als geistesgeschichtliches Dokument der wissenschaftlichen Forschung sowie der interessierten Öffentlichkeit zugänglich machen. Sie bildet dadurch zugleich die Grundlage für die Entwicklung einer kritisch gesicherten infrastrukturellen Wissensressource, auf welche die interdisziplinäre und internationale Forschung zur und mit der Theorie Luhmanns zukünftig zurückgreifen kann.
         Bibliographische Informationen stellen ein besonders wichtiges verbindendes Element zwischen den zu erschließenden und ggf. zu edierenden Materialien aus dem Nachlass Niklas Luhmanns dar. Ihre Modellierung, Zusammenführung und Visualisierung verspricht einen vollständigen Überblick über Grundlagen, Rezeption und Verbreitung des Luhmannschen Werks aber auch detaillierte Einblicke in seine Arbeitsweise. 
         
            Quellen und Forschungsfragen
            
               In Hinblick auf den 
               Werkkosmos 
               Niklas Luhmanns
                
               wurde am Institut für Soziologie in Bielefeld eine Bibliographie aller Publikationen erstellt, die in digitaler Form finalisiert werden soll. Enthalten sind Monographien, Aufsätze, Rezensionen, Festschriften. Gelistet werden außerdem alle 
               bearbeiteten 
               Neu-Auflagen und Übersetzungen. Die insgesamt etwa 1.900 Datensätze sind stark miteinander vernetzt: Sowohl auf der Ebene von Monographien als auch bei Artikeln wird jeweils auf Nachdrucke, Übersetzungen und weitere Auflagen hingewiesen. Die Aufbereitung 
               dieser Informationen soll anschaulich machen, in welchen Medien Luhmann hauptsächlich publiziert hat und wie sich seine Schriften durch Übersetzungen und Nachdrucke international verbreitet haben. 
            
            
               
                  
                  Schematische Darstellung zur Vernetzung der Datensätze
               
            
            
               Den 
               Lesekosmos
                
               Niklas Luhmanns darstellbar und damit erforschbar zu machen, ist ein zweites zentrales Anliegen. Die folgenden Quellen bibliographischer Information liegen dazu vor: 
            
            
               die zitierte Literatur in den Zettelkästen
               eigenständige bibliographische Abteilungen je Zettelkasten (ca. 17.800 Einträge)
               die zitierte Literatur in seinen eigenen publizierten Werken
               die Literaturhinweise in seinen unpublizierten Manuskripten
               die private Arbeitsbibliothek (ca. 4.000)
            
            Nach Aufarbeitung dieser Materialien können wichtige werkhistorische Fragestellungen untersucht werden: Welche und wie viel Literatur (aus welchen wissenschaftlichen Disziplinen) hat Luhmann im Laufe seiner Forschung rezipiert bzw. verarbeitet? Gibt es bestimmte Werke / Autoren, die er wiederkehrend konsultiert bzw. zitiert hat? Gibt es bevorzugte Zeitschriften?
            Der Zettelkasten ist der zentrale Startpunkt für die Reise in den bibliographischen Kosmos. Er enthält durchgehend auf den Notizzetteln Hinweise auf benutzte Literatur und ab der Mitte der 70er Jahre auch eine eigenständige Bibliographie-Abteilung (insgesamt etwa 17.800 Einträge). 
            Eine Auswertung der Zitatnachweise in den gedruckten Bänden Luhmanns soll dann die Verbindung von Zettelkasten und Publikationen erleichtern. Wieviel der im Zettelkasten nachgewiesenen Lektürearbeit ist tatsächlich in die Publikationen eingegangen? Und gibt es Stellen in den Publikationen, die scheinbar keine Wurzeln im Zettelkasten haben? 
            In den unveröffentlichten Manuskripten finden sich neben bereits standardisierten Literaturnachweisen häufig auch handschriftliche Nennungen von Notizzetteln, die den Kreis über den Zettelkasten wieder schließen.
         
         
            Datenmodell
            Zu unseren Zielen gehört die Vernetzung der Datensätze zu visualisieren, um so den Werk- bzw. Lesekosmos erfahrbar zu machen. In Anlehnung an das FRBR-Modell
                    2 verstehen wir die hier zu modellierenden bibliografischen Datensätze als 
                    Manifestationen eines 
                    Werks
                (Wiesenmüller 2008: 350
                    ). Im Fall der Publikationen Niklas Luhmanns selbst, sind bereits Bezugsinformationen zu anderen 
                    Manifestationen (z.B. im Sinne von "ist Teil von") und 
                    Expressions (z.B. im Sinne von "ist Übersetzung von") enthalten. Perspektivisch werden hier noch übergeordnete Datensätze zu 
                    Werken und deren 
                    Expressions entstehen, mit denen die Fachwissenschaftler das Material weiter strukturieren und die Werkgenese genauer beschreiben können.
                    3 Vor allem im Fall der unterschiedlichen Manuskriptfassungen Luhmanns wird die Bestimmung übergreifender 
                    Werke
                eine explizit fachwissenschaftliche Aufgabe sein.
                
            
               Die Basiskodierung erfolgt zunächst ohne 
               weiterführende 
               ontologischen Differenzierungen und Deutungen in voneinander getrennten Datensätzen 
               auf 
               gleicher Ebene
               .
                
               Die TEI bietet 
               hier 
               mit  ein geeignetes standardisiertes Datenmodell
                um 
               Manifestationen
                zu beschreiben, aber auch bereits etablierte Wege wie Personen, Körperschaften, Werktitel und später auch Sachschlagwörter mit Normdatensätzen
               
                  4
               
                verbunden werden können.
               
                  5
               
                Beziehungen zu anderen Datensätzen 
               werden
                über  angereichert. 
               Alle Erwähnungen und Verweise auf das bibliograhierte Item werden über -Elemente
                
               aufgenommen
               . 
               Selbst weiterführende 
               inhaltliche 
               Informationen aus den unterschiedlichen Nachlasszusammenhängen können über  mitgeführt werden. 
            
            
               Die Offenheit einer TEI-Auszeichnung ermöglicht uns, im Gegensatz zu anderen bestehenden bibliografischen Formaten, die meist für Bibliothekszusammenhänge 
               zugespitzt wurden
               , alle vorliegenden Informationen im Datensatz selbst mitzuführen und 
               schwellenlos
                
               projektintern
                für die restlichen in TEI kodierten Materialien 
               nutzbar 
               vorzuhalten
               .
               
                  6
               
            
            Umsetzung im Detail
            
               Die Guidelines empfehlen für strukturierte bibliographische Informationen -Items in Listen ().
               
                  7
               
                
               Wir weichen in diesem Punkt ab und erzeugen für jede 
               Manifestation
                einen eigenständigen bibliographischen Datensatz in Form eines -Single-Files.
               
                
               Jeder Datensatz enthält nur ein  bzw. ein  und ein . Die Dateien erhalten
                
               einen eindeutigen Dateinamen (Name des Autors + Erscheinungsjahr + ggf. Erweiterung) und eine entsprechende xml:id. Jedes Vorkommen, etwa im Zettelkasten oder in einem Manuskript, wird in einem -Element dokumentiert. Hinweise auf Reprints, Übersetzungen und weiterführende Informationen werden in Form von  ergänzt. Da die Dateinamen und xml:ids auf Basis des bibliographierten Werks sprechend benannt wurden, ist eine direkte Verlinkung der Datensätze untereinander problemlos möglich. 
                
            Die Aufspaltung in -Single-Files lässt sich nun auch für die Vorhaltung von unselbstständigen Titeln nutzen. Die Titelinformation für einen Aufsatz wird in  erfasst, die Information zum Sammelband in einem anschließenden -Element. Um die Wiederholung dieser Information für jeden Aufsatz des Sammelbands zu umgehen, wird ein eigenes -Single-File für den Sammelband erzeugt. Über einen XInclude
                    8-basierten Weg wird das -Element des Sammelbands in die Datei des Artikels eingebunden. Damit ist die TEI-Datei des Artikels auch während der Bearbeitung vollständig (zusätzlich zu  wird das externe  des Sammelbands eingebunden).
                    9
            
            
               
                  
                  Gliederungsansicht eines -Elements, Typ  Sammelband
               
            
            
               ”
               
                  
                     
                     Gliederungsansicht eines -Elements, Typ “Artikel in Sammelband
                  
               
            
         
         
            Arbeitsumgebung
            
               Als Arbeitsumgebung zur Bearbeitung und Neuerfassung von bibliographischen Informationen kommt ein speziell für dieses Projekt entwickeltes oXygen-Framework
               
                  10
               
                
               zum Einsatz. Ein solches Framework ist eine Erweiterung für den oXygen XML-Editor, welches spezifische Vorgaben für die grafische Darstellung und Funktionsweise eines Eingabeformulars für das hier entwickelte Datenmodell in oXygen macht. Außerdem werden darin benutzerdefinierte Schaltflächen angelegt, die auf den Workflow des Bearbeiters ausgerichtet sind. Damit ist es für Laien ohne Vorkenntnisse in XML oder TEI auf einfache Weise möglich, bibliographische Informationen auf Grundlage des verwendeten Datenmodells zu erstellen, 
               zu 
               bearbeiten und auszuzeichnen.
                        Das Framework-Verzeichnis wird auf den Rechnern der Bearbeiter zur Verfügung gestellt, von oXygen als solches erkannt und ist damit vom Bearbeiter verwendbar.
                    
            
            
               
               
                  
                     
                     oXygen-Framework (Author-Mode)
                  
               
            
         
         
            Veröffentlichung
            
               Nachdem die Daten in die Datenbank importiert wurden, werden sie automatisiert im Projektportal veröffentlicht. Dies geschieht mit einem modular aufgebauten Web-Präsentationssystem, bestehend aus etablierten Open-Source-Softwarelösungen wie eXist XML Database
               
                  11
               
               , NodeJS
               
                  12
               
               , ReactJS
               
                  13
               
               , sowie dem Design-Framework Material Design Lite
               
                  14
               
               . Die Datenbank verknüpft automatisch die verschiedenen Datensätze und gibt sie aus, sodass der Benutzer des Portals sofort sehen kann, in welchem Verhältnis ein Werk zu verwandten Werken steht. Eine Visualisierung mittels eines Netzwerk-Graphs soll diese Verknüpfungen zusätzlich veranschaulichen.
            
            Die hier dargestellten Workflows setzen ausschließlich auf Open-Source-Softwarelösungen, sowie offene Standards wie TEI. Die Weitergabe des Frameworks mit allen Templates, der ODD, des Schemas und einer Dokumentation wird angestrebt. Die generische Architektur ist nachhaltig und nachnutzbar von anderen Projekten mit ähnlichen Anforderungen.
            Das Luhmann-Projekt eignet sich aufgrund der Heterogenität des bibliographischen Materials sehr gut als Ausgangspunkt zur Entwicklung eines allgemeinen Modells, das vom konkreten Projekt abstrahiert werden kann und soll. Im CCeH wird der Workflow schon von weiteren Projekten eingesetzt und auf seine Tauglichkeit geprüft.
            Die für die Publikationen Luhmanns, ihre Reprints und Veröffentlichungen vergebenen Namen und IDs werden, neben den luhmann-basierten Zettelkennungen, als autoritative Identifikatoren für das Werk Luhmanns nachnutzbar sein.
         
      
      
         
            
               Website des Niklas Luhmann-Archivs 
               
                
               [letzter Zugriff 
               30
               . 
               November 
               2016] 
            
            
                
               “Functional Requirements for Bibliographic Records” (FRBR)
               : 
               
                
               [letzter Zugriff 
               30
               . 
               November 
               2016] 
            
            
                Ein ähnliche Ansatz findet Anwendung im Projekt "Wome
               n
                Writer
               s
                in Review", vgl 
               
                
               [letzter Zugriff 
               30
               . 
               November 
               2016] 
            
            Gemeinsame Normdatei der Deutschen Nationalbibliothek (GND), 
                            
               [letzter Zugriff 
               30
               . 
               November 
               2016] 
            
            Nach dem FRBR-Modell betrifft das die Entitäten der Gruppe 2 und 3 (Tillet 2010: 3)
            
                
               Bei späteren Exporten in andere bibliographische Formate, wie etwa BibTeX, können Hauptfelder gemappt werden, wohingegen Zusatzinformationen - je nach inhaltlicher Zielsetzung des Ausgabeformat
               s
                - 
               schlicht 
               nicht exportiert werden. 
            
            
                
               Vgl. TEI-Guidelines, Abschnitt 3.11: Bibliographic Citations and References (
               
                
               [letzter Zugriff 
               30
               . 
               November 
               2016] 
            
            
               
                
               Vgl. W3C Empfehlung: 
               
                
               und TEI Guidelines 
               
                
               [letzter Zugriff 25. August 2016]
            
             Für die Weitergabe der Daten nach außenwird das XInclude wieder aufgelöst und für die Verlinkung ergänzte IDs werden gelöscht, so dass die Ausgabe von standardkonformen und vollständigen -Files sichergestellt ist.
            
               
                
               Vgl. oXygen Visual (WYSIWYG) XML
                
               Editors:
               
                
               [letzter Zugriff 25. August 2016]
            
            
               
                
               Vgl. 
               
                
               [letzter Zugriff 25. August 2016]
            
            
               
                
               Vgl. 
               
                
               [letzter Zugriff 25. August 2016]
            
            
               
                
               Vgl. 
               
                
               [letzter Zugriff 25. August 2016]
            
            
               
                
               Vgl. 
               
                
               [letzter Zugriff 25. August 2016]
            
         
         
            
               Bibliographie
               
                  Deutsche Nationalbibliothek (Hg.) (2009): 
                        Funktionale Anforderungen an bibliografische Datensätze. 
                        Abschlussbericht der IFLA Study Group on the Functional Requirements for Bibliographic Records. Geänderte und korrigierte Fassung, Februar 2009 (Leipzig/Frankfurt am Main/Berlin, 2009), 
                        . 
                        [letzter Zugriff 30. November 2016].
                    
               
                  Tillett, Barbara (2003): 
                        „What is FRBR?“. Library of Congress Cataloging Distribution Service, 2004 , 
                        in: 
                        Technicalities 25 (5) 
                        
                        [letzter Zugriff 30. November 2016].
                    
               
                  Wiesenmüller, Heidrun / Horny, Silke (2015):
                        Basiswissen RDA: Eine Einführung für deutschsprachige Anwender.
                        De Gruyter Saur.
                    
               
                  Wiesenmüller, Heidrun (2008): 
                        „Zehn Jahre ‚Functional Requirements for Bibliographic Records“: Vision, Theorie und praktische Anwendung“, 
                        in: 
                        Bibliothek, Forschung und Praxis 32 (3).
                    
               
                  Niklas-Luhmann-Archiv:
                        Webseite
                         [letzter Zugriff 30. November 2016].
                    
               
                  TEI:
                        Bibliographic Citations and References in den TEI-Guidelines
                   [letzter Zugriff 30. November 2016].
                    
               
                  IFLA:
                        Functional Requirements for Bibliographic Records (FRBR)
                   [letzter Zugriff 30. November 2016].
                    
            
         
      
   



      
         Vor dem Hintergrund des Akademievorhabens 
                PROPYLÄEN. Forschungsplattform zu Goethes Biographica werden zwei aktuelle Konzeptionsmethoden aus dem Bereich des Software-Engineering vorgestellt, die sich unserer Erfahrung nach als geeignete Grundlage für nachhaltige Konzeptionsprozesse in Digital Humanities Projekten erwiesen haben: 
                Domain Driven Design sowie 
                Behaviour Driven Development. Zunächst erfolgt ein Überblick über Aspekte digitaler Nachhaltigkeit bei der Beantragung geisteswissenschaftlicher Langzeitvorhaben im Akademienprogramm.
            
         Die PROPYLÄEN sindein 2015 im Akademienprogramm gestartetes Forschungsvorhaben der Klassik Stiftung Weimar, der Sächsischen Akademie der Wissenschaften zu Leipzig und der Akademie der Wissenschaften und der Literatur Mainz (
                http://www.goethe-biographica.de). Geplant ist bei einer Gesamtlaufzeit von 25 Jahren Fortführung und Abschluss von vier (Print-)Publikationsreihen: Die Editionen der Briefe von sowie an Goethe, seine Tagebücher und die Zeugnisse seiner Begegnungen und Gespräche. Die gleichzeitig entstehende Forschungsplattform wird sukzessive alle Biographica in einer technisch und inhaltlich offenen Infrastruktur bereitstellen sowie Recherchezugänge und Visualisierungen auf den Datenbestand anbieten.
            
         Eine Voraussetzung für die Aufnahme eines Neuvorhabens in das Akademienprogramm ist ein ausgearbeitetes Digitalisierungskonzept, zu dem unter anderem eine Strategie zur Langzeitarchivierung und Langzeitverfügbarkeit der Forschungsergebnisse gehört (Vgl. Herrmann 2016: 9). Häufig konzentriert sich dieses auf eine enge Definition von Forschungsdaten, die unter diesem Begriff lediglich edierten Text versteht. Im Falle einer Forschungsplattform wie der PROPYLÄEN sind nicht nur die in der Datenschicht – Faksimiles, Editionstext, kurz die Gesamtheit der digitalen Forschungsdaten – befindlichen Komponenten für eine nachhaltige und langfristige Bereitstellung vorzusehen (zu Archivierungsschicht und Präsentationsschicht vgl. Pempe 2012: 141). Daneben sollte auch die Auswahl geeigneter Technologien ein Teil der Antragsstrategie sein. Unter Technologie sei alles vom Datenformat über ein Content-Management-System bis zur virtuellen Forschungsumgebung verstanden.
                 Angesichts des Verhältnisses zwischen langfristiger Projektförderung – die Akademienvorhaben werden 12 bis 25 Jahre gefördert – und der technischen Entwicklung ist die Vorstellung der einmaligen Einrichtung einer digitalen Arbeitsumgebung und Publikationsplattform am Anfang der Projektlaufzeit, welche daraufhin für 25 Jahre verwendet würde, nur mit begleitender Wartung und Weiterentwicklung denkbar. Wird das gewählte Datenformat in 30 Jahren noch maschinenlesbar sein? Absolute Sicherheit gibt es in diesem Fall nicht, aber allgemein gilt die Verwendung von (semistrukturierten) Rein-Textformaten – wie XML nach TEI – gegenüber proprietären Formaten als nachhaltiger.
            
         Der 
                Mehrwert der elektronischen Fassung entsteht aber vornehmlich durch neue Verbindungs-, Gruppier-, Sortier-, Filter- und Suchmöglichkeiten, die auf dynamischen Verarbeitungsmechanismen, also auf der Geschäftslogik der Anwendung, basieren. Diese nicht-statischen Elemente, die sich insbesondere in der Präsentationsschicht einer geisteswissenschaftlichen Webanwendung konkretisieren, müssen in ihrer Funktionalität genauso nachvollziehbar und reproduzierbar über das Projektende hinaus zur Verfügung stehen.
            
         Komplexer als die Datenschicht ist demnach die dauerhafte Erhaltung der Applikationslogik und Präsentationsschicht: Wie kann die webbasierte Verarbeitung von Anfragen und die Wiedergabe von Ergebnissen auch 10 Jahre nach Projektende noch funktionsfähig gehalten werden? Eine aktuell diskutierte Strategie ist die virtuelle Kapselung sämtlicher Softwarekomponenten einer Applikation, die künftig ein Emulieren aller Komponenten auf aktuellen Betriebssystemen ermöglicht (siehe dazu bspw. 
                ). Vorhaben wie die PROPYLÄEN müssen dieser Herausforderung während der Projektlaufzeit begegnen, indem sie eine hohe Flexibilität sowie stete Aktualisierung und Anpassung der Technologien ihrer Infrastruktur in der Grundplanung vorsehen. Um diesen Prozess auf eine transparente Grundlage zu stellen, gilt es bei der informationstechnischen Gestaltung des Forschungsvorhabens eine möglichst 
                gegenstandsnahe Abstraktion zu wählen und die Anforderungen an die Software auf 
                formalisierte Weise zu dokumentieren. Folgende Annahmen und Schlüsse gehen dieser Überlegung voran:
            
         
            Es wird während der Projektlaufzeit intern wie extern neue Erkenntnisse zu Goethes Zeit und Wirken geben, die unseren Blick auf den Forschungsgegenstand verändern. Dennoch ist es möglich, übergreifende Wissensobjekte zu identifizieren und in Form von informatischen 
                    Entitäten abzubilden.
                
            
               Technologieunabhängige, anwendungsorientierte und
                     allgemeinverständliche Funktionsbeschreibungen der Forschungsplattform sind formulierbar. Neue Erkenntnisse im Bereich der Goethe-Forschung wie im Bereich der 
                    usability werden diese Anforderungen innerhalb der Projektlaufzeit höchstwahrscheinlich verändern.
                
         
         In der 
                Konzeptionsphase einer nachhaltigen digitalen Grundlage für den Daten-, Präsentations- und Applikationsteil der Forschungsplattform PROPYLÄEN sind daher zwei Fragen leitend, deren Beantwortung uns nicht nur für Langzeitvorhaben mit Digital-Humanities-Anteil zentral erscheint (vgl. zu Prozessphasen und zur Konzeption hier und folgend: Schrade 2016: Step 14–21, zur Konzeption Step 17):
            
         
            Wie lassen sich 
                    Biographica Goethes für technische und geisteswissenschaftliche Anforderungen nachhaltig modellieren bei gleichzeitiger Bewahrung der Flexibilität in der Anwendungsarchitektur für die Integration gegebenenfalls noch nicht bekannter digitaler Ressourcen?
                
            Wie lassen sich die funktionalen Anforderungen an eine digitale Rechercheumgebung aus der Fachcommunity
                     nachhaltig formulieren und dokumentieren?
                
         
         Zur Beantwortung können zwei Konzeptionsmethoden aus dem Bereich des Software-Engineering fruchtbar gemacht werden: 
                Domain Driven Design (DDD) (siehe Evans 2003, Vernon 2013)
                 und
                 Behavior Driven Development (BDD) (siehe North 2016)
                . Beiden ist gemein, dass sie unabhängig von Datenformaten, Programmiersprachen oder Präsentationstechnologien operieren.
            
         
            DDD nimmt an, dass einem Konzeptions- bzw. Entwicklungsprozess dann die größten Erfolgschancen und daraus resultierend die beste Nachhaltigkeit zukommen, wenn die 
                virtuellen Komponenten des informatischen Modells ausgehend von ihren 
                realen Entsprechungen gebildet werden. Damit kommt der fachwissenschaftlichen Logik, die sich aus dem Wissen von Domänen-Experten (im Anwendungsfall Goethe-Philologen) und deren Niederlegung in publizierten Editionsbänden ableitet, eine zentrale Rolle für die Modellierung zu. Die Ausmodellierung der Wissensdomäne entsteht iterativ in einem konstanten kommunikativen Prozess aller Projektbeteiligten. 
                DDD löst die gängige und nicht optimale Praxis einer ausschließlichen Bedarfsformulierung durch Geisteswissenschaftler und einer darauffolgenden Umsetzung durch Informatiker zugunsten eines gemeinsamen Modellierungsprozesses unter Verwendung einer für alle Beteiligten verständlichen (
                ubiquitären) Sprache auf. Dieser im Rahmen eines Projektes von allen Beteiligten zu entwickelnden Sprache liegen eine Reihe von Komponenten für ein Modell der Wissensdomäne (
                domain model) zugrunde, das in seiner Gesamtheit alle Eigenschaften, Beziehungen und „Geschäftsprozessen“ der zukünftigen Anwendung abbilden kann. Die wichtigsten Komponenten für die Fachdomäne „PROPYLÄEN“ sind:
            
         
            Objekte mit eigener Identität (
                    entities): Briefe, Faksimiles, Personen, Orte, Werke
                
            Objekte, die sich über die Gesamtheit ihrer Eigenschaften definieren (
                    value objects): Geokoordinaten (Länge-Breite), Datierungen (Anfangs-Enddatum), Korrespondenzvorgang (Sender-Empfänger) etc.
                
            Die Beziehungen dieser Objekte untereinander (
                    associations): Person → erwähnt in: Brief
                
            Zusammenfassungen von 
                    entities, value objects und 
                    associations zu
                     Aggregaten
                     (
                    aggregates); der Zugriff auf einzelne Komponenten eines Aggregates darf ausschließlich über die Entität an der Aggregat-Wurzel (
                    aggregate root) erfolgen. Aggregate gewährleisen logische Konsistenz und funktionale Integrität einer Anwendungs- bzw. Wissensdomäne: Brief (Brieftext, Kommentar, Faksimile, Datierung, im Brief erwähnte Orte und Personen)
                
            Dienste zwischen Objekten der Wissensdomäne (
                    services): Sortiere nach Datum etc.
                
         
         Die Fachdomäne kann grafisch dargestellt werden. Abbildung 1 dient der Illustration des DDD-Konzepts am Beispiel eines Teils der PROPYLÄEN-Domäne.
         
            
               
               Abbildung 1: Wissensdomäne Goethe PROPYLÄEN
            Dan Norths Idee des 
                BDD ist, Anforderungen an die geplante Applikation in Form von 
                user stories und verhaltensorientierten Testszenarien im Vorfeld der Realisierung unter Zuhilfenahme einer ubiquitären Sprache zu beschreiben. Durch die Verwendung von Schlüsselwörtern in formalisierter Notation lässt sich eine einheitliche und leicht verständliche Dokumentation erstellen. Aufgrund starker Nutzerorientierung wird 
                BDD im Qualitätsmanagement von Softwareanwendungen zu den 
                Akzeptanztests gerechnet (neben Unit-Tests, Funktionstests, Performance-Tests oder manuellen Tests). 
            
         Abbildung 2 zeigt ein BDD-Testszenario in den Goethe-PROPYLÄEN, das auf Gherkin-Notation basiert (Siehe 
                , Stand 26.08.2016). Die Schlüsselwörter 
                Funktionalität, Beschreibung 
            und 
            Details enthalten übergeordnete Informationen, während im Bereich der 
                Szenarios mit Hilfe der Operatoren 
                Szenario 
            bzw. Gege
            ben, Wenn, Dann, Und einzelne Anforderungen formuliert werden.
                 Die Ausführungen können dabei allgemein gehalten werden und lassen sich im späteren Verlauf auffächern.
            
         
            
               
               Abbildung 2: BDD-Testszenario in Gherkin-Notation
            Ein Vorteil ist, dass Szenarios durch den Einsatz einer verständlichen, formalisierten Sprache nicht nur gemeinsam zwischen Geisteswissenschaftlern und Informatikern während des Entwicklungsprozess formuliert werden, sondern dass bei syntaktisch korrekter Formulierung diese als Grundlage für automatisierte Testverfahren der Präsentationsschicht einer Webanwendung herangezogen werden können (bspw. mit dem BDD Testing-Framework behat; siehe 
                http://docs.behat.org (Stand 26.08.2016)). BDD ermöglicht somit eine zeit-ökonomische Überprüfung aller Bereiche und Funktionalitäten der Präsentationsschicht bei gleichzeitiger Dokumentation der Funktionalitäten in Reintext und natürlicher Sprache. Änderungen in der technischen Infrastruktur einer Plattform (Softwarewechsel der Datenbankschicht, Einführung neuer Programmroutinen) können mit deutlich gesteigerter Verlässlichkeit im Hinblick auf eine gleichbleibende, dauerhafte Funktion der Webanwendung durchgeführt werden.
            
         Die gemeinsame Modellierung einer Wissensdomäne mittels DDD und BDD führt notwendigerweise zu intensiver Auseinandersetzung mit allen geistes- wie informationstechnischen Aspekten des Projektes. Dadurch wird projektintern eine kommunikative Ebene entstehen, die eine gemeinsame und unmißverständliche Sprache verwendet und perspektivisch eine verbesserte Tiefenschärfe in Bezug auf funktionale Aspekte der zu realisierenden Forschungsanwendung ermöglicht.
         DDD und BDD helfen als Konzeptionsmethoden, alle gemeinsam getroffenen Entscheidungen über die gesamte Projektlaufzeit transparent und nachvollziehbar zu dokumentieren. Da es sich gleichzeitig im Bezug auf die BDD-Akzeptanztests um „ausführbares Wissen“ handelt, wird eine dauerhafte und gleichbleibende Funktionalität der Präsentationsschicht geisteswissenschaftlicher Webanwendungen gewährleistet. Dies trägt erheblich zu gesteigerter Nachhaltigkeit digitaler Komponenten eines geisteswissenschaftlichen Forschungsprojektes bei.
      
      
         
            
               Bibliographie
               
                  Evans, Eric (2003): 
                        Domain-Driven Design. Tackling Complexity in the Heart of Software. 
                        Boston et al.
                    
               
                  Herrmann, Dieter (2016): 
                        „E-Humanties im Akademienprogramm“, 
                        in: Union der Deutschen Akademien der Wissenschaften (Hrsg.): 
                        Die Wissenschaftsakademien – Wissensspeicher für die Zukunft. Forschungsprojekte im Akademienprogramm. 
                        Berlin / Mainz 9–11.
                    
               
                  North, Dan (2016): 
                        Introducing BDD. 
                         [letzter Zugriff 26. August 2016].
                    
               
                  Pempe, Wolfgang (2012): 
                        „Geisteswissenschaften“, 
                        in: Neuroth, Heike et al. (eds.): 
                        Langzeitarchivierung von Forschungsdaten – Eine Bestandsaufnahme. 
                        Göttingen 137–159.
                    
               
                  Schrade, Torsten (2016): 
                        Nachhaltige Online-Applikationen in den Geisteswissenschaften – Modellierung und Implementierung. Vortrag vom 11. April 2016, Hochschule Mainz, 
                         [letzter Zugriff 26. August 2016].
                    
               
                  Vernon, Vaugh (2013): 
                        Implementing Domain-Driven Design. 
                        Boston et al.
                    
            
         
      
   



      
         Der vorgeschlagene Beitrag dokumentiert den Fortschritt beim Aufbau unseres digitalen Korpus der literarischen Moderne (KOLIMO), das im Herbst 2016 in der Beta-Version veröffentlicht werden soll (abrufbar unter https://kolimo.uni-goettingen.de/). Im Fokus des Beitrags stehen das Verfahren zur Aufbereitung der Texte (insb. Format und Metadaten in TEI) und das linguistische Tagging (POS).
         Als Teil des laufenden Projektes Q-LIMO (Quantitative Analyse der literarischen Moderne) ist KOLIMO ein repräsentatives und computerlinguistisch solide aufbereitetes Korpus von narrativen fiktionalen Erzähltexten der literarischen Epoche der Moderne. Um durch stratifiziertes Sampling Repräsentativität (verstanden als „extent to which a sample includes the full range of variability in a population“; vgl. Biber 1994) zu ermöglichen, umfasst das Korpus ein möglichst breites Spektrum der literarischen Moderne, verteilt über kanonische und nichtkanonische Texte. So wurden in das Korpus bislang ca. 596.000.000 Wörter aus frei zugänglichen Repositorien importiert (s. Abbildung 1).
         Abbildung 1
         Gesamtanzahl Wörter aus den drei Hauptressourcen (Zwischenstand August 2016)
         
            
         
         Die Datenbank umfasst so neben Texten aus TextGrid und Gutenberg-DE (s. Abbildung 2) und dem DTA auch eine wachsende Zahl von Retrodigitalisaten. Das Sampling ist nicht zuletzt dadurch beeinflusst, dass KOLIMO auch das Kafka/Referenzkorpus (KAREK) beinhaltet, welches zum Ziel hat, Kafkas Texte und Texte, die Kafkas Schreibprozess beeinflusst haben könnten, möglichst umfangreich abzubilden (vgl. Herrmann / Lauer 2016a,b).
         Abbildung 2
         Screenshot KOLIMO-WebApp: Anzahl Wörter, Autoren und Einträge aus TextGrid & Gutenberg-DE (ohne DTA und andere Quellen, Stand August 2016)
         
            
         
         Um philologischen Ansprüchen an den editorischen Status literarischer Texte und die Abbildung von Epochen sowie Gattungskonzepten zu genügen, war eine hohe Genauigkeit und Konsistenz bei der informatischen Vorverarbeitung Textmarkup (XML-TEI) inklusive der Metadaten (Autor, Entstehungszeitpunkt und Gattung) besonders wichtig. Gerade die Auszeichnung der genannten Metadaten stellt eine Schnittstelle zwischen den informatischen und philologischen Dimensionen unseres Projektes dar: so sind Metadaten (a) die unabhängigen Variablen unserer stilistischen Analyse und (b) variieren in den von uns importierten Korpus-Ressourcen stark in qualitativer und quantitativer Hinsicht (Fehler, missing entries, unterschiedliche Ontologien). Der vorgeschlagene Beitrag wird so erstens einen kurzen Einblick in unsere Vorgehensweise geben, wobei Kriterien der Nachhaltigkeit berücksichtigt werden:
         
            Strategien der Textextraktion nach Genre-Kriterien unter Nutzung bestehender Metadatenschemata (ausgeschlossen wurden z.B. alle Texte, deren Metadaten sie als dramatisch und lyrisch ausflaggten, sowie Texte, die keine Absätze [without (tei:p)] enthielten);
            ein transparenter Workflow zur Korpusauszeichnung (internes eXist Webinterface);
            Anwendung eines standardisierten Text-Markups (u.a. Transformation der TextGrid und Gutenberg Header in das DTA-Basisformat TEI);
            Strategien der konsistenten Implementierung und Verbesserung von Metadatenschemata (Ineinandergreifen von händischen und skriptgestützen Workflows, wie Recherche zu [Erst-]Erscheinungsdaten bei missing entries, Zusammenführung der unterschiedlichen Gattungschemata, Überprüfung und ggf. Zuweisung von GNDs für Autoren);
            die nachhaltige Veröffentlichung des Korpus auf einem eigenen Server mit standardisierten Datenschnittstellen;
            Datenbankabbild (nonpublic) zur Langzeitarchivierung.
         
         Zweitens wird der Beitrag unser Vorgehen bezüglich der linguistischen Anreicherung zusammenfassen: Unter der Annahme, dass Stil quantitativ beschreibbar ist (vgl. Herrmann / van Dalen-Oskam / Schöch 2015), und dass Wortarten verlässliche Indikatoren für Register und Genrevariation sind (vgl. z.B. Biber / Conrad 2009), haben wir uns für die linguistische Annotation auf POS (STTS Tagset; vgl. Schiller / Teufel / Thielen 1995) entschieden. POS sind im Vergleich mit anderen Variationsmarkern durch eine relativ akkurate automatische Annotation besonders praktikabel. Das Webinterface liefert variablen Zugriff auf die annotierten Daten, u.a. eine Volltextansicht (siehe Abbildung 3); geplant sind zur Veröffentlichung die Exportierbarkeit in .csv-Files und TCF-Format.
         Abbildung 3
         Screenshot KOLIMO WebApp Textview POS-Tagging
         
            
         
         Zwar liefern bereits trainierte Modelle von einigen Taggern (z.B. TreeTagger) eine gute Genauigkeit für das gegenwärtige Standarddeutsch, angewendet auf ältere Sprachstufen oder vom Standarddeutschen abweichende Register wie „Literatur“ sinkt die Genauigkeit jedoch. Ein bereits auf POS annotiertes Korpus ist das Deutsche Textarchiv (DTA, Berlin-Brandenburgische Akademie der Wissenschaften 2016), ein Referenzkorpus für das Deutsche, das sowohl historische Sprachstufen als auch das Register „Literatur“ enthält. Die POS-Annotation baut hier auf fehlertoleranten linguistischen Analyse historischer Texte auf und verwendet ein Tool zur Morphologisierung (Jurish 2012), ist allerdings hinsichtlich ihrer Qualität noch nicht umfassend evaluiert worden. Ausgehend von diesem Datensatz haben wir zwei Strategien verfolgt: (1) Ein epochensensitives POS-Tagging, das verschiedene Tagger auf dem Datensatz des DTA, aber auf unterschiedlichen literarischen Epochen trainiert (vgl. Paluch et al. in Vorbereitung); (2) eine Überprüfung der Qualität der DTA-POS-Tags durch quantitative und qualitative Verfahren. 
         In Strategie (1) machen wir uns zunutze, dass Annotationsgenauigkeit erhöht werden kann, wenn Tagger auf verschiedene Register/Sprachstände trainiert und diese trainierten Modelle dann auf noch nicht trainierte Texte des gleichen Registers angewendet werden (vgl. Giesbrecht / Evert). Für KOLIMO haben wir u.a. den TreeTagger (vgl. Schmid 1994), Perceptron (vgl. Rosenblatt 1958) und MarMoT (vgl. Müller / Schmid / Schütze 2013) verwendet. Durch die Wahl unterschiedlicher Tagger soll gewährleistet werden, dass die Genauigkeit der POS-Annotation maximiert werden kann, indem nur derjenige Tagger mit den besten Ergebnissen pro Register verwendet wird. Die Auswahl der Tagger basierte einerseits darauf, dass sie unterschiedliche Prinzipien benutzen: So funktioniert der TreeTagger nach dem Hidden Markov Model (HMM, vgl. Baum / Petrie 1966), MarMot nach dem Prinzip der Conditional Random Fields (DRF, vgl. Hammersly / Clifford 1971) und Perceptron nach dem neuronaler Netzwerke. Der Grund für die Wahl des TreeTaggers war zudem seine Prävalenz in der Forschungsliteratur, die nicht zuletzt durch gute Ergebnisse begründet scheint (vgl. Dipper 2012; Giesbrecht / Evert 2009). In einem ersten Schritt (vgl. Paluch et al. in Vorbereitung) wurden hier bereits getaggte Texte aus dem DTA in fünf Epochen geordnet. Neben der Moderne umfassten diese zu Vergleichszwecken auch Barock, Aufklärung, Romantik, und Realismus. Für die Einteilung der Epochen in Zeitperioden sowie der Einteilung von Autoren zu bestimmten Epochen wurden einschlägige Literaturgeschichten zu Rate gezogen (u.a. Beutin 2001; Jørgensen / Bohnen / Øhrgaard 1990; Meid 2009; Schulz 2000; Sprengel 1998, 2004). Anschließend wurden die Tagger auf jeweils eine Epoche trainiert, indem die Texte randomisiert in Trainings- und Evaluationstexte getrennt wurden und eine k-fold cross validation (vgl. Witten / Elbe 2005) für jeden Tagger durchgeführt wurde. Die Ergebnisse (vgl. auch Paluch et al. in Vorbereitung) weisen auf eine gute Genauigkeit insbesondere von Perceptron hin, müssen aber unter dem Vorbehalt betrachtet werden, dass der Status des DTA als Goldstandard für POS-Tagging noch fraglich ist.
         Hier setzen wir mit Strategie (2) an, mit der wir zunächst für alle POS-Tags Übereinstimmung und Abweichung (Matches und Missmatches) des Outputs des Tree-Taggers und MarMots mit dem DTA-Datensatz vergleichen. Aufbauend auf diese quantitative Überprüfung der einzelnen Tag-Zuweisung evaluieren wir zudem händisch Stichproben der Nichtübereinstimmungen in der Annotation der einzelnen Tags. 
         Unsere quantitative Überprüfung ergibt eine generelle Übereinstimmung mit dem DTA-Datensatz in POS-Tags für den TreeTagger und den Marmot Tagger von jeweils 80%. Die generelle Übereinstimmung zwischen den Tags des TreeTaggers und denen des MarMot Taggers hingegen liegt bei 0.78%. 
         Tabelle 1 zeigt Ergebnisse aus der Analyse der Übereinstimmungen (Matches) und Abweichungen (Missmatches) bei der POS-Tagzuweisung von TreeTagger (TT) und MarMot (MM) im Vergleich mit den Tags des DTA. Abgebildet sind hier solche Fälle pro POS-Tag, in denen TT und MM übereinstimmen, aber vom DTA abweichen. Die Tabelle listet die elf POS-Tags, die (von TT und MM gemeinsam) die proportional den höchsten Anteil der Abweichung vom DTA ausmachen.
         Tabelle 1 Abweichung zu POS-Tags des DTA (Übereinstimmung MM und TT)
         
            
               POS-Tag*
               Häufigkeit
               Rel. Häufigkeit 
            
            
               NE
               1444048
               0.12
            
            
               NN
               1443795
               0.12
            
            
               VVFIN
               1326081
               0.11
            
            
               ADJA
               1309006
               0.11
            
            
               ADJD
               741903
               0.06
            
            
               ADV
               618465
               0.05
            
            
               VAFIN
               582791
               0.05
            
            
               FM.la
               404341
               0.03
            
            
               PPOSAT
               397465
               0.03
            
            
               APPR
               362774
               0.03
            
            
               PDAT
               255896
               0.02
            
         
         *STTS Tagset
         Aufbauend auf diesen Daten wird im nächsten Schritt die tatsächliche Qualität der bereits vorhandenen DTA-Tags für den Datensatz der literarischen Texte evaluiert. Auf der Grundlage von randomisiertem Sampling verbessern wir die POS-Annotationen bei tatsächlichen Fehlern händisch, um in der Folge u.a. eigene Sprachmodelle für unser spezifisches Korpus narrativer Texte zu trainieren. So soll schließlich unter Nutzung vorhandener Ressourcen ein Silber- oder sogar Goldstandard für das POS-Tagging historischer literarischer Texte des Deutschen erreicht werden. 
         KOLIMO wird in der Beta-Version zur Tagung veröffentlicht (s. 
                https://kolimo.uni-goettingen.de) und so der Forschungsgemeinschaft zur Verfügung gestellt. Es soll eine hypothesengetriebene, aber auch explorative, quantitative Stilistik ermöglichen (vgl. Herrmann eingereicht); zum Zeitpunkt der Tagung sind erste Ergebnisse zur stilistischen Variation der literarischen Moderne zu erwarten (vgl. schon Herrmann / Lauer / Mattner 2016).
            
         Gleichzeitig planen wir eine detaillierte Dokumentation der Arbeitsschritte zu veröffentlichen, die ähnlichen Projekten als Leitfaden zur Verfügung zu stehen soll. Unser Projekt dokumentiert in seinem gegenwärtigen Status Entscheidungen auf verschiedenen konzeptionellen, analytischen und prozeduralen Ebenen. Es zeigt, dass der Aufbau eines digitalen literarischen Korpus, das den synchronen und diachronen quantitativen Vergleich einer Schwerpunktepoche erlauben soll, bei Weitem keine triviale Aufgabe darstellt. So wurde zum Beispiel deutlich, wie Hypothesen zur Konstitution von Epochen, Autorschaft und Gattungen die Korpuskompilation steuern – und deshalb auf einer möglichst präzisen Modellierung der zugrundeliegenden textwissenschaftlichen Theorien fußen sollten. Gleichzeitig sind Metadaten (u. a. Autor, Titel, Publikationsdatum, Publikationsort, Gattung) und linguistische Parameter (wie POS) gerade die Ansatzpunkte, an denen philologische Fragestellungen in präzise und praktikable Kategorien umgewandelt werden können. Nicht zuletzt deshalb sollten literarische Daten in flexiblen Architekturen gespeichert werden, die zusätzliche Annotationsebenen zulassen – denn hermeneutische Erkenntnisprozesse stellen eine erwachsene Stärke der Geisteswissenschaften dar, die auch im digitalen Zeitalter einen explizit modellierten Platz einnehmen muss.
      
      
         
            
               Bibliographie
               
                  Baum, Leonard E. / Petrie, Ted (1966): 
                        „Statistical inference for probabilistic functions of finite state markov chains“, 
                        in: 
                        The annals of mathematical statistics 37 (6) :1554–1563.  
                    
               
                  Berlin-Brandenburgische Akademie der Wissenschaften (2016): 
                        Deutsches Textarchiv. 
                        http://www.deutschestextarchiv.de/ [letzer Zugriff 24. Mai 2016].
                    
               
                  Beutin, Wolfgang (2001): 
                        Deutsche Literaturgeschichte: von den Anfängen bis zur Gegenwart. 
                        Stuttgart: Metzler.
                    
               
                  Biber, Douglas / Conrad, Susan (2009): 
                        Register, Genre, and Style. 
                        Cambridge: Cambridge University Press.
               
                  Dipper, Stefanie (2012): 
                        „Morphological and part-of-speech tagging of historical language data: A comparison“,
                        in: 
                        Workshop on Annotation of Corpora.
                        http://www.coli.uni-saarland.de/conf/ACRH10/slides/dipper.pdf.
                    
               
                  Gaede, Friedrich (1971): 
                        Humanismus, Barock, Aufklärung: Geschichte der deutschen Literatur vom 16. bis zum 18. Jahrhundert. 
                        Bern: Francke Verlag.
                    
               
                  Giesbrecht, Eugenie / Evert, Stefan (2009): 
                        „Is part-of-speech tagging a solved task? An evaluation of pos taggers for the German web as corpus“, 
                        in: 
                        Proceedings of the fifth Web as Corpus Workshop 27–35. 
                    
               
                  Hammersley, John M. / Clifford, Peter (1971): 
                        Markov fields on finite graphs and lattices.
                         http://www.statslab.cam.ac.uk/~grg/books/hammfest/hamm-cliff.pdf.
                    
               
                  Herrmann, J. Berenike (eingereicht): 
                        „In test bed with Kafka. Introducing a mixed-method approach to digital stylistics“,
                        in: Chambers, Sally / Jones, Catherine / Kestemont, Mike / Koolen, Marijn / Zundert, Joris van (Eds.). 
                        Special Issue DHBenelux 2015, Digital Humanities Quarterly.
                    
               
                  Herrmann, J. Berenike / Lauer, Gerhard (2016a): 
                        „KAREK: Building and Annotating a Kafka/Reference Corpus“,
                        in:
                        DH2016: Conference Abstracts.
                    
               
                  Herrmann, J. Berenike / Lauer, Gerhard (2016b): 
                        „Aufbau und Annotation des Kafka/Referenzkorpus“,
                        in:
                        DHd 2016: Modellierung - Vernetzung - Visualisierung.
                    
               
                  Herrmann, J. Berenike / Lauer, Gerhard / Mattner, Cosima (2016): 
                        Measuring Kafka's Diaries. A Psychostylistic Approach International Society for the Empirical Study of Literature and Media (IGEL), Chicago, USA.
                    
               
                  Herrmann, J. Berenike / van Dalen-Oskam, Karina / Schöch, Christof (2015): 
                        „Revisiting Style, a Key Concept in Literary Studies“,
                        in:
                        Journal of Literary Theory 9 (1): 25–52.
                    
               
                  Jørgensen, Sven Aaage / Bohnen, Klaus / Øhrgaard, Per (1990): 
                        Aufklärung, Sturm und Drang, frühe Klassik: 1740 - 1789. (Boor, Helmut de / Newald, Richard, eds.). 
                        München: Beck.
                    
               
                  Jurish, Bryan (2012): 
                        Finite-state Canonicalization Techniques for Historical German. 
                        PhD, Universität Potsdam.
                    
               
                  Manning, Christopher D. / Raghavan, Prabhakar / Schütze, Heinrich (2008): 
                        Introduction to information retrieval 1. 
                        Cambridge: Cambridge University Press. 
                    
               
                  Meid, Volker (2009):
                        Die deutsche Literatur im Zeitalter des Barock: vom Späthumanismus zur Frühaufklärung: 1570 - 1740. (Boor, Helmut de / R. Newald, Richard, eds.) ([Neuausg.].). 
                        München: Beck.
                    
               
                  Müller, Thomas / Schmid, Helmut / Schütze, Hinrich (2013): 
                        „Efficient higher-order CRFs for morphological tagging“,
                        in:
                        Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing.
                    
               
                  Nekula, Marek (2003): 
                        „Franz Kafkas Deutsch“, 
                        in: 
                        Linguistik online 13 (1)
                        https://bop.unibe.ch/linguistik- online/article/view/879/1533.
                    
               
                  Paluch, Markus / Rotari, Gabriela / Steding, David / Weß, Maximilian / Moritz, Maria (in Vorbereitung): 
                        Non-static analysis of part-of-speech tagging of historical German texts.
                    
               
                  Rosenblatt, Frank (1958): 
                        „The perceptron: a probabilistic model for information storage and organization in the brain“, 
                        in:
                        Psychological Review 65 (6): 386. 
                    
               
                  Schiller, Anne / Teufel, Simone / Thielen, Christine (1995): 
                        „Guidelines für das Tagging deutscher Textcorpora mit STTS“,
                        in:
                        Manuscript, Universities of Stuttgart and Tübingen.
                        http://www.sfs.uni-tuebingen.de/resources/stts-1999.pdf.
                    
               
                  Schmid, Helmut (1994):
                        „Probabilistic part-of-speech tagging using decision trees“, 
                        in: 
                        Proceedings of the international conference on new methods in language processing 12: 44–49. 
                    
               
                  Schulz, Gerhard (2000): 
                        Das Zeitalter der Französischen Revolution: 1789 - 1806. (Boor, Helmut de / Newald, Richard, eds.) (2., neubearb. Aufl.). 
                        München: Beck.
                    
               
                  Sprengel, Peter (1998):
                        Geschichte der deutschsprachigen Literatur 1870 - 1900: von der Reichsgründung bis zur Jahrhundertwende. (Boor, Helmut de / Newald, Richard, eds.). 
                        München: Beck.
                    
               
                  Sprengel, Peter (2004): 
                        Geschichte der deutschsprachigen Literatur 1900 - 1918: von der Jahrhundertwende bis zum Ende des Ersten Weltkriegs. (Boor, Helmut de / Newald, Richard, eds.). 
                        München: Beck.
                    
               
                  Witten, Ian H. / Elbe, Frank (2005): 
                        Data Mining: Practical machine learning tools and techniques. 
                        San Francisco: Morgan Kaufmann Publishers.
                    
            
         
      
   



      
         
            Zusammenfassung: Digitale Daten stellen den zentralen Forschungsfokus der Digital Humanities dar. Fragen der Modellierung, Repräsentations-, Analyse- und Annotationsmöglichkeiten sind dabei wichtige Forschungsdimensionen, ebenso wie etwa die Weiterverarbeitung und Nachnutzbarkeit. Die digitalen Daten sowie die beschrieben Prozeduren werden jedoch auch von EditorInnen bearbeitet und wirken sich auf deren wissenschaftliche Arbeit aus. In diesem Beitrag wird aus qualitativ empirischer Sicht die Perspektive der EditorInnen als besondere Nutzer- und Produzentengruppen im Prozess der Digitalisierung von Musikeditionen vorgestellt. Dabei gilt es weder WissenschaftlerInnen noch Daten singulär zu betrachten, sondern im Akt der Bearbeitung, Analyse, Repräsentation und Annotation eine besondere Perspektive in der Auseinandersetzung von Medien, Materialien und Subjekten zu erschließen und zu reflektieren. In diesem Sinne werden in diesem Abstract zuerst theoretische Verortungen für die Relevanz des Nutzers diskutiert. Darauf aufbauend werden die methodischen Grundlagen der Interviewstudie vorgestellt, um anschließend einen Ausblick auf die Ergebnisse zu geben, der im Vortrag vertieft wird. Dabei stehen die Veränderungen des wissenschaftlichen Arbeitsprozesses von analog zu digital im Vordergrund. Darauf aufbauend stehen die Chancen und Herausforderungen dieses Paradigmenwechsels im Zentrum des Interesses, die sicherlich nicht nur für die Arbeitskontexte der digitalen Musikeditionen zutreffen. Abschließend werden Kristallisationspunkte und Konsequenzen zukünftiger Fragestellungen hinsichtlich der Digitalisierung von Musikeditionen, Veränderungen von Arbeitsstrukturen sowie der Bildungs- und Wissensarbeit resultierend aus diesen Ergebnissen thematisiert. 
            
         Einleitung
         Digitale Musikeditionen bieten potentiell vielerlei Optionen für EditorInnen, WissenschaftlerInnen und geneigte RezipientInnen (vgl. etwa Veit 2010). So beschleunigt und ordnet etwa die Verfügbarkeit digitaler Quellen den editorischen Prozess, die Ansichten des Digitalen ermöglichen größtmögliche Transparenz und Nachvollzieharkeit. Umbestritten sind also die Errungenschaften, die mit den digitalen Musikeditionen einhergehen eine Bereicherung der wissenschaftlichen Arbeit (vgl. ebd.). Vieles was im Kontext der Digital Humanities diskutiert wird, bezieht sich auf die digitale Repräsentation oder aber Transformation der kulturellen Artefakte, deren weitergehenden Analyse bzw. Prozessierbarkeit sowie deren Nachnutzbarkeit. Diese Ebenen werden aus dem Fokus auf die Daten heraus diskutiert. Was hingegen weniger betrachtet wird ist die Forschung im Digitalen als Wissensgenerierungsprozess: Was bedeutet es, sich digitale Techniken anzueignen, digital Quellen zu bearbeiten, zu repräsentieren, zu analysieren? Die Auseinandersetzung mit diesen Praktiken der Aneignung ermöglicht es ein tieferes Verständnis für die Optionen der digitalen Daten und deren wissenschaftliche Relevanz im Arbeitsprozess zu eröffnen. Ebenso lassen sich die zuvor skizzierten Forschungsthemen von Repräsentation, Transformation, Nachnutzbarkeit etc. ebenso aus der Sicht der Subjekte erschließen.
         Theoretische Rahmung
         Neben Fragen der Daten werden somit zunehmend die Arbeitsprozesse interessant, die durch die Digitalisierung der wissenschaftlichen Arbeit beeinflusst werden. An dieser Stelle bietet sich die seltene Gelegenheit die Veränderungsprozesse dieses medialen Paradigmenwechsel und dessen Einfluss auf Forschung und Wissenschaft zu beobachten und zu begleiten. Damit gilt es die NutzerInnen in den Blick zu nehmen (vgl. auch Stone 1982, Edwards 2012, Warwick 2012; Brockman 2001) und vom forensic zum formal layer (vgl. Kirschenbaum 2008) zu wechseln. Aber auch Kirschenbaums formal layer bringt nicht ganz zum Ausdruck, was Drucker (2013) mit der performativen Ebene von Materialität als Nutzungsakt beschreibt: Handeln, der Umgang von NutzerInnen mit kulturellen, auch immateriellen Artefakten, prägen die Wahrnehmung, Beurteilung und die kulturelle Bedeutung dieser Artefakte. Um die Bedeutung von Medien, konkreter von musikeditorischen Ergebnissen unter digitalen Bedingungen, erschließen zu können, ist es notwendig, die vielschichtigen Auseinandersetzungsprozesse der Nutzer mit der Software bzw. der Auszeichnungssprachen und Metadaten zu erforschen. Damit verbunden ist die sogenannte radikale Kontextualisierung in den Cultural Studies, bei der davon ausgegangen wird, dass »Objekt und Subjekt, Medientechnologie und Kontext« (vgl. Winter 2010): sich stetig beeinflussen und miteinander verwoben sind. Erst in der Analyse dieser komplexen Verbindungen kann letztlich das Phänomen konturiert und erforscht werden. Medientechnologien und ihre Nutzer gehen demnach in zahlreichen Auseinandersetzungsprozessen eine Allianz ein, die in dieser Perspektive eine besondere Qualität hervorbringt. Einen Schritt weiter geht Rainer Winter, indem er mit Rekurs auf Heidegger darauf hinweist, dass Medien nicht nur technische Artefakte sind, sondern gerade in ihrer Einbettung in soziale und kulturelle Prozesse, Optionen und Zugänge zur Welt umgestalten (vgl. ebd.). In dieser Hinsicht gilt es weitergehend Wissensgenerierungsprozesse in den Blick zu nehmen. In diesem Beitrag stehen die EditorInnen als besondere Nutzergruppe im Zentrum des Interesses. Diese arbeiten an der Schnittstelle vom computer und cultural layer (Manovich 2001). Sie arbeiten mit Metadaten und Auszeichnungssprachen und müssen somit die Logiken des Prozessierens des Computers verstehen, gleichzeitig arbeiten sie mit den Transformationen an der Oberfläche, lassen sich Teile oder Überblick bestimmte Werkaspekte anzeigen, um editorische Entscheidungen zu treffen und bilden damit einen ganz versierten Nutzer- und Produzententypus ab.
         Methode
         Im Projekt wurde auf die Prinzipien der qualitativen, empirischen Sozialforschung zurückgegriffen, um diesen Prozess möglichst offen und gegenstandsangemessen erforschen zu können (vgl. etwa Flick et al. 2000). Qualitative Sozialforschung birgt zunächst den Vorteil in einem unbekannten Forschungsfeld einsetzbar zu sein. Ergänzend zur quantitativen Forschung wird somit im Projekt ein hypothesengenerierendes und exploratives Verfahren verfolgt. Um die Sicht der EditorInnen in der Annäherung zu erschließen, können die Befragten durch unterschiedliche Methoden beforscht werden (Beobachtung, unterschiedliche Arten von Interviews, Einzel- bzw. Gruppeninterviews etc. vgl. ebd.). Die Auswahl der Erhebungsmethode erfolgt dem Forschungsphänomen entsprechend angemessen. Erhebt also die quantitative Forschung Daten zu vielen Nutzern, um Überblicke zu generieren und Themenfelder zu identifizieren ist die qualitative Forschung mit diesen Erhebungsmethoden dazu in der Lage in die Tiefe zu gehen und bspw. Bedeutungskontexte, implizites Wissen und unbewusste Routinen zu eruieren. Hierbei werden nicht nur Stichworte, sondern Kontextinformationen aus der Sicht der Subjekte aus den Daten herauszuarbeiten (vgl. Flick 2002). Die Erforschung impliziten Wissens, von Arbeitsroutinen, Expertisen und Gewissheiten lässt sich dabei kaum über einzelne direkte Fragen realisieren. Um solchen Phänomenen auszuspüren ist zunächst der Gesamtkontext wichtig. In diesem Sinne nutzt die qualitative Forschung verschiedene Formen von Befragungstechniken, um unterschiedliche Arten von Narrationen zu erhalten, die im Anschluss verschriftlicht und analysiert werden können. Im Auswertungsprozess wird dann über Interpretationen der Gesamtkontext erarbeitet und erschlossen. Dies geschieht indem einzelne Wissensbestände mit anderen Aussagen verbunden werden, die im Gesamtkontext Einblicke in das Zusammenwirken expliziter und impliziter Wissensbestände erlauben. Als Erhebungsform für diese qualitative Studie wurde das teilstandardisierte, narrative Interview gewählt, das zwar einem Leitfaden folgt, in der Interviewsituation allerdings größtmögliche Spielräume hinsichtlich der Frageformulierungen, Nachfragestrategien und der Reihenfolge der Fragen zulässt (vgl. Keuneke 2000) und der Narration der EditorInnen viel Raum gestattet. Der Leitfaden fokussierte besonders Regelstrukturen der Handlungs- und Nutzungsweisen, indem die Befragten nach bestimmten Nutzungssituationen und den damit verbundenen Bedeutungen und Relevanzen befragt wurden. Diese Nutzungssituationen und Bedeutungen wurden allen interviewten Personen gleichermaßen vorgegeben. Die von den Befragten formulierten Antworten konnten anschließend aufeinander bezogen werden. Zur Auswahl der Interviewpartner/innen wurden im Sinne des Theoretical Sampling (vgl. Przyborski/ Wohlrab-Sahr 2009) Expertinnen und Experten, die mit Edirom arbeiten befragt. Die Software Edirom erlaubt es Faksimiles, Digitalisate und digitale Daten von Notentexten oder anderen Quellen einzuarbeiten, zu speichern, zu organisieren, zu kollationieren, zu annotieren und zu analysieren. Dabei handelt es sich nicht um eine Forschungsoberfläche, die Voraussetzungslos für die EditorInnen ist, sondern hier sind Auseinandersetzungen und Erfahrungen mit XML, TEI und MEI erforderlich. Entscheidend war, dass sowohl weibliche als männliche Nutzer/innen befragt wurden. Insgesamt wurden acht Interviews mit sechs Editorinnen und zwei Editoren geführt. Diese dauerten zwischen 90 Min. und 180 Min. Die erhobenen qualitativen Daten wurden durch eine Variante des Kodierens
                1 nach Strauss und Corbin, wie es Przyborski und Wohlrab-Sahr vorschlagen, ausgewertet (vgl. ebd.). Die Auswertung wurde zunächst nicht mittels Auswertungsprogrammen strukturiert, sondern durch Textverarbeitungsprogramme. So konnten die sich herausbildenden Phänomene einer exemplarischen, interdisziplinären Sichtung unterzogen werden. In der Synthese der exemplarischen Auswertung konnte das selektive Kodieren vorangetrieben werden, woraus eine Phänomen- und Kategorienliste resultierte. Diese liefert einerseits konkrete Hinweise für Optimierungen der Software, aber auch Kontextinformationen zu den Arbeitsbedingungen, Routinen und Erfahrungen der EditorInnen. Darüber hinaus liefern die Interviews sehr gute Einblicke in die Änderungsprozesse der Wissensarbeit, des Wissensmanagements als auch des erarbeiteten Wissens als solches.
            
         
            Ausblick auf die Ergebnisse
         
         Wie die empirischen Daten belegen, ist der Wechsel der Arbeit und der beforschten Gegenstände von analog zu digital nicht nur eine technische Änderung, vielmehr gehen damit auch editorische, rechtliche, organisatorische, soziale und nicht zuletzt auch bildungswissenschaftliche Prozesse einher, die betrachtet, reflektiert und weiterentwickelt werden müssen. Sie verändern die wissenschaftliche Arbeitsorganisation, die editorische Tätigkeit und nicht zuletzt die Sicht auf Editionen und die damit verbundenen Erkenntnissen selbst. EditorInnen recherchieren bei ihrer analogen Forschungsroutinen zunächst Quellen, analysieren diese und wählen Haupt- und Nebenquellen aus, um die weitere Editionsarbeit zu gestalten. Im Anschluss daran wurden diese Quellen stetig miteinander verglichen. Dazu musste sehr viel Quellenmaterial physisch verwaltet werden, um die einzelnen Änderungen in der jeweiligen Quelle mit anderen vergleichen und analysieren zu können. Die Arbeit an den digitalen Editionen ist indes ein Konglomerat aus analogen und digitalen Techniken. Zuerst werden die Quellen ebenso recherchiert, analysiert und ausgewertet und ausgewählt. Die ausgewählten Quellen werden von den Hilfskräften im Anschluss digitalisiert, vertaktet und die Konkordanzen festgelegt. Es findet also eine Arbeitsteilung statt, da die Vertaktung delegiert wird. Die Interviews belegen die hohen Vorteile und Freiheitsgrade der digitalen Editionen, den Rezipienten solcher Editionen kann nun erstmals das gesamte Quellenmaterial zur Verfügung gestellt werden. Diese können nun editorische Entscheidungen transparent nachvollziehen und eine eigene Meinung dazu entwickeln. Damit einhergehend sind aber auch ein zunehmendes Maß an Komplexitätssteigerung und wachsenden Aufgaben zu verzeichnen. Bei Printeditionen steht die editorische Tätigkeit im Fokus. Der Wechsel zu digitalen Editionen bedeutet für die Editoren einen weiteren Komplexitätsschub: Nicht nur die musikwissenschaftliche Expertise ist gefragt, sondern auch Kenntnisse verschiedenster Auszeichnungssprachen, wie XML, TEI und MEI. Durch die Arbeitsteilung muss darüber hinaus den Hilfskräften Wissen für die Vertaktung vermittelt, diese angeleitet und kontrolliert werden. Zudem nutzen die EditorInnen notwendigerweise mehr Programme. Um nur einige zu nennen sind dies: Sibelius, Score, Finale, QuarkX, Indesign, OxygenXML, Lillypond, Word, Filemaker, oder aber Verovio. Wie die aufgeführten Notensatzprogramme verdeutlichen, sind EditorInnen nun teilweise auch mit Aufgaben beschäftigt, die vorher von Verlagen erledigt wurden. Durch diese Tätigkeit kann auch das Rechtemanagement von Originalquellen zu einem weiteren Aufgabengebiet werden. Die Ergebnisse abstrahierend betrachtet sind im neuen Medium neue Forschungsfragen entstanden und bilden sich täglich neu aus: Wo sind die Anfangs- und Endpunkte von Editionen, welche Nachnutzbarkeit kann gewährleistet werden, wie kann die Praxis von dem Wissen profitieren und dieses einsehen, wo ist Wissen gesichert erschlossen? Zudem gibt es kaum verbindliche Standards im digitalen Editionsprozess, was nun, bei steigender Editionszahl und entsprechender Annotationsmenge immer offensichtlicher und wichtiger wird. Ein systematischer Wissensaufbau informatischer Grundkenntnisse wird ebenso implizit evident, um die Potenziale der digitalen Repräsentations- und Verarbeitungsoptionen besser erschließen zu können.
      
      
         
             Das Kodieren bezeichnet im Gegensatz zu Semantiken aus der Informatik im Kontext der qualitativen Forschung eine Auswertungstechnik.
         
         
            
               Bibliographie
               
                        Brockman, William S. / Neumann, Laura / Palmer, Carole L. / Tidline, Tonyia J. (2001): 
                        Scholarly Work in the Humanities and the Evolving Information Environment. portal: Libraries and the Academy (Vol. 3). 
                        Digital Library Federation. 
                        10.1353/pla.2003.0012.
                    
               
                  Drucker, Johanna (2013): 
                        „Performative Materiality and Theoretical Approaches to Interface“, 
                        in: 
                        DHQ: Digital Humanities Quartely 7 (1). 
                        http://www.Digitalhumanities.org/dhq/vol/7/1/000143/000143.html.
                    
               
                  Edwards, Charlie (2012): 
                        „The Digital Humanities and Its Users“, 
                        in: Gold, Matthew K. (ed.):
                        Debates in the Humanities.  
                        http://dhdebates.gc.cuny.edu/debates/text/31 [letzter Zugriff 3. September 2015].
                    
               
                  Flick, Uwe / Kardorff, Ernst von / Steinke Ines (eds.). (2000): 
                        Qualitative Forschung: Ein Handbuch. 
                        Reinbeck: Rowohlt. 
                    
               
                  Flick, Uwe (2002): 
                        Qualitative Sozialforschung: Eine Einführung. 6. überarb. und erweiterte Aufl. 
                        Hamburg: Rowohlt. 
                    
               
                  Giesecke, Michael (1991): 
                        Der Buchdruck in der frühen Neuzeit: eine historische Fallstudie über die Durchsetzung neuer Informations- und Kommunikationstechnologien. 
                        Frankfurt a. M.: Suhrkamp.
                    
               
                  Keuneke, Susanne (2005): 
                        „Qualitatives Interview“, 
                        in: Mikos, Lothar / Wegener, Claudia (eds.): 
                        Qualitative Medienforschung: Ein Handbuch.
                        Konstanz: UVK 254–267.
                    
               
                  Kirschenbaum, Matthew (2008): 
                        Mechanisms: New Media and the Forensic Imagination. 
                        Cambridge: MIT University Press.
                    
               
                  Manovich, Lev (2001): 
                        Language of New Media.
                        Cambridge: MIT Press.
                    
               
                  Polanyi, Michael (1985): 
                        Implizites Wissen. 
                        Frankfurt: Suhrkamp. 
                    
               
                  Przyborski, A. / Wohlrab-Sahr, M. (2009): 
                        Qualitative Sozialforschung. Ein Arbeitsbuch.
                        München, Oldenbourg.
                    
               
                  Stone, Sue (1982):
                        „Humanities Scholars: Information Needs and Uses“,  
                        in:
                        Journal of Documentation 38 (4): 292–313. 
                        http://www.emeraldinsight.com/journals.htm?articleid=1649976.
                    
               
                  Veit, Joachim (2010):
                        „Es bleibt nichts, wie es war – Wechselwirkungen zwischen digitalen und ,analogen‘ Editionen“,  
                        in: 
                        editio 24: 37–52 
                        10.1515/9783110223163.0.37.
                    
               
                  Warwick, Claire (2012): 
                        „Studying users in digital humanities“, 
                        in; Warwick, Claire / Terras, Melissa / Nyhan, Julianne (eds.), 
                        Digital Humanities in Practice.
                        London: Facet Publishing 1–21.
                    
               
                  Winter, Rainer (2010): 
                        „Handlungsmächtigkeit und technologische Lebensformen: Cultural Studies, digitale Medien und die Demokratisierung der Lebensverhältnisse“, 
                        in: Pietraß, Manuela / Funiok, Rüdiger (eds.) 
                        Mensch und Medien. Philosophische und sozialwissenschaftliche Perspektiven. 
                        Wiesbaden: VS 139–157.
                    
            
         
      
   



      
         In meinem Vortrag setze ich mich mit der Frage auseinander, welchen Beitrag die Datenbank TInCAP („Tübingen Interdisciplinary Corpus of Ambiguity Phenomena“), die bei der Tagung der Digital Humanities im deutschsprachigen Raum 2016 in Leipzig vorgestellt wurde und die der Sammlung und Annotation von Ambiguitätsbelegen dient, zur Erforschung des Phänomens „Ambiguität“ leisten kann. Den Mehrwert, den TInCAP durch die innovative interdisziplinäre Annotation und die Zusammenführung von Belegen in einer durchsuchbaren Datenbank liefert, werde ich am Beispiel ambiger idiomatischer Ausdrücke in kinderliterarischen Texten illustrieren. 
         Die Datenbank TInCAP entsteht im Rahmen des interdisziplinären Graduiertenkollegs 
                GRK 1808 Ambiguität – Produktion und Rezeption (
                www.ambiguitaet.uni-tuebingen.de; Arbeitsgruppe TInCAP: Wiltrud Wagner, Lisa Ebert, Jutta Hartmann, Gesa Schole, Susanne Winkler) mit dem Zweck, Ambiguitätsbelege aus allen beteiligten Disziplinen zu sammeln und zu annotieren. Hauptziele sind dabei die interdisziplinäre Auseinandersetzung mit dem Phänomen Ambiguität durch die Erstellung eines gemeinsamen Annotationsschemas sowie die nachhaltige Speicherung und Zugänglichmachung der Datensammlung für die nationale und internationale Forschungsgemeinschaft (in Kürze über die Homepage des GRK 1808).
            
         Auch wenn alle an diesem Projekt beteiligten WissenschaftlerInnen das Interesse am Phänomen der Ambiguität verbindet, das hier als Doppel- oder Mehrdeutigkeit in ihren verschiedensten Formen verstanden wird, so sind die zu annotierenden Belege doch sehr divers: Durch die Vielzahl der beteiligten Disziplinen unterscheiden sich die Belege hinsichtlich Medium (aktuell: Schrift, Audio, Bild, Video) und Sprache (aktuell: Deutsch, Englisch, Französisch, Hebräisch, Italienisch, Latein, Spanisch, Griechisch), aber auch Umfang. Im Bestreben, eine gemeinsame Datenbank aufzubauen, sahen wir uns demnach zwei großen Herausforderungen gegenüber gestellt: (1) Der Erarbeitung einer disziplinenübergreifenden Terminologie, die einerseits präzise, andererseits aber nicht an das Vokabular einer der Disziplinen gebunden ist, und (2) der Entwicklung eines interdisziplinären Annotationsschemas, das – trotz der notwendigen Komplexitätsreduktion – den Anforderungen der einzelnen Disziplinen genügt und für alle Beteiligten profitabel ist. 
         Das Ergebnis ist ein Annotationsschema, das die folgenden fünf Punkte fokussiert:
         
            
               Communication level: Auf welcher Ebene der Kommunikation wird die Ambiguität annotiert? Für literarische Texte wird zum Beispiel zwischen der Ebene der fiktiven Charaktere, der Ebene des/der Erzähler(s) und der Ebene des Autors und Lesers unterschieden.
                
            
               Strategic or non-strategic production and/or perception: Wird die Ambiguität strategisch produziert? Wird die Ambiguität strategisch rezipiert?
                
            
               Level of Trigger and Range: Zu annotieren ist, auf welcher Ebene die Ambiguität ausgelöst wird und bis zu welcher Ebene sie relevant ist. Die Ebenen für Auslöser und Wirkung der Ambiguität bilden dabei ein Größenverhältnis ab, analog zum menschlichen Körper, bei dem sich größere Elemente aus kleineren zusammensetzen (z.B. die Ebene 
                    Subelement, die u.a. Phoneme, Grapheme und Morpheme umfasst; die Ebene 
                    Element, die u.a. Worte umfasst; usw.). 
                
            
               Type of Paraphrase Relation: In welchem Verhältnis stehen die möglichen Lesarten zueinander? Sind sie voneinander abgeleitet oder völlig unabhängig voneinander?
                
            
               Phenomenon: Welches Phänomen steht mit der vorliegenden Ambiguität im Zusammenhang? Hier kann und soll disziplininternes Vokabular zur Anwendung kommen, um die Einbindung in den jeweiligen Forschungskontext zu gewährleisten.
                
         
         Zusätzlich ist die Verknüpfung von Annotationen möglich, zum Beispiel, wenn ein Beleg auf verschiedenen Kommunikationsebenen (unterschiedlich) annotiert wird.
         Die Nachhaltigkeit der gesammelten Daten wird durch eine Kombination verschiedener Faktoren gewährleistet: Das von uns entwickelte XML-Schema ist soweit möglich TEI-konform, es wurde für die inhaltliche Annotation der Daten um ein eigenes Schema erweitert. Der gesamte Korpus bzw. Subkorpora können im XML-Format im- und exportiert werden. Diese XML-Dateien werden in Kooperation mit Clarin-D Tübingen im Rahmen der universitären Infrastruktur langfristig gespeichert, katalogisiert und mit PIDs zugänglich gemacht. Teilkorpora können dabei ebenso exportiert werden wie das Gesamtkorpus. Bei Video-, Audio- und Bilddateien halten wir uns an die üblichen Standards für nachhaltige Datenformate (nicht-proprietäre Formate, Formate mit gutem Nachnutzungswert).
         Nach der allgemeinen Vorstellung der Datenbank wende ich mich im zweiten Teil des Vortrags der Frage zu, was die Datenbank im Hinblick auf konkrete Fragestellungen leistet. Die von mir in die Datenbank eingebrachten Ambiguitätsbelege entstammen zum größten Teil meiner Dissertation, die einen interdisziplinären Beitrag zur Ambiguitätsforschung leistet: Der linguistische Teil der Arbeit untersucht, wie idiomatischen Ausdrücken das Potential zur Ambiguität inhärent sein kann. An der Schnittstelle zur Literaturwissenschaft zeigt die Arbeit, wann und wie idiomatische Ausdrücke in Interaktion mit unterschiedlichen Kotexten ihr Ambiguitätspotential entfalten. Am Beispiel von kinderliterarischen Texten wird schließlich dargestellt, wie die aus dieser Interaktion resultierende Bewusstmachung von Ambiguität als sprachspielerisches Potential für literarische Texte produktiv gemacht werden kann. (a)-(c) stellen typische Beispiele aus meinem Korpus dar, die jeweils annotierten Stellen sind fett markiert:
         (a)
         
            One day he went to King Big-Twytt, who was eating a bathtub of roast chicken, custard and chips, and said: 'King - I want a licence to catch ye dragons.'
            
         'What?' said King Twytt. 'But ye dragons are dangerous! They eat ye farm animals.'
         'So do we,' said Sir Nobonk, 'and no one says we're dangerous.'
         'Yea, very well,' said King Twytt, 'I will give you a licence, but 
                be it on your own head.'
            
         So Sir Nobonk strapped the licence to his head.
         Sir Nobonk had been in many wars. Usually […]
         (Spike Milligan: 
                Sir Nobonk and the terrible, awful, dreadful, naughty, nasty Dragon, 1982)
            
         
            
            (b)
            
         
            Draw the drapes
             when the sun comes in.
            
         read Amelia Bedelia. She looked up. The sun was coming in. Amelia Bedelia looked at the list again. "Draw the drapes? That's what it says. I'm not much of a hand at drawing, but I'll try."
         So Amelia Bedelia sat right down and she drew those drapes.
         (Peggy Parish: 
                Amelia Bedelia,1963.)
            
         (c)
         Tom ging auf den frierenden König zu. „Ich bin gekommen, um mein Versprechen einzulösen“, sagte er und warf die Satteltasche auf den Tisch.
         König Knöterich schaute ungläubig auf die Tasche. „Hast du mir etwa ein Paar warme Handschuhe mitgebracht?“
         „Nein, Herr König“, antwortete Tom. „Etwas viel Kostbareres. Ich habe für Euch den goldenen Dings, äh, Kelch erobert.“
         „Aahhh! Oohhh!“, hallte es durch den Saal.
         „Ihr wollt wohl den König 
                auf den Arm nehmen“, sagte Friedrich von Edelstein.
            
         „Ich fürchte, mit den vielen Umhängen und Mützen ist mir der König zu schwer“, grinste Tom.
         (Bernd Schreiber: Ritter Tollkühn und der goldene Dings, 2010.)
         Die Annotation meiner Beispiele mit TInCAP ermöglicht die Sichtbarmachung von Aspekten, die bei der reinen linguistischen oder literaturwissenschaftlichen Analyse möglicherweise verborgen bleiben. Besonderes Gewicht kommt dabei der Möglichkeit zu, Ambiguitäten auf mehreren Kommunikationsebenen zu annotieren und die resultierenden Annotationen zu verknüpfen. Dies möchte ich anhand von Beispielen wie (a)-(c) illustrieren und mich dabei auf folgende Phänomene konzentrieren:
         
            
               strategische vs. nicht-strategische Produktion/Rezeption: In den untersuchten kinderliterarischen Texten erfolgt meist die Produktion auf der innersten Ebene (Ebene der Figuren) nicht strategisch, auf der äußersten Ebene (Ebene des Autors) jedoch strategisch. 
                
            
               Typ der Ambiguitätsverwendung: Sehr häufig wird in den untersuchten kinderliterarischen Texten die Ambiguität auf der innersten Ebene nicht erkannt, auf der äußersten Ebene muss jedoch eine semantische Reanalyse erfolgen, wodurch die Ambiguität sichtbar gemacht wird.
                
            
               Erste Lesart (phrasal vs. kompositional): Die erste (und damit oftmals einzige) Lesart auf der innersten Ebene ist sehr häufig die kompositionale. Auf der äußersten Ebene ist es jedoch die phrasale Lesart, die primär verarbeitet wird, woraus die Notwendigkeit der semantischen Reanalyse resultiert.
                
         
         Diese Phänomene, die erst durch die Annotation mit TInCAP und durch entsprechende Suchabfragen sichtbar werden, zeigen das Potential, das diese Datenbank innerhalb eines Projekts entfaltet. In einem abschließenden Ausblick möchte ich darüber hinaus auf den interdisziplinären Nutzen der Datenbank verweisen, der im Rahmen des GRK 1808 bereits zum Tragen kommt, insbesondere in der Vergleichbarkeit, die über Medien hinweg geschaffen wird.
      
      
         
            
               Bibliographie
               
                  Hartmann, Jutta / Sauter, Corinna / Schole, Gesa / Wagner, Wiltrud / Gietz, Peter / Winkler, Susanne (2016): 
                        TInCAP – ein interdisziplinäres Korpus zu Ambiguitätsphänomenen. Posterpräsentation,
                        in:
                        DHd 2016: Modellierung - Vernetzung - Visualisierung.
                    
               
                  Hartmann, Jutta / Ebert, Lisa / Schole, Gesa / Wagner, Wiltrud / Winkler, Susanne (eingereicht): 
                        „Annotating Ambiguity Across Disciplines: The Tübingen Interdisciplinary Corpus of Ambiguity Phenomena“, 
                        in: Bauer, Matthias / Zirker, Angelika (eds.): 
                        Strategies of Ambiguity.
                    
               
                  Hartmann, Jutta / Ebert, Lisa / Schole, Gesa / Wagner, Wiltrud / Winkler, Susanne (in Vorbereitung): 
                        TInCAP User Manual.
                    
               
                  Klein, Wolfgang / Winkler, Susanne (eds.) (2010): 
                        Ambiguität. Zeitschrift für Literaturwissenschaft und Linguistik 40 (158). 
                        Stuttgart: Metzler.
                    
               
                  TEI Consortium (eds.): 
                        Guidelines for Electronic Text Encoding and Interchange. [6.4.2015]. 
                        http://www.tei-c.org/P5/.
                    
               
                  Wagner, Wiltrud (in Vorbereitung): 
                        Idioms and Ambiguity in Context: Compositional and Phrasal Readings of Idiomatic Expressions. 
                        Dissertation. Tübingen.
                    
               
                  Winkler, Susanne (eds.) (2015): 
                        Ambiguity: Language and Communication. 
                        Berlin: de Gruyter.
                    
               
                  Winter-Froemel, Esme / Zirker, Angelika (2010): 
                        „Ambiguität in der Sprecher-Hörer-Interaktion. Linguistische und literaturwissenschaftliche Perspektiven“, 
                        in: Klein, Wolfgang / Winkler, Susanne (eds.): 
                        Ambiguität. Zeitschrift für Literaturwissenschaft und Linguistik 40 (158). 
                        Stuttgart: Metzler 76–97.
                    
               
                  Winter-Froemel, Esme / Zirker, Angelika (2015): 
                        „Ambiguity in Speaker-Hearer-Interaction: A Parameter-Based Model of Analysis“, 
                        in: Winkler, Susanne (eds.): 
                        Ambiguity: Language and communication. 
                        Berlin: de Gruyter 283–339.
                    
            
         
      
   



      
         
            Ausgangslage
            Die 
                    Historisch-kritische Gesamtausgabe der Werke und Briefe von Jeremias Gotthelf (
                    HKG), begründet von Prof. Dr. Barbara Mahlmann-Bauer und PD Dr. Christian von Zimmermann im Jahr 2003, ist auf 67 Text- und Kommentarbände angelegt und damit eines der grösseren Editionsvorhaben im deutschsprachigen Raum.
                    1
            
            Nach der Publikation der ersten Bände (2012) wurden die Arbeitsprozesse evaluiert, um die Edition adäquat auf digitale Arbeitsverfahren und Publikationsformen ausrichten zu können. Texte und Kommentare sollten nun 
                    TEI-konform erfasst werden, zumal sich inzwischen die erweiterten 
                    TEI-Guidelines als internationaler editionsphilologischer Standard durchsetzen konnten. Die Umstellung sollte aber umfassender sämtliche editorischen Arbeiten von der Transkription und textgenetischen Analyse über die Dokument- und Datenverwaltung, die Registeranbindung oder Kontrollroutinen bis hin zur Publikation in Print und Web berücksichtigen. Ein wichtiges Anliegen war es, aus einem Datensatz mehrere Ausgabenformate erzeugen zu können (d.h.: unterschiedliche Printformate und digitale Präsentationsformen). Schliesslich sollten die ‘endgültigen’ Daten, die bisher allein in der historisch-kritischen Buchedition gedruckt vorlagen, in einem einheitlichen Datenformat in der 
                    Forschungsstelle zentral verfügbar, gesichert und anderweitig verwertbar sein. (Zuvor lagerten die durch Fahnenkorrekturen aktualisierten und in unterschiedlichen Satzprogrammen codierten Satzdaten bei mehreren beauftragten Satzbüros.)
                
            Diese unterschiedlichen Aspekte machen das Umstellungsprojekt aussergewöhnlich komplex; einige der Etappenziele sind bereits erreicht worden. Unser Beitrag handelt von den Erfahrungen der vergangenen vier Jahre und gibt Aufschluss über Chancen und Schwierigkeiten des Umstiegs eines editorischen Grossprojekts auf ein exaktes, weitestgehend inhaltsorientiertes Markup und auf computerphilologische Arbeitsweisen. Von besonderem Interesse ist für uns die Vermittlung zwischen übergreifenden editorischen und digitalen Standards einerseits und individuellen Projektbedürfnissen andererseits. In unserem Vortrag möchten wir zudem den Einfluss institutioneller Rahmenbedingungen (
                    DaSCH/DDZ, SAGW, 
                    SNF, metagrid
               2) auf die Durchführung computerphilologischer Reformen thematisieren.
                
         
         
            Kooperation mit der BBAW und Pagina
            Der Weg zu einer Arbeitslösung für die Edition erfolgt nicht durch eine eigene Programmentwicklung, deren Kosten auch bei einem nationalen Zusammenschluss mehrerer Projekte so nicht tragbar wären. Mittlerweile haben sich unterschiedliche modulare Lösungen etabliert (etwa auch 
                    textgrid etc.). Nach längerer Prüfung entschied sich die Projektgruppe dazu, die Arbeitsumgebung 
                    Ediarum der 
                    BBAW für das eigene Projekt weiterzuentwickeln, zumal 
                    Ediarum bereits für die 
                    Schleiermacher-Edition als digitale Arbeitsumgebung angewendet und auch für andere Projekte angepasst worden war. 
                    Ediarum basiert auf einer 
                    eXist-Datenbank, nutzt den 
                    Oxygen Author und umfasst Module für den Satzprozess sowie die digitale Datenpräsentation (Dumont/Fechner 2012; Arbeitsgruppe Telota o.J.).
                
            Inzwischen sind erste Module unserer Arbeitsumgebung, die Schemata für Handschriften und Drucktexte nebst entsprechenden Anpassungen im 
                    Oxygen Author abgeschlossen. Für die Satzvorbereitung besteht eine Kooperation mit der Firma 
                    Pagina, welche das Satzmodul mit 
                    Tustep für die Druckvorstufe programmiert und in ersten Versionen für Drucktexte und Manuskripte bereits erfolgreich zur Verfügung gestellt hat (drucknaher Satzpreview für den Editionstext und Apparat).
                
         
         
            Schema und Ausgabeformate
            Die Entwicklung (insbesondere des Handschriftenschemas) war vor allem deshalb anspruchsvoll, da die Codierung nicht nur die Einrichtung der historisch-kritischen Ausgabe nach der bisherigen Gestalt sicherstellen musste, sondern auch davon abweichende gedruckte und digitale Präsentationsformen ermöglichen sollte. Dabei sollten etwa an die Stelle der Apparate der Drucktexte in der Webedition medienspezifische Annotationsformen treten können.
            Die kritische Edition stellt einen Editionstext bereit und verzeichnet sämtliche textgenetischen Prozesse in der Handschrift, Emendationen bei Drucktexten und allfällige Varianten zwischen Drucktexten und Handschriften am Seitenende in unterschiedlichen Apparaten (textgenetischer Apparat, Emendationsapparat, Variantenapparat, Textstufenapparat; häufig Kombinationen). 
            Um die Apparate der kritischen Edition in der Printausgabe zu erzeugen, hätte ein Freitextelement wie 
                     völlig ausgereicht. Doch hätte eine solche Lösung, welche die analytischen Stärken der Codierung für die editionsphilologische Arbeit nicht nutzt, den Aufwand für einen Umstieg auf 
                    XML/TEI nicht gerechtfertigt. Gerade die Möglichkeit zur präzisen Erfassung textgenetischer Prozesse einerseits und andererseits der Anspruch, einen Datensatz für unterschiedliche Ausgabeformate zu nutzen, legte den Umstieg nahe. 
                
         
         
            Digitale Präsentation und Korrespondenzedition
            Die digitale Präsentation soll im Wesentlichen im Rahmen der Edition von Gotthelfs Korrespondenzen entwickelt werden (Zihlmann-Märki 2017). Dabei sind drei Ansichten für unterschiedliche Nutzungsszenarien vorgesehen. Neben der historisch-kritischen Ansicht, welche die 
                    Tustep-Routine einbindet, und einer diplomatischen Ansicht stellt eine Inhaltsansicht den finalen Text samt ausgezeichneten Entitäten und Stellenkommentaren bereit. Die Codierung wird die Vorschläge der 
                    TEI Special Interest Group Correspondence berücksichtigen, und die Verwendung standardisierter Daten ist Voraussetzung für eine Integration in externe Suchumgebungen wie 
                    CorrespSearch.
                    3 Ebenso können Informationen aus anderen digitalen Ressourcen dank dem Einsatz von Normdateien und dem BEACON-Format in der digitalen Umgebung angezeigt werden (Stadler 2012; Stadler 2014). 
                
         
         
            Vorzüge und Schwierigkeiten der Reform
            Überblickt man die bisherige Reform, haben sich folgende Vorteile für die Arbeitsprozesse ergeben:
            
               Durch neue Arbeitsabläufe der Texterfassung (angepasste Scann- und OCR-Verfahren bei Drucktexten) konnte eine nicht unerhebliche Zeitersparnis erzielt werden. 
               Unterschiedliche Previewansichten des Editionstextes erlauben die Hervorhebung spezifischer Besonderheiten, die für einzelne Korrekturschritte notwendig sind (etwa Hervorhebung des Zeilenfalles oder Hervorhebung von Stellen, die differenzierter codiert sind, als dies für die Druckedition im Apparat ausgegeben wird etc.). 
               Die Dokumentation der Codierungsrichtlinien ermöglichte es, unterschiedliche Aspekte zu verknüpfen: 1) editorische Prinzipien konnten verbessert und verbindlicher gestaltet werden, 2) Probleme im Übergang von Transkription und Apparatgestaltung entfallen durch die Verbindung beider Prozesse, 3) die Verbindung von Codeerläuterung, Transkriptionsbeispiel und Präsentationsbeispiel hat der Satzfirma die Programmierung der Satzroutinen erleichtert.
               Die Satzfirma 
                        Pagina investiert vor allem in die Konzeption von Satzroutinen und kommt dann nur noch für den Feinsatz der Buchausgabe zum Einsatz. Dank Roundtripping können die Satzinformationen (Seiten- und Zeilenzahlen) in die originalen 
                        XML-Daten zurückgespielt werden; so verfügt die 
                        Forschungsstelle über aktuelle Daten, die alle Informationen zur Druckausgabe enthalten und leicht auch für andere Präsentationsformen oder – in weiteren Reformetappen – für digitale Querverweise und Kommentarverankerungen genutzt werden können.
                    
            
            Zugleich erwies sich der Reformprozess als überaus anspruchsvoll sowie als zeit- und kostenintensiv. 
            
               Als Illusion erweist sich die Vorstellung, es sei möglich, eine Codierung unabhängig von späteren Ausgabeformaten zu entwickeln. Editionen, welche die Daten tatsächlich für mehrere Formate bereit halten wollen und nicht nur auf eine spezifische Ausgabe zielen, stehen hier vor bedeutenden Problemen, da sie die Dateninterpretation durch Webapplikationen ebenso berücksichtigen müssen wie die Eigenheiten von Satzroutinen oder den Wunsch nach einer mediumsspezifischen Apparatgestaltung. Dies gilt umso mehr für Projekte, welche die Umstellung im laufenden Arbeitsprozess bei bereits etablierten Editionsrichtlinien durchführen.
               Innerhalb der modularen Arbeitsumgebung konnte aufgrund der projektspezifischen Bedürfnisse letztlich kein einziges Modul unverändert übernommen werden. Da die Module jeweils spezifischen Projektinteressen der an der Modulentwicklung beteiligten Projektpartner folgen (müssen), sind sie – auch nach Erfahrung von 
                        Telota – in keinem Fall ohne Anpassung nutzbar. Andere Module (Druckvorstufe) konnten dagegen problemlos ausgetauscht werden, und dies wäre wohl eine Grundanforderung überhaupt an modulare Editionsumgebungen.
                    
               Die graphische Oberfläche in 
                        Ediarum können wir für unsere Edition nicht nutzen, weil in ihr komplexe Textphänomene und sich überlagernde Korrekturen nicht übersichtlich darstellbar sind. Allerdings hat sich auch gezeigt, dass die Arbeit in der Code-Ansicht für die meisten Mitarbeitenden unproblematisch ist. Für einzelne Arbeitsschritte wie die Lemmatisierung wäre die Arbeit in der graphischen Oberfläche möglich.
                    
            
            Allein von der Kostenseite aus betrachtet, lohnt sich bei begonnenen Buchausgaben die Umstellung auf eine differenzierte Codierungspraxis nur bei einem bedeutenden Projektvolumen. Erst dann stehen die Entwicklungs- und Anpassungskosten einer digitalen Arbeitsweise in einem Verhältnis zur Einsparung von Satzkosten. Das gilt auch dann, wenn auf eine Buchedition gänzlich verzichtet wird.
         
         
            Standardisierung und heterogene Editionslandschaft
            Freilich wäre es zwecks Ressourcenschonung wünschenswert, dass andere Projekte von Schemata, von Satzroutinen, von der Arbeitsumgebung und der digitalen Präsentation profitieren könnten, die in unserem oder einem anderen Projekt entwickelt wurden. Tatsächlich sehen wir ein gewisses Potential im Erfahrungsaustausch. Dass eigentliche Übernahmen von Projektstrukturen hingegen schwierig sind, liegt weniger an den Codierungsstandards als an heterogenen Editionstypen und -prinzipien. Verbindliche Prinzipien der Textwiedergabe und Apparatierung könnten die Digitalisierungsprozesse wie auch die langfristige Sicherung vereinfachen (
                    ein Prozedere, 
                    ein Umwandlungsschema für alle Daten) und möglicherweise kostensenkend wirken. Die Diversität von Editionen entspringt aber nicht zufälligen Entwicklungen, sondern ist tief in heterogenen Forschungstraditionen verankert, und die Entscheidung für einen Editionstyp geht in der Regel mit einer intensiven Auseinandersetzung mit dem Editionsgegenstand einher. So rechtfertigt die Berner Parzival-Edition ihr Projekt nicht zuletzt durch Annahmen über den mittelalterlichen Textbegriff (Stolz 2002), und die Berner Gotthelf-Edition hebt auf den politisch-publizistischen wie diskursiven Charakter der Texte ab, der nur durch eine umfassende Kommentierung adäquat dargestellt werden kann (von Zimmermann 2014).
                
            Auch die 
                    TEI trägt der Diversität prinzipiell Rechnung, stellt sie doch einen Pool möglicher Codes zur Verfügung, aus denen die Einzelprojekte ihre eigenen Schemata erarbeiten müssen. Ein über das Bekenntnis zu 
                    TEI/XML (oder einer anderen Auszeichnungssprache) hinausgehender, projektübergreifender Standard, der die textphilologische Kernarbeit (Transkription und Apparatierung) betrifft, kann deshalb vermutlich eher nicht entwickelt werden. 
                
         
      
      
         
             Allgemeine Projektinformationen: 
                            http://www.gotthelf.unibe.ch
            
            
               http://www.metagrid.ch
            
            
               http://correspsearch.net/index.xql
            
         
         
            
               Bibliographie
               
                  Dumont, Stefan / Fechner, Martin (2012): 
                        Digitale Arbeitsumgebung für das Editionsvorhaben „Schleiermacher in Berlin 1808–1834“.
                        http://digiversity.net/2012/digitale-arbeitsumgebung-fur-das-editionsvorhaben-schleiermacher-in-berlin-1808-1834 [letzter Zugriff 25. August 2016]. 
                    
               [
                        Arbeitsgruppe Telota]: 
                        Ediarum – Digitale Arbeitsumgebung für Editionsvorhaben.
                        http://www.bbaw.de/telota/software/ediarum [letzter Zugriff 25. August 2016].
                    
               
                  Schweizerische Akademie der Geistes- und Sozialwissenschaften (2015): 
                        Final report for the pilot project „Data and Service Center for the Humanities“ (DaSCH). Swiss Academies Reports 10 (1).
                    
               
                  Stadler, Peter (2012): 
                        „Normdateien in der Edition“,
                        in: 
                        editio 26: 174–183.
                    
               
                  Stadler, Peter (2014): 
                        „Interoperabilität von digitalen Briefeditionen“,
                        in: Delf von Wolzogen, Hanna / Falk, Rainer (Hg.): 
                        Fontanes Briefe editiert. Internationale wissenschaftliche Tagung des Theodor-Fontane-Archivs Potsdam, 18. bis 20. September 2013 (= Fontaneana 12). 
                        Würzburg: Königshausen & Neumann 278–287.
                    
               
                  Stolz, Michael (2002): 
                        „Wolframs ‚Parzival‘ als unfester Text. Möglichkeiten einer überlieferungsgeschichtlichen Edition im Spannungsfeld traditioneller Textkritik und elektronischer Darstellung“,
                        in: Haubrichs, Wolfgang / Lutz, Eckart C. / Ridder, Klaus (Hg.): 
                        Wolfram von Eschenbach – Bilanzen und Perspektiven.Eichstätter Colloquium 2000 (= Wolfram-Studien 17). 
                        Berlin: Schmidt 294–321.
                    
               
                  Zihlmann-Märki, Patricia (2017): 
                        „Kommentierung in gedruckten und digitalen Briefausgaben“, 
                        in: Lukas, Wolfgang / Richter, Elke (Hg.): 
                        Kommentieren und Erläutern im digitalen Kontext (= Beihefte zu editio) [erscheint 2017].
                    
               
                  von Zimmermann, Christian (2014): „Geschichte, Ziele und Perspektiven der Historisch-kritischen Gesamtausgabe der Werke und Briefe von Jeremias Gotthelf (HKG)“, in: Marianne Derron / Christian von Zimmermann (Hg.): 
                        Jeremias Gotthelf. Neue Studien. 
                        Hildesheim / Zürich / New York: Olms 13–37.
                    
            
         
      
   



      
         
            Einleitung
            In Diskussionen über digitale Nachhaltigkeit sind rechtliche Fragen von zunehmender Bedeutung. Häufig entscheiden die rechtlichen Rahmenbedingungen, ob, wie und wie lange Daten und Programme, die im Rahmen von digitalen geisteswissenschaftlichen Forschungsprojekten entwickelt werden, verfügbar sind. Gerade im digitalen Raum, der nationale Grenzen überschreitet, ergibt sich - auch aufgrund der territorialen Beschränkung des jeweiligen Urheberrechts - die Notwendigkeit neuer rechtlicher Regelungen, die einerseits das Urheberrecht des Forschenden schützen, andererseits die Wiederverwendbarkeit ihrer Arbeiten sicherstellen sollen. Offene Lizenzierungsmodelle bieten hier Lösungen, die internationale Forschung an lokal erzeugten Daten und mit individuell entwickelten Softwares ermöglichen.
            In den Geisteswissenschaften, in denen die Publikation von und die Arbeit mit “Rohdaten” im Rahmen der digitalen Wende substantiell an Bedeutung gewonnen haben, beginnt die Forschungscommunity zunehmend, sich diesem Thema im Kontext von Open Access-Diskussionen zu widmen. Obwohl das Bewusstsein für die Notwendigkeit der Auseinandersetzung mit diesen rechtlichen Aspekten zunimmt, fehlt den Forschenden oft der Überblick über die unterschiedlichen Möglichkeiten der Lizenzierung ihrer Daten. Creative Commons-Lizenzen sind weltweit das etablierteste Modell. Während sie in vielen Fällen eine gute Lösung sind, kommen sie häufig auch nur deshalb zum Einsatz, weil den Forschenden Informationen über andere Lizenzierungsmodelle fehlen. Auch wenn Creative Commons-Lizenzen angemessen sind, herrscht in vielen Fällen oft Unklarheit und Unsicherheit über die Wahl der konkret geeigneten Lizenz. Noch komplexer ist die Landschaft an verfügbaren Software-Lizenzen.
            Dieser Workshop möchte zur Information und Aufklärung der Community beitragen, indem zuerst ein allgemeiner Überblick über die unterschiedlichen nationalen rechtlichen Rahmenbedingungen und über verfügbare Daten- und Software-Lizenzen gegeben wird, bevor im zweiten Teil lizenzierte Beispielprojekte vorgestellt werden. Eingegangen wird auch auf die unterschiedlichen Materialtypen (z.B. Manuskript-, Photodigitalisate) und den umgang mit “verwaisten Werken”.  Erwartet wird ein Publikum, das sich einen Überblick über die Lizenz-Möglichkeiten, die sich für einzelne Projekte bieten, verschaffen und die passende Lizenz für das jeweils eigene Projekt finden möchte. Dazu dient der dritte Abschnitt des Workshops, der in der Art einer offenen Sitzung gestaltet ist, in der nach einer Einführung zu Lizenzierungstools Projekte aus dem Publikum diskutiert werden. Die Beitragenden sind Expert/innen für Lizenzierungs- und Rechtsfragen und wünschen sich den Austausch mit Forschenden, die sich ebenfalls mit diesen Themen beschäftigen.
         
         
            Daten-Lizenzen
            
               Creative Commons (CC)
               Creative Commons (CC) ist eine Non-Profit-Organisation, die eine Auswahl an Standard-Lizenzverträgen zur öffentlichen Nutzung von Werken für juristische Laien entwickelt hat. Der Ausgangspunkt dafür war, dass “[t]he idea of universal access to research, education, and culture is made possible by the Internet, but our legal and social systems don’t always allow that idea to be realized.” (
                        
                     https://creativecommons.org/about/
                  ) CC-Lizenzen sind standardisierte Lizenzen, die durch die Creative-Commons-Stiftung entwickelt und ausformuliert wurden und Rechteinhabenden zur freien Verwendung zur Verfügung stehen. Sie sind vorrangig für den Einsatz bei digitalen Werken und der Verbreitung im Internet geschaffen worden, lizenzieren aber gleichzeitig auch die Verwendung im nicht-digitalen Bereich (Druckwerke); sie umfassen alle (wesentlichen) urheberrechtlichen und leistungsschutzrechtlichen Aspekte.
                    
               CC-Lizenzen der Versionen 1.0 bis 3.0 wurden vielfach mit nationalen Recht akkordiert (d.h. “portiert”) und so in spezifischen nationalen Varianten zur Verfügung gestellt. Ab Einführung der aktuellen Version 4.0 erfolgten keine spezifischen nationalen Adaptierungen mehr, um eine einheitliche, international gültige Lizenzierung und Rechtssicherheit zu gewährleisten. In jedem Fall sind aber - wie bei allen Einzel- und Massenlizenzen - die jeweils geltenden nationalen urheberrechtlichen bzw. internationalen Regelungen vorrangig und deshalb zu beachten.
               Alle nach geltendem Recht urheberrechtlich schutzwürdigen Werke können durch die Rechteinhabenden zu jeder Zeit mit Creative Commons (neu) lizenziert werden. Eine einmal erteilte CC-Lizenz kann grundsätzlich nicht widerrufen und damit erteilte Nutzungsrechte können grundsätzlich nicht eingeschränkt werden, jedoch kann die Lizenz in eine weniger einschränkende Lizenz umgewandelt werden. Davon sollte jedoch nur in einzelnen Ausnahmefällen Gebrauch gemacht werden.
               CC-Lizenzen bestehen aus mehreren Modulen, wobei die Zusammensetzung nicht von den Lizenzgebenden gewählt werden kann, sondern vorgegeben ist.
               In den Digital Humanities hat es sich zum de-facto-Standard entwickelt, Projekte, Daten und Ergebnisse unter CC-Lizenzen zur Verfügung zu stellen, wobei die Unterschiede zwischen den verschiedenen Versionen, portierten und nicht portierten Fassungen und unterschiedlichen Lizenz-Inhalten oft zu Unklarheiten führen. In diesem Workshop werden daher die Grundlagen des CC-Modells erklärt und zahlreiche weitergehende Aspekte erläutert.
            
            
               Digital Peer Publishing (DiPP/DPPL)
               Das Hochschulbibliothekszentrum des Landes Nordrhein-Westfalen (hbz) stellt zur Erleichterung der Publikation von Open Access-Journals das System Digital Peer Publishing (DiPP) zur Verfügung, in dessen Rahmen auch die Digital Peer Publishing Lizenzen (DPPL) entwickelt wurden. Aktuell ist die Version 3.0. Die Grundlage dieser Lizenzen bildet das deutsche Recht, was - anders als CC - im internationalen Bereich häufiger zu Problemen führen kann. Um den Teilnehmenden eine breitere Perspektive zu ermöglichen, werden als eine mögliche Alternative zu Creative Commons die DPP-Lizenzen  vorgestellt.
            
         
         
            Software-Lizenzen
            Es ist zu beachten, dass die oben beschriebenen Datenlizenzen nicht zur Lizenzierung von Software geeignet sind, da sie diverse Software-spezifische Aspekte nicht berücksichtigen. Es müssen für diesen Zweck daher spezifische Software-Lizenzen verwendet werden, die in einem weiteren Teil des Workshops vorgestellt werden.
            
               Freie Lizenzen haben im Bereich der Software bereits eine längere Tradition, als dies bei Daten und anderen Inhalten der Fall ist: Die ersten öffentlichen Software-Lizenzen gab es in den 1980er Jahren. Heute ist eine Fülle an freien Softwarelizenzen in Gebrauch. Nach gegenwärtigen Standards gilt eine Lizenz dann als Open Source, wenn sie den von der Open Source Initiative entwickelten Kriterien entspricht (
               
                  https://opensource.org/osd-annotated
               
               ). Die Wichtigsten davon sind: Die Möglichkeiten des Zugangs, der Verbreitung und der Adaptierung von Quellcode.
            
            Des Weiteren können Open Source Lizenzen in drei Kategorien eingeteilt werden:
            
               
                  Freizügig (
                  permissive
                  ): erlaubt breite Verwendung der lizenzierten Software (z.B. BSD, MIT oder Apache Lizenzen)
               
               Starkes Copyleft: “virale” Lizenzen, die den Benutzenden auferlegt, modifizierten Code unter einer kompatiblen Copyleft-Lizenz zu veröffentlichen (GNU GPLs sind die wichtigsten Lizenzen dieser Gruppe)
               Schwaches Copyleft: Lizenzen, die die Benutzenden verpflichten, modifizierte Software unter einer kompatiblen Copyleft-Lizenz zu veröffentlichen, aber die Verbindung mit Bibliotheken erlauben, die andere Lizenzen verwenden (z.B. GNU LGPL)
            
            Aufgrund dieser breiten Auswahl an Möglichkeiten, von denen sich einige nur durch Formalitäten unterscheiden, variiert die Lizenzierungspraxis zwischen Communities und Projekten stark. Das ist insofern problematisch, als dadurch in großen, verteilten Projekten oft komplexe Kompatibilitätsprobleme entstehen. Es ist daher notwendig, gemeinsame “best practices” zu entwickeln, um die Bestrebungen innerhalb der Digital Humanities Community zu harmonisieren.
         
         
            Lizenzierungstools
            Um den Teilnehmenden im praktischen Teil des Workshops nicht nur konkrete Beratung für aktuelle Projekte anbieten zu können, sondern ihnen auch Unterstützung für die Entscheidung von Lizenzierungsfragen im Rahmen zukünftiger Projekte und Kontexte zur Verfügung zu stellen, werden ausgewählte Lizenzierungstools präsentiert und unmittelbar unter Anleitung der Beitragenden getestet. Zu den wesentlichsten Hilfsmitteln gehören:
            
               
                  Europeana Available Rights Statement 
                  
                  
                     http://pro.europeana.eu/share-your-data/rights-statement-guidelines
                  
                  >
               
               
                  CLARIN License Category Calculator 
                  
                  
                     https://www.clarin.eu/content/license-categories
                  
                  >
               
               
                  Licentia License Tool 
                  
                  
                     http://licentia.inria.fr/
                  
                  >
               
               
                  ELRA License Wizard 
                  
                  
                     http://wizard.elda.org/
                  >
                    
               Public License Selector 
                     https://ufal.github.io/public-license-selector/
                  >
                    
            
         
         
            Ablauf
            10.00 Uhr: Einleitung 
            10.15 Uhr: Vorstellung der Lizenzmodelle
            
               Lizenzen für geisteswissenschaftliche Inhalte (Joachim Losehand/Seyavash Amini)
               Free/Open Source Software Licensing (Paweł Kamocki)
               Diskussion
            
            11.45 Uhr: Pause
            13.00 Uhr: Beispielprojekte
            
               Lizenzierungsmodelle in CLARIN (Andreas Witt)
               Das Geisteswissenschaftliche Asset Management-System GAMS: Objekte, Digitalisate und Vervielfältigung (Walter Scholger)  
                         
               
               Internationale Werke: Lizenzierung des Abstractbands der TEI-Konferenz 2016 (Vanessa Hannesschläger)
               Diskussion
            
            14.30 Uhr: Pause
            15.00 Uhr: hands-on: select your license
            
               Einführung: Lizenzierungstools (Walter Scholger)
               Offene Diskussion & Beratungsrunde: Konkrete Beispiele aus dem Publikum
            
            17.00 Uhr: Ende
         
         
            Benötigte Infrastruktur, Teilnehmende
            Erwartet wird ein großes Publikum, da der Workshop konkrete Beratung für individuelle Projekte vorsieht. Um jedoch einen lebendigen Austausch garantieren zu können, möchten wir die Zahl der Teilnehmenden auf 40 Personen beschränken.
            Teilnehmende werden gebeten, ihre eigenen Laptops mitzubringen.
            Benötigt wird ein Raum für 40 Personen mit WLAN und einem Beamer; ein zusätzlicher Computer mit Bildschirm & installiertem Skype (für die Zuschaltung von Seyavash Amini) wäre wünschenswert.
            Eine Mittagspause von 11.45-13.00 Uhr und eine Kaffeepause von 14.30-15.00 Uhr ist geplant. Sollte für die Mittagspause keine Verpflegung verfügbar sein, werden die Teilnehmenden gebeten, sich selbst zu versorgen. Kaffee und Getränke in der Kaffeepause wären wünschenswert.
         
         
            Beitragende
            Vanessa Hannesschläger vanessa.hannesschlaeger@oeaw.ac.at>
                
            studierte Germanistik in Wien. Sie ist wissenschaftliche Mitarbeiterin des Austrian Centre for Digital Humanities der Österreichischen Akademie der Wissenschaften (ACDH-ÖAW) und dort für Rechts- und Lizenzierungsfragen zuständig. Schon im Rahmen eines vorangegangenen Forschungsprojekts am Literaturarchiv der Österreichischen Nationalbibliothek beschäftigte sie sich mit praktischen Fragen des Urheberrechts. Mitarbeit in CLARIN (CLARIN PLUS) und DARIAH (WG Thesaurus Maintenance). Zu ihren Forschungsinteressen gehören digitales Edieren, Text- und Datenmodellierung, das Archiv im digitalen Kontext, Vermittlungsstrategien in den DH sowie digitale Infrastrukturen.
            Joachim Losehand joachim@losehand.at>
                
            ist Kulturhistoriker und studierte u.a. Klassische Archäologie, Alte Geschichte und Altertumswissenschaften in Tübingen, München und Wien. Zwischen 2003 und 2006 war er wissenschaftlicher Lektor und Redakteur, seit 2006 ist er Lehrbeauftragter und Lektor an Universitäten in Bremen, Oldenburg und Wien. 2009/10 war er Mitglied der Lenkungsgruppe im Aktionsbündnis „Urheberrecht für Bildung und Wissenschaft“, seit 2013 ist er Projektkoordinator und Referent für Urheberrecht u.a. im Verband Freier Radios Österreich (VFRÖ) sowie Projektleiter Science Commons bei creative commons Austria.
            Paweł Kamocki kamocki@ids-mannheim.de>
                
            verfügt sowohl im Bereich des Rechts als auch im Bereich der Sprachwissenschaften über breites Fachwissen; derzeit ist er wissenschaftlicher Mitarbeiter am Institut für Deutsche Sprache in Mannheim und Lehr- und Forschungsassistent an der Descartes Universität in Paris und promoviert zu den rechtlichen Fragestellungen der Open Science. Er ist Mitglied des CLARIN Legal Issues Committee und arbeitete als rechtlicher Berater in zahlreichen anderen Projekten und Arbeitsgruppen (z.B. EUDAT, RDA, OpenMinTeD). Neben Urheberrecht und Datenschutz gilt sein Interesse auch den Sprachwissenschaften (insb. rechtliche Fachsprache).
            Walter Scholger walter.scholger@uni-graz.at>
                
            studierte Geschichte und Angewandte Kulturwissenschaften in Graz und Maynooth und ist administrativer Leiter des Zentrums für Informationsmodellierung - Austrian Centre for Digital Humanities an der Universität Graz. In Projekten, internationalen Workshops und universitärer Lehre widmet er sich rechtlichen Aspekten des digitalen Kulturerbes und Fragen offener digitaler Publikationsformen.
            Er ist Mitglied in facheinschlägigen Arbeitsgruppen der Digital Humanities Dachverbände und internationaler Projekte (ADHO, DHd, ICARUS, DARIAH) zu rechtlichen Aspekten, digitalen Publikationen und Lehre im Bereich der Digital Humanities.
            Andreas Witt witt@ids-mannheim.de>
                
            leitet den Programmbereich Forschungsinfrastrukturen am Institut für Deutsche Sprache in Mannheim und ist Honorarprofessor für Digital Humanities an der Universität Heidelberg. Seine Forschungsinteressen konzentrieren sich auf die Texttechnologie, die Digital Humanities, die Informationsmodellierung und Auszeichnungssprachen. Bei CLARIN-D ist er für die Arbeitsgruppe zu juristischen und ethischen Fragen beim Umgang mit digitalen Sprachressourcen verantwortlich.
            Seyavash Amini amini@ivocat.de>
                
            ist Rechtsberater der Universitätsbibliothek Wien, Teamleiter des Clusters E - “Legal and Ethical Issues” im Projekt 
                    e-Infrastructures Austria, Berater der Geschäftsleitung einer Gruppe von Medienunternehmen in Hannover sowie Lehrbeauftragter an den Universitäten Wien und Hannover. Er beschäftigt sich mit Fragen des Informations-, Immaterialgüter-, Medien- und Datenschutzrechts. Im Rahmen der jüngsten Novelle des österreichischen Urheberrechts hat der Gesetzgeber einen von Seyavash Amini mitgestalteten Formulierungsvorschlag aufgegriffen und umgesetzt. Er wird sich per Skype zum Workshop zuschalten.
                
         
      
      
         
            
               Bibliographie
               
                  Amini, Seyavash / Blechl, Guido / Losehand, Joachim (2015): 
                        FAQs zu Creative-Commons-Lizenzen unter besonderer Berücksichtigung der Wissenschaft
                  https://phaidra.univie.ac.at/view/o:408042 [letzter Zugriff 25. August 2016].
                    
               
                  Creative Commons.
                        https://creativecommons.org/ [letzter Zugriff 25. August 2016].
                    
               
                  DiPP - Digital Peer Publishing.
                        http://www.dipp.nrw.de/ [letzter Zugriff 25. August 2016].
                    
               
                  Kamocki, Paweł / Ketzan, Erik (2014): 
                        Creative Commons and Language Resources: General Issues and What’s New in CC 4.0.
                        CLARIN Legal Issues Committee: White Paper Series
                        http://clarin-d.de/images/legal/CLIC_white_paper_1.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Kamocki, Paweł / Ketzan, Erik / Witt, Andreas (2016):
                        „Lizenzauswahlwerkzeuge für die digitalen Geisteswissenschaften“,
                        in:
                        DHd 2016: Modellierung - Vernetzung - Visualisierung 336–337 
                        http://dhd2016.de/boa.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Klimpel, Paul (2013): 
                        Free knowledge thanks to creative commons licenses: Why a non-commercial clause often won’t serve your needs.
                        Wikimedia Deutschland / iRights.info / CC DE.
                        https://www.wikimedia.de/w/images.homepage/1/15/CC-NC_Leitfaden_2013_engl.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Klimpel, Paul / Weitzmann, John H. (2015): 
                        Forschen in der digitalen Welt. Juristische Handreichung für die Geisteswissenschaften.
                        DARIAH-DE Working papers 12
                        https://irights.info/wp-content/uploads/2015/08/Forschen-in-der-digitalen-Welt-Juristische-Handreichung-Geisteswissenschaften-dwp-2015-12.pdf.
                    
            
         
      
   



      
         Die Aufbereitung und Erkennung von handschriftlichen Dokumenten ist sowohl für Menschen als auch für Computeralgorithmen eine technische Herausforderung. Die Bearbeitung von handschriftlichem Material wird bislang von spezialisierten Experten durchgeführt, um technisch und qualitativ hochstehende Resultate aus historischen Dokumenten zu erhalten. Zur Erstellung hochwertiger Editionen ist dafür hilfswissenschaftliches Wissen (Paläographie, Editorik), historisches Hintergrundwissen und technisches Know-how gefragt.
         Im Rahmen des Projekts READ (Recognition and Enrichment of Archival Data) werden unterschiedliche Aufgaben der Automatisierung (weiter-)entwickelt, um qualitativ gute Ergebnisse mit optimalem Ressourceneinsatz zu erhalten. Ein speziell dafür entwickeltes Tool ist die Software Transkribus, die die Arbeit von Experten und maschineller Erkennleistung verkoppelt. Die Software ist frei verfügbar unter www.transkribus.eu. Im Workshop wird Transkribus vorgestellt und kann durch die Teilnehmenden mit eigenen oder zur Verfügung gestellten Dokumenten getestet werden.
         Transkribus unterstützt alle Prozesse vom Import der Bilder über die Identifikation der Textblöcke und Zeilen, die zu einer detaillierten Verlinkung zwischen Text und Bild führt sowie die Transkription und Annotation der Handschrift bis zum Export der gewonnen Daten in standardisierten Formaten.
         Workflow in Transkribus
         Um Texte zu transkribieren oder zu edieren, müssen digitale Bilder hochgeladen und danach mit Layouterkennungswerkzeugen bearbeitet werden. Die Analyse des Layouts kann automatisiert geschehen, wobei die manuelle Kontrolle und falls nötig die Nachbearbeitung im Moment noch sinnvoll ist.
         Texte aus in Transkribus aufbereiteten Dokumenten können entweder mit bereits bestehenden HTR-Modellen (Handwritten Text Recognition) erkannt oder händisch erstellt werden und danach zum Training neuer Modelle genutzt werden. Insbesondere für die Bearbeitung grosser Dokumentenkorpora, die in ähnlichen Handschriften verfasst wurden, lassen sich bereits heute Effizienzgewinne und Vereinfachungen erzielen.
         Aufbauend auf den Transkriptionen ist es möglich eine Vielzahl von Auszeichnungen und Annotationen innerhalb des Textes, aber auch darüber hinaus für Einzeldokumente und ganze Dokumentenbestände anzulegen. Neben der Anreicherung der Dokumente mit der Identifikation von Personen, Orten und Sachwörtern ist somit auch die Möglichkeit der Herstellung von Bestandsbeschreibungen und der Hinterlegung von Transkriptions- und Editionsvorschriften gegeben.
         Ausgabeformate
         Für den Export stehen unterschiedliche Formate und Ausgabeformen zur Verfügung. So ist es möglich XML-Dateien zu exportieren, die den Vorgaben der TEI entsprechen. Ausgehend davon können komplexe digitale Editionen erstellt werden, die jedoch im Unterschied zu herkömmlichen Editionen eine enge Verzahnung mit den verwendeten Bilddateien aufweisen. Dadurch werden Editionen ermöglicht, die den transkribierten Text in der Zusammenschau mit der faksimilierten Vorlage sichtbar machen (analog zu state-of-the-art Editionen, wie beispielsweise die Edition der Briefe Alfred Eschers: https://www.briefedition.alfred-escher.ch/). Daneben sind auch Ausgaben als Druckdaten (PDF) oder zur Weiterbearbeitung für Textverarbeitungsprogramme (DOCX) implementiert. Schliesslich ist auch ein Export im PAGE-Format (zur Anzeige in Viewern für OCR gelesene Dokumente, Pletschacher, 2010) sowie als METS (Metadata Encoding and Transmission) möglich.
         Die Speicherung der Dokumente erfolgt in der Cloud (gehostet auf Servern der Universität Innsbruck). Die importierten Daten bleiben auch während der Bearbeitung unverändert im Dateisystem liegen und werden ergänzt durch METS und PAGE XML, letztere in eigenem Unterordner. Alle bearbeiteten Dokumente und Daten bleiben somit in den unterschiedlichen Bearbeitungsstadien nicht nur lokal verfügbar, sondern können für Projektmitarbeitende geteilt werden. Dank elaboriertem 
                user-management ist die Zuteilung von Rollen möglich. Die Erkennprozesse werden serverseitig durchgeführt, sodass die Ressourcen auf den lokalen Rechnern nicht strapaziert werden. Transkribus ist mit JAVA und SWT programmiert und kann daher plattformunabhängig (Windows, Mac, Linux) genutzt werden.
            
         Zielpublikum
         Die Plattform ist für unterschiedliche Gruppen konzipiert. Einerseits für GeisteswissenschaftlerInnen, die selbst Transkriptionen und Editionen historischer Dokumente erstellen möchten. Andererseits richtet sich die Plattform an Archive, Bibliotheken und andere Erinnerungsinstitutionen, die handschriftliche Dokumente in ihren Sammlungen aufbewahren und ein Interesse an der Aufbereitung des Materials haben. Angesprochen werden sollen auch Studierende der Geistes-, Archiv- und Bibliothekswissenschaften mit einem Interesse an der Transkription historischer Handschriften.
         Das Ziel, eine robuste und technisch hochstehende Automatisierung von Layout und Handschrift, lässt sich nur durch die enge Zusammenarbeit zwischen Geisteswissenschaftlern und Computerspezialisten erreichen, die bezüglich Datenqualität und Herstellung von Transkriptionen von unterschiedlichen Voraussetzungen und Ansprüchen ausgehen. Die Algorithmen werden daher nicht nur bis zu einem Status als 
                proof-of-concept erarbeitet, sondern bis zur Praxistauglichkeit verfeinert und in grösseren Forschungs- und Aufbewahrungsumgebungen getestet und verbessert. Die Computerwissenschaftler sind entsprechend ebenfalls ein wichtiges Zielpublikum, wobei bei ihnen weniger die Nutzung der Plattform als das Beisteuern von Software(teilen) anvisiert wird. Die eingespeisten Dokumente und Daten bleiben privat und vor dem Zugriff Dritter geschützt. Von Projektseite können vorgenommene Arbeitsschritte zwecks besserem Verständnis der ausgeführten Arbeiten und letztlich der Verbesserung der Produkte ausgewertet werden.
            
         Layout- und Texterkennung
         Die zwei zentralen Automatisierungsprozesse basieren auf Algorithmen, die in laufenden Forschungsprojekten entwickelt und verbessert werden. Die 
                document image analysis (DIA) versucht Textblöcke zu identifizieren und von Dreck, Scanfehlern und anderen Störsignalen zu unterscheiden, wobei zwischen handschriftlichen und gedruckten Textblöcken differenziert wird (Zagoris 2012; Stamatopoulos 2015).
            
         In Transkribus werden auf der Layouterkennung aufbauend zwei 
                handwritten text recognition-Engines (HTR) angeboten, die auf unterschiedlichen technischen Grundlagen basieren: Erstens kann eine nach dem Hidden Markov Model (HMM) operierende HTR der Technischen Universität Valencia angewählt werden (Toselli 2015, Puigcerver 2015). Zweitens kann ein Model basierend auf rekurrierenden neuronalen Netzwerken der Universität Rostock genutzt werden (Leifert 2016).
            
         Transkribus und das gesamte Forschungsnetzwerk will die verfügbaren technischen Möglichkeiten den Endnutzern nach möglichst gängigen Workflows aufbereiten, so dass dem schnellen Praxiseinsatz keine Hindernisse im Weg stehen. Im Gegenzug wird die Nutzung im grossen Umfang erhofft, die den Subprojekten wichtige Trainingsdaten und Aufschlüsse bezüglich der Nutzung und den Problemen mit den Algorithmen sowie dem Graphical User Interface geben. Tests zum Einsatz der Technik in Archiven und Bibliotheken und unter unterschiedlichen Bedingungen werden momentan getestet und evaluiert.
         Als Businessmodel ist eine Überführung des Forschungsprojekts in eine Kooperative geplant, die den Stakeholdern möglichst niederschwellige und kostengünstige Angebote unterbreiten soll (Mühlberger, Preprint). Somit vereint das Projekt READ die unterschiedlichsten Ansprüche an Automatisierungs- und Erkennungsroutinen und orientiert sich dabei an gängigen Arbeitsformen im Kontext mit handschriftlichen Dokumenten (siehe auch die Projekthomepage: http://read.transkribus.eu).
         Aus- und Seitenblicke im Workshop
         Zwei unterschiedliche Forschungsaspekte aus READ werden im Rahmen des Workshops als Inputs demonstriert:
         Einerseits der Umgang mit einer speziellen Dokumentenform, Kirchenbüchern, in denen stark strukturierte Daten aus Pfarreien gesammelt wurden (Wurster, 2014 / 2015). Aufgrund der Strukturerkennung und der HTR wird es möglich, spezialisierte Suchroutinen zu produzieren. 
         Andererseits können aufgrund der erhobenen Daten und durch c
                omputer vision Profile der Schreibenden erstellt werden, die die Identifikation der Personen als Schreibende weiterer Dokumente naheliegend macht (Fiel, 2012). Beide Anwendungen versprechen für die Geisteswissenschaften neue Zugänge zu grossen Datenschätzen, die in den handschriftlichen Beständen gehoben werden können.
            
         Programm/Ablauf des Workshops
         
            Einführung in Transkribus (
                Tobias Hodel, Zürich): 30‘
                Aufbau und Funktionieren des Programms, Demonstration des Gebrauchs anhand von Beispielen. Aufzeigen der Möglichkeiten zum Einsatz der Automatisierungen.
            
         
            Strukturierte Daten in Kirchenbüchern (
                Eva-Maria Lang, Passau): 30‘
                Demonstration vom Umgang mit Kirchenbüchern, einer spezifischen und stark standardisierten Dokumentform, die mit Transkribus aufbereitet werden. Eine Suche in den Dokumenten wird über eigene Routinen und Abfragemöglichkeiten gewährleistet.
            
         
            Selbstständiges Arbeiten der Teilnehmenden mit Transkribus: 90‘
                Die Möglichkeiten und Grenzen von Transkribus sollen von den Teilnehmenden (wenn möglich mit eigenen Dokumenten) selbst ausgetestet werden. 
            
         
            Schreiberidentifizierung (
                Stefan Fiel, Wien): 30‘
                Ein über Transkribus hinausgehender Teil des Projekts beschäftigt sich mit 
                computer vision. Ziel ist die Identifizierung unterschiedlicher Personen als Schreibende. Stefan Fiel berichtet über den Stand der Forschung und wie Teilnehmende die Hände wichtiger Schreibender zur Verfügung stellen können.
            
         
            Diskussion über Vor- und Nachteile der Software: 45‘
                Inklusive Evaluation des Tools und der Veranstaltung. Feedbacks werden eingeholt, zur Verbesserung der Software (usability, Umfang und Leistung der Automatisierungen etc.).
            
         Das Projekt READ und somit die Weiterentwicklung von Transkribus werden finanziert durch einen Grant der Europäischen Union im Rahmen des Horizon 2020 Forschungs- und Innovationsprogramms (grant agreement No 674943).
         Kontaktdaten aller Beitragenden (inkl. Forschungsinteressen)
         Tobias Hodel, Staatsarchiv des Kantons Zürich, Winterthurerstrasse 170, CH-8057 Zürich, Schweiz; 
                tobias.hodel@ji.zh.ch (Digital Humanities; Handwritten Textrecognition; eArchiving; Information Retrieval).
            
         Eva-Maria Lang, Archiv des Bistums Passau, Luragogasse 4, DE-94032 Passau, 
                Eva.Lang@bistum-passau.de (Automatic Text Recognition, Digital Archives, Image Recognition and Information Retrieval, Software Architecture).
            
         Stefan Fiel, Technische Universität Wien, Faculty of Informatics
                Institute of Computer Aided Automation, Computer Vision Lab, Favoritenstr. 9/183-2, A-1040 Vienna, Austria; 
                fiel@caa.tuwien.ac.at (Bilderverarbeitung und Dokumentenanalyse).
            
         Zahl der möglichen Teilnehmerinnen und Teilnehmer
         30-40 Personen (auch abhängig von der Raumgrösse)
         Benötigte technische Ausstattung:
                Allgemein: Beamer, evtl. Whiteboard.
                Teilnehmende: Eigener Rechner (wenn möglich Installation von Transkribus; Hilfe zur Installation von Transkribus wird 30 Minuten vor der Veranstaltung angeboten)
            
         Anmeldungen und Rückfragen bitte an tobias.hodel@ji.zh.ch
      
      
         
            
               Bibliographie
               
                  Fiel, Stefan / Sablatnig, Robert (2012):
                        „Writer Retrieval and Writer Identification using Local Features“,
                        in: 
                        10th IAPR International Workshop on Document Analysis Systems
                  http://www.ict.griffith.edu.au/das2012/attachments/FullPaperProceedings/4661a145.pdf.
                    
               
                  Leifert, Gundram / Strauß, Tobias / Grüning, Tobias / Labahn, Roger (2016):
                        Cells in Multidimensional Recurrent Neural Networks
                  https://arXiv.org/abs/1412.2620v02.
                    
               
                  Mühlberger, Günter / Colutto, Sebastian / Kahle, Philip (Preprint): 
                        Handwritten Text Recognition (HTR) of Historical Documents as a Shared Task for Archivists, Computer Scientists and Humanities Scholars: The Model of a Transcription & Recognition Platform (TRP).
                    
               
                  Pletschacher, Stefan / Antonacopoulos, Apostolos (2010):
                        „The PAGE (page analysis and ground-truth elements) format framework“,
                        in: 
                        Proc. ICPR 257–260.
                    
               
                  Puigcerver, Joan / Toselli, Alejandro Héctor / Vidal, Enrique (2015):
                        „Probabilistic interpretation and improvements to the hmm-filler for handwritten keyword spotting“,
                        in: 
                        13th international conference on document analysis and recognition (ICDAR).
                    
               
                  Stamatopoulos, Nikolaos / Gatos, Basilis (2015):
                        „Goal-oriented performance evaluation methodology for page segmentation techniques“,
                        in: 
                        13th international conference on document analysis and recognition (ICDAR) 281–285.
                    
               
                  Toselli, Alejandro Héctor / Vidal, Enrique (2015):
                        „Handwritten text recognition results on the Bentham collection with improved classical n-gram-HMM methods“,
                        in: 
                        International workshop on historical document imaging and processing (HIP). 
                    
               
                  Wurster, Herbert W. (2015):
                        „Schritt für Schritt ins Internet – Europas Matriken online“,
                        in: 
                        insights: Archives and people in the digital age 2: 16–17.
                    
               
                  Wurster, Herbert W. (2014): 
                        „Matrikeln - Ein kulturhistorischer Blick auf die Kirchenbücher“,
                        in: 
                        Zeitschrift für bayerische Kirchengeschichte 83: 87–93.
                    
               
                  Zagoris, Konstantinos / Pratikakis, Ioannis / Antonacopoulos, Apostolos / Gatos, Basilis / Papamarkos, Nikos (2012):
                        „Handwritten and Machine Printed Text Separation in Document Images Using the Bag of Visual Words Paradigm“,
                        in: 
                        Frontiers in Handwriting Recognition (ICFHR), 2012 International Conference 103–108 10.1109/ICFHR.2012.207.
                    
            
         
      
   



      
         
            Motivation
            Aufgrund zahlreicher Sommer-Schulen, Workshops, DH-Studiengänge und vielfältiger online-Tutorials ist die Kodierung eines Textes in XML nach dem de-facto-Standard TEI ein oft anzutreffender Projektbestandteil. Was jedoch häufig fehlt sind einstiegsfreundliche Anleitungen, Tutorials, HowTos zu dem sich an die Kodierung anschließenden Themenkomplex der Publikation einer digitalen Edition. Die Frage nach dem »Wohin?« der oftmals in langer und mühsamer Arbeit erstellten Editionen betrifft vor allem jene Forschende, welche nicht Teil eines größer angelegten Projektes sind oder auch sonst über keine allzu starke Anbindung an eine gut institutionalisierte Forschungsinfrastruktur verfügen. Zwar entwickeln zunehmend mehr Institutionen, vielfach in Verbindung mit konkreten Projekten, Kompetenzen, Workflows und (technische) Infrastrukturen zur Veröffentlichung Digitaler Editionen, aufgrund chronisch knapper Finanzierung können oftmals aber nur wenige und in erster Linie nur eigene/interne Projekte hinreichend betreut werden. 
            Gleichzeitig kann in vielen Digitalen Editionsprojekten eine sehr starre Arbeitsteilung zwischen so genannten FachwissenschlafterInnen und TechnikerInnen beobachtet werden. Obwohl es sicherlich nicht als Nachteil bewertet werden kann, wenn jeder das tut, wofür er ausgebildet wurde und was sie bzw. er demzufolge auch gut kann, so besteht in einem stark arbeitsteiligen Umfeld die Gefahr asymmetrischer Kompetenzverhältnisse und daraus resultierender Abhängigkeiten. Sei es durch unrealistische Wünsche seitens der Fachwissenschaft, die aufgrund mangelnder technischer Kenntnisse an die Technik herangetragen werden. Oder sei es die Verzögerung des Arbeitsfortschritts aufgrund schleppender Implementierung basaler Technologien oder von editorischer Seite dringend benötigter Funktionalitäten. 
            Der hier vorgeschlagene Workshop versucht, beide Problembereiche aufzugreifen, indem gemeinsam mit den Teilnehmern, welche vorzugsweise ihre eigenen XML/TEI Daten mitbringen, eine auf der XML-Datenbank eXist basierte Web-Applikation zur Publikation eigener Editionen entwickelt wird. 
         
         
            Die Applikation
            Die Anforderungen für eine solche Applikation stehen in engem Zusammenhang mit der im Kontext dieses Workshops verwendeten Vorstellung über die Eigenschaften und über potentielle Verwendungszwecke einer Digitalen Edition. Zur Erläuterung: Unter dem Begriff »Digitale Edition« sollen ein kohärenter Text oder mehrere kohärente Texte verstanden werden, die mittels XML/TEI kodiert wurden und worin in der Regel verschiedene Entitäten wie z.B. Personen, Orte, Werke oder ähnliches erfasst, deren Form und Textgenese beschrieben und die um weiterführende Erläuterungen, Annotationen und Anmerkungen ergänzt wurden. Eine solche Digitale Edition wird vorwiegend im ›close reading‹ rezipiert mit dem Zweck, ein tieferes Verständnis über den Text, dessen Inhalt sowie dessen Kontext und Entstehung zu erhalten. Abgesehen von einer solchen eher traditionellen Auseinandersetzung mit einer Digitalen Edition verfügt diese aber auch über den Mehrwert, systematisch und vor allem maschinell gelesen werden zu können.
            Eine ›Digital Edition Web-App‹ sollte ganz generell die kodierten Texte in einer möglichst benutzerfreundlichen Art und Weise präsentieren und den »technischen Unterbau« dem Benutzer nicht aufbürden, wohl aber die computergestützte Weiterverarbeitung der Texte jederzeit ermöglichen. Konkreter formuliert heißt das, dass eine solche Anwendung folgende Anforderungen zu erfüllen hat.
            
               Einstiegsseite
               NutzerInnen sollen auf einer zentralen Einstiegsseite einen möglichst vollständigen Überblick über den kompletten Umfang der Edition erhalten. Dies ist insbesondere dann von großer Bedeutung, wenn die Edition aus mehreren Editionseinheiten besteht, wie zum Beispiel im Falle eines Briefwechsels. 
               In der im Zuge des Workshops zu entwickelnden Applikation wird das in Form einer ListView gelöst, welche sämtliche XML/TEI Dokumente bzw. ausgewählte Informationen aus dem teiHeader in einer von den NutzerInnen such-, filter- und sortierbaren Ansicht präsentiert. Von diesem Inhaltsverzeichnis gelangen die NutzerInnen dann über Verlinkung zu den einzelnen Dokumenten. 
            
            
               Responsive Design
               Da Digitale Editionen im www verfügbar sind, muss davon ausgegangen werden, dass diese generell in digitaler Form, sprich auf einem PC, Notebook, Tablet, eventuell auch auf einem Smartphone gelesen werden. Insofern gilt es, den kodierten Text in einer leserfreundlichen Darstellung anzuzeigen, die die verschiedenen Formate der Anzeigegeräte berücksichtigt (womit einige der Grundlagen des sog. ›responsive design‹ berücksichtigt werden müssen). Andererseits darf aber der Wunsch vieler Nutzer, die Inhalte »klassisch« auf Papier zu nutzen, nicht vergessen werden.
               Die digitale Darstellung im Web eröffnet indes auch die Möglichkeit für dynamische, sprich von den Nutzer/innen frei konfigurierbare, Darstellungsweisen. Abhängig vom konkreten Mark-Up können, um nur ein paar Beispiele zu nennen, etwa Anmerkungen ein- oder ausgeblendet, Abkürzungen aufgelöst, oder Korrekturschritte ausgeblendet werden. 
               In der ›Digital Edition Web-App‹ wird mittels XSLT Transformation aus den XML Dateien eine HTML Dokument ›on the fly‹ generiert. Diese ›DetailView‹ verfügt, sofern aufgrund des Markups des Ausgangsdokumentes möglich, über ein Navigationsmenü, welches eine rasche Orientierung im Text ermöglicht. Über ein weiteres Menü können außerdem verschiedene Darstellungsoptionen (de)aktiviert werden. 
            
            
               Suche
               Die Möglichkeit, eine digitale Edition in ihrer Gesamtheit im Volltext durchsuchen zu können, wird häufig als einer der größten Vorzüge einer digitalen Edition beschrieben. Zusätzlich zu einer so genannten »einfachen Suche« wird darüber hinaus auch gerne eine »erweiterte Suche« angeboten, welche eine spezifizierte Suche wie zum Beispiel nur in Anmerkungen oder über Metadaten ermöglicht. 
               Aufgrund der Integration der Volltext-Suchengine Lucene in die Datenbanksoftware eXist-db ist die Realisierung sowohl einer »einfachen« wie auch einer »erweiterten« Suche im Rahmen der ›Digital Edition Web-App‹ einfach zu bewerkstelligen, wobei die Spezifika der »erweiterten« Suche vom konkreten Markup der einzelnen Editionen abhängt.
               Einige grundlegende Überlegungen zum Erstellen einer Suche werden hierbei anhand konkreter Beispiele mit den Teilnehmern diskutiert und demonstriert werden.
            
            
               Register
               Neben einer Volltextsuche bieten viele digitale Editionen auch eine registerbasierte Suche an, mit deren Hilfe etwa gezielt Personen oder Orte in der Edition identifiziert werden können.
               Je nach Art der Daten wird ein solches Register auf verschiedene Weisen demonstriert werden.
            
            
               PDF-Erzeugung
               Als Nachteil einer digitalen Edition wird oft angesehen, dass ihr die Möglichkeit, einfache Anmerkungen – ähnlich einem eigenen Studienexemplar – anzubringen, fehlt. Aus diesem und anderen Gründen wird häufig die HTML-Seite ausgedruckt.
               Im Rahmen des Workshops werden hierzu zwei verschiedene Lösungswege kurz umrissen, ohne jedoch weiter ins Detail gehen zu können: Einerseits handelt es sich um ein für den Druck spezifisch erarbeitetes CSS-Stylesheet (»print-CSS«), andererseits die Generierung einer Datei für das Satzprogramm LaTeX.
            
            
               Schnittstellen
               Da die Texte in einer (einigermaßen) standardisierten Art und Weise kodiert sind, können diese auch maschinell prozessiert werden. Dafür ist es notwendig, dass nicht nur eine HTML Darstellung der Daten veröffentlicht wird, sondern auch die eigentlichen XML/TEI-Daten.
               Die ›Digital Edition Web-App‹ wird ihre Daten über die in der eXist-db integrierte ›REST-Style Web API‹ veröffentlichen.
            
         
         
            Ziel und Zielgruppe des Workshops
            Ziel des Workshops ist es, den TeilnehmerInnen einen ersten Einblick in weit verbreitete Workflows, Technologien und Terminologien sowie Konzepte zur Umsetzung der genannten Funktionalitäten zu vermitteln. Sie erhalten somit Grundlagen zur Weiterentwicklung oder auch Beurteilung anderer Plattformen und Tools.
            Die von den TeilnehmerInnen im Zuge des Workshops erarbeitete Web-App wird – auch aufgrund der Heterogenität der von den TeilnehmerInnen gestellten Daten – keine produktionsreife Applikation sein, die alle Aspekte einer digitalen Edition umsetzt. Allerdings bildet die im Workshop teilweise selbst geschriebene Software eine solide Basis für weiteres Selbststudium, woraus sich später für die einzelnen Teilnehmer oder Institutionen einfache, aber auf die spezifischen Bedürfnisse zugeschnittene Plattformen entwickeln können.
            Die TeilnehmerInnen des Workshops sollten über Erfahrung in der Kodierung in XML/TEI verfügen und im besten Fall an einem konkreten Projekt arbeiten und über XML/TEI Dateien verfügen, auf deren Grundlage sie im Workshop ihre eigene ›Digital Edition Web-App‹ entwickeln können.
         
         
            Ablauf und Teilnehmeranzahl
            Die TeilnehmerInnen erhalten vorab eine detaillierte Anleitung zur Installation der notwendige Software (eXist-db).
            Im eigentlichen Workshop werden die jeweiligen Arbeitsschritte von einem der Organisatoren live vorgeführt (dafür wird ein Beamer benötigt). Die konkreten Inhalte orientieren sich dabei an dem gleichnamigen Blog (Andorfer/Kampkaspar 2016), welcher von den Organisatoren im Rahmen der TEI-Konferenz 2016 offiziell präsentiert wurde.
            Während des Workshops werden wir bei auftretenden Fragen und Problemen den Teilnehmenden helfend zur Seite stehen. Um eine möglichst gute Betreuung der TeilnehmerInnen gewährleisten zu können, sollte die Teilnehmerzahl 25 nicht überschreiten.
         
         
            Organsiatoren
            Peter Andorfer
            hat im Zuge seiner Dissertation eine digitale Edition erstellt und war im Editionsprojekt »Die Korrespondenz von Leo von Thun-Hohenstein« für die technische Umsetzung des Projektes (Entwicklung der Web-Applikation) verantwortlich. Gemeinsam mit Dario Kampkaspar schreibt er außerdem für den Blog »HowTo build a digital edition web app«.
            Dario Kampkaspar 
            erstellt im Rahmen seines Dissertationsprojektes eine Edition einer frühneuzeitlichen Handschrift. An der HAB ist er im Rahmen zweier Projekte (Andreas Bodenstein von Karlstadt; Johannes Rist) intensiv mit Edition und Entwicklung beschäftigt. Gemeinsam mit Peter Andorfer schreibt er außerdem für den Blog »HowTo build a digital edition web app«. 
            Marcus Baumgarten 
            ist langjähriger Mitarbeiter an der HAB und betreut unterschiedliche Editionsprojekte. Zur Zeit arbeitet er in einem Kooperationsprojekt mit dem historischen Seminar der Universität Freiburg (die »Tagebücher des Fürsten Christian II. von Anhalt-Bernburg«) und gemeinsam mit dem Leibniz-Institut für europäische Geschichte in Mainz (»Digitale Edition europäischer Religionsfrieden zwischen 1500 - 1800«). 
            Gemeinsam mit Timo Steyer und Studierenden der TU Braunschweig betreibt er das Weblog www.digital-ist-besser.net
            Timo Steyer 
            ist aktuell in den Bereichen Metadaten und Datenmodellierung im Forschungsverbund Marbach Weimar Wolfenbüttel am Standort Wolfenbüttel tätig. In diesem Kontext beschäftigt er sich mit Fragen und Methoden zu den Themen der Interoperabilität von digitalen Editionen und der Retrodigitalisierung von bereits im Druck vorliegenden Editionen (z. B. »Controversia et Confessio« und »Die Briefe der Fruchtbringenden Gesellschaft«).
         
      
      
         
            
               Bibliographie
               
                  Andorfer, Peter / Kampkaspar, Dario (2016):
                        How to build a Digital Edition Web-App
                  http://www.digital-archiv.at/howto-build-a-digital-edition-web-app/.
                    
            
         
      
   



      
         Die Forschungstätigkeiten Georeferencing und Entity Linking sind wichtiger Bestandteil vieler DH-Projekte. Webservices/APIs und Tools versuchen diese Tätigkeiten zu vereinfachen und zu beschleunigen. Eines der bekannteren Tools, wenigstens im deutschsprachigen Raum ist dabei vermutlich der ‘DARIAH-DE Datasheet Editor’. Dieser zeichnet sich durch seine einfache Benutzung aus, sei es was den Datenimport (ausfüllen einer Tabelle oder Hochladen einer CSV-Tabelle) betrifft oder die anschließende Disambiguierung/Verifizierung der vom ‘Getty Thesaurus of Geographic Names’ zurückgelieferten Treffer über ein Graphical User Interface. 
         Das Projekt TEIHencer greift diese Vorzüge des ‘DARIAH-DE Datasheet Editors’ auf und versucht diese einerseits mit der ‘TEI-Welt’ zu verknüpfen sowie mit GeoNames und der GND zwei alternative Normdaten Ressourcen einzubinden.
         Konkret handelt es sich bei TEIHencer um ein Plug-In zu dem Python/Django basierten prosopographisch-geographischen Informationssystem APIS. Mit Hilfe des TEIHencers ist es möglich, XML/TEI kodierte Texte in denen Lokalitäten, Orte ausgezeichnet sind, über eine Webformular in APIS zu importieren. Während des Imports werden die Orts-Entitäten entsprechend eines vom Benutzer wählbaren X-Path Ausdruckes geparst, gegen GeoNames und GND abgeglichen und im Falle von Übereinstimmung angereichert und in einer relationalen Datenbank gespeichert. Die gespeicherten Entitäten können anschließend über das APIS-Web-Interface im Falle mehrerer Treffer disambiguiert werden. Dies erfolgt über eine Kartendarstellung, in welcher die verschiedenen Treffer zu einer Entität aufscheinen. Darüber hinaus können über das APIS-Web-Interface noch weitere Informationen zu den Entitäten ergänzt (z.B. alternative Schreibweisen, Datierungen) sowie die einzelnen Entitäten miteinander in typisierte Beziehungen gesetzt werden (z.B. Ort A ist Nachfolger von Ort B.; Ort A ist Teil von Ort B).
         Die mit Hilfe von TEIHencer angereicherten Daten können dann wieder als XML/TEI Dokument (kodiert als  Element) exportiert bzw. über HTTP GET request abgerufen und so etwa in andere Applikationen eingebunden werden.
         Im Zuge der Posterpräsentation soll der TEIHencer der einschlägigen DH-Comunity vorgestellt werden und zwar an dem konkreten Fallbeispiel der “Andreas Okopenko: Tagebücher aus dem Nachlass (Hybridedition)”. Dabei handelt es sich um ein digitales Editionsprojekt, das eine Auswahl der Tagebücher Andreas Okopenkos im Zeitraum von 1949 bis 1955 inhaltlich erschließen und einem breiteren Publikum zugänglich machen möchte. Einer der Schwerpunkte des Projekts liegt hierbei auf der inhaltlichen Erschließung des örtlichen Wirkungs- und Schaffensraums des Nachkriegsavantgardisten, indem nicht nur erwähnte Orte (), sondern nach Maßgabe auch Werke und Organisationen ( und ) mit geographischen Normdaten verknüpft werden, um so ein umfassenderes Bild von Okopenkos kulturellem Kontext vermitteln zu können.
         Neben der eigentlich Applikation und des konkreten Use-Cases wird am Poster auch das Konferenzthema “Kritik der Digitalen Vernunft” bzw. das Subthema “Kritik digitaler Angebote, Projekte und Werkzeuge” in Form der Frage nach der Nachhaltigkeit des vorgestellten Tools reflektiert. Eine solche glauben wir nämlich insofern gewährleisten zu können, als das Tool a) in ein konkretes Projekt (Okopenko) eingebettet ist, b) einen weit verbreiteten Standard (TEI) unterstützt, c) auf bestehende Eigenentwicklungen (APIS) aufbaut und d) Teile des Codes als selbstständige Module (TEI-Modul als python-package) konzipiert sind, die auch jenseits der konkreten Applikation Anwendung finden können. Darüber hinaus, e) ist der gesamte Code auf GitHub publiziert [3].  
      
      
         
            
               
                    http://www.getty.edu/research/tools/vocabularies/tgn/index.html
                
            
            
               
                    https://geobrowser.de.dariah.eu/edit/index.html
                
            
            
               
                    https://github.com/acdh-oeaw/teihencer
                
            
            
               
                    https://github.com/acdh-oeaw/apis-core
                
            
            
               
                    https://www.onb.ac.at/bibliothek/sammlungen/literatur/forschung/projekte/andreas-okopenko-tagebuecher-aus-dem-nachlass-hybridedition/
                
            
         
      
   



      
         
            Einleitung
            Durch die wachsende Menge an digitalen Daten im Bereich Digital Humanities bedarf es zunehmend der Arbeit von Datenkuratoren. Aufgrund der ständigen Steigerung der Zahl und des Umfangs der zu verarbeitenden Quellen ist jedoch der übliche Modus Operandi der Datenkuratoren (manuelle Konversionen und konsekutive Anpassungen) untragbar geworden.
            Dieser Vortrag stellt eine neue Methode für die Kuration digitaler Daten vor, die auf den Prinzipien der funktionalen Programmierung, der unix-Tools und der XML-Technologien basiert. Diese Methode wurde vom Cologne Center for eHumanities der Universität zu Köln seit 2014 im Rahmen des Lazarus-Projekts (CCeH 2014) und danach in verschiedenen anderen DH-Projekten angewendet.
            Die Besonderheit dieser Methode besteht in der Aufgliederung des Kurationsprozesses in eine Pipeline von Miniprogrammen, von denen jedes einzelne einen präzisen Schritt des Kurationsprozesses darstellt. Wird ein Fehler in den resultierenden Dateien bemerkt, kann ein neuer Schritt geschrieben werden und in die Kette eingebaut werden, oder die existierenden Schritte korrigiert werden. Anschließend wird die Kurationspipeline neu laufen gelassen.
                
            Der Hauptgrundsatz lautet: Keine Datei wird „manuell“ modifiziert, jede Operation muss von einem Miniprogramm ausgeführt werden.
            Konsequenz dieses Grundsatzes und Hauptvorteil der vorgestellten Methode ist, dass die ganze Arbeit des Kurators — mitsamt seiner Entscheidungen und seiner bevorzugten Arbeitsweisen — in dieser Pipeline von Miniprogrammen dokumentiert ist und in sie eingebettet ist. Des weiteren sind der Kurationsprozess und seine Ergebnisse einfach zu reproduzieren und zu verifizieren. Das führt zu einer besseren Nachvollziehbarkeit der Kurationsarbeit, auch wenn die Kuration von einem Team durchgeführt wird.
         
         
            Kuration von Digitalen Daten
            Eine der Aufgaben der Datenkuration und der Datenkuratoren ist: »[to] intervene in the research process in order to translate or migrate data into new formats, to enhance it through additional layers of context or markup, to create connections between data sets, and to otherwise ensure that data is maintained in as highly-functional a form as possible.« (Flanders & Muñoz 2017).
            Praktisch können wir die Arbeit der Datenkuratoren auf folgende Art und Weise grob zusammenfassen:
            
               Die Daten werden von den Forschern zu den Kuratoren übertragen.
               Die Kuratoren studieren die Daten, sowohl ihren Inhalt als auch ihr Format.
               Die Kuratoren reichern die Daten mit den nötigen Metadaten an und sie sorgen dafür, dass eventuelle Inkohärenzen zwischen den originalen Formaten und den Zielformaten ausgeglichen werden.
               Die so verarbeiteten Daten werden archiviert, publiziert oder in anderen Projekten verwendet.
            
         
         
            Datenkuration am CCeH: Das Cologne-Sanskrit-Lexicon-Projekt
            Ein praktisches Beispiel von Datenkuration sind die Wörterbücher des Cologne Sanskrit Lexikons. Diese wurden von verschiedenen Wissenschaftlern der Universität zu Köln seit den 90er Jahren (pre XML und pre Unicode) erarbeitet, innerhalb des Lazarus-Projektes vom CCeH kuratiert (d.h. in TEI/Unicode umgewandelt) und 2015 zugänglich gemacht.
            Die anfängliche Arbeitshypothese, welche im Verlauf jedoch fallen gelassen wurde, sah einen eher klassischen Workflow vor: Die originalen Dateien in XML umwandeln, danach eine XSLT-Transformation nutzen, um diese in TEI umzuwandeln und schließlich die durch die Transformation entstandenen kleinen Fehler per Hand verbessern.
            Wir haben es vorgezogen, diesen Weg aus zwei Gründen nicht einzuschlagen.
            Erstens: Von Beginn an sind verschiedene Versionen der zu kuratierenden Dateien aufgetaucht. Hätten wir in der Zwischenzeit neue Versionen der Dateien entdeckt, wäre die bis dahin geleistete Arbeit vergebens gewesen.
            Zweitens: Die Arbeitsgruppe bestand aus drei Personen aus unterschiedlichen Fächern und mit unterschiedlichen Herangehensweisen an das Thema Kuration. Einen einheitlichen Stil bei zu behalten wäre nicht einfach — wahrscheinlich unmöglich — gewesen.
            Die Arbeitsgruppe hat sich dann für eine andere Arbeitsweise entschieden: Eine „wiederholbare Pipeline“-Methode, die auf der funktionalen Programmierung basiert, statt manueller Konversionen und Anpassungen.
            In dieser Methode ist jeder Arbeitsschritt formell durch ein sehr kleines Programm beschrieben, das in einer funktionalen und deklarativen Programmiersprache implementiert ist und im Durchschnitt aus weniger als 15 Instruktionen besteht. In dieser Art sind sehr unterschiedliche Schritte implementiert, z.B. die Normalisierung der Lemmata, die Behebung von technischen Fehlern, die Integration von externen Quellen.
            Diese Programme, die die Schritte der Kurationsarbeit darstellen, sind im Sinne einer Pipeline organisiert, d.h. der Output des einen ist der Input eines anderen. Die Kuratoren erklären, welches die kommenden Schritte sind und welche Abhängigkeiten zwischen den einzelnen Schritten bestehen (z.B. dass die Umwandlungsschritte dem Abrufschritt folgen sollen). Der folgende Abschnitt enthält verschiedene konkrete Beispiele von Kurationspipelines und Schritten/Miniprogrammen.
            Alle diese Schritte sind reine 
                    idempotente Funktionen, d.h. dass ihr Ergebnis nur von den Input-Daten anhängig ist. Konkret bedeutet das, dass man die Kurationspipelinemehrmals durchlaufen kann, und immer das gleiche Ergebnis erhält. Dies steht im Gegensatz zu den klassischen Skript-basierten Methoden.
                
         
         
            Lazarus-Kurationsworkflow: XML-Pipelines, Makefiles und Schematron
            In der Essenz bedeutet das konkret, dass der Kurationworkflow, der im Lazaruz-Projekt und in anderen folgenden CCeH-Projekten benutzt wurde, aus drei großen Komponenten besteht:
            
               Die Makefiles. Ein Makefile ist eine Datei, die den gesamten Kurationsprozess mittels des unix-Tools make steuert. Der Makefile erklärt, wie man eine Datei X (genannt 
                        Target) durch die Dateien A, B und C (genannt Abhängigkeiten von X) herstellen kann. Im Fall des Cologne-Sanskrit-Lexicon-Projekts erklären die Makefiles wie man die target TEI-Deteien herstellen und testen kann, d.h. wo man die originalen Dateien mit den Sanskrit-Wörterbüchern finden kann, wie man sie herunterladen kann, wie man den Konversionprozess durch die Konversionspipelines durchführen kann usw.
                    
               Die Konversionspipelines. Jede Pipeline ist für die Konversion bestimmter Dateien verantwortlich und besteht aus verschieden Schritten. Jeder Schritt ist implementiert durch eine XSLT-Transformation (die funktionalen Miniprogramme, wie oben erwähnt). Die Pipelines selbst sind XProc-basierte XML-Pipelines (Walsh 2007).
               DieTests. Verschiedeneautomatisierte Schematron-basierte Tests kontrollieren, dass die hergestelltenDateien valide sind, dass keine altenschon behobenen Fehler erneut eingepflegt werden, sowie dass keine Informationen verloren gehen.
            
            Um die Methode und die Beziehungen zwischen den verschieden Komponenten besser zu verstehen, stellen wir hier ein konkretes Beispiel vor: Das Monier Sanskrit-English Wörterbuch, Teil des Cologne-Sanskrit-Lexicon-Projekts.
            Die ursprünglichenDateien mit den Digitalisatenund den Transkriptionen des Monier-Wörterbuches, Nachlass der Arbeit von Thomas Malten et al., sind auf einem Server der Universität zu Köln gespeichert und archiviert. Um dieses Wörterbuch in dasneue Cologne Sanskrit-Lexicon zu integrieren, müssen die Kuratoren folgenden Operationen durchführen:
            
               Die Originaldateien vom Server abrufen;
               Kleine Markup-Fehler beheben;
               Die Daten in TEI/XML umwandeln;
               Verweise zu externen Datenbanken/Quellen integrieren;
               Prüfen, ob Fehler unterlaufen sind, bzw. dass kein Lemma verloren wurde;
               Die kuratiertenDaten zur Verfügung stellen, sodass sie auf den Produktionsserver hochgeladen werden können.
            
            Diese Operationen, die denKurationsprozess grob zusammenfassen, sind imMakefile monier.mkbeschrieben. Die Operationensind im Makefile in der folgenden Form ausgedrückt: „1) Datei X wird ausgehend von Datei Y hergestellt. 2) Wenn X fehlt oder älter als Y ist, wird X hergestellt, indem man Y anProgramm K mit bestimmten Parametern übergibt“. Abbildung 1 zeigt einige Regeln, die im Vergleich zum Original vereinfacht dargestellt sind.
            
               
                  
                  Abbildung 1: Makefile für die Kuration des Monier Wörterbuches. In den ersten Zeilen werden verschiedene Parameter eingerichtet. Dann werden die Abhängigkeiten der target-Datei monier.tei beschrieben. Schließlich wird das Kommando eingerichtet, das man benötigt, um die target-Datei herzustellen.
                        
               
            
            Die Makefiles geben in 
                    imperativerWeise vor, welche Daten die 
                    funktionalePipeline durchlaufen sollen und wo das finale Ergebnis gespeichert werden soll. Dies macht unter anderem das Testen der Kurationspipeline anhand eines Auszugesdes Wörterbuches möglich, ohne die Pipeline an sich zu verändern; es werden lediglich Änderungen einiger Parameter in den Makefiles vorgenommen.
                
            Operationen 2, 3 und 4 (die Behebung von Fehlern, die Umwandlung der originalen Daten in TEI/XML und deren Verlinkung mit externen Datenbanken) sind das Herzstück des Kurationsprozesses und wird durch verschiedeneXProcPipelines durchgeführt. Diese Pipelinesbestehen insgesamt aus 35 verschiedenen XSLT-Transformationen, von denen jede einzelne einen spezifischen Schritt des Kurationsprozesses darstellt. Beispiele für diese Schritte sind:fehlplatziertes Markup verschieben, falsch kodierte Devanagari-Buchstaben richtig stellen, bibliographische Referenzen hinzufügen. Abbildungen2 und 3 zeigen Ausschnitte von zwei XSLT-Transformationen.
            
               
                  
                  Abbildung 2: Ein Miniprogramm. Dieser Schritt korrigiert nur einen bestimmten Fehler.
                        
               
            
            
               
                  
                  Abbildung 3: Auszug aus der Haupt-TEI-Transformation. Da die vorherigen Schritte die kleinen Fehler schon behoben haben, kann diese Transformation kurz und bündig sein.
                        
               
            
            Diese Miniprogramme lesen oderschreiben keine Datei. Sie fungieren als rein funktionale Filter, die die Daten empfangen, einige Anteile modifizieren, und die modifizierten Daten zurückgeben. Wie bereits erwähnt ist die Aufgabe des Makefiles zu entscheiden, welche Daten in die Pipeline eingespeist werden sollen und wo die Ergebnisse gespeichert werden sollen. Die Pipeline kümmert sich nicht um diese Details. Dies verringert den Aufwand für die Entwicklung der Miniprogramme erheblich.
            Die Miniprogramme, aus denen die Pipeline besteht, spiegeln die Entscheidungen des Teams wider, das diese Daten kuratiert hat (Peter Dängeli, Martina Gödel und Gioele Barabucci, mit der wissenschaftlichen Kollaborationvon Felix Rau). Weil die Entwicklung des Codes der Pipeline und der Miniprogramme mittels eines Repositorium auf GitHub stattgefunden hat,ist es möglich, den Entwicklungsprozessder Kurationsarbeit nachzuvollziehen. Insbesondere ist es möglich, zu sehen, wie einzelneSchritte,die sich alsirrtümlich herausgestellt haben, durch bessere Schritte ersetzt werden, ohne das die übrigen Teile der Pipeline verändert werden müssen.
            Schließlich Operation 5 (testen, dass kein Fehler untergelaufen ist)ist durch Schematron-basierte Testsimplementiert. Während dieser Operation wird getestet, 1) dass der Umwandlungsprozess keine Daten unabsichtlich entfernthat und, 2) dass alte Fehler, die schon korrigiert worden sind, wiedereingeführt wurden. Die Wiedereinfürung von alten Fehlernist ein Ereignis, das in über Jahre andauerndenProjekten leider häufig vorkommt. Diese Art von Test ist von den Best Practices der Continous Integration in der Softwareentwicklung inspiriert.
         
         
            Vorzüge der vorgestelltenMethode
            Der Gebrauch dieser Methode hat viele Vorteile, sowohl im Hinblick auf die methodologische Stringenz als auch die Technik an sich:
            
               Jede einzelne Handlung der Kuratoren ist formalisiert und dokumentiert (durch XSLT-Code und Code-Kommentare). Die geringe Größe der Miniprogramme macht deren Code praktisch selbst dokumentierend.
               Jeder Wissenschaftler kann unabhängig nachvollziehen, wie die Ergebnisse entstanden sind.
               Jeder Schritt kann einzeln getestet werden.
               Man kann zurückverfolgen, welcher Schritt ein bestimmtes Konstrukt in den Ergebnissen generiert hat.
               Die Wiederverwendung von Schritten ist möglich und leicht nachzuvollziehen.
               Eine methodologische Kohärenz kann über Jahre hinweg beibehalten werden, auch wenn neue Kuratoren diese Daten verwalten werden.
               Dank der Speicherung verschiedener Schritte in Versioning-Systemen wie Git, kann man sehen, wie der Kurationsprozess entwickelt worden ist.
            
            Die Hauptregel dieser Methode, dass keine Datei „manuell“ modifiziert wird und alles durch Miniprogramme ausgeführt wird, garantiert, dass jede Operation an den gegebenen Daten klar definiert und ausdrücklich formuliert ist.
                
         
         
            Die Rolle der Kuratoren
            Diese Methode ändert nicht die Rolle oder die Verantwortung der Kuratoren, aber sie verändert grundlegend ihre tägliche Arbeit. Die Arbeit der Kuratoren besteht nicht mehr in dem Modifizieren von Dateien in einem Editor, sondern in dem Schreiben von Arbeitsschritten und in der korrekten Verwaltung von den Abhängigkeiten zwischen Arbeitsschritten.
            Die Kurationsarbeit ist dann in zwei Teile aufgeteilt. Der erste Teil besteht im Schreiben und in der schrittweisen Präzisierung von Kurationsprogrammen, welches die Hauptaufgabe der Kuratoren darstellt. Hierin zeigt sich die Fähigkeit, die Erfahrung der Kuratoren sowie die von ihnen präferierten anwendbaren Richtlinien. Der zweite Teil ist die Generierung von kuratierten Daten, welche in sterilerArt und Wiese von einemKoordinationsprogramm vollzogen wird. Es führt die verschiedenen Schritte in der von Kuratoren bestimmten Reihenfolge innerhalbwenigerMinuten durch.
            Eine letzte wichtige Auswirkung dieser Methode ist, dass was registriert/gespeichert wird, nicht nur die Endergebnisse sind, sondern der ganze Kurationsprozess: Vom Abrufen der originalen Daten bis zu der Speicherung der kuratierten Daten. Es besteht die Möglichkeit diesen Prozess der Öffentlichkeit zugänglich zu machen, nicht nur um eine bessere Transparenz zu schaffen, sondern auch um die Zusammenarbeit mit externen Kuratoren zu erleichtern.
            Zusammenfassend ergibt sich das Besondere dieserMethode aus der Tatsache, dass nicht nur das Ergebnis der Kuration, sondern auch der Kurationsprozess dokumentiert wird.
         
         
            Verwandte Arbeiten
            Für die Kuration digitaler Daten wurde oft ein einfachererWorkflow vorgeschlagen: die Daten ändern und danach die Ergebnisse in einem Git-Repositorium speichern. (Reeve 2016, Crowley et at. 2017). Die Idee der Befürworter dieses Workflows ist, dass die Speichrung des Datenstatuses nach jedem Arbeitsschritt den Kurationsprozess adäquat widerspiegeln. Das greift jedoch zu kurz. Git speichert nur was geändert wurde, nicht mit welcher Absicht eine Änderung durchgeführt wurde. Natürlich könnten diese Absichten und die entsprechenden Begründungen in einer Commit-Nachricht beschrieben werden, aber oft sind sie es nicht und in jedem Fall können Commit-Nachrichten nicht so präzise wie ein Stück Code sein. Des weiteren löst die Änderungen mithilfe von Git nachzuvollziehen nicht die Probleme, welche entstehen, wenn die originalen Daten verändert werden: In diesem Fall muss die ganze Arbeit von vorne begonnen werden.
                
            Workflows wie der in diesem Beitrag beschriebene, in welchen die Hauptaufgabe der Kuratoren ist, Pipelines zu schreiben, finden sich häufig in der Informatik (Doltra & Löh 2008; Schoen & Perry 2014) und in der Physik (Peng 2009).
            Diese sind auch im Bereich Digital Scholarly Editing vorgeschlagen worden, z.B. von van Zundert (2016) oder Barabucci und Fischer (2017).
         
      
      
         
            
               Bibliographie
               
                  Barabucci, Gioele / Fischer, Franz (2017): „The formalization of textual criticism: bridging the gap between automated collation and edited critical texts“, in: Advances in Digital Scholarly Editing: Papers presented at the DiXiT conferences in The Hague, Cologne, and Antwerp. Sidestone Press.
               
                  CCeH (2014). „sanskrit-dict-to-tei: TEI-fy existing Sanskrit dictionaries.“, https://github.com/cceh/sanskrit-dict-to-tei (Das Repository wird im Laufe des Jahres 2018 veröffentlicht werden).
               
                  Crowley, Ronan / Reeve, Jonathan / Schäuble, Johannes (2017). open-editions/corpus-joyce-ulysses-tei: Zenodo release (Version v0.1.1). Zenodo. 10.5281/zenodo.583139
               
                  Dolstra, Eelco / Löh, Andres (2008). „NixOS: A purely functional Linux distribution“, in: ACM Sigplan Notices, 43(9): 367-378.
               
                  Duvall, Paul M. / Matyas, Steve / Glover, Andrew (2007). „Continuous integration: improving software quality and reducing risk“. Pearson Education.
               
                  Flanders, Julia / Muñoz, Trevor (2017). „An Introduction to Humanities Data Curation“, http://guide.dhcuration.org/contents/intro/ [letzter Zugriff 2018-01-10].
               
                  Peng, Roger D (2009). „Reproducible research and biostatistics“, in: Biostatistics, 10(3): 405-408.
               
                  Reeve, Jonathan (2016). „Git-Lit: an Application of Distributed Version Control Technology toward the Creation of 50,000 Digital Scholarly Editions“, in: Digital Humanities 2016: Conference Abstracts. Jagiellonian University & Pedagogical University, Kraków, pp. 657-658.
               
                  Schoen, Seth / Perry, Mike (2014). „Why and how of reproducible builds: Distrusting our own infrastructure for safer software releases“, https://air.mozilla.org/why-and-how-of-reproducible-builds-distrusting-our-own-infrastructure-for-safer-software-releases/ [letzter Zugriff 2018-01-10].
               
                  Walsh, Norman (2007). „XProc: An XML pipeline language“ in: XML Prague 2007.
               
                  van Zundert, Joris J. (2016). „Close Reading and Slow Programming — Computer Code as Digital Scholarly Edition.“, in: ESTS 2016.
            
         
      
   



      
         
            Im Nachlass des portugiesischen Dichters Fernando Pessoa (1888-1935) finden sich zahlreiche Listen geplanter Publikationen. Diesen stehen nur wenige zu Lebzeiten tatsächlich realisierte Veröffentlichungen gegenüber. Daraus ergibt sich ein Kontrast zwischen der Ebene des Möglichen und des Verwirklichten in der Literatur und Literaturproduktion. Vor diesem Hintergrund entsteht die digitale Edition “Fernando Pessoa. Projekte und Publikationen” und mit ihr die im Folgenden vorgestellte Netzwerkvisualisierung. Mit ihr wird ein Werkzeug zur Verfügung gestellt, das die Exploration des Personen- und Figurenkosmos in Pessoas Publikationsplänen über die Zeit ermöglicht.
            
         
            
               Pessoas Werk zwischen Planung und Publikation
                
            
               Die Dynamik der Schriften Pessoas ist im Spannungsverhältnis zwischen dem projizierten Werk, das der Dichter im Sinne eines vollkommen Ganzen konzipiert hat, und dem tatsächlich Geschriebenen und nur in geringem Maße Publizierten zu verstehen. Pessoa war bei der Veröffentlichung seiner Werke extrem selektiv und die Dynamik seines Schreibens weist auf eine ständige Bedeutungsverschiebung hin, die sowohl an fragmentarischen Schriften seines Nachlasses als auch an seinen Publikationsplänen zu erkennen ist (vgl. dazu Cunha 1987, Martins 2003, Gusmão 2003 und Sepúlveda 2013). Diese Bedeutungsverschiebung hängt stark mit dem Wahl der Autorennamen zusammen, die ebenfalls einem ständigen Wechsel unterlag und in den Plänen zur Edition und Publikation des Werkes eine hohe Bedeutung gewann. Zur Definition eines Publikationsvorhabens gehörte für Pessoa die Zuordnung einer bestimmten Autorfigur, deren fiktionale Persönlichkeit sowohl durch ein bestimmtes Werk konstruiert werden sollte als auch dieses Werk in seiner Besonderheit definieren würde.
                
            
               Die Spannung zwischen Planung und Publikation des Werkes wird daher noch dadurch verstärkt, dass Pessoa unter verschiedenen, insgesamt etwa 120 Autorennamen geschrieben hat - oder geplant hat zu schreiben (vgl. dazu Pessoa 2012). Eine besonders wichtige Rolle in Pessoas Werk und seinen Werkplänen spielen dabei die Namen Alberto Caeiro, Álvaro de Campos und Ricardo Reis, die er (in Abgrenzung zu den weiteren Pseudonymen) Heteronyme genannt hat.
                
         
         
            
               Digitale Edition “Projekte und Publikationen”
                
            
               Die Notizzettel, Seiten aus Notizbüchern und andere Papiere aus Pessoas Nachlass, auf denen er die Pläne für seine Werke handschriftlich oder mit Schreibmaschine geschrieben festgehalten hat, werden in einer Kooperation zwischen dem Institut für Literatur und Tradition (IELT) der Neuen Universität Lissabon und dem Cologne Center for eHumanities (CCeH) der Universität zu Köln digital ediert (Sepúlveda und Henny-Krahmer 2017).
                    
            
            
               Neben den Dokumenten aus dem Nachlass umfasst die digitale Edition auch die zu Lebzeiten von Pessoa publizierten Gedichte. Gegenstand des hier vorgestellten Netzwerkes sind jedoch ausschließlich die Dokumente, auf denen die Figuren und Personen genannt sind und deren Beziehungen ausgehend von ihrer gemeinsamen Erwähnung über die Zeit untersucht werden. Die Dokumente sind in der Edition in TEI codiert, wobei die Namensvorkommen erfasst werden und eine Identifikation der hinter den Namen stehenden Personen und Figuren in einem zentralen Index erfolgt. Transkriptionen und Index bilden zusammen mit den für jedes Dokument festgehaltenen Metadaten die Datengrundlage für das Figuren- und Personennetzwerk, wobei insbesondere die für die Chronologie relevante Datierung zu nennen ist.
                
         
         
            
               Netzwerkvisualisierung zu Personen und Figuren
                
            
               Die Vorkommen von Namen in Pessoas Publikationsplänen werden hier mit Hilfe einer interaktiven Netzwerkvisualisierung analysiert, um zu untersuchen, wie sich der von ihm in den Dokumenten entworfene Personen- und Figurenkosmos über die Zeit entwickelt. Für dynamische Netzwerkvisualisierungen gibt es in den DH bereits verschiedene Ansätze (vgl. u. a. Rigal et al. 2016, Xanthos et al. 2016). Für das Pessoa-Netzwerk ergibt sich die Dynamik aus der Möglichkeit, das Gesamtnetzwerk der Personen und Figuren über alle Dokumente hinweg auf Dokumente aus bestimmten Zeiträumen oder Jahren einzugrenzen. Es ist als heuristisches Instrument gedacht, um Hypothesen zur Chronologie von Personen- und Figurenkonstellationen zu generieren und im Ansatz überprüfen zu können.
                
            
               
                    Für das Netzwerk, das unter 
                    verfügbar ist, sind 249 Dokumente ausgewertet worden, die insgesamt 369 Namen enthalten. Es werden sowohl historische Personen als auch fiktive Figuren aus Pessoas Werkwelt gezeigt. Analysiert wird das Vorkommen der Namen auf Pessoas Publikationsplänen von 1913 bis zu seinem Tod im Jahr 1935. Die frühen Dokumente (vor 1913) werden hier nicht berücksichtigt, da sie noch in Bearbeitung sind.
                
            
               Die Netzwerkdaten sind mit Hilfe von XSLT aus den TEI-Dokumenten generiert worden und liegen im JSON-Format vor.
                     Die interaktive Visualisierung ist mit der Bibliothek D3 erstellt worden, wobei ein Netzwerklayout von Mike Bostock adaptiert und um weitere Funktionalitäten ergänzt wurde.
                     Erweiterungen, die für die vorliegende Anwendung vorgenommen wurden, sind u. a. die Möglichkeit, Teile des Netzwerks ein- und auszublenden (nach Chronologie; Teilnetzwerke für die Verbindungen, die von einzelnen Personen ausgehen; nur Knoten mit oder auch Knoten ohne Verbindungen) sowie Optionen für die Darstellung (Einblenden von Labels; Dichte bzw. Weite der Anzeige des Netzwerks).
                
            
               
                  
                  Abbildung 1: Optionen für die Anzeige des Netzwerks
                        
               
               Die Größe der Knoten im Netzwerk zeigt an, wie häufig einzelne Namen auf den Dokumenten erwähnt werden. Zur Ermittlung der Knotengröße wurde die Formel 2 + log2(size) angewandt, wobei size für die tatsächliche Häufigkeit des Vorkommens steht. Um zwischen fiktiven Figuren und historischen Personen unterscheiden zu können, sind die Knoten unterschiedlich eingefärbt (türkis = fiktiv; dunkelbau = historisch). Bei den Netzwerkkanten verdeutlicht die Dicke, wie häufig Namen gemeinsam auf Dokumenten vorkommen: je häufiger das gemeinsame Auftreten, umso dicker die Linien in der Visualisierung. Dabei ist die minimale Kantendicke 1 Pixel (bei einer gemeinsamen Erwähnung). Pro weiterer gemeinsamer Erwähnung nimmt die Kantendicke um 1 Pixel zu.
                
            
               
                  
                  Abbildung 2: Teilnetzwerk mit Fernando Pessoa als zentralem Knoten
                        
               
               Zentrale Funktionalitäten in der Netzwerkanwendung sind Auswahloptionen, welche die Chronologie der Namenserwähnungen betreffen. So kann das Netzwerk neben einer Gesamtdarstellung auch für Vorkommen in einzelnen Jahren angezeigt werden. Darüber hinaus ist eine Anzeige nach Perioden möglich (z. B. 1919-1927). Da es in der Pessoa-Forschung konkurrierende Vorschläge für eine Periodisierung des Werkes gibt, werden zwei verschiedene Einteilungen in Perioden zur Auswahl angeboten. Auf diese Weise wird es möglich, die Entwicklung von Pessoas Personen- und Figurenkosmos, wie er sich in den Publikationsplänen darstellt, kritisch zu untersuchen.
                
            
               
                  
                  Abbildung 3: Gesamtnetzwerk mit Pessoa als häufigstem Namen
                        
               
               Wenn man das Vorkommen aller Namen im gesamten Netzwerk betrachtet, so ist zu erkennen, dass Fernando Pessoa als Name am häufigsten vorkommt, direkt gefolgt von den Namen der Heteronyme Alberto Caeiro, Ricardo Reis und Álvaro de Campos, was etabliertes Wissen zu der Bedeutung der Heteronyme in Pessoas Werk bestätigt (vgl. Abb. 2 und 3). An zweiter Stelle stehen dann William Shakespeare, José Almada-Negreiros, Edgar Allan Poe, Mário de Sá-Carneiro und António Mora. Dabei ist beispielsweise interessant zu sehen, dass die Heteronyme (und Mora, der zeitweise als starker Kandidat für die Rolle eines Heteronyms galt) auch zusammen mit Fernando Pessoa selbst, aber besonders unter sich verbunden sind, was auf eine gewisse Autonomie des heteronymischen Universums hindeutet. Die Namen von Shakespeare und Poe weisen auf die zwei wichtigsten Referenzen Pessoas aus der englischsprachigen Literatur hin, während Almada und Sá-Carneiro die zwei für ihn bedeutendsten zeitgenössischen portugiesischen Schriftsteller waren.
                
            
               Betrachtet man das Netzwerk chronologisch anhand ausgewählter Jahre und Perioden, innerhalb derer die Publikationspläne verfasst wurden, so sind Tendenzen zu erkennen, die historisch, editorisch und auch poetisch für das Werk von maßgeblicher Bedeutung sind. So kann man beispielsweise sehen, wie in den Jahren von 1913 bis 1919, und besonders zwischen 1914 und 1915, die Namen der Heteronyme zusammen mit dem von Fernando Pessoa am häufigsten vorkommen und vor allem untereinander verbunden sind (vgl. Abb. 4). 
                
            
               
                  
                  Abbildung 4: Teilnetzwerk 1915
                        
               Namen anderer Schriftsteller und historischer Figuren kommen tendenziell in späteren Perioden, etwa ab den 20er Jahren, häufiger vor. Sie zeigen ein zunehmendes Interesse Pessoas an der Veröffentlichung eigener Übersetzungen von einigen für ihn entscheidenden Werke der Weltliteratur. Auch die wichtigsten Beziehungen Pessoas zu zeitgenössischen portugiesischen Schriftstellern und Kritikern sind im Netzwerk deutlich zu erkennen, besonders zwischen 1913 und 1918 (vgl. Abb. 5). Es handelt sich dabei vor allem um die modernistische Generation, die sich um die Zeitschrift Orpheu versammelt hat, und ab 1928 und bis zu Pessoas Tod 1935 dann die sogenannte zweite modernistische Generation um die Zeitschrift Presença.
                
            
               
                  
                  Abbildung 5: Teilnetzwerk 1913-1918
                        
               
            
         
         
            Fazit
            
               Die aus der Netzwerkinterpretation gewonnenen literaturhistorischen Erkenntnisse bestätigen, dass für Pessoa die Edition und Publikation des Werkes und dessen Planung nicht von der Bedeutungsebene des Werkes selbst zu unterscheiden sind. Dass bestimmte Namen insgesamt oder zu bestimmten Zeiten besonders häufig auf den Publikationsplänen auftauchen, ist gleichbedeutend mit deren Wichtigkeit für das Werk in der entsprechenden Zeitperiode. Das gilt neben dem Vorkommen einzelner Namen auch für die gemeinsamen Vorkommen mehrerer Namen, wodurch die Bedeutung bestimmter Konstellationen zu bestimmten Zeiten sowohl in Pessoas Publikationsplänen als auch in seinem Werk deutlich wird. Diese grundlegende Erkenntnis für die Interpretation von Pessoas Werk bestätigt einige Intuitionen der Kritiker über den größeren Zusammenhang mehrerer Ebenen von Pessoas Werk (z. B. der Ebene der Edition des Werkes, vgl. dazu Sepúlveda und Uribe 2016), sowie die Vorstellung von Pessoas Werkganzem, die einer materiellen Fragmentarität seiner Schriften gegenübersteht (vgl. dazu Martins 2003, Gusmão 2003, Sepúlveda 2013, Feijó 2015).
                
            
               Methodisch eröffnet die Visualisierung durch das interaktive Element und den höheren Grad der Abstraktion gegenüber den edierten Dokumenten, die in der digitalen Edition studiert werden können, neue Interpretationsspielräume. Dabei ist es jedoch sehr wichtig, die Dokument-, Text- und Datengrundlage sowie die methodischen Wege zum Netzwerk und zur visuellen Darstellung stets im Blick zu behalten. Für diesen Beitrag bieten wir dafür eine Dokumentation an, die direkt mit dem interaktiven Netzwerk verbunden ist.
                     Der Zusammenhang zwischen Quellen und Analyse wird auch dadurch hergestellt, dass die interaktive Visualisierung an die digitale Edition angebunden ist.
                    
            
         
      
      
         
            
               
                        Die digitale Edition ist in einer Beta-Version unter 
                        http://www.pessoadigital.pt
                        [letzter Zugriff 14. Januar 2018] verfügbar; die Entwicklung wird fortlaufend über GitHub organisiert: 
                        
                        [letzter Zugriff 14. Januar 2018]. Zur editorischen Herangehensweise aus digitaler Perspektive vgl. Henny-Krahmer und Sepúlveda 2017.
                    
            
               
                        Die dem Netzwerk zugrunde liegenden Daten sind unter
                        
                        [letzter Zugriff 14. Januar 2018] einsehbar.
                    
            
                Das Ausgangs-Layout von Bostock trägt den Titel “Force Layout with Mousover” (Bostock 2017).
                    
            
               
                        [letzter Zugriff 14. Januar 2018].
                    
            
               
                        Derzeit über den Menüpunkt “Chronologie”, vgl. 
                        
                        [letzter Zugriff 14. Januar 2018].
                    
         
         
            
               Bibliographie
               
                  Bostock, Mike (2017): “Force Layout with Mouseover Labels”, in: Mike Bostock’s Blocks.
                        
                        [letzter Zugriff 14. Januar 2018].
                    
               
                  
                  Cunha, Teresa Sobral (1987): "Planos e projectos editoriais de Fernando Pessoa: uma velha questão", in: Revista da Biblioteca Nacional, Série 2, Vol. 2, N. 1: 92-107.
                    
               
                  
                  Feijó, António M. (2015): “Uma admiração pastoril pelo diabo (Pessoa e Pascoaes)”, in: Pessoana. Ensaios. Lissabon: Imprensa Nacional-Casa da Moeda.
                    
               
                  
                  Gusmão, Manuel (2003): "O Fausto — um teatro em ruínas", in: Românica 12: 67-86.
                    
               
                  
                        Henny-Krahmer, Ulrike / Sepúlveda, Pedro (2017): “Pessoa’s editorial projects and publications: the digital edition as a multiple form of textual criticism”, in: Boot, Peter / Cappellotto, Anna / Dillen, Wout / Fischer, Franz / Kelly, Aodhán / Mertgens, Andreas / Sichani, Anna-Maria / Spadini, Elena / van Hulle, Dirk (eds.): Advances in Scholarly Editing. Papers presented at the DiXiT conferences in The Hague, Cologne, and Antwerp. Leiden: Sidestone Press, 125-133 
                        
                        [letzter Zugriff 14. Januar 2018].
                    
               
                  
                  Martins, Fernando Cabral (2003): "Breves notas sobre a alta definição", in: Românica. N.º 12. 157-164.
                    
               
                  
                  Pessoa, Fernando (2012): Teoria da Heteronímia. Hsg. von Fernando Cabral Martins und Richard Zenith. Lissabon: Assírio & Alvim.
                    
               
                  
                  Rigal, Alexandre / Rodighiero, Dario / Cellard, Loup (2016): “The Trajectories Tool: Amplifying Network Visualization Complexity”, in: Digital Humanities 2016. Conference Abstracts. 
                        Kraków: Jagiellonian University & Pedagogical University, 328-330 
                        
                        [letzter Zugriff 14. Januar].
                    
               
                  
                  Sepúlveda, Pedro (2013): Os livros de Fernando Pessoa. Lissabon: Ática.
                    
               
                  
                  Sepúlveda, Pedro / Henny-Krahmer, Ulrike (eds., 2017): Fernando Pessoa – Digitale Edition. Projekte und Publikationen. Editorische Leitung Pedro Sepúlveda, technische Leitung Ulrike Henny-Krahmer. Lissabon und Köln: IELT, Neue Universität Lissabon und CCeH, Universität zu Köln
                        http://www.pessoadigital.pt
                        [letzter Zugriff 14. Januar 2018].
                    
               
                  
                  Sepúlveda, Pedro / Uribe, Jorge (2016): O Planeamento editorial de Fernando Pessoa. Lissabon: Imprensa Nacional-Casa da Moeda.
                    
               
                  
                  Xanthos, Aris / Pante, Isaak / Rochat, Yannick / Grandjean, Martin (2016): “Visualizing the Dynamics of Character Networks”, in: Digital Humanities 2016. Conference Abstracts. Kraków: Jagiellonian University & Pedagogical University, 417-419 
                        
                        [letzter Zugriff 14. Januar 2018].
                    
            
         
      
   



      
         
            I.
            Ausgangspunkt war eine reine Metadaten-Sammlung zu Werken und Quellen Johann Sebastian Bachs (1999-2008: „Göttinger Bach-Katalog“), die zum Abschluss der ‚Neuen Bach-Ausgabe‘ am Johann Sebastian Bach-Institut Göttingen erfolgte. Dieses Metadatensammlung war 2008 die Basis für den Projektstart von „Bach digital“. Seitdem erfolgte mittels kontinuierlicher Förderung durch die DFG ein mehrstufig angelegter Ausbau:
            Das erste Digitalisierungsprojekt umfasste die sogenannten Originalquellen zu Johann Sebastian Bachs Musik, also Autographen und originales Aufführungsmaterial Bachs, die sich zu etwa 90 % im Besitz der oben genannten Bibliotheken befinden. 2010 ging diese erste Stufe als www.bach-digital.de online.
            Daran schloss sich von 2013 bis 2016 die Digitalisierung von sogenannten Sekundärquellen Bachscher Musik aus der Generation der Bach-Söhne und -Schüler an, ein Bestand, der besonders viel Tastenmusik J. S. Bachs umfasst, die vielfach nicht autograph überliefert ist und damit Forschungen zu individuellen Fassungen ermöglicht sowie Bachs Arbeitsweise in der Klavier- und Orgelmusik zwischen Kunstwerk und Unterrichtspraxis transparent zu machen hilft.
            Mittlerweile wurde die dritte Stufe gezündet: die konsequente Ausweitung der Datenbank in Metadaten und Digitalisaten auf die Musik der Bach-Söhne im Projekt „Quellenkorpus Bach-Söhne – Erschließung und Digitalisierung der Primärüberlieferung zu Werken Wilhelm Friedemann, Carl Philipp Emanuel, Johann Christoph Friedrich und Johann Christian Bach sowie deren Einbindung in das zu erweiternde Portal Bach digital“.
            Daneben werden Schritt für Schritt auch Werkverzeichnisse der Publikationsreihe „Bach-Repertorium“ sowie das derzeit neu erarbeitete „Bach-Werke-Verzeichnis III“ integriert. Mittelfristig werden dazu auch musikalische Incipits implementiert bzw. suchbar gemacht.
            Diese stufenweise Bearbeitung eines Kernbestandes der Musik des 18. Jahrhunderts strukturiert und systematisiert das gesamte musikalische Quellenmaterial und bildet den soliden Ausgangspunkt für eine quellenbasierte Forschung zur Musik der Bach-Familie: nicht nur als wichtiges Hilfsmittel der Bach-Forschung, sondern auch z. B. als Vergleichsobjekt für Studien zu anderen Repertoires, als unterstützendes Material für Forschungen zu Mitteldeutschland, als Kernbestandteil zur Provenienzforschung wichtiger Sammlungen wie Poelchau, Breitkopf etc. etc. Die Nutzungsmöglichkeiten sind in den vergangenen zehn Jahren stark angewachsen; und damit auf die Verantwortung, ein möglichst den diversen Anforderungen gerecht werdendes Material bestmöglich aufzubereiten. 
            Das MyCoRe-basierte Projekt wurde dabei von Anfang an durch eine Dokumentation begleitet, die es problemlos nachnutzbar macht: https://www.bach-digital.de/content/documentation.xml?XSL.lastPage.SESSION=/content/documentation.xml.
            Abseits der Bach-Forschung bzw. Musikwissenschaft werden Daten und Digitalisate von „Bach digital“ auch von einer breitgefächerten Bach-Community gesucht: das sind die weltweit großen Nutzerkreise musikinteressierter Laien sowie auch Musiker, die z. T. direkt nach originalen Quellen-Digitalisaten musizieren. Das Spektrum der Nutzer weitet sich nach unserer Erfahrung mit statistischen Daten zur Datenbank: je divergenter das ins Netz gestellte Material, desto vielfältiger die Nutzung. Inwiefern eine ursprünglich für die Bach-Forschungscommunity entwickelte Datenbank dieser Entwicklung noch stärker Rechnung tragen soll, wäre zu diskutieren.
         
         
            II.
            Die Menge der Datensätze an sich (es sind Stand Januar 2018 immerhin 8230 Musik-Quellen zu 3870 Werken der Bach-Familie) sowie der Umfang der Metadaten innerhalb eines Datensatzes ist nur mit hohem personellem Aufwand auf dem neuesten Stand der Forschung zu halten. Digitalisate und Metadaten werden so gut es geht laufend überprüft, auch mithilfe von Nutzer-Feedbacks – besonders jenen für die Bach-Forschung so wichtigen Power-Usern aus aller Welt. Dies ist eine ständige Anforderung, die die Daten selbst stellen, sobald sie öffentlich sichtbar sind. An der Aktualisierung der Datensätze aufgrund von Neuerkenntnissen der Bach-Forschung sollen deshalb nun auch mehr Mitarbeiter in der Forschungsabteilung des Bach-Archivs beteiligt werden als es Projektmitarbeiter für „Bach digital“ gibt. Ziel ist es, der Veraltung von Forschungsdaten entgegenzuwirken. Das Bach-Archiv sieht sich hier in der Verantwortung, die einmal publizierten Forschungsdaten mit den „Bach digital“-Nutzern möglichst zu teilen. Hierzu gehört auch die Mehrsprachigkeit, die derzeit nur mit Hilfe von strukturierten Daten umgesetzt werden kann. Fließtexte zu übersetzen ist mangels dafür vorhandener Projektmittel nur sehr begrenzt möglich. Alle anderen Daten sind aber mittlerweile auch in Englisch, Japanisch, Französisch (und Anfang 2018 auch Italienisch und Spanisch) recherchierbar. Hierbei sind wiederum die Nutzer der Datenbank selbst behilflich. Geplant ist als nächstes eine Nutzerbefragung, die über die Interessen und Wünsche sowie Kritik oder weitere Formen der Common Science-Beteiligung Auskunft geben soll. Inwieweit dieses Ergebnis zu einer Umstrukturierung von Daten oder der Präsentation von Modulen führen wird oder muss, ist derzeit noch offen.
            Bei dieser prinzipiell optimistischen Sicht auf „Bach digital“ sollen weitere kritische Punkte nicht außer Acht gelassen werden, die aus dieser langjährigen Erfahrung mit den Metadaten resultieren:
            Die Datenbank-Struktur suggeriert Eindeutigkeit, suggeriert, dass die Daten dem - in letzter Zeit in den Polit-Medien - so beliebten Faktencheck standhalten. Die Herkunft der Daten, gerade auch bei Neuerkenntnissen, wird dabei oft nicht präzise offengelegt. Die Datenbankstruktur suggeriert indes meist, dass es hier um Fakten geht. Unsicherheiten können nur sehr begrenzt formuliert werden, gerade im Fall von strukturierten Daten. Gerade auch die gegenüber den Printmedien so einfach zu handhabende Datenänderung ist also ein Problem für die Transparenz von Forschungsdaten.
            Ein einfacher Daten-Austausch per Schnittstelle, sicher allgemein gewünscht und praktiziert, ist nur insofern dauerhaft praktikabel, so lange ermöglicht wird, dieses Procedere mehrfach zu wiederholen, gerade auch bei Richtigstellungen von Forschungsdaten. Ansonsten finden sich mehrere Versionen von Quellen- oder Werkdaten im Netz, die sicherlich unerwünscht sind, selbst wenn man mit Versionierungsangaben arbeitet.
         
         
            III.
            Immer mehr Datenbanken zu Musikern und musikalischen Quellen tummeln sich im Netz. Doch selbst wenn es inhaltliche Überschneidungen gibt, arbeiten sie zumeist aneinander vorbei. Dabei ist das größte Problem nicht einmal die Vergeudung von Ressourcen, sondern die prinzipielle Unmöglichkeit eines Datenaustauschs bzw. einer einfachen Nachnutzbarkeit der Metadaten – selbst wenn sie in den sogenannten „Quasi-Standards“ TEI oder MEI vorliegen. Bei öffentlich geförderten Projekten ist heutzutage Grundvoraussetzung, dass die „langfristige Sicherung von“ und der „grundsätzlich offene Zugang zu“ Forschungsdaten gewährleistet sein muss, es aber bislang unklar ist, was genau dies heißt. Ist der Zugang schon „offen“ wenn man die Informationen im Internet finden kann, oder erst dann, wenn sie über eine Schnittstelle bereitgestellt werden? Solange Forschungsprojekte nur „digitale Inseln“ errichten, bringt das „Digitale“ keinen wirklichen Mehrwehrt.
            Zwar existieren bereits verschiedene Formate, die speziell für den Datenaustausch gedacht sind, wie z. B. MARC21 und METS/MODS, doch sind diese nur sehr eingeschränkt für (musikwissenschaftliche) Forschungsprojekte und Datenbanken einsetzbar. Auch spezielle Ontologien stehen bereit, die vom W3C zu den „Good Ontologies“ gezählt werden, also Ontologien, die vollständig dokumentiert, dereferenzierbar, von unabhängigen Datenlieferanten verwendet und möglicherweise von bestehenden Tools unterstützt werden („ontologies that are fully documented, dereferenceable, used by independent data providers and possibly supported by existing tools“). Beispiele dafür sind „
                    
                  Dublin Core“
                und „
                    
                  The Music 
                  Ontology
                  “
               . Doch auch hier bleibt das Problem, dass diese Formate zu flexibel, zu schwammig gestaltet sind, um einen sinnvollen, nachvollziehbaren Datenaustausch zu gewährleisten, oder aber spezielle Forschungs-Erkenntnisse nicht hinreichend darin abgebildet werden können – ganz abgesehen davon, dass derlei Lösungen überhaupt erst einmal implementiert werden müssen. Das vielgepriesene RDF, das versucht, einige dieser Probleme zu lösen (oder zu umschiffen), kann dabei kein Selbstläufer sein.
                
            Auch können Projekte a-priori nicht immer vorhersehen, welche Daten genau anfallen werden, bzw. welche von anderen Forschern oder Projekten nachgenutzt werden könnten. Es ist also nicht unbedingt zielführend, Daten in allen möglichen Formaten anbieten zu wollen, selbst wenn die Ressourcen es gestatten verschiedene Daten-Export-Möglichkeiten bereitzustellen (und zu pflegen).
            Können RESTful APIs die Lösung aller Probleme sein? Diese ermöglichen es zwar, sehr spezielle Kombinationen aus Metadaten zusammen zu stellen. Dennoch bleibt das Problem der intern verwendeten Formate bestehen; beschreibt ein Feld „date“ ein Aufführungsdatum oder das Datum der Werkgenese?
            Um digitale Gräber zu verhindern, sind spezielle, klar definiert und strukturierte Datenformate vonnöten, die für klar definierte Anwendungsfälle einen echten Austausch ermöglichen und somit auch erstmals dezentrale Suchmaschinen ermöglichen. Solche Suchmaschinen können abseits von Google überhaupt erst wirkliche Interdisziplinarität herstellen, denn mit wachsender Zahl an digitalen Projekten – so begrüßenswert dies auch sein mag – steigt die Gefahr, dass man „den Wald vor lauter Bäumen nicht sieht“, also Ergebnisse anderer (vielleicht fachfremder) Projekte nicht wahrnimmt, und dadurch möglicherweise den eigenen Erkenntnisprozess behindert.
            Der aktuelle Umgang mit gesammelten Metadaten soll am Beispiel von „Bach digital“ gezeigt sowie mögliche Auswege skizziert und diskutiert werden. Vorgestellt werden dabei standardisierte Formate, die bereits heute den Informationsaustausch und -fluss ermöglichen und aufzeigen, was dadurch zukünftig möglich sein könnte, aber auch, wo die größten Lücken und dringendsten Desiderate bislang bestehen blieben.
         
      
   



      
         
            Einleitung
            Die Verwendung von Referenzdaten für das Training und die Auswertung statistischer Annotations- und Analyseverfahren ist ein Kernmerkmal empirischer Forschung, zu der auch die Digital Humanities zählen möchten.
                   Die wichtigste Grundlage für den erfolgreichen Einsatz statistischer Verfahren liegt in der Verwendung geeigneter, den Algorithmen zugrunde liegender Modelle. Für deren Erstellung ist neben einem passenden Lernverfahren das Vorhandensein von Trainingsdaten eine wesentliche Voraussetzung. Werden Forschungsdaten, die mit quantitativen Methoden entstanden sind, mit Referenzdaten verifiziert und interpretiert, ist ein kritischer Blick auf Auswahl, Erstellung und Umgang mit selbigen ein häufig vernachlässigter Bereich.
                
            Vor diesem Hintergrund richtet der vorliegende Beitrag einen kritischen Blick auf die Rolle von und den Umgang mit Ground-Truth-Daten im Bereich der Digital Humanities. Drei Beobachtungen sollen zur Diskussion gestellt werden:
            
               Es fehlt den Digital Humanities an einheitlichen und etablierten Ground-Truth-Datensets für die Evaluierung von Forschungsergebnissen auf Basis quantitativer Methoden.
               Es fehlt den Digital Humanities an Richtlinien zur Erstellung und Verfahren zur Verifizierung von Ground Truth.
               Es fehlt den Digital Humanities an akzeptierten und operationalisierbaren Metriken zur Qualitätsbestimmung von Ground Truth und abgeleiteten Datenanalysen.
            
            Anhand der automatischen Texterfassung, die als Analogie für ein allgemeines Modell eines empirischen Forschungsprozesses gesetzt wird, sollen im Folgenden die Probleme diskutiert und Möglichkeiten gezeigt werden, wie diesen Defiziten begegnet werden kann.
                
            
               
               Abbildung : Gegenüberstellung eines Modells eines Forschungsprozesses aus der empirischen Sozialforschung und dessen Anwendung auf den Prozess der automatischen Texterfassung
                    
            
            Unter Ground Truth wird in diesem Kontext die Dokumentation ausgewählter Merkmale (Zeichen, Zeilen, Absätze, Spalten, Abbildungen, usw.) des Textes in Form einer digitalen Transkription verstanden. Dabei ist je nach Anwendung zwischen allgemeineren Referenz- und spezifischeren Trainingsdaten zu unterscheiden.
            Die Volltext-Digitalisierung von Archiv- und Bibliotheksbeständen, größeren Dokumentsammlungen oder Korpora wird heute von unterschiedlichen Seiten verfolgt: So werden beispielsweise seit 2005 massenhaft Bibliotheksbestände von Google im Rahmen von öffentlich-privaten Partnerschaften sowohl als Bild als auch als Text digitalisiert. Daneben unterstützen Stiftungen, Fördereinrichtungen wie die DFG sowie die Haushaltsmittel der Institutionen die Digitalisierungen im Rahmen spezifischer Projekte. Im Ergebnis dieser Digitalisierungsbemühungen stehen Volltextsammlungen höchst unterschiedlicher Qualität, Vollständigkeit, Interoperabilität und Nachnutzbarkeit.
            Mit der OCR-D-Initiative wird erstmals versucht, die technischen und organisatorischen Grundlagen dafür zu schaffen, einen breiten heterogenen Bestand von Drucken aus dem 16. – 18. Jahrhundert vollständig und einheitlich in elektronischen Volltext umzuwandeln und frei zur Verfügung zu stellen. Unter Einbeziehung einzelner Modulprojekte wird vom DFG-geförderten Koordinierungsprojekt die Transformation der Drucke in strukturierten Volltext konzeptuell und prototypisch vorbereitet. Dazu werden im Rahmen von OCR-D Anwendungen, Adaptionen und Weiterentwicklungen von Verfahren der Optical Character Recognition (OCR) für historische Drucke geprüft bzw. implementiert und in einer finalen, prototypischen Produktionsumgebung kombiniert. Eine zentrale Aufgabe von OCR-D besteht dabei in der Bereitstellung eines umfassenden Ground-Truth-Korpus, das sowohl Referenz- als auch Trainingsdaten sowie Richtlinien zur Transkription von Texten für deren Verwendung als Ground Truth umfasst. Damit können sowohl Texte bezüglich ihrer Zeichengenauigkeit transparent geprüft als auch spezielle statistische Modelle für die Text- und Strukturerkennung trainiert werden.
                
         
         
            Diskussion
            1. Gegenstand der Digital Humanities ist das digitale Objekt, beispielsweise digitaler Text. Vergleicht man den Bereich der automatischen Texterfassung mit einem in der empirischen Sozialforschung etablierten Modell des Forschungsprozesses, wird deutlich, dass die Referenzdaten in beiden Prozessen, im Besonderen in der Phase der Evaluation nach Abschluss der Datenanalyse, eine bedeutende Rolle spielen. In dieser Phase wird auf entsprechende Referenzdaten oder -systematiken zurückgegriffen, die in der Phase der Theoriebildung identifiziert und angesammelt wurden. Anzumerken ist, dass der Forschungsprozess nicht isoliert zu betrachten ist, da er schon zu Beginn bei Formulierung und Auswahl des Forschungsproblems auf vorhandene Forschungsdaten zurückgreift. Der traditionelle Forschungsprozess hat durch ein System von Referenzendaten (u. a. Wörterbücher, Editionen, Nachschlage- und Quellenwerke) ein System der 
                    referenzbasierten Evaluation geschaffen. Dieses System wird gestützt durch Konventionen der Zitierung, Dokumentation, Verzeichnung und Aufbewahrung in Institutionen sowie spezifischen Publikationsformen. Der Forschungsprozess in den Digital Humanities kann auf diesen Hintergrund nur teilweise bzw. gar nicht zurückgreifen, da bisher zu wenige digitale Daten vorliegen.
                
            Auf der anderen Seite haben die Digital Humanities bezüglich der Verfügbarmachung von Quellcode und Forschungsdaten einen großen Vorsprung gegenüber anderen datenbasiert arbeitenden Disziplinen (etwa der kognitiven Psychologie). 
                    Reproducible Science wird sowohl gefördert als auch propagiert. Problematisch sind aber die fehlende Vereinheitlichung und Transparenz bei der Datenerhebung sowie der Einsatz mangelhaft erfasster Daten bzw. deren ad-hoc Surrogaten bedingt durch die mangelnde Verfügbarkeit an (insbesondere annotierten) Forschungsdaten.
                
            Die Arbeitsweise mit Referenzdaten, wie sie in den Naturwissenschaften und Teilen der Geisteswissenschaft Anwendung findet, möchte neben der Evaluation, Verifikation gerade die Vergleichbarkeit der Forschungsergebnisse stützen. Das setzt voraus, dass diese Daten in ihrer Qualität diesen Ansprüchen genügen müssen und durch entsprechende Normungen Interpretationsspielräume definiert sind.
            Trotz des wissenschaftlichen Anspruches der Digital Humanities auf Objektivität und dem Bemühen Referenzdaten zu schaffen, werden unterschiedliche Interpretationen, die auf Grundlage von mangelhaften Referenz- und Trainingsdaten entstehen, möglich. Solch ein „hinzunehmendes Übel“ wird erkannt und mit „Pragmatismus“, mit der „Flexibilität“ oder „Austauschbarkeit von Konzepten im Konkreten der Texte“ gerechtfertigt, eine Vergleichbarkeit der so entstandenen Texte bzw. Forschungsergebnisse damit aber wesentlich behindert. Um eine Vergleichbarkeit im Sinne von 
                    Reproducible Science zu erreichen, ist somit der Gegenstand der Digital Humanities um die Fragen der Objekterstellung zu erweitern.
                
            2. Mit dem Begriff OCR wird üblicherweise der Gesamtprozess der automatischen Texterfassung bestehend aus den Teilaufgaben Bildvorverarbeitung, Struktur- und Texterkennung sowie gegebenenfalls (automatisierter) Nachkorrektur bezeichnet. Damit eine OCR vorgenommen werden kann, sind entsprechende Erkennungsmodelle notwendig. Diese werden durch sogenanntes Training unter Nutzung von Ground Truth induziert. Generische Modelle werden vom Anbieter der OCR-Software zur Verfügung gestellt, domänenspezifische Modelle müssen trainiert werden. Das Training dient immer der Verbesserung der Endergebnisse des Erkennungsprozesses. Somit sollten bei einem universellen Anspruch Ground-Truth-Daten sowohl spezifische als auch allgemeine Normungen enthalten, damit sowohl generische als auch spezifische Modelle trainiert werden können. Ziel ist es, die unterschiedlichen Bedürfnisse und Schwerpunkte in den Digital Humanities zu bedienen.
            3. Um diesem sehr breiten Anspruch gerecht zu werden, reicht eine Akklamation, dass diese oder jene Sammlung von Daten als Ground Truth bezeichnet und genutzt werden kann, nicht aus. Es bedarf hingegen eines Ground-Truth-Konzepts. Dieses Konzept dokumentiert sowohl dessen inneren Aufbau als auch dessen Erstellung. Damit können im Bereich der Texterfassung beispielsweise folgende Punkte im Umgang mit Ground Truth erreicht werden:
            
               (weitere) Reduktion bzw. Standardisierung der Freiheitsgrade bei der Transkription (z. B. langes s, Ligaturen, Zeilenumbruch)
               weitergehende Operationalisierung der Überprüfbarkeit der Validität
                        der Transkription
                    
               Ergänzung von (weiteren) Anweisungen zum Umgang mit koordinatenbasierten Phänomenen
            
            Illustriert werden kann ein solches Konzept am Beispiel der 
                    OCR-D Ground-Truth-Guidelines. Um die Freiheitsgrade von Interpretationen (Punkt A) zu normieren werden drei Level von Erfassungsgraden angeboten. Die einzelnen Level sollen nachvollziehbare Interpretationsentscheidungen sowohl festlegen als auch dokumentieren und damit die Möglichkeit der maschinellen Überprüfbarkeit eröffnen.
                
            Tabelle : Beispiel der Anwendung der Level bei der Ligatur ct.
            
               
                  Zeichen
                  Level 1
                  Level 2
                  Level 3
               
               
                  
                     
                        
                     
                  
                  ct
                            Die Ligatur wird in zwei einzelne Zeichen aufgespalten.
                        
                  ct
                            Die Ligatur wird aufgespalten und mit einer zusätzlichen Annotation, dass es sich um eine Ligatur handelt, im PAGE-Format versehen.
                            textStyle{offset:0; length:2;ligatur:true;}
                        
                  &#xEEC5;
                            Die Ligatur wird als 
                            ein Zeichen interpretiert und mit dem entsprechenden Unicode-Zeichen wiedergegeben.
                        
               
            
            Damit werden Anweisung und Richtlinien weitgehend operationalisiert und eine computergestützte Validierung umsetzbar (Punkt B). Das entsprechende Ground-Truth-Korpus liegt im XML-basierten PAGE-Format vor. 
                    Das PAGE-Format hat sich im Rahmen des EU-Projekts IMPACT sowie durch seine Verbreitung im Rahmen von Wettbewerben bei wissenschaftlichen Konferenzen (z.B. ICDAR, ICFHR, DAS) als de-facto Standard für XML-basierter Ground Truth etabliert. 
                    Mit Hilfe von Schematron-Regeln kann nun, wie das Beispiel zeigt, geprüft werden, nach welchem Level die „ct“-Ligatur kodiert ist:
                
            
               
            
            Die Wahl des PAGE-Formates ermöglicht im Unterschied zum TEI-basierten Basisformat des DTA (DTABf) eine unkomplizierte Lösung für die Repräsentation von koordinatenbasierten Phänomenen (Punkt C).
                
            
               
            
         
         
            Fazit
            Ground Truth stellt im Texterkennungsprozess und im dazu betrachteten Forschungsprozess in den Digital Humanities eine entscheidende Rolle dar. Ergebnis dieser beiden Prozesse sind immer Publikationen, die als Forschungsdaten in neuen Forschungszusammenhängen nachgenutzt werden. Gelingt es den Digital Humanities nicht, ein Referenzsystem mit Konventionen zur Prüfung, Dokumentation, Verzeichnung und Aufbewahrung ihrer Daten in Institutionen sowie spezifischen Publikationsformen aufzubauen, ist die Vergleichbarkeit der Forschungsergebnisse nicht immer gegeben. Dabei bietet der Aufbau so genannter Forschungsdatenrepositorien erste positive Entwicklungen in Richtung dokumentierter Forschungsprozesse.
            
            Der Gegenstand der Digital Humanities ist nicht nur auf das digitale Objekt zu beschränken, sondern auch auf dessen Erstellung zu erweitern. Die Erstellung ist ein Prozess, der von den Digital Humanities zu dokumentieren ist, da nur so eine Referenzierbarkeit und Reproduzierbarkeit der Forschungsergebnisse möglich wird. Der kritische Umgang mit diesen Daten stellt eine Aufgabe der Wissenschaft dar. 
            Daher täten die Digital Humanities gut daran, in einen aktiven Dialog mit digitalisierenden Einrichtungen und Förderern einzutreten um gemeinsame Standards und Richtlinien zur Dokumentation zu etablieren, die transparent Auskunft über die Provenienz eines digitalen Objekts geben sowie klar über Möglichkeiten sowie Einschränkungen zu dessen Nachnutzbarkeit informieren. Entscheidungen, Kriterien, Daten und Ergebnisse sollten, im Unterschied zur bisherigen, in diesem Beitrag kritisierten Praxis, zukünftig möglichst transparent, operationalisierbar sowie validierbar digital zur Verfügung gestellt werden. 
         
      
      
         
             Vgl. dazu These 1.1: „Die Digital Humanities bereichern die traditionellen Geisteswissenschaften konzeptionell und methodisch - ihre Werkzeuge und Verfahren ergänzen das „Wie“ unserer Praxis um 
                            eine empirisch ausgerichtete Epistemologie.“ [Thesenpapier des Fachverbands „Digital Humanities im deutschsprachigen Raum“ 2014]
                        
             siehe [Schnell, Hill, Esser 2011: 4]
             siehe Fußnote 2
             OCR-D: Koordinierungsprojekt zur Weiterentwicklung von Verfahren der Optical Character Recognition (OCR), http://www.ocr-d.de/.
             „Es werden weitere korpusbasierte Analysen angestrebt. Diese erfordern aber eine Voraussetzung: Im Zuge der ‚digitale[n] Wende‘ [Schöch 2014: 130] ist es weiterhin wünschenswert und erforderlich, dass immer mehr literarische Texte digital zur Verfügung stehen oder diese durch leichte und praktikable Verfahren der Texterkennung (OCR, optical character recognition) digitalisierbar gemacht werden können.“ [Mihm 2016: 200]
             „Dass die von uns einbezogenen Online-Repositorien hinsichtlich der editionsphilologischen Textqualität variieren ist ein hinzunehmendes Übel, dem wir zum einen pragmatisch (Wahl der bestmöglichen verfügbaren Ausgabe; Ziel, die Fehlermarge unter 2% zu halten), zum anderen unter Hinweis auf die flexible Struktur des Korpus (Austausch durch eine qualitativ hochwertigere Version ist möglich) begegnen. Durch die nahtlose Dokumentation des Korpus wird zudem die nötige Transparenz gewährleistet um auch Nachnutzern flexible Kontrolle der Daten zu ermöglichen.“ [Herrmann-Wolf, Lauer 2016: 159]
             Page Analysis and Ground Truth Elements, siehe http://www.primaresearch.org/publications/ICPR2010_Pletschacher_PAGE und https://github.com/PRImA-Research-Lab/PAGE-XML.
             Vgl. https://www.digitisation.eu/tools-resources/image-and-ground-truth-resources/.
             ISO/IEC-Standard 19757-3:2006
             Deutsches Textarchiv, DTA-Basisformat, http://www.deutschestextarchiv.de/doku/basisformat/.
             Vgl. z.B. https://rdmorganiser.github.io/
         
         
            
               Bibliographie
               
                  Herrmann-Wolf, J. Berenike / Lauer, Gerhard (2016): „Aufbau und Annotation des Kafka/Referenzkorpus“ in: Burr Elisabeth (ed): 
                        DHd 2016 : Modellierung, Vernetzung, Visualisierung : die Digital Humanities als fächerübergreifendes Forschungsparadigma : Konferenzabstracts : Universität Leipzig 7. bis 12. März 2016. [Duisburg]: nisaba 158-160.
                    
               
                  Mihm, Melanie (2016): „Weibliches Erzählen im Expressionismus? Eine Stilometrie von Mela Hartwigs Prosa“ in: Burr Elisabeth (ed): 
                        DHd 2016 : Modellierung, Vernetzung, Visualisierung : die Digital Humanities als fächerübergreifendes Forschungsparadigma : Konferenzabstracts : Universität Leipzig 7. bis 12. März 2016. [Duisburg]: nisaba 198-200.
                    
               
                  Schöch, Christof (2014): „Corneille, Molière et les autres. Stilometrische Analysen zu Autorschaft und Gattungszugehörigkeit im französischen Theater der Klassik“, in: Schöch, Christof / Schneider, Lars (eds.): 
                        Literaturwissenschaft im digitalen Medienwandel (=Philologie im Netz Beiheft 7) 130-157 http://web.fu-berlin.de/phin/beiheft7/b7t08.pdf [letzter Zugriff 25.September 2017].
                    
               
                  Schnell, Rainer / Hill, Paul B. / Esser, Elke (2011): Methoden der empirischen Sozialforschung. 9., aktualisierte Aufl. München: Oldenbourg
                    
               
                  Thesenpapier des Fachverbands „Digital Humanities im deutschsprachigen Raum“ (DHd) (2014): „Digital Humanities 2020“, vorgestellt im März 2014 auf der ersten Jahrestagung des Verbandes in Passau. http://dig-hum.de/digital-humanities-2020.
                    
            
         
      
   



      
         
            Vorhaben
            Cäsar Flaischlens „Graphische Litteratur-Tafel“ von 1890 stellt den Versuch dar, die Entwicklung der Deutschen Literatur mit ihren Einflüssen aus anderen Nationalliteraturen graphisch in der Form eines Flusses darzustellen. Gegenstand des Vortrags ist die digitale Edition und Bereitstellung des Vorwortes und der Karte sowie der entwickelte Workflow: Für die Edition wurde das graphische Karteninventar kodiert. Mithilfe computergestützter Bildanalyse können nicht-textuelle Informationen der Visualisierung erfasst und einer quantitativen Analyse zugeführt werden.
         
         
            Visualisierung von Literaturgeschichte
            In den letzten Jahren lässt sich ein Trend innerhalb der Literaturwissenschaft – u.a. der Literaturgeschichtsschreibung – ausmachen, Fragestellungen auf der Grundlage von großen Datenkorpora zu beantworten. Charakteristisch für diese Art von Literaturwissenschaft ist ein Methodenimport aus Natur- und Sozialwissenschaften, der sich nicht zuletzt in den Darstellungsformen deutlich zeigt.
            Auch wenn sich diese Zugänge gegenwärtig großer Beliebtheit erfreuen, sind Darstellungsweisen wie jene in Morettis einflussreichem Buch „Kurven, Karten, Stammbäume: Abstrakte Modelle für die Literaturgeschichte“ (Moretti 2007) keineswegs ein Phänomen der Gegenwart, denn Literaturgeschichtsschreibung bedient sich bereits seit der Antike Bildmedien und anderer – nicht rein textueller – Präsentationsformen. Darstellungen von AutorInnen, wie etwa jene auf Raffaels berühmtem Parnassfresko in den Vatikanischen Museen lassen sich aus heutiger Perspektive wie Diagramme lesen. Information zu Relevanz sowie Verbindungen einzelner AutorInnen sind hier im Bildmedium kodiert. (vgl. Hölter/Schmitz-Emans 2013, Hölter 2005)
            Das Parnassfresko ist nur eine Art, wie sich Kanonbildung, Rezeption und die Zugehörigkeit zu einer AutorInnengruppe darstellen lassen. Häufig gewählte Darstellungsformen sind (Stamm-)Baum (Lima 2014) und Fluss. Die Literaturwissenschaft greift damit Formen auf, die erst mit Fortschritten in der Buchproduktion durch die Entwicklung der Lithographie möglich geworden sind und zunächst in der Geschichtsschreibung Anwendung gefunden haben (vgl. Rosenberg/Grafton 2010).
         
         
            Cäsar Flaischlens „Graphische Litteratur-Tafel“
            Wie produktiv sich diese tradierten Denkbilder auf die Konzeptualisierung von (Literatur-)Karten auswirken können, zeigt die „Graphische Litteratur-Tafel” (1890) des deutschen Autors Cäsar Flaischlen (1864–1920). Flaischlens großformatige Karte (58x86,5 cm) visualisiert den – wie es im Untertitel heißt – „Einfluss fremder Literaturen” auf die deutsche Literatur und bedient dafür die Denkfigur geschichtlicher Prozesse als Fluss, bestehend aus einer Summe von Einflüssen. Er knüpft damit an eine Darstellungstradition von Weltgeschichte an, wie sie durch die Graphik „Strom der Zeiten“ (1804) des österreichischen Historiographen Friedrich Strass maßgeblich geprägt wurde (vgl. Rosenberg und Grafton, 2010). 
            In Cäsar Flaischlens Œuvre nimmt die aufwendig gestaltete Litteratur-Tafel eine Sonderstellung ein: Die für ihre Zeit ungewöhnliche literaturwissenschaftliche Arbeit erschien beinahe zeitgleich mit seiner Promotion 1899 und sollte Flaischlens einzige Publikation zur Literaturgeschichte bleiben. Flaischlen verließ die akademische Welt und war als Mitherausgeber und Redakteur von Literatur- und Kunstzeitschriften tätig. Heute ist der Autor hauptsächlich für seine Mundartgedichte und Erzählungen bekannt. 
            Auf der Karte wird die deutschsprachige Literatur von ihren Anfängen bis in Flaischlens Gegenwart dargestellt. Was in der Grafik zunächst als zwei sich schlängelnde Bäche der „Volks- und Kunstpoesie“ um 750 beginnt, entwickelt sich im Laufe der Jahrhunderte zu einem breiten Strom, in welchen über den gesamten (Zeit-)Verlauf Zuflüsse aus anderen (hauptsächlich) europäischen Nationalliteraturen einmünden. In der Legende der Karte führt Flaischlen folgende Einflüsse auf: Altes- und Neues Testament, Englisch, Französisch, Nordisch, Orientalisch, Klassisches Altertum, Spätlateinisch, Niederländisch, Italienisch, Spanisch, Schwedisch/Dänisch/Norwegisch, Russisch. 
            Als Vertreter positivistischer Denkrichtung unternimmt Cäsar Flaischlen also den Versuch, die Darstellung der Zeit als Fluss mit den exakten Wissenschaften zu verbinden. Die Tatsache, dass er keine Quellen für die Zusammenstellung seiner Tafel nennt, spricht dafür, dass er den gängigen Kanon abbildet. Im 8-spaltigen Vorwort zur Tafel spielt Flaischlen zwar den Zusammenhang von quantitativem Befund und Visualisierung herunter – so gibt er etwa zu bedenken, dass die Breite des Flusses „nicht mathematisch berechnet” sei, die Platzierung der Autoren folgt jedoch einem gewissen Prinzip: Die Autoren habe er am „Höhepunkt“ ihres Schaffens eingezeichnet.
            Der Informationsgehalt der „Litteraturtafel“ ist sehr hoch: Auswahl, Platzierung und Größe der beeinflussenden und beeinflussten Autoren ermöglichen die Rekonstruktion von Flaischlens Datengrundlage und lassen Rückschlüsse auf die hierfür verwendeten Quellen zu.
            So sind etwa in der Tafel Namen von Autoren, kanonischen Texten, literarischen Gruppierungen und literarischen Schulen enthalten. Ferner nutzt Cäsar Flaischlen typographische Gestaltungsmöglichkeiten wie Schriftart, Schriftgröße, Farbwahl und Unterstreichung), Symbole (Kreise in verschiedenen Größen, römische und lateinische Ziffern) und die farbliche Schraffur der (Zu-)Flüsse. Am unteren Rand der Karte ist zwar eine Legende angebracht, diese weist jedoch lediglich die Bedeutung der Schraffur aus (z.B. blau für Einflüsse aus der Englischen Literatur, rot für Einflüsse aus der Französischen Literatur, etc.). Weitere Angaben, insbesondere Erläuterungen zu verwendeten Schriftarten und -größen fehlen jedoch.
         
         
            Technische Umsetzung
            Das vorgestellte Projekt „Cäsar Flaischlens Graphische Litteratur-Tafel digital“ unternimmt den Versuch eines ‚reverse engineering‘ und erschließt dieses Dokument früher Visualisierung literaturgeschichtlicher Daten mit Methoden der Digital Humanities. 
            Dazu wurden die Koordinaten von angeführten Personen, Texten, literarischen Strömungen und Schulen unter Verwendung des GIMP ImageMap-Editors ermittelt und entsprechend den in den TEI P5 Guidelines (TEI Consortium 2017) definierten Transkriptionskonventionen („Advanced Uses of surface and zone“) erfasst, um die spatiale Dimension (Kodierung von Information über räumliche Anordnung) ebenfalls zugänglich machen zu können. Die Personen- und Werkreferenzen wurden mit den entsprechenden Normdaten (GND, VIAF, wikidata) verknüpft. Das kartographische Inventar der Karte wurde unter Rückgriff auf CSS innerhalb von @style erfasst. Dies ermöglicht nun, neben den textuellen Informationen zusätzlich die typographische Gestaltung des Textes auszuwerten.
            Die TEI-Daten werden über ein github-repository bereitgestellt und als Webseite aufbereitet. Das Interface fügt die drei separaten Abschnitte von Flaischlens Karte zusammen. In einer Print- Ausgabe wäre der zusammengefügte Fluss beinahe drei Meter lang und somit nur schwer lesbar. Die Web-Version erlaubt über das Scroll-Interface jedoch einen komfortablen Zugang. Die kodierten Informationen sind über Register erschlossen. Ein Prototyp der Edition ist unter http://litteratur-tafel.weltliteratur.net zugänglich.
         
         
            Computergestützte Analyse des kartographischen Inventars (Zwischenergebnisse)
            Durch eine Analyse des kartographischen Inventars lässt sich das Zeichensystem rekonstruieren, das zudem gängigen kartographischen Konventionen folgt. Beeinflusste und beeinflussende Autoren werden durch Unterstreichung unterschieden. Die Farbe der Unterstreichung entspricht der farblichen Kennzeichnung der Zuflüsse und ordnet somit die Autoren einer der beeinflussenden Nationalliteraturen zu. Durch die Verwendung unterschiedlicher Schriftgrößen wird die „Relevanz“ eines Autors bzw. Textes für die deutsche Literatur zum Ausdruck gebracht. Besonders wichtige deutsche Autoren sind in Konturschrift ausgeführt, vergleichbar der Beschriftung von Städten in Abhängigkeit von ihrer Einwohnerzahl (vgl. Kohlstock 2004: 100). EIne Analyse der Typografie erlaubt es somit, Flaischlens „Verständnis“ der deutschen Literatur zu rekonstruieren.
            Um die farblich kodierten Informationen der Karte ebenfalls berücksichtigen zu können, wurde ein Zugang über Verfahren aus dem Bereich computergestützter Bildanalyse gewählt. Mittels der Open Source Computer Vision Library (openCV) wurden die Pixel nach Farbbereichen klassifiziert und quantitativ ausgewertet, um die „Einflüsse“ zu messen und ihre Veränderungen im Laufe der Zeit visualisieren zu können. Abbildung 1 zeigt jene Pixel, der Kategorie „rot“ und somit dem französischen Einfluss zugeordnet wurden. 
            
               
               Abbildung 1: Original und klassifizierte Pixel
            
            Für Abbildung 2 wurde wurden die Pixel nach Jahren gruppiert und als Liniendiagramm visualisiert. Deutlich erkennbar sind Spitzen um 1620, 1665, 1715 und 1800, die sich verschiedenen Epochen der französischen Literaturgeschichte zuordnen lassen: Dem Französischen Klassizismus und der Aufklärung. Der Peak in den 1860er Jahren ist mit dem französischen Naturalismus verbunden.
            
               
               Abbildung 2: Französischer Einfluss auf Basis der Pixelklassifikation
            
         
         
            Ausblick
            Das Projekt erschließt nicht nur einen inspirierenden Vorläufer gegenwärtiger Versuche von Visualisierung literaturgeschichtlicher Daten mithilfe von ‘Graphen, Karten und Stammbäumen’, sondern erprobt auch auf einer methodologischen Ebene Möglichkeiten und Workflows zur Erschließung und Kodierung älterer graphischer Darstellungen von (Literatur-)geschichte. Darüber hinaus liefert es Impulse für die Arbeit mit der TEI für das Encoding von Bildmaterial, indem die Eignung von primär zur Kodierung von Manuskripten eingesetzten Tags für Grafiken und Diagramme überprüft werden. 
         
      
      
         
            
               Bibliographie
               
                  Flaischlen, Cäsar (1890): 
                        Graphische Litteratur-Tafel. Die deutsche Litteratur und der Einfluß fremder Litteraturen auf ihren Verlauf vom Beginn der schriftlichen Ueberlieferung an bis heute in graphischer Darstellung. Berlin: Behr's Verlag.
                    
               
                  Hölter, Achim (2005): „Überlegungen zu Raffaels Parnass-Fresko als Kanonbild“ in: Heimböckel, Dieter / Werlein, Uwe (eds.): 
                        Der Bildhunger der Literatur. Festschrift für Gunter E. Grimm. Würzburg 51-68.
                    
               
                  Hölter, Achim / Schmitz-Emans, Monika (eds.) (2013): 
                        Literaturgeschichte und Bildmedien. Heidelberg: Synchron.
                    
               
                  Kohlstock, Manuel (2004): 
                        Kartographie. Eine Einführung. Stuttgart: UTB.
                    
               
                  Lima, Manuel (2014): 
                        The Book of Trees. Visualizing Branches of Knowledge. New York: Princeton Architectural Press.
                    
               
                  Moretti, Franco (2007): 
                        Graphs, Maps, Trees. Abstract Models for Literary History. London, New York: Verso.
                    
               
                  openCV: 
                        Open Source Computer Vision Library. http://opencv.org [letzter Zugriff 24. September 2017].
                    
               
                  Rosenberg, Daniel / Grafton, Anthony (2010): 
                        Cartographies of Time. New York: Princeton Architectural Press.
                    
               
                  Strass, Friedrich (1803): 
                        Der Strom Der Zeiten. http://www.davidrumsey.com/luna/servlet/detail/RUMSEY~8~1~281767~90054624 [letzter Zugriff 24. September 2017].
                    
               
                  TEI Consortium (2017). 
                        TEI P5 – Guidelines for Electronic Text Encoding and Interchange. Version 3.2.0. http://www.tei-c.org/release/doc/tei-p5-doc/en/html/index.html [letzter Zugriff 25. September 2017].
                    
            
         
      
   



      
         In terms of longevity and collation of textual data in the humanities, digital data, notwithstanding its potential, still falls short the qualities of the traditionally printed book.
         To streamline the diverse and idiosyncratic Digital Editions of the time and to establish a cross- and re-usable, durable digital archive of textual cultural artifacts, in 1988 the Text Encoding Initiative (TEI) was established with the goal to present a commonly shared standard for the transcription of literary, scientific and other forms of text.
         As data model, the extensible markup language XML was chosen to assure longevity and exchangeability of the data. However, it turns out that XML, and with it, the data model of the hierarchically ordered tree are questionable choices for the recording of complex texts – as they are commonly found in the humanities – by potentially rendering the data ambiguous on semantic level.
         The abstract idea behind the commonly shared tag set for the description of textual data is reflected in the 
                TEI abstract model (TEI Consortium 2016b) which uses XML as a serialisation format – but to which it is not bound:
            
         The rules and recommendations made in these Guidelines are expressed in terms of what is currently the most widely-used markup language for digital resources of all kinds: the Extensible Markup Language (XML) […]. However, the TEI encoding scheme itself does not depend on this language […], and may in future years be re-expressed in other ways as the field of markup develops and matures.
         In the following, fundamental limitations of the tree data model are highlighted in spotlight fashion and contrasted with a graph based model for the sustainable recording and long-term archiving of complex textual data.
         
            Limitations of the tree model
            Paradoxically, Digital Editions as well as digital archives, tools, platforms and data repositories are not as interoperable in practice as one would theoretically expect from standardised sources. To be able to cross- or re-use data or tools between projects, in practice, serious refactoring and rededication is necessary – e.g. existing web platforms cannot readily be re-used by another project, notwithstanding the fact that the data repositories are fully validating, validating TEI-P5 sources. How is this possible?
            As will be shown, this paradoxical situation of factually unattainable interoperability of editions and tools are a direct consequence of the choice of data model.
            The decision towards XML and the tree data model is based on the OHCO assumption of text as an Ordered Hierarchy of Content Objects (DeRose et al. (1990); revised in Renear, Mylonas, and Durand (1993)). Contrasting the original goals (TEI Consortium 2016c) of interoperable long-term archivable data repositories with the status quo, this decision towards XML as the serialisation format needs to be critically questioned – particularly since the TEI Guidelines themselves very early on make clear that the assumption of data model behind XML is an improper simplification (TEI Consortium 2016a):
            Surprisingly perhaps, this grossly simplified view of what text is […] turns out to be very effective for a large number of purposes. It is not, however, adequate for the full complexity of real textual structures, for which more complex mechanisms need to be employed.
            Already two most basic constellations can lead to a necessary departure from the tree paradigm which could be described as ‘Complex XML’.
            These situations are commonly resolved by using workarounds (TEI Consortium 2016d). Although 
                    syntactically permissible on the level of XML markup, these workarounds establish structures beyond the data model of the tree and can lead to misrepresentation of the data on 
                    semantic, modelling level, seriously harming effective re-use and long-term archiving.
                
            
               Data as well as tools inevitably become idiosyncratic, i.e. they irrevocably need to be handled on individual, project-specific basis; projects increasingly develop ‘private dialects’ and couple philologists and data scientists for actually accessing the data; data and tools are inaccessible to cross- and re-use between projects; finally, the possibility of a common digital archive is lost beyond recall.
            
            
               Complex textual structures demand additional annotation to help and guide downstream tooling to not misrepresent the data. The transcription – in spite of valid, conforming data w.r.t. to the XML Schema – cannot automatically, i.e. without human intervention, be unambiguously resolved into its textual variants. 
               The necessary supplementary annotation to one-unambiguously describe and model the source sets in motion a vicious circle of exponentially growing complexity in the data. Project-specific, idiosyncratic tools become necessary and must match this complexity. Moreover, such repositories typically suffer from overtagging (Hanrahan 2015), or, in the worst case need to be abandoned entirely (Schmidt et al. 2006).
            
            
               Any further annotation or commentary only ever increases the complexity: any further annotation must match the existing complexity of the amended tree structure to accordingly be integrated; data and tools suffer from a ‘Heisenberg-Effect’ in that any further, more precise description of the source makes the data only ever more imprecise.
            
         
         
            Complex XML
            In contrast to a simple edition, i.e. one of linear text without any further annotation, the need for ‘Complex XML’, on most fundamental, level arises through: 
            
               the edition of a non-linear text
               the edition of a linear text, open for annotation
            
            In essence, anything that is beyond linear text free of annotation cannot adequately be represented by a mono-hierarchical tree model and will need “more complex mechanisms” (TEI Consortium 2016a).
            
               Complex XML through non-linear text
               Non-linear text results from editorial operations such as insertions, deletions, substitutions. For instance, recording the genealogical writing process of two undecided variants within the same sentence, yields four different, non-linear potential readings.
               
                  
                     
                  
                  These four different readings derived from mechanical re-combination potentially are not intended and to be reduced to specific readings only.
               
               
                  
                     
                  
                  Constraining these combinatorial permutations cannot be done in general ways within the mono-hierarchical tree data model. The tree model exposes a general limitation – even without the prevalence of overlapping structures.
               
               
                  
                     
                  
                  While interconnecting nodes across the tree’s boundaries by (ab-)using attributes is syntactically possible it nevertheless makes the data idiosyncratic on semantic level, i.e. project-specific rules are introduced and must individually be followed when working with the data.
               
               
                  
                     
                  
                  These interconnections to constrain the combinatorics to specific readings cannot formally be made part of the tree structure itself. To build a tree, any node in the tree must have exactly one parent. A different data model and data structure is necessary to model more than one parent for one node, namely the data model of the graph.
               
               
                  
                     
                  
               
            
            
               Complex XML through meta-data
               Complex XML can also result from linear text, open for annotation. The following schematic example shows a linear text with overlapping annotation:
               
                  
                     
                  
               
               Corresponding serialisation using XML and the segmentation method (TEI Consortium 2016d):
               
                  
                     
                  
               
               The necessary interconnection and recombination of fragmented nodes cannot be modelled within the tree structure in general ways:
               
                  
                     
                  
               
               Another representation shows how one node in the tree is made the child of two parents:
               
                  
                     
                  
               
            
         
         
            The relationship between graphs and trees
            Trees and graphs are closely related: An ordered tree is a special form of graph with the properties of 
                    a) it is a directed graph without cycles, 
                    b) has one designated root node and 
                    c) any node has exactly one parent node.
                
            As was shown in the previous basic examples, there is strictly no possibility to interconnect nodes of the tree across branches of the tree. By trying to associate two parents to one node, the tree paradigm is effectively abandoned, and results in a permanent need for case-specific handling to resolve potential ambiguities in the data.
         
         
            Conclusion
            Digital Editions wanting to model more than just simple structures can – notwithstanding the syntactical possibilities of XML – not be represented in interoperable ways within the paradigm of the tree data model, making longevity and uniformly re-usable digital archives impossible.
            Alternative, graph-theoretic attempts to solve this problem have been suggested and could implement the 
                    TEI abstract model through an adequate data structure (Huitfeldt 1994; Barnard et al. 1995; Sperberg-McQueen and Huitfeldt 2000; Huitfeldt and Sperberg-McQueen 2001; Durusau and O'Donnell 2002; Tennison and Piez 2002; Dipper 2005; Dekhtyar and Iacob 2005; Banski and Przepiórkowski 2009; Di Iorio, Peroni, and Vitali 2010; Di Iorio, Peroni, and Vitali 2011; Schmidt and Colomb 2009; Schmidt 2014; Götze and Dipper 2006; Peroni, Vitali, and Di Iorio 2009; Witt 2007; Kuczera 2016).
                
            Yet, the question of an adequate serialisation and exchange format to any such data structure remains open. To be able to give guarantees of long term storage and archiving, any such serialisation format must be able to one-unambiguously represent the source as well as data structure. Ideally, any such serialisation format should be both machine readable as well as human intelligible and independent of existing computer hardware and software.
            Previous graph-based approaches for the recording of complex textual data either did not catch on or have been abandoned for reasons of complexity in implementation or usage.
            Because of the choice of data model, current repositories are idiosyncratic and tools and data must be handled on individual basis. In order to be able to build general digital archives fully interoperable data repositories are necessary. Interoperability is closely connected to the choice of data model. The TEI abstract model should be implemented as a graph structure, however, the graph structure is in need of a suitable exchange and serialisation format.
            The commonly shared property between former graph-based approaches is the use of embedded markup. It is conjectured that future research on suitable serialisation formats for graph-based approaches should re-evaluate standoff based markup for the durable recording of Digital Editions.
         
      
      
         
            
               Bibliography
               
                  Banski, Piotr, and Adam Przepiórkowski. 2009. “Stand-Off TEI Annotation: The Case of the National Corpus of P
                        olish.” In Proceedings of the Third Linguistic Annotation Workshop, 64–67. Association for Computational Linguistics.
                    
               
                  Barnard, David T, Lou Burnard, Jean-Pierre Gaspart, Lynne A Price, CM Sperberg-McQueen, and Giovanni Battista Varile. 1995. “Hierarchical Encoding of Text: Technical Problems and SGML Solutions.” In 
                        Text Encoding Initiative, 211–31. Springer.
                    
               
                  Dekhtyar, Alex, and Ionut E Iacob. 2005. “A Framework for Management of Concurrent XML Markup.” 
                        Data & Knowledge Engineering 52 (2). Elsevier:185–208.
                    
               
                  DeRose, Steven J., David G. Durand, Elli Mylonas, and Allen H. Renear. 1990. “What is text, really.” 
                        Journal of Computing in Higher Education 1 (2). Springer Nature:3–26. 
                        .
                    
               
                  Di Iorio, Angelo, Silvio Peroni, and Fabio Vitali. 2010. “Handling markup overlaps using OWL.” In 
                        Knowledge Engineering and Management by the Masses, 391–400. Springer.
                    
               
                  Di Iorio, Angelo, Silvio Peroni, and Fabio Vitali. 2011. “A Semantic Web Approach to Everyday Overlapping Markup.” 
                        Journal of the American Society for Information Science and Technology 62 (9). Wiley Online Library:1696–1716.
                    
               
                  Dipper, Stefanie. 2005. “XML-Based Stand-Off Representation and Exploitation of Multi-Level Linguistic Annotation.” In 
                        Berliner XML Tage, 39–50.
                    
               
                  Durusau, Patrick, and M Brook O'Donnell. 2002. “Concurrent Markup for XML Documents.” In 
                        Proc. XML Europe.
                    
               
                  Götze, Michael, and Stefanie Dipper. 2006. “ANNIS: Complex Multilevel Annotations in a Linguistic Database.” In 
                        Proceedings of the 5
                  th
                   Workshop on NLP and XML: Multi-Dimensional Markup in Natural Language Processing, 61–64. Association for Computational Linguistics.
                    
               
                  Hanrahan, Elise. 2015. “Over-Tagging with XML in Digital Scholarly Editions.” In 
                        DHd2015 Conference – von Daten Zu Erkenntnissen. Book of Abstracts., edited by Various, 162–65. Graz, Austria.
                    
               
                  Huitfeldt, Claus. 1994. “Multi-Dimensional Texts in a One-Dimensional Medium.” 
                        Computers and the Humanities 28 (4-5). Springer:235–41.
                    
               
                  Huitfeldt, Claus, and CM Sperberg-McQueen. 2001. “TexMECS: An Experimental Markup Meta-Language for Complex Documents.” 
                        URL Http://Www. Hit. Uib. No/Claus/Mlcd/Papers/Texmecs. Html.
                    
               
                  Kuczera, Andreas. 2016. “Digital Editions Beyond Xml – Graph-Based Digital Editions.” In 
                        Proceedings of the 3
                  rd
                   Histoinformatics Workshop on Computational History (Histoinformatics 2016), edited by Johannes Preiser-Kappeller Marten Düring Adam Jatowt.
                    
               
                  Peroni, Silvio, Fabio Vitali, and Angelo Di Iorio. 2009. “Towards markup support for full GODDAGs and beyond: the EARMARK approach.” 
                        .
                    
               
                  Renear, Allen H, Elli Mylonas, and David Durand. 1993. “Refining Our Notion of What Text Really Is: The Problem of Overlapping Hierarchies.” Oxford University Press.
               Schmidt, Desmond. 2014. “Towards an Interoperable Digital Scholarly Edition.” 
                        Journal of the Text Encoding Initiative, no. 7. Text Encoding Initiative Consortium.
                    
               
                  Schmidt, Desmond, and Robert Colomb. 2009. “A Data Structure for Representing Multi-Version Texts Online.” 
                        International Journal of Human-Computer Studies 67 (6). Elsevier:497–514.
                    
               
                  Schmidt, Thomas, Christian Chiarcos, Timm Lehmberg, Georg Rehm, Andreas Witt, and Erhard Hinrichs. 2006. “Avoiding Data Graveyards: From Heterogeneous Data Collected in Multiple Research Projects to Sustainable Linguistic Resources.” In 
                        6
                  th
                   E-Meld Workshop, Ypsilanti.
                    
               
                  Sperberg-McQueen, C Michael, and Claus Huitfeldt. 2000. “GODDAG: A Data Structure for Overlapping Hierarchies.” In 
                        Digital Documents: Systems and Principles, 139–60. Springer.
                    
               
                  TEI Consortium. 2016a. “A Gentle Introduction to XML” In 
                        TEI P5: Guidelines for Electronic Text Encoding and Interchange, by TEI Consortium, Version 3.2.0. Last updated on 10th July 2017, revision 0fcf651. 
                        .
                    
               ———. 2016b. “About These Guidelines.” In 
                        TEI P5: Guidelines for Electronic Text Encoding and Interchange, by TEI Consortium, Version 3.2.0. Last updated on 10th July 2017, revision 0fcf651. 
                        .
                    
               ———. 2016c. “Design Principles.” In 
                        TEI P5: Guidelines for Electronic Text Encoding and Interchange, by TEI Consortium, Version 3.2.0. Last updated on 10th July 2017, revision 0fcf651. 
                        .
                    
               ———. 2016d. “Non-Hierarchical Structures.” In 
                        TEI P5: Guidelines for Electronic Text Encoding and Interchange, by TEI Consortium, Version 3.2.0. Last updated on 10th July 2017, revision 0fcf651. 
                        .
                    
               
                  Tennison, Jeni, and Wendell Piez. 2002. “The Layered Markup and Annotation Language (LMNL).” In 
                        Extreme Markup Languages.
                    
               
                  Witt, Andreas. 2007. “Guideline: Multiple Hierarchies.” In 
                        Dagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum für Informatik.
                    
            
         
      
   



      
         Die vom Institut für die Geschichte der deutschen Juden (IGdJ) erstellte, seit Juli 2015 von der DFG geförderte und seit September 2016 frei zugängliche zweisprachige (deutsch/englisch) Online-Quellenedition „Hamburger Schlüsseldokumente zur deutsch-jüdischen Geschichte“ (
                http://juedische-geschichte-online.net/) wirft am Beispiel von derzeit etwa 75 ausgewählten Quellen thematische Schlaglichter auf zentrale Aspekte der deutsch-jüdischen Geschichte Hamburgs.
            
         Mit der Auswahl und Digitalisierung von Text-, Bild-, Ton- und Sachquellen, die exemplarisch Einblick in historische Zusammenhänge und Ereignisse von der frühen Neuzeit bis in die Gegenwart bieten – den sog. Schlüsseldokumenten – führt sie das aufgrund von Vertreibung und Migration verstreute jüdische Erbe der Stadt digital wieder zusammen und trägt zu seiner langfristigen Sicherung für zukünftige Generationen bei. Ziel ist dabei, das Digitale nicht nur als ein weiteres Medium zu begreifen, sondern als einen Werkzeugkasten, mit dem das Material auf unterschiedlichen Ebenen bearbeitet werden kann. Zum einen führt die Digitalisierung selbst zur besseren Zugänglichkeit und nachhaltigen Sicherung, zum anderen erlauben die technische Auszeichnung nach TEI und Verknüpfung der bereitgestellten Materialien die Auswertung bislang nicht systematisch erfasster Informationen. Und schließlich bietet eine digitale Publikationsumgebung die Möglichkeit, neben Textquellen Bild-, Ton- und Videodokumente (sowie zukünftig 3D-Repräsentationen von Objekten) einzubinden und damit in den Geschichtswissenschaften bislang eher stiefmütterlich behandelte Quellengattungen verstärkt in den Blick zu nehmen.
         Von diesen Überlegungen ausgehend, bilden die digitalisierten und technisch aufbereiteten Quellen konsequenterweise den Dreh- und Angelpunkt der Edition, die zugleich so strukturiert ist, dass sie hypertextuell angelegt und modular aufgebaut ist. Dass die Auseinandersetzung über konkrete Deutungen und Einordnungen am Beispiel konkreter Dokumente erfolgt und diese zugleich neuartig aufbereitet präsentiert werden, erlaubt ihre Fruchtbarmachung für neue Fragestellungen und kann Impulse für die deutsch-jüdische Geschichte geben. Alle Quellen werden als Transkript und digitales Faksimile bereitgestellt. Sowohl Quellen als auch Interpretationstexte werden nach TEI P5 gemäß dem Basisformat des Deutschen Textarchivs ausgezeichnet (Haaf et al 2015). Dies erlaubt neben der Auszeichnung der grundlegende Textstruktur die Verknüpfung mit Normdaten (Personen, Institutionen, Orte) sowie eine interne Verlinkungen mit weiteren Quellen, um so den Texten eine zweite Informationsebene einzuschreiben. Zugleich werden alle bereitgestellten Materialien mit einer DOI versehen, die die langfristige Referenzierbarkeit sicherstellt und die Bearbeitungshistorie transparent werden lässt.
         Da die Digitalisierung und Online-Stellung von Quellen jedoch auch immer ein Herauslösen aus dem Überlieferungszusammenhang bedeutet und damit mit einer Entkontextualisierung und Entmaterialiserung verbunden ist, wird bei dieser Edition Wert darauf gelegt, neben der Bereitstellung der digitalisierten Quelle, diese durch begleitende Interpretations- und Hintergrundtexte verstärkt in ihre historischen Kontexte einzubetten und zusätzliche Informationen zur Überlieferung, Rezeptionsgeschichte und zu wissenschaftlichen Kontroversen bereitzustellen.
         Indem für die Digitalisierung, Textauszeichnung und Metadatenerschließung auf existierende Standardformate digitaler Editionen und der Langfristarchivierung wie MODS (Katalogdaten), METS (Digitalisate), TEI (Textauszeichnung der Transkriptionen und Übersetzungen), DOI (persistente Adressierung) sowie GND-Beacon-Dateien zurückgegriffen wird und bestehende Werkzeuge (Oxygen XML Editor) und technische Infrastrukturen (MyCoRe, Zotero) nachgenutzt werden, zugleich aber die Nutzerfreundlichkeit und Bedienbarkeit im Vordergrund steht, wurde eine innovative digitale Quellenedition zur jüdischen Geschichte Hamburgs geschaffen, die das Digitale als eine Möglichkeit ansieht, analoge Quelle neuartig zu präsentieren und mit weiteren (Informations-)schichten anzureichern und damit neue Impulse für die Forschung zu geben. Der Quellcode der Webanwendung ist für andere Projekte frei nachnutzbar (https://github.com/burki/jewish-history-online).
         Neben der Auswahl und Aufbereitung des ausgewählten heterogenen Quellenmaterials zeichnet sich das Angebot durch umfassende Recherchemöglichkeiten (Karte, Zeitstrahl, Themen) sowie eine attraktive Präsentationsform aus. Auf diese Weise werden digitale Edierungstechniken für Quellen zur jüdischen Geschichte erprobt und einer breiten Öffentlichkeit vorgestellt. Die systematische Auszeichnung von Personen, Organisationen und Orte mit Normdaten ermöglicht die bidirektionale Verknüpfung der Edition mit externen Angeboten. So können ergänzende Informationen aus Linked Data Services wie denen der DNB und von Getty automatisiert ergänzt werden. Umgekehrt ermöglicht die Generierung von eigenen GND-Beacon-Listen externen Anbietern eine einfache Verknüpfung ihrer Angebote mit den entsprechenden Inhalten im Quellenportal.
         Unser Poster zeigt die zentralen Eigenschaften der Online-Edition und hilft, den konzeptionellen Rahmen sowie die technische Umsetzung zu verstehen. Es illustriert die Verknüpfung zwischen TEI-Kodierung der Dokumente sowie ihrer Präsentation und Navigation. Es soll damit den konzeptionellen Grundgedanken des Projektes veranschaulichen, das Digitale mit seinen Möglichkeiten ernst zu nehmen, jedoch nicht in Konkurrenz, sondern in Ergänzung zum Analogen.
      
      
         
            
               Bibliographie
               
                  Haaf, Susanne / Geyken, Alexander / Wiegand, Frank (2015): "The DTA 'Base Format': A TEI Subset for the Compilation of a Large Reference Corpus of Printed Text from Multiple Sources", in: Journal of the Text Encoding Initiative 8, December 2014 - December 2015, http://journals.openedition.org/jtei/1114 [letzter Zugriff 10. Januar 2018].
                    
            
         
      
   



      
         
            Einleitung
            Schon länger wird die Frage nach der Lehre der Digital Humanities gestellt (cf. u. a. Hirsch (2012), Gold (2012), Warwick / Terras / Nyhan (2012)). Zudem sind Initiativen wie die EU geförderte 
                    #dariahTeach entstanden, die Materialien für die Lehre von Digital Humanities entwickelt und online zur Verfügung stellt. 2017 haben dann Fotis Jannidis, Hubertus Kohle und Malte Rehbein ein umfangreiches deutschsprachiges Lehrbuch zu 
                    Digital Humanities vorgelegt. In keiner dieser Publikationen wird aber, soweit wir sehen können, der Frage nachgegangen, wie der Auftrag, den die neue Epistemologie 
                    Digital Humanities als den ihren erkennt, nämlich ein umfassenderes und kritischeres Wissen von Artefakten sowie deren Relationen untereinander mittels der Nutzung von computationellen Methoden zu schaffen, in der Lehre umgesetzt werden kann. Einen Versuch in diese Richtung unternehmen wir in einem romanistischen Master-Modul, wo frühe gedruckte Grammatiken und Sprachtraktate zunächst als „Kinder des Buchdrucks“ eingeordnet und dann mittels Transkription und Auszeichnung für Forschungen im digitalen Zeitalter verfügbar gemacht werden. 
                
         
         
            Grundlagen des Versuchs
            Dem Versuch zugrunde liegt ein Projekt, das es sich zum Ziel setzt, die kulturellen, politischen, gesellschaftlichen, ideologischen, genderbedingten etc. Vorstellungen, Ideen und Ziele, die sich hinter sprachbetrachtenden, sprachbeschreibenden und sprachnormierenden Werken, wie Grammatiken und Sprachtraktate, der frühen Gutenberg-Ära verbergen, vergleichend und in ihren Zusammenhängen über die einzelnen romanischen Kulturen und Sprachen hinweg zu untersuchen. Um diese Werke systematisch erforschbar zu machen, bedarf es allerdings zunächst einmal ihrer Digitalisierung und Erschließung. 
            Bei den Texten, die bisher berücksichtigt wurden, handelt es sich um:
            
               Francesco Fortunio (1516): 
                        Regole grammaticali della volgar lingua. Ancona: Bernardin Vercellese
                    
               Louis Meigret (1550): 
                        Le trętté de la GRAMMĘRE FRANCOĘZE. Paris: Chrestien Wechel
                    
               Speron Sperone (1542): “Dialogo delle lingue”, in: Speron Sperone: 
                        I dialogi di Messer Speron Sperone. Vinegia: Aldus 106-131
                    
               Joachim Du Bellay (1549): 
                        Deffence, et illustration de la langue francoyse. Paris: Arnoul l’Angelier.
                    
            
            Als „Kinder des frühen Buchdrucks“ weisen diese Texte nicht nur eine Vielzahl von heute nicht mehr gebrauchten Lettern, von Abkürzungen und Variationen auf, sondern sie sind zugleich auch Ausdruck der durch den Buchdruck maßgeblich geförderten Hinwendung zur eigenen romanischen Volkssprache, zur eigenen Nation, zur Abgrenzung gegenüber anderen bzw. zum Vergleich oder gar zur Konkurrenz mit anderen. Zudem wird hier ein Modell der jeweiligen romanischen Sprache entwickelt und eine (soziale) sprachliche Norm fixiert, an der gerade auch die Drucker (nicht zuletzt aus ökonomischen Gründen) ein großes Interesse haben (cf. z. B. Tory (1529)). Darüber hinaus legen sie auch Zeugnis ab von der Entstehung eines Marktes, eines Lesepublikums und der Entwicklung von Layout- und Strukturierungsverfahren, die das Lesen und Verstehen von Texten fördern.
         
         
            Umsetzung des Versuchs
            Der Versuch wird in einem grundständigen und aus zwei Seminaren bestehenden Master-Modul zur französischen und italienischen Sprachwissenschaft unternommen. In diesem Modul sollen sich die Studierenden mit der Geschichte der Sprachbetrachtung und Normbildung bzw. Normierung und den dabei eingesetzten Werkzeugen auseinandersetzen. 2015 habe ich diesem Modul das oben genannte Projekt unterlegt und so konzipiert, dass die Studierenden selbst dazu einen Beitrag leisten können. Die beiden Seminare werden seither dazu genutzt, den Studierenden zu ermöglichen, alle für das Projekt benötigten Kenntnisse und Fertigkeiten zu erwerben und dabei auch ein Verständnis für die 
                    Digital Humanities, ihre Ziele und Methoden zu entwickeln.
                
            Umfassendes und kritisches Wissen von diesen „Kindern des Buchdrucks“ und ihrer Bedeutung für die romanischen Sprachen sowie ein Verständnis für die Implikationen von Medienrevolutionen allgemein und der Gutenberg-Ära sowie der digitalen Revolution im Besonderen erwerben die Studierenden in den beiden Seminaren durch das gemeinsame Aufarbeiten von Medientheorien (cf. Kloock / Spahr (1997)), Darstellungen zum Buchdruck und seinen Folgen (cf. Eisenstein (1997), Giesecke (
                    42006)) und Abhandlungen zur Grammatikographie der romanischen Sprachen und zu romanische Sprachen fokusierenden Sprachtraktaten (cf. u. a. Bierbach / Pellat (2003), Lüdtke (2001), Lubello (2003), Poggi Salani (1988), Swiggers (1990)). Die dabei zum Einsatz kommenden Technologien (Etherpad, Wiki, Datenbanken, Stylesheets bzw. Formatvorlagen) fordern sie zudem zu einem Umdenken bezüglich der Wissensproduktion im digitalen Zeitalter heraus (cf. z. B. Schöch (2017)), lange bevor sie sich – gestützt durch ein Tutorium - der Modellierung der zu digitalisierenden Texte, ihrer Transkription und Auszeichnung zuwenden. Bei letzterer spielen dann die Dokumentation des 
                    Deutschen Textarchiv (cf. DTA 2016), die Guidelines der 
                    Text Encoding Initiative (cf. TEI 2015), der 
                    Unicode Standard (cf. Uni 1991–2017) und die 
                    Medieval Unicode Font Initiative (cf. Haugen 2011) sowie der Oxygen XML-Editor eine zentrale Rolle.
                
         
         
            Poster
            Das komplexe Zusammenspiel von verschiedenen Wissensdomänen und einer Vielzahl von Technologien sowie die intendierten konzeptuellen Änderungen wollen wir in unserem Poster visualisieren. Dabei wollen wir nicht nur eine kritische Reflektion über dieses Vorgehen einbringen, sondern auch Ergebnisse, die sich in den Modulabschlussarbeiten der Studierenden zeigen, werten.
         
      
      
         
            
               Bibliographie
               
                  Albrecht, Jörn (2001): „Sprachbewertung / Évaluation de la langue“, in: Holtus, Günter / Metzeltin, Michael / Schmitt, Christian (eds.): 
                        Lexikon der romanistischen Linguistik (LRL). Band I / 2: 
                        Methodologie (Sprache in der Gesellschaft / Sprache und Klassifikation / Datensammlung und -verarbeitung) / Méthodologie (Langue et société / Langue et classification / Collection et traitement des données). Tübingen: Niemeyer 526-540.
                    
               
                  Bierbach, Mechtild / Pellat, Jean-Christophe (2003): "Histoire de la réflexion sur les langues romanes: le français / Geschichte der Reflexion über die romanischen Sprachen: FranzöRomanische Sprachgeschichte / Histoire linguistique de la Romania. Ein internationales Handbuch zur Geschichte der romanischen Sprachen / Manuel international d'histoire linguistique de la Romania 1 (= Handbücher zur Sprach- und Kommunikationswissenschaft 23). Berlin / New York: De Gruyter 226-229. 
                    
               
                  Du Bellay, Joachim (1549): 
                        Deffence, et illustration de la langue francoyse. Paris: Arnoul l’Angelier (Digitalisat: PDF, BnF Gallica).
                    
               
                  DTA (12.01.2016): „Dokumentation“, in: 
                        Deutsches Textarchiv  [25.09.2017].
                    
               
                  Eisenstein, Elisabeth I. (1997): 
                        Die Druckerpresse. Kulturrevolution im frühen modernen Europa. Wien / New York: Springer. 
                    
               
                  Fortunio, Francesco (1516): 
                        Regole grammaticali della volgar lingua. Ancona: Bernardin Vercellese / Bernardino Guerralda (Digitalisat: TIFF, Universitätsbibliothek Eichstätt-Ingolstadt 03.06.2015).
                    
               
                  Giesecke, Michael (
                        42006): 
                        Der Buchdruck in der frühen Neuzeit. Eine historische Fallstudie über die Durchsetzung neuer Informations- und Kommunikationstechnologien. Frankfurt am Main: Suhrkamp. 
                    
               
                  Gold, Matthew K. (ed.) (2012): 
                        Debates in the Digital Humanities. Minneapolis / London: University of Minnesota Press.
                    
               
                  Haugen, Odd Einar (ed.) (2011): 
                        Medieval Unicode Font Initative  [25.09.2017].
                    
               
                  Hirsch, Brett D. (ed.) (2012): 
                        Digital Humanities Pedagogy. Practices, Principles and Politics. Cambridge: Open Book Publishers.
                    
               
                  Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): 
                        Digital Humanities. Eine Einführung . Stuttgart: J. B. Metzler.
                    
               
                  Kloock, Daniela / Spahr, Angela (1997): 
                        Medientheorien. Eine Einführung (= UTB 1986). München: Wilhelm Fink.
                    
               
                  Lubello, Sergio (2003): "Storia della riflessione sulle lingue romanze: italiano e sardo / Geschichte der Reflexion über die romanischen Sprachen: Italienisch und Sardisch", in: Ernst, Gerhard / Gleßgen, Martin-Dietrich / Schmitt, Christian / Schweickard, Wolfgang (eds.): 
                        Romanische Sprachgeschichte / Histoire linguistique de la Romania. Ein internationales Handbuch zur Geschichte der romanischen Sprachen / Manuel international d'histoire linguistique de la Romania. Berlin: De Gruyter 208-225.
                    
               
                  Lüdtke, Jens (2001): "Romanische Philologie von Dante bis Raynouard / La philologie romane de Dante à Raynouard", in: Holtus, Günter / Metzeltin, Michael / Schmitt, Christian (eds.): 
                        Lexikon der romanistischen Linguistik (LRL). Band I / 1: Geschichte des Faches Romanistik. Methodologie (Das Sprachsystem) / Histoire de la philologie romane. Méthodologie (Langue et système). Tübingen: Niemeyer 1-35. 
                    
               
                  Meigret, Louis (1550): 
                        Le tretté de la GRAMMeRE FRANCOEZE. Paris: Chrestien Wechel (Digitalisat: PDF, Bayrische Staatsbibliothek, Münchner Digitalisierungszentrum Digitale Bibliothek 05.09.2015, JPG 10.11.2017).
                    
               
                  Oxygen XML Editor (2002-2017)  [25.09.2017].
                    
               
                  Poggi Salani, Teresa (1988): "Italienisch: Grammatikographie / Storia delle grammatiche", in: Holtus, Günter / Metzeltin, Michael / Schmitt, Christian (eds.): 
                        Lexikon der Romanistischen Linguistik (LRL) IV: Italienisch, Korsisch, Sardisch / Italiano, Corso, Sardo. Tübingen: Niemeyer 774-786.
                    
               
                  Schöch, Christof (2017): „Digitale Wissensproduktion“, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): 
                        Digital Humanities. Eine Einführung . Stuttgart: J. B. Metzler 206-212.
                    
               
                  Schreibman, Susan / Ping Huang, Marianne / Benardou, Agiatis/ Tasovac, Toma / Scagliola, Stef / Durco, Matej / Clivaz, Claire (2015-2017): 
                        #dariahTeach. DH Training Materials  [24.09.2017].
                    
               
                  Sperone, Speron (1542): “Dialogo delle lingue”, in: Speron Sperone: 
                        I dialogi di Messer Speron Sperone. Vinegia: Aldus 106-131 (Digitalisat: TIFF, Universitätsbibliothek Leipzig 27.04.2017).
                    
               
                  Swiggers, Pierre (1990): “Französisch: Grammatikographie / Grammaticographie“, in: 
                        Lexikon der Romanistischen Linguistik (LRL), V, 1: Französisch, Okzitanisch, Katalanisch / Le français, L'occitan, Le catalan. Tübingen: Niemeyer 843-869. 
                    
               
                  TEI (05.10.2015): “Guidelines”, in: 
                        Text Encoding Initiative  [25.09.2017].
                    
               
                  Tory, Geoffroy (1529): 
                        Champ Fleury. Paris: Geoffroy Tory / Giles Gourmont.
                    
               
                  Uni (1991–2017): “The Unicode Standard“, in: 
                        The Unicode Consortium  25.09.2017].
                    
               
                  Warwick, Claire / Terras, Melissa / Nyhan, Julianne (eds.) (2012): 
                        Digital Humanities in Practice. London: Facet Publishing.
                    
            
         
      
   



      
         
            Motivation
            
                    »Angesichts der steigenden Sichtbarkeit der Digital Humanities, auch und gerade bei universitären Schwerpunktsetzungen, ist die Frage, wie sie am sinnvollsten gelehrt werden sollen, von steigender Bedeutung« […] »Um diese Bemühungen längerfristig in der Community zu verankern, wurde auf der ersten Jahreskonferenz der Digital Humanities der deutschsprachigen Länder im März 2014« […] »eine Arbeitsgruppe der DHd gegründet. Die Proponenten der Arbeitsgruppe schlugen vor,« […] »die bisher losen Diskussionen stärker auf ein »Referenzcurriculum« zu fokussieren.«
                    (https://dig-hum.de/ag-referenzcurriculum-digital-humanities)
            
            
                    Diesem Anliegen fühlt sich der als Dozent für die Vermittlung von Digitalisierungskompetenz an der Universitäsbibliothek Würzburg arbeitende Autor verpflichtet. Er engagiert sich in der 
                    
                  AG Referenzcurriculum Digital Humanities,
                ist Mitglied des Würzburger Arbeitskreis Digitale Editionen und unterstützt das Editionsprojekt 
                    
                  Narragonien digital.
                
                    Er plädiert mit seinem Beitrag dafür, die DH-Ausbildung bezüglich der Bilddigitalisierung und des OCR (Optical Character Recognition) stärker zu kanonisieren. Mit seiner eigenen 
                    Lehrveranstaltung
               
                  Bilddigitalisierung und OCR für Geisteswissenschaftler
               , deren Schwerpunkt auf der Erstellung der Digitalisate und dem OCR liegt, bietet er eine Referenz, die er hiermit zur Diskussion in der Community stellt.
                
            Das Ziel der Lehrveranstaltung ist die Vermittlung von Kenntnissen des gesamten Digitalisierungsprozesses für MA-, BA- und LA-Studentinnen und Studenten aller geisteswissenschaftlichen Fachrichtungen. Diese nachfolgend als Zielgruppe Bezeichneten sollen in die Lage versetzt werden, selbständig strukturiert ausgezeichnete, digitale Volltexte zu erzeugen und die Ergebnisse der Erstellung derselben beurteilen zu können. Diese Kenntnisse sind wichtig, weil Projekte häufig auch dadurch gefährdet sind, dass Vertretern der Zielgruppe die mangelnde Qualität der ihnen vorliegenden Digitalisate zu spät bewusst wird. Darum müssen sie von Beginn an Digitalisate auf ihre Brauchbarkeit für die automatische Texterkennung beurteilen können. Strukturiert ausgezeichnete, digitale Volltexte sind unabdingbar beispielsweise für Topic Modeling oder Sentiment Analysis auf größeren Textcorpora und als Zwischenstufe für die Erstellung digitaler Editionen.
         
         
            Ablauf
            Kurz gefaßt sind die sechs Arbeitschritte dafür
            
               die Sensibilisierung für juristische Aspekte der Bilddigitalisierung,
               die Suche nach vorhandenen oder die Erstellung von eigenen Digitalisaten,
               deren Vorverarbeitung,
               das OCR,
               die Anreicherung der Digitalisate und des generierten Rohtextes – im Idealfall einer diplomatische Transkription – mit Metadaten, sowie
               die inhaltliche Auszeichnung des generierten Rohtextes.
            
            
               2.1. Rechtliche Grundlagen der Bilddigitalisierung
               Um den Teilnehmern den typischen Arbeitsablauf der Textdigitalisierung möglichst stringent und ohne thematische Abschweifungen vorzuführen, werden sie zuerst intensiv mit den juristischen Grundlagen der Bilddigitalisierung vertraut gemacht. Dazu gehören
               
                  die Vorstellung des UrhG beziehungsweise speziell der § 60d und 60g UrhG in der novellierten Fassung des UrhWissG 2017,
                  die Unterscheidung zwischen Immaterial- und Materialgüterrecht und was daraus für die Digitalisierung zweidimensionaler Objekte folgt,
                  die Persönlichkeitsrechte des Urhebers und weiterer Betroffener sowie
                  der Umgang mit Werken, die unter der Creative Commons Lizenz stehen und die Möglichkeit, diese selbst zu benutzen.
               
            
            
               2.2. Suche nach vorhandenen Digitalisaten beziehungsweise deren Erstellung
               Am Anfang der Transformation vom gedruckten zum digitalen Corpus steht die Suche nach möglicherweise bereits vorhandenen Digitalisaten. Diese Suche setzt neben nicht DH-spezifischen Kenntnissen der Erschließung das Wissen um Metadaten zu digitalen Bildformaten voraus. Die Vertreter der Zielgruppe müssen in dieser Situation wissen, dass beispielsweise die Chancen eines erfolgreichen OCR mit einem JPEG mit 72 dpi deutlich geringer sind als mit einem unkomprimierten TIFF, True Color und 300 dpi. Sollte er schließlich feststellen, dass ihm Digitalisate in der gewünschten Form nicht zugänglich sind, bedarf er der Kenntnisse zu digitalen Bildformaten genauso, um im nächsten Schritt erfolgreich den Scan selbst durchzuführen oder nach seinen Vorgaben durchführen zu lassen.
               Ausgehend von den skizzierten Anforderungen werden den Vertretern der Zielgruppe in der Veranstaltung die Grundlagen der Bilddigitalisierung nahe gebracht. Vertieft wird hier auf das menschliche Sehen und die Farbreproduktion, die Entstehung digitaler Bilder (Rastergraphik, optische und interpolierte Auflösung, Farbtiefe), Farbräume, Color-Management-Systeme, verschiedene Graphikspeicherformate, Speichermedien und verschiedene Scannertypen eingegangen. Auch für die Berücksichtigung konservatorischer Aspekte werden die Teilnehmer sensibilisiert.
            
            
               2.3. Aufbereitung der Digitalisate für das OCR
               Die Forschung im Bereich OCR, insbesondere auf Inkunabeln und Wiegendrucken, sowie die eigene Praxis des Autors belegen die Bedeutung einer vorherigen Aufbereitung der Digitalisate für das OCR, der darum entsprechend Platz in der Veranstaltung eingeräumt wird (Springmann 2015: 9). Als Tätigkeiten sind hier in der Reihenfolge ihrer Ausführung die Bereinigung und anschließende Binarisierung der Digitalisate, deren Segmentierung in einzelne Textabschnitte und schließlich einzelne Textzeilen sowie die Transkription (›ground truth‹) einer Anzahl der Textzeilen zu nennen. Die gesamte Aufbereitung der Digitalisate für das OCR führen die Vertreter der Zielgruppe in der Veranstaltung selbst an ausgewähltem Trainingsmaterial durch. Nach der Teilnahme an der Veranstaltung sollen sie auch diesen Arbeitsschritt selbständig erledigen und Arbeitsergebnisse anderer in diesem Bereich beurteilen können.
            
            
               2.4. OCR
               Entsprechend der zunehmenden Spezialisierung des Digitalisierungszentrums der UB Würzburg ist es erstens wünschenswert, in der Lehrveranstaltung besonders auf das Training eigener Modelle beispielsweise mit 
                    
                     OCRopus
                   einzugehen. Zweitens soll ein Arbeitsablauf für die Digitalisierung eigener Texte vorgestellt werden, der von den Vertretern der Zielgruppe selbständig mit möglichst geringem technischen Aufwand realisierbar ist. Diese Form der Digitalisierung von Texten soll als handhabbares Mittel zum Zweck wahrgenommen werden.
                    
               Schließlich wird in diesem Zusammenhang auf die Frage nach dem Zeitpunkt der Normalisierung des mit dem OCR erstellten Textes eingegangen. Soll bereits mit dem OCR ein normalisierter Text erstellt werden und wenn ja, nach welchen Regeln? Ist also beispielsweise von der verwendeten OCR-Software das Schaft-s bereits automatisch als Rund-s zu lernen und anschließend zu transkribieren? Oder soll das Ergebnis des OCR graphisch so dicht wie möglich am Original bleiben und normalisierende Eingriffe erst hinterher erfolgen?
            
            
               2.5. Auszeichnung/Anreicherung
               Nach dem OCR wird erst die Notwendigkeit der Auszeichnung sowohl der Digitalisate mit Metadaten als auch des Rohtextes erläutert, die die Teilnehmer dann auch selbst vornehmen sollen. Für den extrahierten Rohtext gilt das in zweifacher Hinsicht: Erstens sind ihm Metadaten hinzuzufügen, welche die spätere, eindeutige Identifikation des Werkes und dessen Auffindbarkeit ermöglichen. Zweitens muss der Text strukturiert mit inhaltsbezogenen Elementen angereichert werden.
               Konkret sind bei der digitalen Repräsentation eines Romans beispielsweise die Figuren, Orte, Zeitpunkte und gegebenenfalls weitere signifikante Entitäten im Text für das spätere, automatisierte Retrieval zu kodieren. Andere Anforderungen stellen digitalierte Transkriptionen gesprochener Sprache und wiederum andere die Erstellung einer Urkundenedition (Vogeler 2015). Für die visuelle Präsentation, beispielsweise auf einem Webportal, sind textstrukturierende Merkmale wie die Einteilung nach Kapiteln, Abschnitten, Fußnoten etc. zu kennzeichnen. Dem unterschiedlichen Kenntnisstand der Teilnehmer geschuldet muss hier vor der Vorstellung der TEI Guidelines zuvor zweifelsohne XML dargestellt werden. Wie ausführlich daneben 
                    Dublin Core und bibliotheksspezifische Formate (MARC21) thematisiert werden, ist noch nicht entschieden. Wieviel Zeit für weiterführende Themen wie 
                    Named Entity Recognition, 
                    PID und Normdaten wie GND bleibt, muß ebenfalls die Praxis weisen.
                    
            
         
         
            Forschungsbezug und Weiterentwicklung
            Die eine Stärke der Würzburger Lehrveranstaltung ist die Praxisorientierung, der nach der zweitägigen Einführung noch stärker mit einer drei Tage dauernden Übung Rechnung getragen wird. Bei dieser werden die Vertreter der Zielgruppe mit vorbereiteten Scans selbständig die genannten Arbeitsschritte von der Bereinigung und anschließenden Binarisierung über die Segmentierung und Transkription, dem OCR bis zum anschließenden Auszeichnen beziehungsweise der Anreicherung des digitalen Corpus mit den nötigen Metadaten vornehmen.
            Unverzichtbar für die gesamte Ausbildung im DH-Bereich ist neben dem Praxisbezug die Orientierung am neuesten Stand der Forschung in allen Teilbereichen. Dem wird bei der skizzierten Lehrveranstaltung erstens durch die Forschung einzelner Mitglieder des Digitalisierungszentrums als die Veranstaltung verantwortende Abteilung Rechnung getragen (1. Reul/Wick/Springmann/Puppe: 2017. 2. Springmann: 2016. 459–462). Zweitens ist die enge Zusammenarbeit des Digitalisierungszentrums mit dem Lehrstuhl für Informatik VI der Universität Würzburg zu nennen.
            Denkbare Erweiterungen für eine zukünftig breiter angelegte Lehrveranstaltung der beschriebenen Art sind die Vorstellung a) des OCR von Handschriften, beispielsweise in der Kooperation mit einer 
                    Transkribus anwendenden Institution, idealerweise der 
                    
                  Digitalisierung und elektronische Archivierung – DEA
                der Universität Innsbruck, und b) des Einsatzes virtueller Forschungsumgebungen zur Herstellung digitaler Ressourcen.
                
            Aus Sicht des Autors ist nach der erfolgreichen Durchführung die kritische Diskussion mit den Autors ähnlicher oder gleicher Veranstaltungen von anderen Institutionen unverzichtbar. Ausgehend von
                    http://cceh.uni-koeln.de/digitale-geisteswissenschaften-studiengange-2011/ befragt er aktuell die Mitarbeiter einschlägiger Institutionen nach deren Angeboten im Bereich der Digitalisierung von Texten. Er hofft mit seinem Beitrag wie dem AG Treffen an der DHd2018 auf einen fruchtbaren Meinungsaustausch.
                
         
      
      
         
            
               Bibliographie
               
                  Corbach, Almuth: 
                        Bestandsschonendes Digitalisieren von schriftlichem Kulturgut. In: Digital und analog. Die beiden Archivwelten. 46. Rheinischer Archivtag. Ratingen 21.-22. Juni 2012.
                    
               
                  Jannidis, Fotis / Hubertus Kohle / Malte Rehbein [Hrsg.]: 
                        Digital Humanities. Eine Einführung. Springer-Verlag GmbH Deutschland, 2017.
                    
               
                  Kneißl, Michael: 
                        Scannen wie die Profis : Text- und Bildvorlagen perfekt digitalisieren. München: DTV. (2)2002.
                    
               
                  Loewenheim, Ulrich / Adolf Dietz / Gerhard Schricker: 
                        Urheberrecht. Kommentar. München: Beck. (4)2010.
                    
               
                  Reul, Christian / Christoph Wick / Uwe Springmann / Frank Puppe: 
                        Transfer Learning for OCRopus Model Training on Early Printed Books. In: 
                        Zeitschrift für Bibliothekskultur. Bd. 5, Nr. 1 (2017).
                    
               
                  Springmann, Uwe: 
                        A high accuracy OCR method to convert early printings into digital text. A Tutorial. 
                        Center for Information and Language Processing (CIS). LMU. München. 2015. S. 9.
                    
               
                  Springmann, Uwe: 
                        OCR für alte Drucke. 
                        Informatik-Spektrum. 39(6):459–462. 2016.
                    
               
                  Vogeler, Georg: 
                        Die Text Encoding Initiative (TEI) als Werkzeug des Urkundeneditors – Erfahrungen und Desiderate. In: Fees, Irmgard Prof. Dr.; Hotz, Benedikt; Schönfeld, Benjamin (Hrsg.): 
                        Papsturkundenforschung zwischen internationaler Vernetzung und Digitalisierung. Neue Zugangsweisen zur europäischen Schriftgeschichte. Göttingen. 2015.
                    
               
                  Weitzmann, John H. / Paul Klimpel: Rechtliche Rahmenbedingungen für Digitalisierungsprojekte von Gedächtnisinstitutionen. Berlin: Zuse Institute Berlin. digiS – Servicestelle Digitalisierung Berlin. (3)2016.
            
         
      
   



      
         
            Ausgangslage
            Die Vielschichtigkeit der Aufklärungszeit, die Funktionsweise der Gelehrtenrepublik und ihre Wechselwirkungen mit Wirtschaft, Politik und Gesellschaft, aber auch die Ausdifferenzierung der Naturgeschichte werden an der Universität Bern seit den frühen 1990er-Jahren intensiv beforscht.
                     Zentrale Figur des Forschungsinteresses ist dabei Albrecht von Haller (1708-1777), Schweizer Universalgelehrter und Schöpfer eines vielfältigen Oeuvres, das unter anderem in einem außerordentlich reichhaltigen handschriftlichen Nachlass überliefert ist. 
                
            Wichtigstes Hilfsmittel ist seit 1991 eine Forschungsdatenbank, die schrittweise durch den Zusammenschluss mit mehreren Forschungsprojekten (etwa zur Oekonomischen Gesellschaft Bern) zur Verbunddatenbank erweitert und bis heute laufend ausgebaut wurde.
                     Sie enthält detaillierte und untereinander stark verlinkte Daten zu rund 40
                    '000 Publikationen (Hallers Bibliothek, Forschungsliteratur), 22
                    '000 Personen, 20
                    '000 Briefen, 3
                    '000 Pflanzenarten, 2
                    '500 Orten und 800 Institutionen, jeweils in projektspezifisch gewachsenen Datenstrukturen abgelegt.
                
            Um diese wertvolle Forschungsquelle umfassender verfügbar und sie zugleich als Grundlage für die anstehende Edition von Primärquellen nutzbar zu machen, wird die Datenbank gegenwärtig in einer Forschungskooperation zwischen dem Historischen Institut der Universität Bern und dem Cologne Center for eHumanities (CCeH) bereinigt, umstrukturiert und in eine zugleich entstehende Forschungs- und Editionsplattform integriert.
                     Zielformat sind XML-Daten, die sich eng an die TEI-Guidelines anlehnen.
                    
            
         
         
            Charakteristika der Datenbank und der entstehenden Editionen
            Das Potenzial der Datenbank lässt sich gut anhand der Personenobjekte veranschaulichen, die für rund 24
                    '000 historische Persönlichkeiten (dazu zählen Hallers rund 1
                    '050 Korrespondenten und 156 Korrespondentinnen) verschiedene Aspekte sehr detailliert und mit Quellennachweisen versehen beschreiben. Je nach Objekttyp enthalten die Personendatensätze bis zu 385 Felder: Neben den gängigen Lebensdaten wurden zu diesen Personen alle Verwandtschaftsbeziehungen, Ausbildungsgang, sowie Arbeits-, Forschungs- und Reisetätigkeiten erhoben und wo möglich mit den entsprechenden Datenbankobjekten verlinkt.
                     Für die historische Forschung stellt dieses dichte und qualitativ gesicherte Informationsnetz eine herausragende Forschungsressource dar.
                
            Die Datenbank wird im Rahmen der Forschungskooperation ergänzt durch eine prototypische Korrespondenzedition des Briefverkehrs zwischen dem von Hannover aus wirkenden Universitätskurator Gerlach von Münchhausen und Albrecht von Haller.
                     Die Edition, die sowohl innerhalb der Forschungs- und Editionsplattform als auch mit externen Ressourcen wie correspSearch.net verlinkt wird, dient als Modell für die 
                    Open Access-Edition von zunächst 8
                    '000 Briefen und 9
                    '000 Rezensionen Hallers, die ab 2018 im Rahmen des SNF-Projekts 
                    Online-Edition der Rezensionen und Briefe Albrecht von Hallers: Expertise und Kommunikation in der entstehenden Scientific community ediert werden.
                
            Hinsichtlich digitaler Forschungspraktiken sind die laufenden Modellierungs-, Konversions- und Entwicklungsarbeiten in zweierlei Hinsicht von Interesse: einerseits werfen sie die Frage auf, inwiefern Datenstrukturen durch die zugrunde liegenden Werkzeuge beeinflusst werden und bieten zugleich einen Rahmen konkreter Reflexion, andererseits erfordert die Konzeption eines übergreifenden Portals für unterschiedliche, aber zusammenhängende Inhalte eine geeignete Präsentation und Kommunikation.
         
         
            Datenmodellierung und -transformation als Reflexionsprozess
            Die 
                    ad hoc gewachsenen Datenstrukturen werden in ein neu definiertes Textformat überführt, das den TEI-Guidelines inhärente bestehende semantische Konzepte nutzt und sich mithin (zu einem gewissen Grad) selbst beschreibt. Angestrebt wird Verständlich- und Nachvollziehbarkeit nicht nur in der Präsentation, sondern auch auf der Datenebene. Da die TEI-Guidelines für viele vorliegende Forschungsdaten (z.B. Eigenschaften von Personen) keine entsprechenden Elemente kennen, wurden generische Lösungen durch Abstraktion gesucht. Damit bedingt die Überführung aus einer Feld-, Tabellen- und Maskenbasierten Datenbank in ein TEI-basiertes Format eine analytische Durchdringung der Datenstruktur, die auch den mit den Daten vertrauten Forschenden einen neuen Blick auf ebendiese gibt. Aus dieser Perspektive betrachtet erscheinen die Forschungsdaten deutlicher als Objekte der realen Welt mit semantisch klar definierten Eigenschaften. Die Reflexion verstärkt auf diese Weise die gegenseitige Wechselwirkung zwischen Datenmodellierung und wissenschaftlicher Analyse. Die Generalisierung spezifischer Datenbeschreibungsformen ist wesentlicher Bestandteil, um die neue Datenbank nicht nur im bestehenden, um Albrecht von Haller und die Oekonomische Gesellschaft Bern zentrierenden Kontext nutzen zu können, sondern z.B. auch für die Naturforschende Gesellschaft Zürich. Sie ermöglicht inskünftig auch Anschluss an Schnittstellen Dritter wie 
                    correspSearch
                im Bereich der Korrespondenzdaten oder die im Entstehen begriffene 
                    prosopogrAPhi
                im Bereich der Personendaten.
                
         
         
            Spezifizierung eines übergreifenden Editionsportals
            Das entstehende Editionsportal will eine einheitliche Grundlage schaffen für verschiedene Inhalte, seien es beschreibende und Metadaten, digitale Neu-Editionen bestehender gedruckter Briefeditionen oder neu erarbeitete digitale Brief- und Rezensionseditionen mit unterschiedlichen thematischen, geographischen und personellen Schwerpunkten. Dabei sollen bestimmte Inhalte der Plattform den Nutzern in verschiedenen Kontexten (z.B. ein Brief als atomares Datum, als Bestandteil einer “Sammlung”, als edierter Brief) jeweils so präsentiert werden, dass die Situierung innerhalb des Portals jederzeit nachvollziehbar bleibt. Der Anspruch des Portals geht dadurch konzeptionell beträchtlich über den Standardfall einer digitalen Edition (im Sinne tiefer editorischer Erschließung) eines einzelnen Textes oder verschiedener Texte eines Autors hinaus. Zugleich greift durch die Tiefe der Erschließung aber auch der Vergleich mit digitaler Datenbereitstellung, wie sie in Bibliotheken und Archiven oftmals betrieben wird, zu kurz. Eine einfache und verständliche visuelle Kommunikation und Umsetzung dieser Ansprüche erweist sich in der konzeptuellen Arbeit als Herausforderung.
            
               
               Abbildung 1: Segmentierte Landingpage der in Entwicklung begriffenen Webanwendung: Datenbank, Sammlungen, Editionen
            
         
      
      
         
             Wegweisend war das SNF-Projekt 
                            Albrecht von Haller und die Gelehrtenrepublik der Aufklärung (Laufzeit 1991-2003, Leitung: Urs Boschung, medizinhistorisches Institut der Universität Bern, in Kooperation mit der Burgerbibliothek Bern).
                        
             Realisiert wurde die Datenbank mit dem Dokumentations- und Retrievalsystem 
                            FAUST Professional der Firma Land Software: ursprünglich in der Version 1.0, zuletzt wurde das Programm in der Version 6.0 genutzt.
                        
             Der Umbau ist das Ziel des Projekts Haller Online (Laufzeit 2016-2019), das von der Burgergemeinde Bern mit der Albrecht von Haller-Stiftung sowie der Universität Bern finanziert wird. Über den Abschluss des Projekts hinaus ist die Zugänglichkeit und der Unterhalt der Plattform institutionell durch die Haller-Stiftung und die Burgerbibliothek Bern abgesichert.
             Die Datentransformation erfolgt in XProc-orchestrierten XSL-Transformationen und die künftige Datenpflege im oXygen-Autorenmodus; die Webanwendung vereinigt ein XML-Backend (XSLT 3.0, Solr) und ein Vue.js-Frontend, die Digitalisate sollen über eine IIIF-Schnittstelle eingebunden und verfügbar gemacht werden.
             Verlinkungen können sowohl auf andere Personen wie auch auf andere Objekte (wie Orte, Pflanzen oder Institutionen) referenzieren.
             Otto Sonntag (ed.): The Albrecht von Haller-Gerlach Adolph von Münchhausen Correspondence, Digitale Edition, Bern 2018 (in Vorbereitung).
             Stefan Dumont et al.: correspSearch, URL: http://correspsearch.net (14.1.2018).
             Georg Vogeler et al.: prosopogrAPhI, URL: https://github.com/GVogeler/prosopogrAPhI (14.1.2018).
         
         
            
               Bibliographie
               
                  Boschung, Urs et al. (eds.): Repertorium zu Albrecht von Hallers Korrespondenz 1724-1777 (Studia Halleriana VII), Basel: Schwabe 2006.
               
                  Dumont, Stefan et al.: correspSearch, URL: http://correspsearch.net (14.1.2018).
               
                  Flückiger, Daniel / Stuber, Martin: Vom System zum Akteur. Personenorientierte Datenbanken für Archiv und Forschung, in: Kirchhofer, André et al. (eds.), Nachhaltige Geschichte. Festschrift für Christian Pfister, Zürich: Chronos 2009, S. 253-269.
               
                  Sonntag, Otto (ed.): The Albrecht von Haller-Gerlach Adolph von Münchhausen Correspondence, Digitale Edition, Bern 2018 (in Vorbereitung).
               
                  Steinke, Hubert: Archive databases as advanced research tools: the Haller Project, in: Monti, Maria Teresa (ed.), Antonio Vallisneri. L'edizione del testo scientifico d'età moderna, Firenze: Olschki 2003, S. 191–204.
               
                  Steinke, Hubert / Profos, Claudia: Bibliographia Halleriana: Verzeichnis der Schriften von und über Albrecht von Haller (Studia Halleriana VIII), Basel: Schwabe 2004.
               
                  Stuber, Martin: Findmittel und Forschungsinstrument zugleich. Die Datenbank des Berner Haller-Projekts, in: Arbido 14, 1999, S. 5-10.
               
                  Stuber, Martin / Hächler, Stefan / Lienhard, Luc (eds.): Hallers Netz. Ein europäischer Gelehrtenbriefwechsel zur Zeit der Aufklärung, Schwabe: Basel 2005.
               
                  Vogeler, Georg et al.: prosopogrAPhI, URL: https://github.com/GVogeler/prosopogrAPhI (14.1.2018).
            
         
      
   



      
         
            Einleitung
            In den letzten Jahren wurden verschiedene Werkzeuge zur Annotation von Dokumenten entwickelt, z.B. 
                    WebAnno (Eckart de Castilho et al. 2016), 
                    Cor
               A (Bollmann et al. 2014) und 
                    CATMA (Meister et al. 2016). Diese Werkzeuge bieten zahlreiche Funktionalitäten, die für viele Sprachen und Anwendungen ausreichend sind. Die Unterstützung verschiedener Skripten erweckt den Eindruck, dass diese Werkzeuge für die Annotation aller Sprachkorpora erfolgreich eingesetzt werden können, was solange korrekt ist, wie man sehr flache Annotation durchführt.
                
            Tiefere Annotationen, insbesondere für Sprachen außerhalb der Europäischen Sprachfamilie oder für historische Sprachen, dagegen gestalten sich problematischer. Häufig jedoch wird die Verwendung bereits etablierter Annotationstools trotzdem bevorzugt, weil diese eine reibungslose Integration mit weiteren Analyse- und Visualisierungs-Tools wie z.B. 
                    ANNIS (Krause & Zeldes 2016) versprechen. In diesem Beitrag werden wir zeigen, dass die Entwicklung dedizierter Annotationswerkzeuge dann als Lösung in Betracht gezogen werden kann, wenn gleichzeitig Schnittstellen (z.B. zu Analyse-Tools) entwickelt werden, die die Neuentwicklung wiederum an vorhandene Software und Infrastrukturen anbinden. Auf diese Weise können die Nachteile einer Neuentwicklung unter Gesichtspunkten der nachhaltigen Entwicklung von Forschungssoftware gegenüber der Anpassung bestehender Tools - bei gleichzeitiger Ermöglichung eines spezialisierten Anwendungsfalles - weitgehend abgeschwächt werden. Der Vorteil eines solchen Verfahrens ist die Realisierung eines Annotationsmodells, das exakt den Besonderheiten der Sprache entspricht. 
                    Insbesondere für diachrone Analysen ist es häufig nötig, dass man eine sehr detaillierte Modellierung der morphologischen, syntaktischen und semantischen Merkmale vornimmt, da die Unterschiede häufig nur im Detail reflektiert sind. Im Einzelnen werden wir die Entwicklung des 
                    GeTa-Annotationstools, eines Mehrebenenannotationswerkzeugs für die Altäthiopische Sprache, und dessen Integration mit dem Such- und Visualisierungsframework 
                    ANNIS darstellen.
                
         
         
            Spezielle Anforderungen für die Altäthiopische Sprache
            Das Altäthiopische (Geʿez), ist eine südsemitische Sprache. Es war bis ins 19. Jahrhundert hinein die bedeutendste Schriftsprache des christlichen Äthiopien. Die reiche christlich-äthiopische Literatur ist zunächst von Übersetzungen - anfangs aus dem Griechischen und später aus dem Arabischen - geprägt, bevor sich eine mannigfaltige indigene Literatur mit ganz eigenen Zügen entwickelt. Insbesondere Texte, die einzig im Altäthiopischen vollständig überliefert, und deren Textzeugen in anderen Sprachen entweder vollständig verloren, oder von denen nur Fragmente erhalten sind (z.B. das Henochbuch) erlangen dabei ganz besondere Bedeutung (Vertan et. al. 2016).
            Das Altäthiopische hat aus einer südsemitischen Schrift ein eigenes Silbenalphabet entwickelt, das bis heute in mehreren modernen Sprachen Äthiopiens und Eritreas Verwendung findet. Innerhalb der semitischen Sprachen fällt es durch die verwendete Rechtsläufigkeit auf; außerdem werden die Vokale vollständig geschrieben. Das äthiopische Silbenalphabet bringt dabei mit sich, dass Morphemgrenzen in der Schrift nicht darstellbar sind, sodass beispielsweise ein einzelner Vokal als Bestandteil einer Silbe eine eigenständige Wortart darstellen kann und tokenisiert werden muss; z.B. ist im zweisilbigen Wort ቤቱ፡ /be·tu/ das /u/ als pronominales Suffix zu bet-
                    u (
                    sein Haus) zu segmentieren. Eine solche Annotation kann nur auf der Transkription erfolgen. Annotationen auf anderen Ebenen (z.B. Seiten- Spaltenumbrüche, Textkorrekturen) müssen auf dem Fidal-Skript realisiert werden. Dies bedeutet, dass Original und Transkription synchronisiert im Annotationswerkzeug dargestellt werden müssen. Eine korrekte Darstellung einer Transliteration benötigt die Transkription. Während diese regelbasiert automatisch durchgeführt werden kann, ist korrekte Transliteration (z.B. Konsonantendopplung) in vielen Fällen nur zusammen mit morphologischer Analyse möglich. Daher muss das Annotationstool Korrekturen am Basistext während der Annotation zulassen und zuverlässig verarbeiten können. Keins der existierenden Annotationswerkzeuge erfüllt diese Voraussetzungen. CorA arbeitet mit Listen von Wertkombinationen von Attributen. Die benötigte morphologische Annotation im Fall der altäthiopischen Sprache umfasst 30 Merkmale, mit je mindestens drei möglichen Werten. Die Handhabung solch einer umfangreichen Liste macht manuelle Annotation praktisch unmöglich. Weder WebAnno noch CATMA ermöglichen die Korrektur des zugrunde liegenden Textes während der Annotation. Die Implementierung einer solchen Funktion auf Basis eines dieser Werkzeuge würde tief in die Architektur der Software eingreifen, was innerhalb der Laufzeit des Projektes nicht realisierbar war. Auch die semi-automatischen Annotationsmöglichkeiten unter Supervision des Benutzenden sind für diese beiden Werkzeuge nicht leicht erweiterbar.
                
         
         
            Das GeTa Annotationstool
            Im TraCES-Projekt
                    , das als Hauptziel die Entwicklung eines diachronen, tief annotierten Korpus für die Altäthiopische Sprache hat, implementieren wir eine neuartige Architektur, die sowohl Änderungen im Text als auch eine Mehrebenenannotation ermöglicht.
                
            Wir betrachten als Grundtext den Originaltext in der altäthiopischen Schrift. Die Transliteration bildet die erste, die morphologische Annotation die zweite Ebene, wobei die Transliteration und der Originaltext bei allen Arbeitsschritten synchronisiert bleiben. Im folgenden Abschnitt beschreiben wir das Datenmodell, das diese Architektur ermöglicht.
            Die Basiseinheit in unserem System ist ein Wort, das eine einmalige ID zugewiesen erhält (graphische Einheit). Ein Wort hat folgende Komponenten:
            
               Eine Liste der einzelnen Fidal
                        -Objekte, wobei ein Fidal-Objekt aus einer ID und einem Label (dem Fidal-Buchstaben) besteht.
                    
               Eine Liste einzelner Silben-Objekte, wobei ein Silben-Objekt aus einer ID und einer Liste von einzelnen Buchstaben-Objekten besteht.
               Ein Buchstaben-Objekt hat immer eine ID und ein Label (das graphische Symbol).
            
            Graphische Einheiten können in mehreren Tokens geteilt werden. Die Tokens sind getrennt gespeichert (mit eigenenen IDs) und verlinkt mit der graphischen Einheit. Elemente der Textstruktur werden durch selbständige Objekte dargestellt, die mit den beinhalteten Tokens verlinkt werden. Editorische Annotationen werden mit den graphischen Einheiten verlinkt. Eine derartig stark vernetzte komplexe Struktur ist mit XML schwierig zu modellieren, insbesondere bei manueller Annotation. Auch die maschinelle Verarbeitung einer derartig tief vernetzten TEI-Struktur wäre sehr kompliziert. Alternativ bietet JSON für dieses Modell eine bessere Handhabbarkeit. Wir haben uns daher entschieden, die Daten in JSON zu speichern und zusätzlich XML-Export zu implementieren. Die Datenstruktur ist in vier JSON-Dateien gespeichert: eine für die graphischen Einheiten, eine für die Tokens, je eine für weitere Annotationsebenen (Textstruktur und Edition). Diese JSON-Dateisammlung wird via Konverterschnittstelle nach 
                    ANNIS importiert.
                
         
         
            corpus-tools.org: 
                    ANNIS, 
                    Pepper, 
                    Salt
            
            
               ANNIS (Krause & Zeldes 2016) ist eine Such- und Visualisierungsplattform für linguistische Daten, die mehrebenenfähig ist, d.h., verschiedene Annotationsebenen eines Korpus darstellen kann und dabei verschiedenste Annotationsarten berücksichtigt. Dies macht 
                    ANNIS zu einem hervorragenden Analysetool für die Daten des TraCES-Projekts. Eine besondere Rolle dabei spielt die Mächtigkeit der 
                    ANNIS-eigenen Abfragesprache AQL (ANNIS Query Language), die neben Freitextsuche und regulären Ausdrücken sich vor allem dadurch auszeichnet, linguistische Strukturen über mehrere Ebenen suchen zu können.
                
            Durch 
                    ANNIS‘ beschriebene Eigenschaften und seine Konfigurierbarkeit ist die Software für die Verwendung in den unterschiedlichsten Analyseszenarien geeignet und wird in der Tat bereits in verschiedenen Communities verwendet, z.B. in historischer Linguistik, Typologie, Sprachdokumentation u.v.m.
                
            Diese Eignung wird verstärkt durch eine hohe Kompatibilität mit vielen unterschiedlichen linguistischen Datenformaten, die erreicht wird durch Einsatz von 
                    Pepper (Zipser et al. 2011), einem Konvertierungsframework für liguistische Daten. 
                    Pepper nutzt ein generisches, graphbasiertes Zwischenmodell, 
                    Salt (Zipser & Romary 2010), das unterschiedlichste, auch über mehrere Ebenen eines Korpus verteilte, Annotationsarten aufnehmen kann und so weiterverarbeitbar macht. 
                    Pepper zeichnet sich weiterhin durch eine Plugin-Architektur aus, die es ermöglicht mit geringem Aufwand sowohl Import- als auch Manipulations- und Exportmodule zu entwickeln, die die Quelldaten in ein 
                    Salt-Modell im Hauptspeicher übertragen, das dort manipuliert werden und anschließend in ein Zielformat überführt werden kann.
                
            
               ANNIS, 
                    Pepper und 
                    Salt sind Komponenten der Softwarefamilie 
                    corpus-tools.org (Druskat et al. 2016).
                
         
         
            Schnittstelle zwischen 
                    GeTa und 
                    ANNIS
            
            Diese Infrastruktur ermöglicht es ohne Weiteres, die in 
                    GeTa annotierten Daten über ein dediziertes 
                    GeTa-Importmodul für 
                    Pepper und die Verwendung der bereits existierenden 
                    ANNIS-Module für 
                    Pepper – hier das Exportmodul – in 
                    ANNIS verarbeitbar zu machen. Mit 
                    GeTaModules (Druskat 2018) haben wir einen solchen Importer entwickelt, dessen Funktionsweise hier kurz beschrieben werden soll. 
                    GeTaModules ist in Java implementiert und wird als OSGi-Bundle ausgeliefert, das von 
                    Peppers OSGi-Plattform verwaltet werden kann. 
                    GeTaModules ist Open Source unter der Apache License, Version 2.0.
                
            Um Performanz und Speicherökonomie zu optimieren nutzt der 
                    GeTaModules-Importer eine Kombination aus Streaming und Object-Mapping-Methoden, um die aus 
                    GeTa exportierten JSON-Dateien einzulesen. Dabei werden die kleinsten Einheiten der Transliteration auf den graphischen Einheiten genutzt, um eine Tokenisierung in 
                    Salt zu erstellen und einen virtuellen Primärtext aufzubauen. Auf den so erstellten Modelltokens werden rekursiv die Silben- und Fidal-Objekte als Spannen aufgebaut. Im Anschluss werden die linguistischen Annotationen der 
                    GeTa-Tokens sowie die weiteren Annotationen aus den JSON-Objekten per Identifikator auf die entsprechenden Einheiten projiziert.
                
            In diesem Zustand hält 
                    Pepper einen kompletten 
                    Salt-Dokumentgraphen für das entsprechende Korpusdokument im Hauptspeicher. In einem weiteren Schritt werden Ordnungsrelationen jeweils zwischen den Knoten der Fidal und der transliterierten Wortebene erstellt, damit diese später in 
                    ANNIS als Segmentierungsgrundlage angezeigt werden können.
                
            Im dritten Schritt werden mit Hilfe des 
                    ANNIS-Exportmoduls
                     die für den Import in 
                    ANNIS benötigten Dateien im nativen Format geschrieben. Diese können nun in 
                    ANNIS über dessen graphische Benutzeroberfläche importiert werden.
                
            Die Konfiguration der Visualisierungen erfolgt durch Anpassung einer während des Exportvorgangs generierten Konfigurationsdatei. Im Fall der 
                    GeTa-Daten müssen hier lediglich die anzuzeigenden Annotationsebenen und ihre Reihenfolge eingestellt werden. In den 
                    ANNIS-Daten wird weiterhin ein dedizierter HTML-Visualisierer konfiguriert, der Wortartenannotationen auch graphisch Fidalwörtern zuordnet.
                
            Mit 
                    Pepper erfolgt die Konvertierung auf der Kommandozeile und die Konfiguration in Textdateien. Da dies für einige potenzielle Anwendergruppen unbekanntes Terrain bedeuten könnte haben wir den Prototypen einer Desktopanwendung für 
                    Pepper für die Verwendung mit 
                    GeTaModules angepasst, die eine grafische Oberfläche für die Verwendung von 
                    Pepper bietet: 
                    Pepper Grinder (Druskat 2017). 
                    Pepper Grinder (TraCES Edition) bietet so den Workflow für die Konvertierung der 
                    GeTa-Daten nach 
                    ANNIS quasi per Knopfdruck an. 
                    Pepper Grinder wird in Zukunft in eine vollfunktionale Anwendung ausgebaut, die Modi für verschiedene Anwendergruppen anbieten wird.
                
            Neben der projektinternen Nutzung von 
                    GeTaModules für die Konvertierung in das 
                    ANNIS-Format eröffnet die Software weitere Nachnutzungsszenarien für die annotierten Daten. Durch die Existenz von Manipulator- und Export-Modulen für 
                    Pepper für die verschiedensten Datenformate können die Daten mit anderen Werkzeugen nachgenutzt und etwa um zusätzliche Annotationsebenen angereichert, oder mit anderen Visualisierungen zu weiteren Forschungsfragen analysiert werden. 
                    Gleichzeitig leistet die Nachnutzung und Anpassung von 
                    Pepper durch 
                    GeTaModules einen Beitrag zur Vernachhaltigung dieses Frameworks. Durch die Ermöglichung des Imports von GeTa-Daten in ANNIS wird aus dem GeTa-Modell eine durchaus attraktive Modellierungsalternative für andere Sprachen mit ähnlichen Merkmalen. Derzeit wird das Modell in laufenden Projekten zu Mayasprachen und Jiddisch erprobt.
                
            Zusammenfassend stellt unser Ansatz eine Alternative dar zu Tendenzen der „Format-Hoheit“, also der Modellierung auf Grundlage eines – eventuell de facto standardisierten – Datenformats im Gegensatz zur Modellierung auf Grundlage der Forschungsfrage und der vorliegenden Daten. Die Einrichtung von Schnittstellen, wie z.B. 
                    GeTaModules, die eine Nachnutzbarkeit der Daten auch über die ursprüngliche Erstellung hinaus gewährleisten können, ermöglicht eine optimierte Modellierung und die Entwicklung spezifischer, den Bedürfnissen der Forschung und ihrer Daten hochgradig angepasster Werkzeuge, wie etwa 
                    GeTa.
                
         
      
      
         
            
               .
            
             "Fidal" ist der Terminus technicus für das äthiopische Silbenalphabet.
            
               .
            
         
         
            
               Bibliographie
               
                  Bollmann, Marcel / Petran, Florian / Dipper, Stefanie / Krasselt, Julia (2014): „CorA: A web-based annotation tool for historical and other non-standard language data“, in: 
                        Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities (LaTeCH). Gothenburg, Sweden: 86-90.
                    
               
                  Druskat, Stephan (2018): „GeTaModules (Version 0.9.0)“. Zenodo. DOI: 10.5281/zenodo.1146985. 
                        
               
               
                  Druskat, Stephan (2017): „Pepper Grinder (Version 0.1.7)“. Zenodo. DOI: 10.5281/zenodo.1041735. 
                        
               
               
                  Druskat, Stephan / Gast, Volker / Krause, Thomas / Zipser, Florian (2016): "corpus-tools.org: An Interoperable Generic Software Tool Set for Multi-layer Linguistic Corpora", in: 
                        Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016): 23–28.
                    
               
                  Eckart de Castilho, Richard / Mújdricza-Maydt, Éva / Yiman, Seid Muhie / Hartmann, Silvana / Gurevych, Iryna / Frank, Anette / Biemann, Chris (2016): „A Web-based Tool for the Integrated Annotation of Semantic and Syntactic Structures“, in: 
                        Proceedings of the LT4DH workshop at COLING 2016, Osaka, Japan: 76-84.
                    
               
                  Krause, Thomas / Zeldes, Amir (2016): "ANNIS3: A new architecture for generic corpus query and visualization", in: 
                        Digital Scholarship in the Humanities: 118-139.
                    
               
                  Meister, J.C. / Petris, M. / Gius, E. / Jacke, J. (2016): CATMA 5.0 [Software for text annotation and analysis] 
                        http://www.catma.de [letzter Zugriff 10.01.2018]
                    
               
                  Vertan, Cristina / Ellwardt, Andreas / Hummel, Susanne (2016): "Ein Mehrebenen-Tagging-Modell für die Annotation altäthiopischer Texte", in: 
                        Proceedings der DHd-Konferenz 2016
                  http://www.dhd2016.de/abstracts/vortr%C3%A4ge-061.html [letzter Zugriff 25.09.2017].
                    
               
                  Zipser, Florian / Romary, Laurent (2010): "A model oriented approach to the mapping of annotation formats using standards", in: 
                        Proceedings of the Workshop on Language Resource and Language Technology Standards (LREC 2010)
                   [letzter Zugriff 25.09.2017].
                    
               
                  Zipser, Florian / Zeldes, Amir / Ritz, Julia / Romary, Laurent / Leser, Ulf (2011): "Pepper: Handling a multiverse of formats", Poster, 
                        33. Jahrestagung der Deutschen Gesellschaft für Sprachwissenschaft, Göttingen.
                    
            
         
      
   



      
         Vernetzung von Texten und anderen Ressourcen war und ist ein großes Versprechen des digitalen Zeitalters im Allgemeinen, aber auch der digitalen Editionsphilologie im Besonderen. Während man um 2000 darunter noch vorwiegend das manuelle Setzen von einzelnen Links verstand, wurde spätestens seit Beginn dieses Jahrzehnts die automatisierte Verknüpfung in den Blick genommen. So wurde in Portalen, Websites und auch digitalen Editionen (vornehmlich des deutschsprachigen Raums) die automatisierte Verlinkung von Personenregistereinträgen eingeführt, die mit Hilfe der Gemeinsamen Normdatei und von BEACON-Schnittstellen ermöglicht wird. Diese Vernetzung auf Basis von BEACON-Schnittstellen gehört heute zum Standard digitaler Editionen und ist nicht mehr wegzudenken (Stadler 2012). 
         Mit den Fortschritten im Internet allgemein wuchs aber seit einigen Jahren der Wunsch, digitale Editionen über solche basalen Verlinkungen hinaus zu verknüpfen. Im Bereich der Briefeditionen z.B. wurde schon recht früh der Wunsch geäußert, Briefe editionsübergreifend durchsuchbar zu machen und zu vernetzen. Basis dafür sollten die in TEI-XML kodierten Briefmetadaten, also die wichtigsten Kopfdaten eines Briefes, sein. Mit Entwicklung des Correspondence Metadata Interchange Format (TEI Correspondence SIG 2015) und des da-rauf aufbauenden Webservices correspSearch (
                http://correspSearch.net) konnte dies realisiert werden. Nicht nur können Briefeditionen jetzt digitale Briefverzeichnisse bereitstellen, die durch correspSearch zentral recherchierbar gemacht werden. Digitalen Briefeditionen ist es hier nun möglich, über die API von correspSearch Briefmetadaten zu beziehen und passend zu einem – in der eigenen Edition vorliegenden – Brief zu präsentieren (Dumont 2018). Damit werden schon einige methodische Probleme der Briefedition überwunden, andere bleiben hingegen noch offen.
            
         So kann es vorkommen, dass ein Brief mehrfach ediert vorliegt. Diese Situation kann dadurch entstanden sein, dass ältere Editionen (etwa des 19. Jahrhunderts) durch zeitgemäße abgelöst werden. Sie kann aber auch dadurch aufgekommen sein, dass der Brief nicht in einer Briefwechselausgabe, sondern in zwei Gesamtausgaben erschienen ist: Einerseits in derjenigen Gesamtausgabe, die die Korrespondenz des Autors ediert, andererseits in derjenigen, die die Korrespondenz des Empfängers ediert. In so einem Fall entstehen zum einen Kommentierungen, die in beiden Editionen ähnlich vorgenommen werden (z.B. Identifizierung von Personen etc.). Zum anderen werden aber auch Kommentierungen vorgenommen, die aus der spezifischen Perspektive der Edition entstehen – d.h. mit Fokus auf diejenige Person, deren Korrespondenz ediert wird. Dadurch entstehen zwei Annotationslayer, die sich im Idealfall ergänzen. Die Einzelstellen(-kommentierung) aus der jeweils anderen Edition kann also u.U. äußerst wertvoll für einen Nutzer sein. 
         In digitalen Briefeditionen besteht nun prinzipiell die Möglichkeit, Daten aus anderen digitalen Editionen bzw. Portalen zu integrieren. Anstelle eines einmaligen, manuellen Importes bzw. Übernahme dieser Daten, ist heutzutage deren automatisierter Abruf über ein Application Programming Interface (API) zu favorisieren. Daten aus externen Quellen können so schneller aggregiert, aktualisiert und zusammen mit den eigenen Forschungsdaten – hier: edierte Briefe – angeboten werden. Daher erscheint die Nutzung von APIs und standardisierten Datenformaten angeraten. Im Regelfall liegen Einzelstellenkommentare in digitalen Briefeditionen heutzutage als Bestandteil der TEI-XML-Dokumente vor (etwa im Element ). Sollen diese Kommentare allerdings über eine Schnittstelle zur freien Nachnutzung durch externe Editionen angeboten werden, müssen sie deutlich mehr Informationen tragen, als im Kontext der Edition – nämlich Metadaten (Autor des Kommentars, Bezugstext etc.). Auch eine Übermittlung des ganzen Briefes mitsamt seinen Einzelstellenerläuterungen ist nicht gewünscht, liegt der Text doch gerade im hier geschilderten Fall bereits vor. 
         In Betracht kommt daher ein Datenformat, dass dediziert für den Austausch und die Aggregation von Kommentaren, also vorwiegend textuell vorliegender Annotation, konzipiert ist. Zwar wäre es möglich ein solches Format in TEI-XML neu zu definieren, allerdings gibt es für diesen Bereich bereits einen Standard, der auch schon breite Anwendung findet: das Web Annotation Data Model (Sanderson, Ciccarese, und Young 2017). Es ist für genau dieses Nutzungsszenario – Austausch von Kommentaren – konzipiert und durch das W3C standardisiert. Es wird daher auch von DARIAH-DE empfohlen (Lordick u. a. 2016). 
         Das Poster stellt nun anhand eines Beispiels diesen Ansatz exemplarisch vor und demonstriert seinen Nutzen. Als Beispiel ist der überlieferte Briefwechsel zwischen Alexander von Humboldt und Samuel Thomas Soemmerring gewählt, der zum einen in 
                edition humboldt digital (
                http://edition-humboldt.de) vorliegt, zum anderen in einer (derzeit noch unveröffentlichten) digitalen Edition zu Soemmerrings Biographica. Konkret sieht die Implementierung so aus: Einzelstellenerläuterungen aus der Soemmerring-Edition, die ursprünglich als TEI-XML vorliegen, werden über eine API im Web Annotation Data Model angeboten. Diese API wird von correspSearch abgefragt und die Daten aggregiert. Andere digitale Editionen, hier beispielhaft die 
                edition humboldt digital, können anhand einer Kalliope-URI, also der eindeutigen und maschinenlesbaren Archivkennung, den Webservice correspSearch auf Annotationen zu einem bestimmten Brief hin abfragen. Im Erfolgsfall werden die Annotationen ausgeliefert und in der 
                edition humboldt digital am Brief angezeigt (mit Quelle, Autor etc.). 
            
         Das Poster stellt nicht nur Konzept und Implementierung vor, sondern diskutiert auch die noch offenen Probleme einer solchen API, wie z.B. die korrekte und stabile Referenzierung des Bezugstextes (d.h. der kommentierten Textstelle).
         
            
            Beispielhafte Implementierung in der Entwicklungsversion von edition humboldt digital
                
         
      
      
         
            
               Bibliographie
               
                  Dumont, Stefan. 2018. „CorrespSearch – Connecting Scholarly Editions of Letters“. Journal of the Text Encoding Initiative 10. [Im Erscheinen].
                    
               
                  Lordick, Harald, Rainer Becker, Michael Bender, Luise Borek, Canan Hastik, Thomas Kollatz, Beata Mache, Andrea Rapp, Ruth Reiche, und Niels-Oliver Walkowski. 2016. „Digitale Annotationen in der geisteswissenschaftlichen Praxis“. Bibliothek Forschung und Praxis 40 (2): 186–199. doi:10.1515/bfp-2016-0042.
                    
               
                  Sanderson, Robert, Paolo Ciccarese, und Benjamin Young. 2017. „Web Annotation Data Model. W3C Recommendation“. W3C. https://www.w3.org/TR/annotation-model/.
                    
               
                  Stadler, Peter. 2012. „Normdateien in der Edition“. editio 26: 174–83.
                    
               
                  TEI Correspondence SIG. 2015. „Correspondence Metadata Interchange Format (CMIF)“. https://github.com/TEI-Correspondence-SIG/CMIF.
                    
            
         
      
   



      
         
            Einleitung
            Die Anforderungen an die Erstellung von Editionen sind im letzten Jahrzehnt gestiegen.
                     Es ist unstreitig, dass digitale Editionen Annäherungen an den edierten Text ermöglichen, die weit über den statischen Druck hinausgehen (Sahle 2016). Gleichzeitig gibt es aber auch noch immer Anforderungen, etwa die Zitierbarkeit, die im Druck gelöst sind, zu denen es aber in der Webpublikation noch keine einheitliche Entsprechung gibt. Insbesondere stellt sich die Frage nach der Nachhaltigkeit und der Langzeitarchivierung.
                    
            
            Ein Lösungsansatz besteht darin, Techniken der Data Curation zu etablieren (Pempe 2012: 141f.). Damit ist aber ein Aufwand verbunden, der linear mit der Zahl der Webpublikationen wächst und sogar polynomial ansteigt, wenn auch die Schnittstellen zu verknüpften Ressourcen gepflegt werden müssen. Eine besondere Herausforderung stellt sich, wenn man sinnvollerweise annimmt, dass die Darstellung und die Funktionalitäten Teil der Edition selbst sind (Ralle 2016, Pierazzo 2015: 127-146, Porter 2016 und Turska et al. 2016). Wenn also nur die Forschungsdaten nachhaltig archiviert werden, geht die ursprüngliche Präsentation letztendlich verloren. Es wird zwar momentan auf die Einführung von Standards gesetzt, 
                     einen wirklichen Mehrwert für den Gebrauch von digitalen Editionen entfalten diese aber erst, wenn sie durch das Angebot von technischen Schnittstellen unterstützt werden.
                     Bisher fehlt es jedoch an einem klar definierten Interface, welches die Forschungsdaten in eine nachhaltige funktionale Präsentationsform mit allen Aspekten einer digitalen Edition übersetzt.
                    
            
         
         
            Entwurf einer Schnittstelle
            Hier wird nun ein System für die Nachhaltigkeit von Webpublikationen entworfen. Es verbindet die Daten mit der Präsentationsschicht und kann mithilfe einer projektspezifischen, archivierbaren Konfiguration über eine Schnittstelle gesteuert werden. Durch den Einsatz einer solchen Schnittstelle können Webpublikationen inklusive der entsprechenden Funktionalitäten aus den Forschungsdaten reproduziert werden.
                     Digitale Editionen unterscheiden sich technisch zwar zurzeit noch stark voneinander (Robinson 2016), der hier gemachte Vorschlag und das sich anschließende Software-Beispiel zeigen daher, wo es schon jetzt Möglichkeiten zur Standardisierung gibt und wie diese aussehen könnten. 
                
            Vorbild für das System ist der erfolgreiche IIIF-Standard, der zu einem ähnlichen Zweck für Bilder eingeführt wurde (Cramer 2011). Der IIIF-Standard sieht eine Aufteilung von Server- und Clientstruktur vor, und als Austauschformat fungiert eine Manifest-Datei. Übertragen auf Digitale Editionen heißt dies, dass eine archivierbare Manifestdatei notwendige Definitionen festhält, mit denen es einem Viewer möglich ist, die vollständige Präsentation und Funktionalität der jeweiligen digitalen Edition herzustellen.
            
            
               Manifestdatei
               Der Vorschlag für eine Manifestdatei, aus der die Funktionalitäten hergestellt werden können, lautet wie folgt (vgl. auch die nachstehende Tabelle):
               Ganz konkret sollten die notwendigen Metadaten der digitalen Edition definiert werden. Dabei kann darüber diskutiert werden, welche Informationen für eine digitale Edition notwendig und welche für wünschenswert gehalten werden.
               Mit der Definition einer Gliederung der Materialien, also von Editionstexten, Kommentaren und Begleitmaterial, können sinnvolle hierarchische Navigationselemente in der Darstellung umgesetzt werden.
               In der Präsentationsoberfläche muss es möglich sein, zu den verschiedenen Datentypen zu navigieren und entsprechende Überblickslisten anzeigen zu lassen. Dafür werden die Typen von Datenobjekten (etwa Textsorten oder Register) definiert. Für die Unterstützung von facettierten Filtern, müssen diese entsprechend festgelegt werden. 
               Die Darstellung der Dokumente in der Einzelansicht kann unter Einbindung von Schnittstellen für die Transformation geschehen.
                         Auch können für die Navigation zu Abschnitten im Dokument, die entsprechenden Teile definiert werden. Zu jedem Objekttyp sollten auch die vorhandenen Beziehungen zu Teilen, als auch zu anderen Objekttypen festgehalten werden. Damit können verschiedene Forschungsdaten in einer dynamischen Ansicht zusammengeführt werden.
                    
               Für eine nachhaltige Einbindung externer Ressourcen sollte definiert werden, auf welche Ressourcen die Edition Bezug nimmt und welche Schnittstellen dazu genutzt werden. Die Integration externer Ressourcen kann problematisch sein, wenn deren Verfügbarkeit noch nicht gesichert ist.
                        
               
               
                  
                     
                        Zu definierende Elemente
                     
                     
                        Zu definierende Eigenschaften
                     
                     
                        Steht in Beziehung zu
                     
                  
                  
                     
                        Metadaten
                     
                     Titelei, Impressum, Lizenzangaben
                     
                  
                  
                     
                        Gliederung der Edition
                     
                     Hierarchie, Verweise auf Objekte / Objekttypen
                     Objekttypen
                  
                  
                     
                        Gesamtschau
                     
                     Typ, Sortierungen
                     
                  
                  
                     
                        Filterdefinitionen
                     
                     
                        Name, Typ, ID
                        Kontext: Zugehöriger Objekttyp
                        Wurzel des Filterobjekts (XPath)
                        Pfad zum Filterwert (XPath oder XQuery)
                        Labelingfunktion (XPath oder XQuery)
                     
                     
                  
                  
                     
                        Dokumente und Objekte
                     
                     
                        Name, Typ
                        Schnittstelle: Zugang zu den Objekten
                        Objekt-Wurzel
                        Pfad zur Objekt-ID (XPath oder XQuery)
                        Pfad zum Objekttitel
                        Thumbnail: Funktionsdefinition für Kurzanzeige (HTML)
                     
                     Gesamtschau, Filter, Einzelpräsentation, Binnenstruktur, Beziehungen, Schnittstellen, Zitation
                  
                  
                     
                        Einzelpräsentation
                     
                     Schnittstelle: Formate und Transformationskripte (ODD)
                     
                  
                  
                     
                        Binnenstruktur
                     
                     
                        Name, ID
                        Wurzel der referenzierbaren Objektteile (XPath)
                        Pfad zur jeweiligen ID (XPath)
                        Titel und Thumbnail für die Darstellung in einer Liste
                     
                     
                  
                  
                     
                        Beziehungen
                     
                     
                        Name, ID
                        Subjekt, Prädikat, Objekt-Beziehungen
                     
                     Objekttypen, Teile von Objekttypen
                  
                  
                     
                        Schnittstellen
                     
                     Label
                                Pfad zu den Elementen (XPath)
                                Schnittstellen (URLs)
                            
                     
                  
               
               Tabelle 1: Definitionen für die Schnittstelle
            
         
         
            Prototyp
            Im Forschungprojekt „ediarum“ wird momentan ein Prototyp entwickelt, der das vorgestellte Konzept umsetzt und die Anforderungen der Darstellung erfüllen soll (Dumont/Fechner 2014 und 
                    http://www.bbaw.de/telota/software/ediarum
               ). Dieser enthält eine Programmbibliothek, die die Funktionalitäten zur Anzeige bereitstellt. Kern der spezifischen Darstellung einer digitalen Edition wird durch eine Manifestdatei nach obigem Konzept gebildet. Mit diesem Prototypen ist es bereits möglich mithilfe der Manifestdatei und wenigen Anpassungen, die vor allem das Layout betreffen, eine Webseite für eine Digitale Edition zu erstellen. Mit dem Einsatz des Prototyps für mehrere Editionen werden die einzelnen Funktionalitäten und Konfigurationsmöglichkeiten ausgetestet und verbessert. Schließlich soll er als Viewer zur Verfügung stehen, der zur Präsentation lediglich die Manifestdatei und Zugang zu den Daten über eine entsprechende Serverinfrastruktur benötigt. Durch das Zusammenspiel der Kernkomponente, die alle Funktionalitäten bereitstellt, und der projektspezifischen Komponente, die im Layout angepasst werden kann, wird eine hohe Flexibilität erreicht. Somit kann für jedes Projekt ein individueller Auftritt erzeugt werden.
                
            
               
               Abbildung 1: Manifestdatei des Prototyps
            
         
         
            Fazit
            Hier wird die Entwicklung einer neuen Standardschnittstelle vorgeschlagen, um die Nachhaltigkeit digitaler Editionen zu verbessern. Denn es braucht für die Langzeitarchivierung digitaler Editionen auch eine Standardisierung ihrer Funktionalitäten. Editionen sind zwar sehr unterschiedlich, doch mit dem hier beschriebenen Interface und dem zugehörigen Austauschformat wird beispielhaft ein praktikabler Ansatz vorgestellt, um diese Lücke zu schließen. Eine Weiterentwicklung des hier gemachten Entwurfs und die Integration weiterer Standards unter Einbindung unterschiedlicher Editionsprojekte kann in Zukunft die Unabhängigkeit digitaler Editionen von einzelnen technischen Systemen erhöhen und unterstützt damit die Langzeitarchivierung.
         
      
      
         
            
                    Ziele von Editionen finden sich bei (Förderkriterien 2015, Eggert 2016, Ralle 2016 und Sahle 2016).
                
            
                    Für die Forschungsdaten selbst ist die Langzeitarchivierung grundsätzlich gelöst. Digitalen Editionen wird jedoch nur geringe Zuverlässigkeit zugeschrieben  (Pierazzo 2015: 169).
                
            
                    So wird das Format der Text Encoding Initiative (TEI) verbreitet eingesetzt, das nur ein erster Schritt zur Standardisierung ist (Holmes 2017). Weitere Standards und Identifikatoren sind etwa die GND für Personen, GeoNames-IDs für Orte oder der Canocical Text Service (CTS) für Zitationen, das DITA- oder DocBook-Format für (technische) Dokumentationen. Als Langzeitarchivierungsformat für Metadaten gibt es etwa LMER (Steinke 2005).
                
            
                    Gute Ansätze einer Präsentationsoberfläche für Einzeldokumente bietet der "teiPublisher", der auf dem Datenformat ODD aufbaut (Meier 2017, Meier/Turska 2016 und Turska et al. 2016). 
                
            
                    Die Benutzbarkeit digitaler Editionen leidet unter mangelnden Interfaces (Robinson 2016). Pierazzo sieht es als Nachteil, dass viele digitale Editionen unterschiedliche User Interfaces besitzen, hält aber eine zukünftige Angleichung für wahrscheinlich (Pierazzo 2015: 162).
                
            
                    Es gibt dabei sehr unterschiedliche Anforderungen, die an die Präsentation digitaler Editionen gestellt werden (Shillingsburg 2016, Ralle 2016: 154f. und Sahle 2014).
                
            
                    Für die Langzeitarchivierung könnte eine Kompatibilität der Manifestdatei etwa mit LMER hergestellt werden (Steinke 2005).
                
            
                    TEI-Dokumente können etwa mit ODD zur Einzelpräsentation transformiert werden (Meier 2017).
                
            
                    Externe Daten können auch in eine "Standalone" Version überführt werden (Holmes 2017).
                
         
         
            Bibliographie
            
               Cramer, Tom (2011): The International Image Interoperability Framework (IIIF): Laying the Foundation for Common Services, Integrated Resources and a Marketplace of Tools for Scholars Worldwide. Blogpost. URL: 
                    https://www.cni.org/topics/information-access-retrieval/international-image-interoperability-framework
            
            
               Dumont, Stefan / Fechner, Martin (2014): “Bridging the Gap: Greater Usability for TEI encoding”, in: Journal of the Text Encoding Initiative 8. URL: 
                    http://jtei.revues.org/1242
            
            
               Eggert, Paul (2016): “The reader-oriented scholarly edition”, in: Digital Scholarship in the Humanities 31, 4: 797–810. DOI: 
                    10.1093/llc/fqw043
            
            Förderkriterien für wissenschaftliche Editionen in der Literaturwissenschaft. In: Informationen für Geistes- und Sozialwissenschaftler/innen (11) 2015.
            
               Holmes, Martin (2017): “Whatever happened to interchange?” In: Digital Scholarship in the Humanities 32, suppl_1: i63–i68. DOI: 
                    10.1093/llc/fqw048
            
            
               Meier, Wolfgang (2017): teiPublisher. The instant publishing toolbox. Version v2.2.0, Stand 8. September 2017. URL: 
                    http://teipublisher.com/index.html
            
            
               Meier, Wolfgang / Turska, Magdalena (2016): “TEI Processing Model Toolbox: Power To The Editor”, in: Digital Humanities 2016: Conference Abstracts: 936. URL: 
                    http://dh2016.adho.org/abstracts/401
            
            
               Pempe, Wolfgang (2012): „Geisteswissenschaften“, in: Langzeitarchivierung von Forschungsdaten: Eine Bestandsaufnahme. Hg. v. Heike Neuroth et al., Version 1.0, Stand 2012: 137-159. URN: 
                    urn:nbn:de:0008-2012031401
            
            
               Pierazzo, Elena (2015): Digital Scholarly Editing: Theories, Models and Methods. Abingdon, England.
                
            
               Porter, Dot (2016): "What is an edition anyway?" My Keynote for the Digital Scholarly Editions as Interfaces conference, University of Graz. Blogpost, in: Dot Porter Digital. URL: 
                    http://www.deporterdigital.org/?p=309
            
            
               Ralle, Inga Hanna (2016): „Maschinenlesbar – menschenlesbar. Über die grundlegende Ausrichtung der Edition“, in: Editio 30, 1: 144-156. DOI: 
                    10.1515/editio-2016-0009
            
            
               Robinson, Peter M. W. (2016): “Project-based digital humanities and social, digital, and scholarly editions”, in:
                    Digital Scholarship in the Humanities 31, 4: 875–889. DOI: 
                    10.1093/llc/fqw020
            
            
               Sahle, Patrik (2014): Kriterienkatalog für die Besprechung digitaler Editionen. Version 1.1, Stand Juni 2014. URL: 
                    https://www.i-d-e.de/publikationen/weitereschriften/kriterien-version-1-1/
            
            
               Sahle, Patrik (2016): “What is a Scholarly Digital Edition?” In: Digital Scholarly Editing: Theories and Practices: 19-40. DOI: 
                    10.11647/obp.0095.02
            
            
               Shillingsburg, Peter (2016): “Reliable social scholarly editing”, in: 
                    Digital Scholarship in the Humanities 31, 4: 890–897. DOI: 
                    10.1093/llc/fqw044
            
            
               Steinke, Tobias (Redaktion) (2005): LMER Langzeitarchivierungsmetadaten für elektronische Ressourcen.Version 1.2, Stand 7. April 2005. URN: 
                    urn:nbn:de:1111-2005041102
            
            
               Turska, Magdalena / Cummings, James / Rahtz, Sebastian (2016): Challenging the Myth of Presentation in Digital Editions, in: Journal of the Text Encoding Initiative 9. DOI: 
                    10.4000/jtei.1453
            
         
      
   



      
         
            Die literarische Netzwerkanalyse hat sich in den letzten Jahren zu einer gefragten Methode der digitalen Literaturwissenschaft entwickelt. Dabei rangiert die Größe der Arbeitskorpora im Sinne des »scalable reading« (Martin Mueller) von der Betrachtung von Einzeltexten (Schweizer/Schnegg 1998, Moretti 2011) über kleinere Korpora bis hin zur Untersuchung hunderter oder gar tausender Dramen (Fischer u. a. 2016, Trilcke u. a. 2016, Algee-Hewitt 2017). Dabei zeigt sich auch immer wieder Interesse an bestimmten, etwa autorzentrierten Subkorpora (Wade 2017).
            
         In diesem Kontext siedelt sich auch unser Posterprojekt an, in dessen Mittelpunkt die extrahierten Netzwerkdaten zu den Stücken des russischen Dramatikers Anton Tschechow (1860–1904) stehen. Die Datengrundlage bildet das von uns aufgebaute und betriebene Russian Drama Corpus (RusDraCor), das es sich zur Aufgabe gestellt hat, russischsprachige Stücke in der Zeitspanne zwischen den 1740er-Jahren (Sumarokow, Lomonossow u. a.) und den 1930er-Jahren (mit Texten von Autoren wie Majakowski oder Gorki) im TEI-Format zur Verfügung zu stellen (Fischer u. a. 2017). Neben Large-Scale-Analysen zur strukturellen Evolution des russischen Dramas ergibt sich so auch die Möglichkeit zur Betrachtung von nach verschiedenen Kriterien portionierten Teilkorpora, etwa der Stücke einzelner Autoren.
         Anton Tschechow gehört zu den meistgespielten russischen Dramatikern, dessen Werke bis heute inszeniert werden, gerade auch an deutschsprachigen Bühnen, vor allem seine vier letzten Stücke, »Die Möwe«, »Onkel Wanja«, »Drei Schwestern« und »Der Kirschgarten«. Von der Figurenkonstellation her haben diese Werke einen hohen Wiedererkennungswert: Es gibt keine wirklichen Protagonisten; die Redeanteile und Gesprächssituationen sind relativ gleichmäßig über eine Gruppe von Figuren verteilt. Dies zeigt sich sofort auch in den Netzwerkgraphen: Die Knoten (von denen jeder für eine Figur des jeweiligen Dramas steht) bilden einencharacter space
                , der bei der Visualisierung einem Polyeder gleicht. Die einzigen Figuren, die nicht am gemeinsamen Gesprächskreis teilhaben, sind die Diener und sonstige Gehilfen, deren Redeanteile sich auf Dialoge mit ihren direkten Weisungsbefugten beschränken. Dieses Sichtbarwerden der sozialen Zweiteilung des Dramenpersonals ist eine der Leistungen der Netzwerkvisualisierung. Zieht man die Werkchronologie als Größe hinzu, wird außerdem deutlich, wie sich die für Tschechow typischen Personenkonstellationen allmählich herausbilden, ab den frühen Stücken »An der Landstraße« (1884), »Iwanow« (1887) und »Der Waldteufel« (1889), über mehrere Kurzdramen oder Etüden wie »Der Bär«
                (1888), »Tragödie wider Willen« (1889) oder »Das Jubiläum« (1891), bis 1895 mit der »Möwe« die typische Tschechow’sche Charakterkonstellation gefunden ist.
            
         Die Beschaffenheit des Russian Drama Corpus erlaubt es, quantitative Analysen auch zugeschnitten auf bestimmte Figurengruppen zu beschränken, etwa gesondert nach Geschlecht oder sozialem Status. Bereits eine simple Worthäufigkeitsanalyse kann so etwa zeigen, dass weibliche und männliche Rollen in Tschechow-Stücken von den Redeanteilen und dem Vernetzungsgrad her vergleichbar sind (anders als etwa bei allen anderen Autoren im Korpus). Diese Verteilungsdiagramme sowie netzwerktheoretische Werte wie Dichte, Diameter, Clustering-Koeffizient und Average Path Length ergänzen die chronologisch sortierten Netzwerkvisualisierungen.
         Die im Poster geschaffene Übersicht über alle Tschechow-Dramen hat auch enzyklopädischen Charakter, enthält sie doch etwa alle Figuren im Kontext ihres Auftretens im Tschechow’schen Dramenkosmos. Der netzwerkanalytische Blick ist somit durchaus geeignet, als Brücke zur inhaltlichen Auseinandersetzung mit den Werken Tschechows zu dienen.
            
      
      
         
            
               Bibliographie
               
                  
                  Algee-Hewitt, Mark (2017): Distributed Character: Quantitative Models of the English Stage, 1500–1920. DH2017, Montréal. URL: 
                        >.
                    
               
                  Fischer, Frank; Göbel, Mathias; Kampkaspar, Dario; Kittel, Christopher; Meiners, Hanna-Lena; Trilcke, Peer; Vogel, Andreas (2016): Distant-Reading Showcase. 200 Years of Literary Network Data at a Glance. DHd2016, Leipzig. DOI: 
                        >.
                    
               
                  Fischer, Frank; Orlova, Tatyana; Skorinkin, Danil; Palchikov, German; Tyshkevich, Natasha (2017): Introducing RusDraCor – A TEI-Encoded Russian Drama Corpus for the Digital Literary Studies. CORPORA2017, St. Petersburg. Abstractband, S. 28–31.
                    
               
                  Schweizer, Thomas; Schnegg, Michael (1998): Die soziale Struktur der »Simple Storys«. Eine Netzwerkanalyse. URL: 
                        >.
                    
               
                  Trilcke, Peer; Fischer, Frank; Göbel, Mathias; Kampkaspar, Dario; Kittel, Christopher (2016): Dramen als ›small worlds‹? Netzwerkdaten zur Geschichte und Typologie deutschsprachiger Dramen 1730–1930. DHd2016, Leipzig. URL: 
                        >.
                    
               
                  Wade, Karen (2017): Jane Austen’s Social Networks. In: The Sea of Books, 4. Juli 2017. URL: 
                        >.
                    
            
         
      
   



      
         Das Projekt 
                Altägyptische Kursivschriften (AKU 2015) an der Akademie der Wissenschaften und der Literatur Mainz unter der Leitung von Prof. Dr. Ursula Verhoeven-van Elsbergen (JGU Mainz) in Kooperation mit Prof. Dr. Andrea Rapp (TU Darmstadt) besteht seit April 2015. Ziel ist es, in verschiedenen Modulen im Verlauf von maximal 23 Jahren eine digitale Paläographie zum Hieratischen und zu den Kursivhieroglyphen zu erstellen sowie verschiedene Aspekte der Kursivschrift-Kultur systematisch unter Einbeziehung digitaler Methoden zu untersuchen. 
            
         Im Alten Ägypten gab es neben den monumentalen und detailliert ausgeführten Hieroglyphen auch kursive (Hand-)Schriften, die als Hieratisch, Kursivhieroglyphen, Kursivhieratisch und Demotisch bezeichnet werden. Sie wurden mit Pflanzenstengeln und Rußtusche auf Papyrus, Leinen, Leder, Holz, Ton oder Stein geschrieben oder eingeritzt. Die Kursivschriften spielten unter Gelehrten, Priestern, Beamten und Schreibern eine wesentliche Rolle in den Bereichen der Kommunikation und Verwaltung, aber auch in der Dichtung, den Wissensgebieten sowie religiösen und funerären Texten. Das Hieratische war über 3000 Jahre lang in Gebrauch und wurde von den Schülern als erste Schriftart noch vor den Hieroglyphen erlernt.
         Bis heute ist die knapp 100 Jahre alte 
                Hieratische Paläographie von Georg Möller das Standardwerk für die ägyptologische paläographische Forschung (Möller 1909–1912). Er hat aus nur 32 gut datierten Schriftzeugnissen (vor allem Papyri) alle identifizierbaren Grapheme faksimiliert und in übersichtlichen Listen erfasst, die die Zeitspanne von der 5. Dynastie (ca. 2500 v. Chr.) bis zur römischen Kaiserzeit (3. Jh. n. Chr.) abdecken; die drei Bände bestehen aber zusammengenommen aus nur etwa 220 Seiten. Da ihm für manche Epochen nur sehr wenige oder gar keine Schriftquellen zur Verfügung standen, sind einige Zeiträume nicht oder nur unzureichend dokumentiert. Möller selbst betrachtete diese Listen als Vorarbeiten für weitergehende Untersuchungen, was er aber aufgrund seines frühen Todes nicht realisieren konnte. Erst ca. 70 Jahre später formulierte Posener (1973) seine Anforderungen an eine zukünftige paläographische Forschung und einen 
                nouveau Möller. Mit den damaligen technischen Voraussetzungen hätten die komplexen Anforderungen in Verbindung mit der Materialfülle selbst von einer Forschergruppe nicht erfüllt werden können. So erklären sich die zahlreichen Teilpaläographien, die in den nachfolgenden Jahrzehnten im Rahmen ägyptologisch-paläographischer Forschung entstanden sind (z. B. Goedicke 1988, Verhoeven 2001, Allen 2002, Lenzo 2011). Diese halten sich bis heute an das Prinzip von Möller, ordnen die Zeichen allerdings nach der Standardliste (
                Sign-list), die Gardiner in seiner 
                Egyptian Grammar (Gardiner 1927, 31973: 438-548) publiziert hat. Da bei Gardiner aber nicht alle hieroglyphischen Entsprechungen zu den 
                Hieratogrammen (Verhoeven 2001: 1) zu finden sind, kam es in den verschiedenen Teilpaläographien zu diversen Erweiterungen, die keinem einheitlichen Prinzip folgen und somit nicht eindeutig referenzierbar sind (Gülden 2016: 3).
            
         Digitale Ansätze zur Analyse von Handschriften beschreiben beispielsweise Stokes (2009) für das europäische Mittelalter und Quirke (2011) für die hieratische Schrift des Alten Ägypten. Das AKU-Projekt entwickelt erstmals eine Paläographiedatenbank, in der nach und nach das gesamte Zeichenrepertoire der altägyptischen Kursivschriften erfasst wird: ca. 600 Grapheme ¬– sowohl Laut- als auch Deutzeichen, Zahlen, Maße und Korrekturzeichen sowie Ligaturen, Zeichengruppen und besondere Orthographien (Verhoeven 2015: 32). Fü
         Während für alphabetische Schriften bereits zahlreiche Vorarbeiten im Bereich der Handschriftenerkennung vorliegen, muss dies für die Handschrift des Alten Ägypten erst entwickelt werden, um eine Grundlage für automatisierte Prozesse bei der Zeichenerkennung, -erfassung und -auswertung dieser komplexen Schrift zu ermöglichen.
         Zunächst werden die einzelnen Schriftzeichen (
                Hieratogramme) auf der Basis hochauflösender Digitalisate der Textträger faksimiliert (umgezeichnet). Diese werden sowohl als Vektor- und Rastergrafiken gespeichert. In der Datenbank, die in den nächsten Jahren als 
                open access online tool zur Verfügung stehen soll, werden sie kategorisiert und annotiert. Dadurch soll die Auswertung mit unterschiedlichen Verfahren (z. B. 
                shape matching, 
                image retrieval und 
                pattern recognition) ermöglicht werden. 
            
         Für die Hieratistik sind das vor allem Fragen zu Entwicklung und Diversität der Kursiven, Bezügen zur Hieroglyphenschrift sowie kontextuellen und funktionellen Anpassungen. Hinzu kommen Aspekte zur Schriftökonomie, Schreibrichtung, zu Abkürzungen, Diakritika und Ligaturen sowie zum Layout. Mithilfe von Clusteranalysen können Datierungen, Schreiberpersonen und regionale Unterschiede identifiziert werden. 
         Langfristig erhofft sich das Projekt mit den erfassten Zeichen zudem eine Basis für weitere automatisierte Verfahren (
                machine learning) zu schaffen, damit auch umfangreiche Textträger (bspw. verfügt ein 23 m langen Papyrus hochgerechnet über 140.000 Einzelzeichen) analysiert werden können. Für ein Repositorium, aber auch für Auswertungen, Visualisierungen und 
                linked open data, sollen die Daten in verschiedene Formate übertragen werden, z. B. in ein TEI konformes XML-Schema und als csv-Files.
            
      
      
         
            
               Bibliographie
               
                  Allen, James P. (2002): 
                        The Heqanakht papyri. Publications of the Metropolitan Museum of Art Egyptian Expedition 27. New York: Metropolitan Museum of Art.
                    
               
                  AKU (2015): 
                        Altägyptische Kursivschriften. Digitale Paläographie und systematische Analyse des Hieratischen und der Kursivhieroglyphen (AKU). Akademie der Wissenschaften und der Literatur Mainz http://aku.uni-mainz.de [letzter Zugriff 22. September 2017].
                    
               
                  Gardiner, Sir Alan (1927 
                        31973): 
                        Egyptian Grammar. Oxford 1927. Third edition. Oxford: Oxford University Press, 
                        31973.
                    
               
                  Goedicke, Hans (1988): 
                        Old Hieratic Paleography. Baltimore: Halgo.
                    
               
                  Gülden, Svenja A. (2016): „
                        Ein ‚nouveau Möller‘? Grenzen und Möglichkeiten. Ein working paper zum gleichnamigen Vortrag“. Hieratic Studies Online 1 urn:nbn:de:hebis:77-publ-557584.
                    
               
                  Gülden, Svenja A. / Krause, Celia / Verhoeven, Ursula (2017): „Prolegomena zu einer digitalen Paläographie des Hieratischen“ in: Fischer, Franz / Sahle, Patrick / Busch, Hannah (eds.):
                         Kodikologie & Paläographie im digitalen Zeitalter 4. Schriften des Instituts für Dokumentologie und Editorik 11. Norderstedt: Books on Demand 253-273 kups.ub.uni-koeln.de/7774/.
                    
               
                  Gülden, Svenja A. / Krause, Celia / Verhoeven, Ursula (im Druck): „Digital Palaeography of Hieratic“, in: Davies, Vanessa / Laboury, Dimitri (eds.):
                         Oxford Handbook of Epigraphy and Palaeography, Oxford: Oxford University Press.
                    
               
                  Lenzo, Giuseppina (2011): „Paleografia“, in: Roccati, Alessandro: 
                        Magica Taurinensia. Il grande papiro magico di Torino e i suoi duplicati. Analecta Orientalia 56. Roma: Gregorian & Biblical Press 193–255.
                    
               
                  Möller, Georg (1909–1912): 
                        Hieratische Paläographie. Die Aegyptische Buchschrift in ihrer Entwicklung von der fünften Dynastie bis zur Römischen Kaiserzeit I–III. Leipzig: J. C. Hinrichs, 1909–1912. I–IV: Leipzig: J. C. Hinrichs, 
                        21927–1936. Neudruck Osnabrück: Otto Zeller, 1965.
                    
               
                  Posener, Georges (1973): „L’écriture hiératique“, in: 
                        Textes et langages de l’Égypte pharaonique, cent cinquante années de recherches I
                        , 1822–1972. Bibliothèque d’Étude 64, 1. Le Caire: Institut français d’archéologie orientale 25-30.
                    
               
                  Quirke, Stephen (2011): „Agendas for Digital Palaeography in an Archaeological Context: Egypt 1800 BC“, in: Fischer, Franz et al. (eds.): 
                        Kodikologie und Paläographie im digitalen Zeitalter. Schriften des Instituts für Dokumentologie und Editorik 3. Norderstedt: Books on Demand 279-294 kups.ub.uni-koeln.de/4354/.
                    
               
                  Stokes, Peter (2009): „
                        Computer-Aided Palaeography, Present and Future“, in: Rehbein, Malte et al. (eds.): 
                        Kodikologie und Paläographie im digitalen Zeitalter. Schriften des Instituts für Dokumentologie und Editorik 2. Norderstedt: Books on Demand GmbH 310-338 kups.ub.uni-koeln.de/2978/.
                    
               
                  Verhoeven, Ursula (2001): 
                        Untersuchungen zur späthieratischen Buchschrift. Orientalia Lovaniensia Analecta 99. Leuven: Peeters.
                    
               
                  Verhoeven, Ursula (2015): „Stand und Aufgaben der Erforschung des Hieratischen und der Kursivhieroglyphen“, in: Verhoeven, Ursula (ed.): 
                        Ägyptologische „Binsen“-Weisheiten I–II, Neue Forschungen und Methoden der Hieratistik, Akten zweier Tagungen in Mainz im April 2011 und März 2013. Abhandlungen der Akademie der Wissenschaften und der Literatur Mainz, Einzelveröffentlichungen Nr. 14. Mainz und Stuttgart: Franz Steiner Verlag 23–63 urn:nbn:de:hebis:77-publ-547544.
                    
            
         
      
   



      
         
            Einleitung
            Der Beitrag diskutiert kritisch, welche nicht allein arbeitsökonomischen, sondern vor allem intellektuellen, methodologischen und wissenschaftstheoretischen "Kosten" mit der Digitalisierung und dem dadurch etablierten Fokus auf Fragen der Interoperabilität entstehen. Die deutlichen wissenschaftlichen Vorteile von Interoperabilität müssen nämlich einem Umbau (und z.T. Rückbau) in wissenschaftlichen Modellierungen und im Verständnis wissenschaftlicher Erkenntnisvermehrung gegenüber gestellt werden. Dabei wird im Sinne des doppelten Genitivs "Kritik der digitalen Vernunft" sowohl das naheliegende Phänomen diskutiert, dass die digitale Transformation Hoffnungen weckt und Lösungen wissenschaftlich-methodologischer Schwierigkeiten nahelegt, die sich (z.T. erst bei der Implementierung) als wissenschaftlich nicht akzeptabel erweisen, als auch der umgekehrte Fall, dass die digitale Transformation ein kritisches Umdenken in der Ausrichtung der eigenen wissenschaftlichen Arbeit und eine Umorientierung wissenschaftlicher Ambitionen erzwingt.
            Diese Diskussion wird im Durchgang durch einige beispielhafte Entwicklungen im seit vier Jahren laufenden Projekt "Die Schule von Salamanca" geführt, bevor ein Versuch der Verallgemeinerung unternommen wird. Dass Interoperabilität signifikante wissenschaftliche Kosten mit sich bringt, heißt im Übrigen nicht, dass diese nicht womöglich durch die Vorteile aufgewogen würden. Es ist allerdings in der Orientierung der Projektarbeit wichtig, sich an den „Grenzen der Interoperabilität“ abzuarbeiten und sich regelmäßig beide Seiten dieser Bilanz vorzulegen.
         
         
            Interoperabilität
            Mit der digitalen Transformation sind die Möglichkeiten des überregionalen und interdisziplinären Austauschs und der Weiterverwendung von Forschungsdaten in einer ganz neuen Weise möglich geworden. So hat sich der Begriff der Interoperabilität als zentrales Paradigma in den Digital Humanities etabliert, um den Anspruch zu beschreiben, diese Möglichkeiten methodisch auszubauen und ein Evaluationskriterium für Forschungsleistungen und -ergebnisse anzubieten. Interoperabilität ist auf mehreren Ebenen zu verstehen (vgl. Gradmann 2009) und wird auf den konkreten technischen und syntaktischen Ebenen vor allem durch Standards für Datenformate und Schnittstellen, sowie auf der pragmatischen Ebene u.a. durch den Bezug auf Normdaten realisiert. Mag Interoperabilität auf der semantischen Ebene schließlich vor einiger Zeit noch als ein utopisches Ziel angemutet haben (vgl. Baumann 2011), so haben sich etwa durch die Selbstreflexion digitaler geisteswissenschaftlicher Forschungs-Arbeit (Hughes et al. 2016) oder in der Beschreibung von kulturellen und geistigen Phänomenen (Le Boeuf et al. 2017; Stead et al. 2015) deutliche Fortschritte in der Modellierbarkeit geisteswissenschaftlicher Phänomene und in der Verfügbarkeit und Relationierbarkeit von semantischen Forschungsdaten und ‑ergebnissen ergeben.
            Wie Forschungsprojekte den derart gewachsenen Interoperabilitätserwartungen gerecht werden können, kann jedoch kaum allgemein, sondern nur im Rahmen ihrer jeweils spezifischen Arbeit ermittelt werden (zu einer projektunabhängig formulierten Handreichung vgl. aber Beer et al. 2014). Dabei stellt sich die Frage nach dem Nutzen, den Grenzen und den Kosten interoperabler Techniken im Projekt "Die Schule von Salamanca" insofern auf besondere Weise, als dieses einerseits über einen langen Zeitraum und mit Blick auf eine langzeitverfügbare Erschließung von relativ großen (Text-)Datenmengen operiert, andererseits aber auch die Erforschung innovativer Textaufbereitungsverfahren und Webanwendungsfunktionen zur Ermöglichung neuartiger Forschungserkenntnisse für die beteiligten Fachwissenschaften anstrebt.
         
         
            Das Projekt
            Im durch die Akademie der Wissenschaften und der Literatur | Mainz geförderten und insgesamt auf 18 Jahre angelegten Projekt "Die Schule von Salamanca. Eine digitale Quellensammlung und ein Wörterbuch ihrer juristisch-politischen Sprache" (Duve et al. 2013) werden voraussichtlich insgesamt etwa 120 Texte der gleichnamigen Schule iberischer Theologen und Juristen des 16. und 17. Jahrhunderts nach und nach digitalisiert und als Volltexte erfasst. Die in TEI-XML ausgezeichneten und aufwändig normalisierten Texte werden nicht nur strukturell, sondern auch im Hinblick auf als Linked Open Data (LOD) referenzierbare Entitäten – etwa Personennamen – erschlossen; dabei werden die Text-Digitalisate, die Volltexte und die LOD-Datensammlungen online bereit gestellt. Hinzu kommt ein digitales (und schließlich auch gedrucktes) Wörterbuch, in dem sowohl biographische Informationen zu den in der Edition vertretenen Autoren als auch zentrale Begriffe der Rechts- und politischen Ideengeschichte und deren Entwicklung im Diskussionszusammenhang der "Schule von Salamanca" erfasst werden. In diesen beiden Säulen des Projekts ist die Erschließung und Repräsentation der Struktur der internen Verweise (Autoren, die sich wechselseitig zitieren, Wörterbuchartikel, die auf Textstellen verweisen) eines der zentralen wissenschaftlichen Ergebnisse.
            Die sowohl für die Benutzung als auch für die Bereitstellung der Daten als zentrales Portal dienende Webseite des Projekts (
                    https://salamanca.school/) ist technisch in Form einer komplex modularisierten Webanwendung implementiert, die fachwissenschaftlichen BenutzerInnen eine Vielzahl von Funktionen bieten soll, z.B. eine geräteübergreifende und performante Darstellung der Editionstexte, eine intelligente Suchfunktion, die den frühmodern-lateinischen und -spanischen Volltexten angepasst ist, und eine feinkörnige Referenzierung der Texteinheiten. Durch technische Mechanismen (content negotiation, API, RESTful Microservice-Architektur), Exportfunktionen (abschnitts-, text- oder corpusweise, plaintext-, TEI- oder andere Formate) und die Orientierung an Formatstandards wird Anforderungen der Interoperabilität explizit Rechnung getragen. Der Quelltext der Webanwendung wird bis Januar 2018 ebenfalls veröffentlicht und fortan in Open Source weiterentwickelt werden.
                
         
         
            Interoperabilitäts-"Konflikte": Einige Beispiele
            Einhergehend mit der Open Source-Veröffentlichung der Webanwendung soll über einige repräsentative Aspekte der Projektarbeit reflektiert werden, in denen sich an vermeintlich technischen Herausforderungen Konflikte um die wissenschaftlichen Implikationen von Interoperabilität entzünden:
            
               a) Textrepräsentation:
               Während in diesem Kernbereich der Projektarbeit gegenwärtig noch mit einem eigens spezifizierten und den bisherigen Forschungsanforderungen entsprechenden "idiosynkratischen" TEI-Format gearbeitet wird und die Datenmodellierung somit eher als "research-driven" (vgl. Flanders/Jannidis 2016: 233) bezeichnet werden kann, wird eine langfristige Verfügbarmachung der Texte in einem interoperableren, "curation-driven" Format wie etwa 
                        TEI Simple (Text Encoding Initiative Consortium 2016) erwägt. Dies ist nicht zuletzt auch durch den Blick auf die Wissenschaftsförderung motiviert (vgl. etwa DFG 2015, Anhang), die die Interoperabilität von Textauszeichnungen dem erst noch nachzuweisenden Erkenntnisgewinn 'reicher' Annotationen gegenüberstellt. Nun beinhaltet das XML-Ökosystem der Edition allerdings auch Informationen wie etwa Metadaten zur Zeichenkodierung (die nicht zuletzt Interoperabilitäts-Funktionen erfüllen), die sich nach derzeitigem Stand nicht ohne weiteres in 
                        TEI Simple abbilden lassen. So ergibt sich eine nur durch einigen Aufwand aufzulösende Spannung zwischen detaillierter wissenschaftlicher Gegenstandsbeschreibung und Maßnahmen zur Förderung der Nachnutzbarkeit der eigenen Ressourcen (z.B. des Angebots mehrerer alternativer Datenformate), und es stellt sich die Frage, wann und durch wen jener Aufwand erbracht werden soll. Selbst eine gegenüber den "Experten"-Annotationen tolerantere Zielvorgabe, die sich etwa am Konzept des 'Interchange' (als einer durch menschliche Interpretation vermittelten Nachnutzung, vgl. Holmes 2017, Baumann 2011) orientiert und im Vergleich zum Modell einer bruchlosen Weiterverwertbarkeit der Daten durch automatische Prozesse gemäßigtere Ansprüche erhebt, sieht sich ähnlichen Fragen der Standardisierung von Schnittstellen und Datenformaten ausgesetzt.
                        
            
            
               b) Modulare Infrastruktur:
               Im Zuge der Einrichtung einer Linked Open Data-Infrastruktur haben wir 
                        content-negotiation-Mechanismen, Weiterleitungen und die Adressierung unterschiedlicher Funktionen über verschiedene Server und Server-Adressen eingeführt. Diese Entwicklung legt eine Fortsetzung nahe, die den Umbau der Web-Anwendung insgesamt in ein Ensemble von Microservices bedeuten würde (vgl. Wolff 2016), in dem Daten und Dienste aufs Engste verschränkt sind. Während dies die Interoperabilität, d.h. konversions- und barrierearme Nachnutzbarkeit der Daten enorm verbessert (etwa dadurch, dass auf verschiedenen Ebenen in verschiedenen Formaten und verschiedenen Granularitäten Daten und Dienste verfügbar sind, vgl. Turska et al. 2016) hat es jedoch auch den Nachteil, dass der Daten- und Anwendungszusammenhang – der eben auch 
                        als Zusammenhang eine Forschungsleistung darstellt – nur in einer sehr viel aufwändigeren Weise repliziert und ggf. archiviert werden kann: Während erste Infrastrukturen die Archivierung und langfristige Zugänglichkeit von Javascript-Anwendungen erlauben sollen (vgl. Bingert/Buddenbohm 2016; kritischer: Brunelle 2016), so ist dies für eine solche Infrastruktur mit mehreren kooperierenden und kommunizierenden Servern nur sehr viel schwerer vorstellbar.
                    
            
            
               c) Adressierung, Versionierung und Persistenz:
               Im Zusammenhang mit der Adressierung von einzelnen Textpassagen haben wir ein System eingeführt, das sowohl semantische Aussagen über Text-Entitäten als auch die Realisierung der Verweisstrukturen auf implementierungs- und plattformunabhängige, intellektuell intuitive Weise erlaubt (im Anschluss an das Canonical Text Services-Schema, Blackwell/Smith 2014; vgl. Wagner 2016). Diese Struktur der komplexen Verknüpfung von Ressourcen und Entitäten untereinander verträgt sich aktuell nicht mit etablierten Methoden, Dokumente persistent zu identifizieren und die Überarbeitungshistorie der Ressourcen in einer Versionierung transparent und nachvollziehbar zu machen. Diese Methoden spezifizieren nämlich zumeist den Umgang mit einem ganzen Dokument und vernachlässigen den Bedarf, Entitäten innerhalb des Dokuments (persistent) zu referenzieren oder die Einbettung jenes Dokuments in ein Corpus zu verwalten. Wenn beispielsweise ein Dokument verändert wird, muss es unter anderem normalerweise eine neue persistente ID erhalten - damit entsteht ein Aktualisierungs- oder mindestens Kontrollbedarf bei den Querverweisen innerhalb des Dokuments sowie in allen weiteren Dokumenten des Corpus (und in unserem Falle in allen Artikeln des Wörterbuchs), welche Verweise auf das aktuelle Dokument enthalten. Mechanismen wie das Memento framework (van de Sompel/Nelson 2015) bieten eine transparente und flexible Versionierung, die für Web-Dokumente wie für Semantische Ressourcen gleichermaßen funktioniert, sind jedoch in ihrer Integration mit PID-Systemen ebenfalls noch nicht erprobt. Versteht man diese Motive in einem Interoperabilitäts-Zusammenhang, lesen sie sich wie ein Konflikt zwischen etablierten Lösungen auf verschiedenen Interoperabilitäts-Ebenen und es fragt sich, wie kanonische Verweissysteme (funktional-pragmatische Ebene nach Gradmann 2009) und Persistent Identifiers (technische Ebene) miteinander harmonieren können.
            
         
         
            Diskussion
            Die hier am Beispiel digitaler Projektarbeit aufgezeigten Probleme etwa einer gesteigerten Spannung zwischen „Interoperabilität und Expressivität“ (Baumann 2011) weisen auf grundlegendere wissenschafts- und erkenntnistheoretische Fragestellungen hin, die nicht zuletzt das Selbstverständnis der Digital Humanities und die mit ihnen verbundenen Hoffnungen berühren: Ist Expertenwissen am Ende vielleicht gar nicht "interoperabel"? Erweisen sich die Möglichkeiten, durch Verknüpfung digitaler Forschungsdaten und ‑anwendungen verschiedenster Kontexte neuartige Einsichten zu generieren, als stark begrenzt? Eine wissenschaftspolitische Pointe des Beitrags besteht so darin, auf die entscheidende Rolle aufmerksam zu machen, die die Erwartung darüber spielt, an welchem Ort – zwischen den Disziplinen oder “tief” in den fachwissenschaftlichen Spezialdiskursen – wissenschaftlicher Fortschritt erzielt wird.
         
      
      
         
            
               Bibliographie
               
                  Baumann, Syd (2011): “Interchange vs. Interoperability”, in: 
                        Proceedings of Balisage: The Markup Conference 2011 7, 
                        https://
                  
                     doi
                  
                  .org/
                  10.4242/BalisageVol7.Bauman01 [letzter Zugriff: 13.01.2018].
                    
               
                  Beer, Nikolaos / Herold, Kristin / Kolbmann, Wibke / Kollatz, Thomas / Romanello, Matteo / Rose, Sebastian / Walkowski, Niels-Oliver (2014): "Interdisciplinary Interoperability", DARIAH-DE Working Papers Nr. 3, 
                        http://nbn-resolving.de/urn.nbn.de.gbv:7-dariah-2014-1-0 [letzter Zugriff: 18.01.2018].
                    
               
                  Bingert, Sven / Buddenbohm, Stefan (2016): "Die HDC-Anwendungskonservierung - ein Dienst zur Archivierung und Bereitstellung komplexer Forschungsergebnisse", in: 
                        GWDG-Nachrichten 11/2016: 7-9 
                        https://www.gwdg.de/documents/20182/27257/GN_11-2016_www.pdf [letzter Zugriff: 23.09.2017].
                    
               
                  Blackwell, Christopher / Smith, Neel (2014): "The Canonical Text Services protocol, version 5.0.rc.2", 
                        https://cite-architecture.github.io/cts_spec/ [letzter Zugriff: 25.09.2017].
                    
               
                  Brunelle, Justin F. / Kelly, Mat / Weigle, Michele C. / Nelson, Michael L. (2016): "The impact of JavaScript on archivability", in: 
                        International Journal on Digital Libraries 17/2: 95–117, 
                        https://doi.org/10.1007/s00799-015-0140-8 [letzter Zugriff: 25.09.2017].
                    
               
                  DFG (2015): "Förderkriterien für wissenschaftliche Editionen in der Literaturwissenschaft", 
                        http://www.dfg.de/download/pdf/foerderung/grundlagen_dfg_foerderung/informationen_fachwissenschaften/geisteswissenschaften/foerderkriterien_editionen_literaturwissenschaft.pdf [letzter Zugriff: 12.1.2018].
                    
               
                  Duve, Thomas / Lutz-Bachmann, Matthias / Birr, Christiane / Niederberger, Andreas (2013): "Die Schule von Salamanca: eine digitale Quellensammlung und ein Wörterbuch ihrer juristisch-politischen Sprache. Zu Grundanliegen und Struktur eines Forschungsvorhabens". Mainz: Akademie der Wissenschaften und der Literatur. 
                        http://nbn-resolving.de/urn/resolver.pl?urn:nbn:de:hebis:30:3-324011 [letzter Zugriff: 12.1.2018].
                    
               
                  Flanders, Julia / Jannidis, Fotis (2016): "Data Modeling", in: Schreibman, Susan / Siemens, Ray / Unsworth, John (eds.): 
                        A New Companion to Digital Humanities. Chichester: Wiley Blackwell 229-237.
                    
               
                  Gradman, Stefan (2009): "Interoperability. A key concept for large scale, persistent digital libraries". 
                        Digital Preservation Europe
                  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.363.6311&rep=rep1&type=pdf [letzter Zugriff: 25.09.2017].
                    
               
                  Holmes, Martin (2017): "Whatever happened to Interchange?", in: 
                        Digital Scholarship in the Humanities 32 (Issue suppl_1): i63–i68 
                        https://doi.org/10.1093/llc/fqw048 [letzter Zugriff: 25.09.2017].
                    
               
                  Hughes, Lorna / Constantopoulos, Panos / Dallas, Costis (2016): “Digital Methods in the Humanities: Understanding and Describing their Use across the Disciplines“, in: Susan Schreibman / Ray Siemens / John Unsworth (eds.), 
                        A New Companion to Digital Humanities, Wiley & Sons, 150–170, 
                        https://doi.org/10.1111/b.9781118680643.2016.00013.x [letzter Zugriff: 13.1.2018].
                    
               
                  Le Boeuf, Patrick / Doerr, Martin / Ore, Christian Emil / Stead, Stephen et al. (2017): “Definition of the CIDOC Conceptual Reference Model”, Version 6.2.2, 
                        http://www.cidoc-crm.org/sites/default/files/2017-09-30%23CIDOC%20CRM_v6.2.2_esIP.pdf
                         [letzter Zugriff: 13.1.2018].
                    
               
                  Schmidt, Desmond (2014): “Towards an Interoperable Digital Scholarly Edition”, in: 
                        Journal of the Text Encoding Initiative 7, 
                        https://doi.org/10.4000/jtei.979 [letzter Zugriff: 13.1.2018].
                    
               
                  Stead, Stephen / Doerr, Martin et al. (2015): “CRMinf: the Argumentation Model. An Extension of CIDOC-CRM to support argumentation”, 
                        
                     http://www.cidoc-crm.org/crminf/sites/default/files/CRMinf-0.7%28forSite%29.pdf
                   [letzter Zugriff: 13.1.2018].
                    
               
                  Text Encoding Initiative Consortium (2016): "TEI Simple" 
                        https://github.com/TEIC/TEI-Simple [letzter Zugriff: 25.09.2017].
                    
               
                  Turska, Magdalena / Cummings, James / Rahtz, Sebastian (2016): "Challenging the Myth of Presentation in Digital Editions", in: 
                        Journal of the Text Encoding Initiative 9 
                        http://jtei.revues.org/1453 [letzter Zugriff: 25.09.2017].
                    
               
                  van de Sompel, Herbert / Nelson, Michael L. (2015): "Reminiscing about 15 Years of Interoperability Efforts", in: 
                        D-Lib Magazine 21 (11/12) 
                        http://www.dlib.org/dlib/november15/vandesompel/11vandesompel.html [letzter Zugriff: 25.09.2017].
                    
               
                  Wagner, Andreas (2016): "What’s in a URI? Part I: The School of Salamanca, the Semantic Web and Scholarly Referencing" 
                        https://blog.salamanca.school/2016/11/15/whats-in-a-uri-part-1/ [letzter Zugriff: 22.09.2017].
                    
               
                  Wolff, Eberhard (2016): "Microservices. Grundlagen flexibler Softwarearchitekturen". Heidelberg: dpunkt.verlag.
            
         
      
   



      
         
            Ausgangslage
            Digitale Editionen haben sich bereits als geeignete Publikationsform für die Präsentation von umfangreichen Textbeständen im Bereich des kulturellen Erbes etabliert. Für ein so umfangreiches und komplexes textgenetisches Korpus wie Robert Musils literarischen Nachlass und die daran entwickelte Datenstruktur der Klagenfurter Ausgabe stehen jedoch keine fertigen Modelle zur Verfügung. Im Rahmen des Panels soll einerseits diskutiert werden, welche Kriterien eine digitale Edition erfüllen muss, um eine Grundlage zur Erforschung von Robert Musils Gesamtwerk zu erstellen und andererseits, ob die derzeit geltenden Standards zur Langzeitarchivierung, interoperablen Repräsentation und Online-Kommentierung von digitalen Textkorpora noch zeitgemäß sind. 
         
         
            Gegenstand
            Der umfangreiche literarische Nachlass des österreichischen Schriftstellers Robert Musil umfasst 12.000 Manuskriptseiten und wird bereits seit 1985 digital ediert. Die wichtigsten bisherigen Publikationsetappen markieren die CD-ROM-Ausgabe 
                Robert Musil: Der literarische Nachlass (Hg. F. Aspetsberger, K. Eibl, A. Frisé, Rowohlt 1992) und die DVD-Edition Robert Musil: 
                Klagenfurter Ausgabe. Kommentierte Edition sämtlicher Werke, Briefe und nachgelassener Schriften. Mit Transkriptionen und Faksimiles aller Handschriften (Hg. W. Fanta, K. Amann, K. Corino, Robert-Musil-Institut/Kärntner Literaturarchiv, AAU Klagenfurt 2009). Um für das Editionsvorhaben, das mittlerweile mehrere Computer-Generationen, System- und Formatwechsel sowie Veränderungen der editorischen Vorgaben und Richtlinien erlebt hat, eine befriedigende und zukunftssichere Lösung zu entwickeln, entsteht am Robert-Musil-Institut/Kärntner Literaturarchiv seit 2016 eine Hybrid-Edition. Sie soll einerseits die Bedürfnisse der LeserInnen und der wissenschaftlichen UserInnen befriedigen, andererseits auch für nachhaltige Verfügbarkeit der Daten sorgen. Die TeilnehmerInnen des vorliegenden Panels präsentieren die einzelnen Lösungsansätze aus theoretischer und methodologischer Sicht in exemplarischer Weise und stellen sie als Best-Practice-Modelle im Bereich des digitalen Edierens zur Diskussion: 
                
            a) Die 
                    Hybrid-Edition setzt sich aus der 
                    Musil-Gesamtausgabe in 12 Bänden (Salzburg, Jung und Jung, 2016-2022) und dem Internetportal 
                    musilonline (Prototyp seit 2016: 
                    www.musilonline.at) zusammen. Die Buchausgabe enthält einen Lesetext für die literarische Lektüre in leserfreundlicher Ausstattung. 
                    musilonline wird unter 
                    Musil-Text die digitale Version des Lesetexts, unter 
                    Archiv das gesamte textgenetische Dossier (Faksimiles, XML/TEI-Dateien) und unter 
                    Kommentar neben dem textkritischen bzw. textgenetischen auch einen interdiskursiven Kommentar bieten. Im Kurzvortrag wird die Hybrid-Edition in Hinblick auf ihre medienhistorische Bedeutung, intermediale Funktion, literaturdidaktische Vermittlungsleistung und Leser/User-Orientierung erläutert. (Anke Bosse, Robert-Musil-Institut/Kärntner Literaturarchiv, AAU Klagenfurt)
                
            b) Das künftige 
                    Interface für die literaturwissenschaftliche, insbes. textgenetische Forschung auf 
                    musilonline soll sich aus folgenden Komponenten zusammensetzen: 
                
            
               
                  Suchmaschinen (zur Treffergenerierung im edierten Musil-Textkorpus sowie im XML/TEI-ausgezeichneten Text- und Metadatenbereich)
                    
               
                  Navigation entlang hypertextueller Verknüpfungen zwischen ediertem Text, Faksimiles, XML/TEI-Dateien und Kommentarbereich
                    
               
                  Bild-Browser (zum Studium der Originalmanuskripte)
                    
               
                  Textdarstellung (zur visuellen Inszenierung der Textstufen und ‑schichten am Manuskript, aus XML/TEI-Dateien generierte HTML-Lösungen). Diese Funktionen werden derzeit auf der Grundlage von zwei zentralen Manuskriptmappen aus Musils Nachlass entwickelt. 
                    
            
            Im Kurzvortrag werden noch keine fertigen Lösungen vorgestellt, sondern es erfolgt ein kritischer Aufriss der Problemlage in Folge der komplexen Struktur von Musils Manuskripten und die Präsentation eines Grundkonzepts an Hand von exemplarischen Ausschnitten aus dem Manuskriptbestand. (Walter Fanta, Robert-Musil-Institut/Kärntner Literaturarchiv, AAU Klagenfurt)
            c) Die fachgerechte 
                    Speicherung des gesamten Text- und Metadatenbestands zu Robert Musils autor-autorisierten und nachgelassenen Schriften 
                    via XML/TEI ist vorgesehen und bereits begonnen worden. Die Manuskripte des Musil-Nachlasses stellen eine besondere Herausforderung für die Textauszeichnung dar, weil sie äußerst komplexe Varianzbeziehungen auf der Ebene der Makrovarianz zwischen den Entwurfsfassungen und der Ebene der Mikrovarianz – Korrekturschichten – enthalten. Es stellt sich heraus, dass die Struktur der großen philosophischen und literarischen Fragmente der Moderne (Nietzsche, Wittgenstein, Musil, Bachmann) den Rahmen sprengt, der von XML/TEI (Baumstruktur) vorgegeben ist. Im Kurzvortrag erfolgt ein Problembericht zu Auszeichnungsschwierigkeiten mit XML/TEI. Im Rahmen des österreichischen Kompetenzzentrums für Digitale Edition (KONDE), sowie mit Hilfe der TEI Guidelines und DariahTeach wurden bereits Lösungen gefunden. In Hinblick auf die Datenkonservierung und –interoperabilität muss aber diskutiert werden, ob XML/TEI für die Langzeitarchivierung des Musil-Nachlasses geeignet ist. (Katharina Godler, Robert-Musil-Institut/Kärntner Literaturarchiv, AAU Klagenfurt)
                
            d) Die 
                    Migration des gesamten Textdaten-Korpus erfolgt aus dem Flatfile des Formats FolioViews der Klagenfurter Ausgabe in das Zielformat XML/TEI. Dabei werden Scripts entwickelt, welche die im Flatfile enthaltenen Kodierungen (Formatierungen und Sprungverknüpfungen von FolioViews) sowie die diakritischen Zeichen der 1992 publizierten Transkription soweit wie möglich automatisch im automatischen Austausch in sachadäquate XML/TEI-Auszeichnungen umsetzen. Der Kurzvortrag erläutert den erreichten Stand und die Probleme dieser Migrationsprozesse; sie bestehen kurz gesagt im Alter der Nachlass-Transkription (entstanden 1984-1990), in der chaotischen Struktur der FolioViews-Infobase mit insgesamt 735.000 Einträgen und ca. 250.000 Verknüpfungen, zahlreichen Redundanzen, Inkonsistenzen, Fehlern und Ergänzungsbedarf. Für die drei Hauptbereiche gedruckte Quellen, Nachlassmanuskripte, Metadaten müssen jeweils eigene Lösungen gefunden werden. (Gerrit Brüning, Goethe-Universität Frankfurt / Freies Deutsches Hochstift)
                
            e) Der 
                    interdiskursive Online-Kommentar auf 
                    musilonline wird 2018-2022 in einem vom Österreichischen Wissenschaftsfonds (FWF) geförderten Projekt am Robert-Musil-Institut / AAU Klagenfurt entwickelt. Die Grundidee besteht darin, über die herkömmliche (textkritische) Erläuterungsfunktion von Kommentaren hinaus zu wirken, auf der Bedeutungsebene anzusetzen und die Bedeutungsvielfalt in Musils Texten dadurch zu bewahren, dass Interpretamente im Musil-Textkorpus identifiziert und mit den Deutungen der bisherigen Interpretationsliteratur verknüpft werden. Aus der Sicht der Digital Humanities stellt sich die Herausforderung, für die Online-Inszenierung der Diskurse um Musils Texte eine digitale Struktur zu finden, die den heterogenen, teils widersprüchlichen Anforderungen und Erwartungen unterschiedlicher Usergruppen Genüge tut. Im Kurzvortrag wird das Vorhaben mit Fokus auf den Online-Kommentar als Desiderat der Literaturvermittlung exemplarisch skizziert. (Artur Boelderl, Institut für Germanistik, AAU Klagenfurt)
                
            Das Panel integriert höchst unterschiedliche Bereiche und Aspekte der Digital Humanities, die sich im Projekt 
                    musilonline verknüpft finden. Der Stoßrichtung der Tagung, eine 
                    Kritik der digitalen Vernunft zu formulieren, tragen die geplanten Diskussionsbeiträge in besonderem Maße Rechnung, da nicht in allen Belangen schon mit fertigen Lösungen aufgewartet wird, sondern das Erkennen von Problemen, die Verbesserung etablierter Strukturen, die Suche nach Alternativen und die Erfindung neuer Konzepte im Vordergrund stehen. Die Präsentationen und Diskussionen des Panels reflektieren u.a. auch die neuen editionswissenschaftlichen Forschungsansätze und Best Practices, die im Rahmen des österreichischen Kompetenznetzwerks digitale Edition (KONDE) seit 2017 entwickelt werden.
                
         
      
      
         
            
               Bibliographie
               
                  Burghart, Marjorie (2017): 
                        Creating a Scholarly Digital Edition with the Text Encoding Initiative. Demm: 
                        https://www.digitalmanuscripts.eu/digital-editing-of-medieval-texts-a-textbook/ (letzer Zugriff 12. Jänner 2018)
                    
               
                  Burnard, Lou (2014): 
                        What Is the Text Encoding Initiative? How to Add Intelligent Markup to Digital Resources. Encyclopédie Numérique. Marseille: OpenEdition Press: 
                        http://books.openedition.org/oep/426 (Letzter Zugriff 12. Jänner 2018)
                    
               
                  Fanta, Walter (2016): 
                        "Editionsgeschichte.",in: 
                        Nübel, Birgit/ Wolf, Norbert Christian:
                        Robert-Musil-Handbuch. Berlin: Walter de Gruyter, S. 799-810.
                    
               
                  Fanta, Walter (2016): 
                        "Nachlass.",in: 
                        Nübel, Birgit/ Wolf, Norbert Christian (eds):
                        
                  Robert-Musil-Handbuch. Berlin: Walter de Gruyter, S. 470-497.
                    
               
                  Fanta, Walter (2010): 
                        "Robert Musil – Klagenfurter Ausgabe.",in: 
                        editio, S. 117-148.
                    
               
                  Fanta, Walter (2011): 
                        "Zur Immortalität elektronischer Korpora am Beispiel der Musil-Edition.",in: 
                        Braungart, Georg / Gendolla, Peter /
                         Jannidis, Fotis (eds): 
                        Jahrbuch für Computerphilologie online: 
                  http://computerphilologie.tu-darmstadt.de/jg09/fanta.html (letzter Zugriff 12. Jänner 2018)
                    
               
                  Fanta, Walter (2008): 
                        "Das Zögern vor dem letzten Schritt. Zur digitalen Edition von Robert Musils „Mann ohne Eigenschaften", in: 
                        Golz, Jochen /
                         Koltes, Manfred (eds.): 
                        Autoren und Redaktoren als Editoren - Tagung der Arbeitsgemeinschaft für germanistische Edition an der Klassik Stiftung Weimar, Tübingen: Max Niemeyer Verlag, S. 342-352
                    
               
                  Pierazzo, Elena (2015): 
                        Digital Scholarly Editing: Theories, Models and Methods. Farnham, Surrey; Burlington, VT: Ashgate.
                    
               
                  Sperberg-McQueen, C.M. / Burnard, Lou (2017): 
                        TEI P5: Guidelines for Electronic Text Encoding and Interchange. Text Encoding Initiative Consortium.
                    
            
         
      
   



      
         
            Einleitung
            Inwieweit verändert der Einsatz digitaler Verfahren die sozialwissenschaftliche Forschung? Diese Frage lässt sich am Beispiel der Digitalisierung des Zettelkastens Niklas Luhmanns stellen, die im Rahmen des Forschungsprojektes “Niklas Luhmann - Theorie als Passion. Wissenschaftliche Erschließung und Edition des Nachlasses” erfolgt. Das Langzeitvorhaben (2015-2030), in dem die Fakultät für Soziologie der Universität Bielefeld mit dem Cologne Center for eHumanities kooperiert, wird im Akademienprogramm durch die Nordrhein-Westfälische Akademie der Wissenschaft und der Künste gefördert.
                
            
               Luhmann (1927-1998) zählt zu den bedeutendsten Soziologen des 20. Jahrhunderts. Im Laufe seiner 35-jährigen Forschungstätigkeit entwickelte er eine universale Sozial- und Gesellschaftstheorie, die er in annähernd fünfzig Monographien und 500 Aufsätzen publiziert hat. Als Basis für diese erstaunliche Produktivität diente Luhmann ein Zettelkasten, den er über vierzig Jahre lang systematisch gefüllt und gepflegt hat. Im Zuge der seit 2015 laufenden Nachlasserschließung wurden die ca. 90.0000 Zettel digitalisiert, in einem zweiten Schritt werden sie nun transkribiert und fachwissenschaftlich editiert sowie in eine Internetpräsentation überführt. Ziel dieses Prozesses ist zunächst eine digitale Reproduktion des Zettelkastens, die aber zugleich die Möglichkeiten der modernen digitalen Technik nutzt, um die nicht linear strukturierte Sammlung lesbar und ihre Genese nachvollziehbar zu machen. Indem die Digitalisierung über die reine Reproduktion hinausgeht, macht sie den Kasten selbst zu einem Forschungsobjekt.
                
         
         
            Der (analoge) Zettelkasten Niklas Luhmanns
            Die Zettelsammlung ist durch vier Merkmale gekennzeichnet, deren Kombination das besondere theoretische Kreativitätspotential der Sammlung begründet:
            (a)
                    Nichthierarchische Ordnungsstruktur:
                    Luhmann verzichtet weitgehend auf eine vorher festgelegte
                    
                    systematische Ordnung der Sammlung; diese ist primär ein historisches Produkt seiner Lektüre- und Forschungsinteressen. Aufgrund seines spezifischen Einstellprinzips führt die jeweilige thematische Erstfestlegung nämlich nicht zu einer monothematischen Reihung von Zetteln: Die für den Zettelkasten konstitutive Idee ist, dass
                    ein Zettel thematisch nur in irgendeiner Weise an den
                    vor ihm stehenden anschließen muss, ohne sich an einer übergeordneten (systematischen) Themenstruktur zu orientieren. Damit korrespondiert eine spezifische Art der Notizgenerierung, bei der Luhmann Nebengedanken weiterverfolgt, indem er diese zusätzlichen Notizen, auf einen an dieser Stelle einzuschiebenden Zettel notiert, so dass ein Wachstum des Zettelkastens ‚nach innen‘ erfolgt.
                
            (b)
                     Nummerierungssystem:
                    
                    Mit der skizzierten Ablagetechnik in einem konstitutiven Zusammenhang steht das besondere Nummerierungssystem: Jeder Zettel erhält eine von seinem jeweiligen Einstellplatz abhängige Nummerierung, die zugleich auf das Problem reagiert, wie Luhmann neue Zettel in den Altbestand einfügen kann, ohne die schon bestehende Nummerierung in Frage zu stellen. Zunächst erfolgt eine einfache Durchnummerierung der Zettel entsprechend des Zeitpunkts des jeweiligen Eintrags (1, 2, 3 usw.). Ein später erstellter Zettel, der einen einzelnen Aspekt (also gerade nicht zwingend monothematisch) von Zettel 1 weiterverfolgt, wird mit 1a nummeriert und zwischen den Zettel 1 und 2 eingeschoben. Daran kann wiederum monothematisch 1b anschließen oder aber eine thematisch abzweigende weitere Verzettelung folgen, beispielsweise 1a1. Das führt dazu, dass zwischen zwei ursprünglich direkt hintereinander eingestellten Zetteln im Extremfall bis zu 1000 später erstellte Zettel mit einem entsprechend komplexen Nummernsystem eingestellt werden.
                
            (c)
                    Verweisungssystem:
                    Das skizzierte Ablagesystem macht es außerdem nötig, dass die thematisch oder konzeptionell miteinander zusammenhängenden, aber eben verstreut in der Sammlung stehenden Zettel aufeinander verweisen, indem auf den Zetteln jeweils die entsprechenden Zettelnummern notiert werden. Neben Einzelverweisen findet man häufig am Beginn eines thematischen Abschnitts auf einem einleitenden Zettel auch
                    eine ganze Sammlung von Verweisen, die thematisch verwandte Bereiche des Zettelkastens systematisch erschließen. Aufgrund einer stichprobenartigen Auszählung kann man davon ausgehen, dass sich in der Sammlung insgesamt ca. 50.000 Verweise befinden. Luhmann selbst nennt diese netzwerkartige Verweisungsstruktur ein „spinnenförmiges System“.
                
            (d)
                    Schlagwortverzeichnis:
                    Um Einstiegspunkte in dieses Verweisungsnetz zu erhalten, hat Luhmann ein ca. 4000 Einträge umfassendes Schlagwortverzeichnis erstellt. Dieses Schlagwortregister war das zentrale Werkzeug für seine Nutzung des Kastens, da nur so Notizen zu einem bestimmten Thema zuverlässig wiedergefunden werden konnten. Das Schlagwortregister des Kastens beansprucht dabei aber keinen Anspruch auf Vollständigkeit hinsichtlich der Erfassung der einschlägigen Stellen in der Sammlung. Vielmehr notiert Luhmann in der Regel nur maximal drei Systemstellen, an denen der jeweilige Begriff zu finden ist, da er annimmt, dass man dann über das interne Verweisungsnetz schnell die anderen relevanten Stellen findet.
                
         
         
            Technische Umsetzung
            
               Datenmodell und Arbeitsumgebung
               Der Anlage des Zettelkastens folgend verstehen wir die Zettel als semantisch freie Einheiten und erzeugen für jede Zettelseite eine XML-TEI-Datei, in der die Transkription des Zettelinhalts erfolgt. Der Kopf () bietet Raum für Metainformationen, in einem umfangreichen Anhang zur Transkription () betten wir den Zettel in das Zettelnetzwerk ein. Wir unterscheiden mehrere Navigationswege durch den Kasten: Erstens eine physische Navigation in der von Zettelvorderseite zu folgender Vorderseite gesprungen wird. Zweitens geht es um übergeordnete Gliederungsverläufe. In einer dritten Navigation modellieren wir logische Abfolgen. In Form von Sprungzielen wird hier der nächste, in einem Gedankenstrang folgende Zettel angegeben, dabei werden von Luhmann eingeschobene Zettel zunächst ausgeblendet. Diese Abfolgen werden in eigenen Feldern klassifiziert und explizit gemacht.
               
                  
                     
                     Abbildung 1: Ausschnitt aus der oXygen-Arbeitsoberfläche zur Transkription und Bearbeitung von Zettelseiten (Zettel 1-5B1c).
                  
               
               Um den Fachwissenschaftlern die Arbeit mit den XML-Texten zu erleichtern, arbeiten wir mit dem Author-Mode des oXygen XML-Editors. Die von uns für die Zetteltranskription und Einbettung in das Netzwerk entwickelte Arbeitsoberfläche setzt im Verknüpfungsanhang auf fest eingerichtete Standardfelder, in die die Editoren IDs vorausliegender Zettel eintragen. Auf Basis dieser Einträge - die im Arbeitsprozess einer stetigen Weiterbearbeitung unterliegen - ergänzen wir über Skripte die Verbindung zu den davor liegenden Zetteln, um für die Portalnavigation beide Richtungen anbieten zu können.
                    
               Für die Transkription selbst stellen wir im Framework ein Mixed-Content Feld zur Verfügung, in dem weitere Textphänomene und von Luhmanns selbst explizit formulierte Zettelverweise über Buttons ausgezeichnet werden können.
            
            
               Forschungsportal
               Das Forschungsportal soll dem Benutzer eine intuitive Möglichkeit bieten, durch den Zettelkasten zu navigieren und ihn dabei nicht nur so benutzen zu können, wie Luhmann es vorgesehen hat, sondern die Nutzbarkeit durch die digitalen Möglichkeiten zu erweitern. Dazu wird für jeden Zettel dessen Ort in der im Rahmen des Projekts erstellten Inhaltsübersicht sowie eine interaktive Visualisierung seiner inhaltlich-logischen Einordnung angezeigt. Über indikative Buttons ist erkennbar, welche Navigationsmöglichkeiten sich für den momentan angezeigten Zettel für den Benutzer bieten (siehe Abbildung 2). Die Buttons sind gruppiert nach dem Navigationstypus.
               Innerhalb der Transkription sind Schlagwörter, bibliographische Angaben, Personennamen und vor allem Verweise zu anderen Zetteln verlinkt. Außerdem ist eine facettierte Volltext-Suche (z.B. nach Schlagworten oder bibliographischen Angaben) über vom Benutzer definierbare Bereiche des Zettelkastens möglich.
               
                  
                     
                     Abbildung 2: Ansicht eines Zettels im Forschungsportal
                  
               
               Neben optimierten Darstellungen für einzelne Zettelseiten gibt es Visualisierungen, die größere Bereiche des Kastens abdecken. Dazu zählt eine Visualisierung der im Rahmen der Edition erstellten inhaltlich-logischen Einordnungs- und Navigationsstruktur des Zettelkastens (siehe Abbildung 3), sowie ein Arc Diagram zur Darstellung von Zettelverweisen innerhalb verschiedener Bereiche (siehe Abbildung 4), um die Dichte der Vernetzung innerhalb und zwischen den verschiedenen Abteilungen des Zettelkastens zu verdeutlichen.
                    
               Die Datenaufbereitung der TEI-XML-Dateien geschieht jeweils über im Rahmen des Projekts entwickelte Node.js-Skripte, bevor die Visualisierungen mit der JavaScript-Bibliothek D3.js erstellt werden.
                    
               
                  
                     
                     Abbildung 3: Ausschnitt einer Visualisierung der im Rahmen der Edition erstellten inhaltlich-logischen Einordnungs- und Navigationsstruktur des digitalen Zettelkastens.
                  
               
               
                  
                     
                     Abbildung 4: Visualisierung der internen Zettelverweise des ersten Auszugs von Zettelkasten I (ca. 3300 Zettel). Man erkennt die deutlichen Unterschiede der Vernetzungsdichte innerhalb und zwischen den 21 Abteilungen.
                  
               
            
         
         
            Fazit: Der Zettelkasten als Subjekt und Objekt der Forschung
            Die Digitalisierung des Zettelkastens erfolgt als ein Editionsprojekt im Rahmen einer Nachlasserschließung, insofern ist das primäre Ziel eine digitalisierte
                    Reproduktion mit der Intention, die Nutzbarkeit im Luhmannschen Sinne zu rekonstruieren und zu erleichtern. Die
                    dafür notwendige digitale Modellierung hatte eine vertiefte Reflexion über das Design des Zettelkastens zur Folge, insbesondere aufgrund der damit einhergehenden Notwendigkeit einer eindeutigen Typendifferenzierung von Zettelanschlüssen. Durch diese Differenzierung, die ihren Niederschlag in einer entsprechend komplexen Navigationsstruktur des digitalen Kastens gefunden hat
                    , die eine erleichterte Nutzbarkeit des Kastens ermöglicht, kam es zu einer deutlichen Konkretisierung der zweiten fachwissenschaftlichen Intention der Publikation: der Möglichkeit einer werkgenetischen Lesbarkeit des Kastens über eine Rekonstruktion der Einstellhistorie. Gerade weil die Zettel selbst undatiert sind, ist eine solche historisierende Lesart nur über die (fachwissenschaftliche) Identifizierung von ursprünglichen Zettelanschlüssen und späteren Einschüben möglich; die Lesbarkeit der dadurch implizierten Zettelfolgen, die sich von der physischen Stellordnung der Zettel in Teilen unabhängig macht, ist dann aber nur aufgrund der entsprechenden technischen Umsetzung möglich.
                
            Die technische ‚Aufrüstung‘ des digitalen Kastens führt allerdings auch dazu, dass diese Version kein reines Abbild des analogen Kastens mehr darstellt, sondern den Zettelkasten nun in einer Weise verfügbar macht, wie Luhmann selbst ihn nie genutzt hat. Diese 
                    Differenz zum ursprünglichen Zettelkasten wird verstärkt durch die weiteren o.g.  Recherchemöglichkeiten im Rahmen der digitalisierten Version sowie durch die  Visualisierungsmöglichkeiten der Verweisungsstruktur zwischen den Zetteln, die zudem die Netzwerkförmigkeit der Sammlung transparent macht. Diese Form der Edition des Zettelkastens rekonstruiert den Kasten selbst also nicht mehr als das Forschungssubjekt, das er für Luhmann als ‚Denkmaschine‘ und Theorieapparat war, sondern macht den Kasten selbst schon bei der Editionsarbeit zu einem Forschungsobjekt und bietet die für die Beantwortung von jetzt noch unbekannten Forschungsfragen notwendigen Instrumentarien an.
                
         
      
      
         
            
               
                  www.niklas-luhmann-archiv.de
               
            
            Vgl. oXygen Visual (WYSIWYG) XML Editors:
                
            
            Arc diagram:
                
            
            Node.js:
                
            
            D3.js:
                
            
         
         
            
               Bibliographie
               
                  Krajewski, Markus (2002):
                        ZettelWirtschaft. Die Geburt der Kartei aus dem Geiste der Bibliothek
                        . Berlin: Kadmos.
                    
               
                  Schmidt, Johannes F.K. (2016): "Niklas Luhmann´s Card Index: Thinking Tool, Communication Partner, Publication Machine", in: Alberto Cevolini (ed.): Forgetting Machines. Knowledge Management Evolution in Early Modern Europe. Leiden: Brill, 289-311.
                    
               
                  Watts, Duncan (2004): "The »new« science of networks", in: Annual Review of Sociology 30, 243-270.
                    
            
         
      
   



      
         
            Die Hieroglyphenschrift des Klassischen Maya
            Die logosyllabische Hieroglyphenschrift der Maya umfasst rund 1000 Zeichen und wurde etwa zwischen 350 v. Chr. und 1550 n. Chr. im südlichen Mesoamerika zur Aufzeichnung der Hochsprache des Klassischen Maya verwendet. Die Anzahl der Graphe ist raum-zeitlich betrachtet mit rund 3000+ Formen weitaus höher, da Zeichen gleichzeitig mehrere Graphvarianten aufweisen können, die von einer Vollform abgeleitet sind.
            Einzelne Graphe werden in einem meist mit einem Wort oder morphemischem Verbund identischen Hieroglyphenblock arrangiert, ähnlich dem koreanischen Hangul. Allerdings offenbart das Maya aufgrund seines Variantenreichtums eine wesentlich größere kalligraphische Freiheit als die einzelnen Varianten nur durch simples Aneinanderreihen im Block zu schreiben. Je nach Platzbedarf und Ästhetik können Graphe etwa miteinander verschmelzen, infigiert oder gedreht werden.
            Bei der Katalogisierung der Graphe muss weiterhin mehr als ein Jahrtausend paläographischer Entwicklung berücksichtigt werden, ebenso unterschiedliche Stile in skulptierten oder gemalten Texten. Wegen all dieser Eigenheiten und der herausfordernden Struktur widersetzt sich die Maya-Schrift aktuell, Teil des Unicode-Standards zu werden.
         
         
            Zeichenkataloge als Hilfskonstrukte der Epigraphik
            Bis in die 1950er Jahre war die Maya-Schrift nicht entziffert und blieb es in großen Teilen bis in die 1980er Jahre, als eine Reihe bahnbrechender Erkenntnisse einen Kaskadeneffekt in Gang setzte, etwa Stuart (1987). Bis heute kennt man ebensowenig die genaue Zahl der Zeichen und ihrer graphischen Repräsentationen, da alle bisher publizierten Verzeichnisse aufgrund des Entzifferungsprozesses unvollständig und unzulänglich sind. Über 300 Zeichen sind bis heute nur vage oder gar nicht entziffert. Für viele dieser Fälle  existieren  konkurrierende Entzifferungsvorschläge, die vielleicht nur in ausgewählten Kontexten valide sind, sich aber wegen möglicher Polyvalenz nicht gegenseitig ausschließen müssen. Es gilt nicht nur, das Zeicheninventar vollständig zu erfassen, sondern existierende Entzifferungsvorschläge kritisch im Textzusammenhang zu prüfen, ob sie verifizierbar sind, und wo sie sich als falsch oder nicht überprüfbar herausstellen und somit nicht weiter berücksichtigt werden müssen.
            Die elf bisher publizierten Zeicheninventare, vor allem Thompson (1962), weisen viele Schwachstellen auf, besonders problematisch sind dabei Mehrfachinventarisierungen von Allographen als verschiedene Zeichen. Ein weiterer offensichtlicher Nachteil der traditionellen Zeichenkataloge ist die unveränderbare Natur einer gedruckten Fassung. Dies verhindert, dass gegebene Mehrfach- und Fehlklassifikationen korrigiert oder neue Beziehungen zu Zeichen und zu verschiedenen Zeichenfunktionen erstellt werden können, wobei zudem neue Entzifferungen nicht berücksichtigt werden können.
            Um einen Überblick über die bisher geleistete Zeichenklassifikationsarbeit der Mayaschriftforschung zu schaffen und den Vergleich der Inventare zu ermöglichen, fließen die bisher publizierten Kataloge in unseren digitalen Zeichenkatalog als Konkordanz ein. Die fehlerhaften Klassifikationen werden durch unseren Katalog korrigiert, bleiben aber nachvollziehbar dokumentiert, da wir die konkordanten Katalogeinträge beim jeweiligen Graph, das optional einem Zeichen zugeordnet werden kann, erfassen.
         
         
            Ein digitales Zeichen- und Graphinventar für das Klassische Maya
            Als Resultat interdisziplinärer Arbeit, bei dem die Modellierung und Verarbeitung der Daten auf Grundlage epigraphischer Prinzipien und Forschungsfragen erfolgte, ist unser digitaler Zeichenkatalog so konzipiert, dass er sowohl bisherige Forschungsergebnisse kritisch abbilden als auch noch zu erwartende Erkenntnisse flexibel einbinden kann.
            Der Katalog basiert auf einem innovativen Konzept der flexiblen Zuordnung von Zeichen zu ihren Graphen: Zeichen als Träger sprachlicher Informationen und Graphe als Form ihrer schriftlichen Realisierung werden getrennt erfasst, und erst die Verbindung eines Zeichens mit seinen Graphen macht es zu dessen Allograph, deren Gesamtheit bildet das Graphem eines Zeichens, das phonemisch KV-Silben oder freie und gebundene Morpheme, Diakritika und Zahlen wiedergibt.
            Die ontologisch-vernetzte Struktur des Modells und dessen Implementierung in RDF erlauben es, semantische Relationen über persistente URIs zwischen eindeutig referenzierbaren Entitäten herzustellen. Dadurch ist es möglich, Zuordnungen flexibel anzupassen und Allographe durch neue Verknüpfungen hinzuzufügen oder Falschzuweisungen zu korrigieren, etwa wenn ein Graph in Wirklichkeit aus zwei Graphen verschiedener Zeichen besteht.
            
               Sobald die Neuinventarisierung abgeschlossen ist (voraussichtlich Mitte 2018), wird der Katalog auf unserem zukünftigen Projektportal (
                    
                  https://classicmayan.org/
               ) publiziert und die RDF-Daten über einen SPARQL-Endpoint zugänglich gemacht. Zusätzlich werden die Daten auch im TextGrid Repository (
                    
                  https://textgridrep.org/
               ) veröffentlicht, wo sie mittels einer OAI-PMH Schnittstelle auch für externe Nutzer abrufbar sind. Die Dokumentation des digitalen Zeichenkatalogs ist unter 
                    
                  http://idiom-projekt.de/catalogue
                erreichbar.
                
         
         
            Bewertung und qualitative Einstufung von Lesungshypothesen
            Bei einer noch nicht vollständig entschlüsselten Schrift machen Epigraphiker zwangsläufig verschiedene Annahmen zur phonemischen Lesung von Zeichen. Es ist notwendig, alle plausiblen und nicht eindeutig widerlegten Entzifferungsvorschläge zu dokumentieren, vor allem aber, deren Qualität nach formalen Kriterien bewertbar zu machen. Dazu haben wir ein neutrales, transparentes System modelliert, das anhand formaler Kriterien eine qualitative Einstufung von Lesungshypothesen ermöglicht und deren Plausibilität im Textkorpus überprüfbar macht.
            Die Zeichen werden Zeichenklassen (syllabisch, morphographisch, diakritisch und/oder numerisch) zugewiesen, die jeweils einen breiten Transliterationswert ohne allophonische Varianz haben, z.B. “ku” für das Silbenzeichen 528. Aufgrund der Polyvalenz hat das Zeichen noch zwei logographische Lesungen mit distinktem Transliterationswert, statt des normalen “TUN” wird “CHAHUK” gelesen, wenn es als Tagesname verwendet wird. Für die Konfidenz eines Transliterationswertes werden jene Kriterien ausgewählt, auf die er sich stützt (siehe Abb. 1). Für jede Art der Zeichenfunktion wurde ein eigenes Kriterien-Set basierend auf Kelley (1962) und Houston (2001) entwickelt, das sich vor allem am graphematischen und sprachlichen Nutzungskontext orientiert, z.B. hat die Übereinstimmung mit einer bestimmten Wortart durch das Syntagma für ein Silbenzeichen keine Bedeutung, jedoch für Logogramme - hier besonders auch der Nachweis in modernen Maya-Sprachen.
            
               
               Abb. 1: Polyvalentes Zeichen mit drei Lesungen und jeweils unterschiedlichen Konfidenzen.
            
            Die Kriterien sind mittels Aussagenlogik so miteinander in Bezug gesetzt, dass je nach Kombination eine qualitative Einstufung vorgenommen wird. Dabei steht “1” für die höchstmögliche Konfidenz und eine evidente Lesung. Die Anzahl der Konfidenzstufen je Zeichenfunktion ist unterschiedlich: während Logogramme eine granulare Einteilung benötigen, brauchen Silbenzeichen weniger Stufen, da deren Permutationen im Kontext eines Wortes recht eindeutig sind.
            Das Wortzeichen “CH’AM” etwa taucht mit phonemischen Komplementen auf, die entsprechenden Kriterien ergeben Stufe 2. Damit liegt eine wahrscheinliche, aber ohne funktional äquivalente syllabische Substitution noch keine gesicherte Entzifferung vor. Die Kriterien sind bewusst streng gehalten, um für jede Lesungsaussage eine kritische Evaluation gegenüber den Zeichenvorkommen im von uns TEI-kodierten Textkorpus durchführen zu können und damit unserem Wörterbuch eine hohe Zuverlässigkeit für den Nutzer zu geben. Lesungen unterhalb einer bestimmten Konfidenz werden nämlich nicht aufgenommen, so dass in unserem Wörterbuch bestimmten Entzifferungsvorschlägen der normative Charakter genommen wird, den sie vielleicht in der epigraphischen Forschung durch Zitierung gewonnen haben.
         
         
            Zusammenspiel von Zeichenkatalog, Textkorpus und linguistischer Analyse
            Ohne eine sichere Identifizierung aller Graphe und mit vielen Zeichen unbekannter oder umstrittener Lesung kann das TEI-kodierte Textkorpus weder aus einem festen Schriftzeichensatz wie Unicode noch aus phonemisch transliterierten Werten bestehen. Für ein flexibles Korpus, das auf neue Entzifferungen und verschiedene Lesungshypothesen reagieren kann, nutzen wir den Zeichenkatalog als eine Art “Grundbaukasten” bei der Korpuserstellung.
            Im kodierten Text wird jede Hieroglyphe mittels Katalognummer und einer Referenz zur URI im Zeichenkatalog erfasst. Mittels einer Software zur linguistischen Analyse, Teil unserer virtuellen Arbeitsumgebung, wird in einem weiteren Prozessierungsschritt die numerische Transliteration (Katalognummern) zunächst in eine graphemische Transliteration (Transliterationswerte) überführt. Dies geschieht wegen der Polyvalenz semi-automatisch, wenn der Epigraphiker nach Verwendungskontext eine Entscheidung fällen muss - erst damit wird der Text “menschenlesbar”.
            Die qualitative Einstufung der Entzifferungsvorschläge im Zeichenkatalog wird jetzt relevant: Die Analyse kann anhand hoher oder niedriger Konfidenzstufen durchgeführt werden. Die Entzifferungsaussagen können in ihrem Verwendungskontext überprüft werden, was idealerweise zu neuen Erkenntnissen für die Entzifferung führen kann, aber auch der Vorbereitung der zweiten Stufe der phonemischen Transliteration dient. Jetzt werden die quasi als Container genutzten Transliterationswerte aus dem Zeichenkatalog kontextuell der korrekten sprachlichen Lesung angepasst. So besitzt das Zeichen “CHAN” üblicherweise die Lesung “chan” - “Himmel”, syllabische Substitutionen in Nordwest-Yukatan zeigen aber, dass das Zeichen dort in einem vernakularen Kontext “káan” ausgesprochen wurde. Der Einfluss lokaler Maya-Sprachen (relevant sind drei Sprachfamilien) auf die Schriftsprache ist noch nicht systematisch erforscht und mit Ergebnissen der historischen Linguistik abgeglichen worden.
            Das Tool zur linguistischen Analyse ermöglicht darüber hinaus die Anlage paralleler, als gleichwertig anzusehende Textanalysen, damit wird den verschiedenen Entzifferungsvorschlägen Rechnung getragen. Durch die Verbindung von Zeichenkatalog, Textkorpus und linguistischer Analyse entsteht letztendlich ein dynamischer Text, der je nach Forschungsfrage individuell generiert werden kann. Dieser Ansatz der ontologischen Vernetzung der Komponenten dürfte auch für die Erforschung weiterer nicht entzifferter Schriften von Interesse sein.
         
         
            Neue Perspektiven für die Maya-Epigraphik
            Auch ein digitaler Zeichenkatalog des Klassischen Maya kann nur so gut sein wie die epigraphische Forschung, und ist vor allem vom Grad der kritischen Selbstreflektion abhängig. Konkurrierende Entzifferungsvorschläge werden erst einmal als gleichwertig aufgenommen und erst dann anhand formaler Kriterien kategorisiert. Die Kriterienvergabe folgt den Argumenten der Hypothese und ist damit faktisch. Die Aussagelogiken zur Festlegung der Konfidenzstufen sind dabei eine kritische Zusammenführung fast 70-jähriger epigraphischer Forschungspraxis, auch im Vergleich mit den Methoden bei der Entzifferung anderer nicht-alphabetischer Schriftsysteme. Die Konfidenz eines Entzifferungsvorschlags ist damit weit mehr als ein Wahrscheinlichkeitsbegriff. Das Datenmodell ist dabei dem noch nicht gefestigten Erkenntnisstand zum Mayaschriftsystem angepasst.
            Die Arbeit mit digitalen Methoden hat so manche Kritik an der eigenen Forschungstradition erst in Gang kommen lassen, lenkt diese aber auch in eine vorher nicht denkbare Richtung. Die Möglichkeit eines digitalen Zeichenkatalogs und eines digitalen Textkorpus erlaubt erstmals, den gesamten Schriftschatz des Klassischen Maya nach Entzifferungskontexten zu durchsuchen, anstatt sich auf sein “cerebrales” Textkorpus verlassen zu müssen. Erst die Verbindung von geisteswissenschaftlichen und digitalen Methoden erlaubt es, die Maya-Epigraphik in eine neue Phase eintreten zu lassen.
         
      
      
         
            
               Bibliographie
               
                  Houston, Stephen (2001): 
                        The Decipherment of Ancient Maya Writing. Norman: University of Oklahoma Press: 3–19.
                    
               
                  Kelley, David H. (1962): Review of “A Catalog of Maya Hieroglyphs, by J. Eric S. Thompson. Pp. xiv + 458, Including pls 16. University of Oklahoma Press, Norman, in Cooperation with the Carnegie Institution of Washington, 1962.” 
                        American Journal of Archaeology 66(4): 436–438.
                    
               
                  Stuart, David (1987): 
                        Ten Phonetic Syllables. Research Reports on Ancient Maya Writing 14. Center for Maya Research, Washington, D.C.
                    
               
                  Thompson, J. Eric S. (1962): 
                        A Catalog of Maya Hieroglyphs. Norman: University of Oklahoma Press.
                    
               
                  Wichmann, Søren (2006): Mayan Historical Linguistics and Epigraphy: A New Synthesis. 
                        Annual Review of Anthropology 35, 279–294.
                    
            
         
      
   



      
         
            Einführung
            Im Bereich digital basierter Untersuchungen wird zunehmend eine Verzahnung quantitativen und qualitativen Arbeitens gefordert. In der konkreten Arbeit der Korpusanalyse wird aus dieser scheinbaren Dichotomie jedoch schnell eine Methodenvielfalt, denn gerade durch Kombinationen verschiedener Perspektiven auf die Daten werden unterschiedliche Phänomene greifbar und entfaltet sich das volle Potential quantitativ-qualitativen Arbeitens. Das hier präsentierte Poster soll dies an einem konkreten Beispiel veranschaulichen.
         
         
            Fragestellung
            Inhaltlicher Ausgangspunkt ist die Frage nach Textgliederungsprinzipien, welche für bestimmte erbauliche Textsorten kennzeichnend sind. Mittel der Textgliederung können als Mittel der Markierung von Teiltexten innerhalb eines Gesamttextes beschrieben werden (Hausendorf/Kesselheim 2008: 41) und können wiederum für Textsorten charakteristisch sein. Sie finden sich auf verschiedenen Ebenen des Textes, u.a. im Bereich der Typographie (Stein 2003: 422). 
            Gerade diese typographischen Gliederungsmerkmale stellen einen guten Ausgangspunkt für eine quantitative Analyse von Textgliederungsmerkmalen dar, da sie in TEI-annotierten Korpora z.B. durch die XML-Strukturierung automatisch greifbar werden. Anders als bei dem Verfahren, textbezogene Phänomene reduziert auf bestimmte TEI-Strukturen zu untersuchen (z.B. Schöch 2016: 351ff., Haaf 2016), gelangen hier die im Korpusvergleich möglicherweise signifikanten Häufigkeiten der TEI-Strukturierungen selbst in den Blick.
            Die inhaltliche Frage nach Textgliederungsprinzipien erbaulicher Textsorten wird ausführlich behandelt in Haaf (in Vorber.). Im vorliegenden Beitrag stehen – der thematischen Ausrichtung der Konferenz entgegenkommend – Überlegungen zur adäquaten Methodik einer solchen Untersuchung im Vordergrund. 
         
         
            Korpus- und Analysegrundlage 
            Der hier präsentierten Studie liegen drei Teilkorpora des 17. Jahrhunderts aus dem Deutschen Textarchiv (2017) zugrunde: 
            
               Prosaische Erbauungsliteratur: 25 Bände (10 Autoren, 10.501 Seiten)
               Funeralschriften: 334 Schriften (14.316 Seiten)
               Referenzkorpus: 187 Bände verschiedener Textsorten (60.798 Seiten)
            
            Die Texte des DTA-Korpus wurden nach einheitlichen Richtlinien und mittels eines TEI-Subsets, das Ambiguitäten der Auszeichnung möglichst reduziert, ausgezeichnet (Haaf et al. 2014/15). 
            Für die vorliegende Untersuchung wurden einzelne TEI-Strukturen hinsichtlich der Häufigkeit ihres Auftretens (relativ zur Token-Anzahl) und ihrer Verteilung im jeweiligen Korpus verglichen, um speziell die Unterschiede in der Textgliederung zwischen den untersuchten Textsorten herauszuarbeiten. Dabei wurden solche Tags einbezogen, die voraussichtlich Textgliederungsmerkmale repräsentieren. So kann z.B. tei:div die Kapitelstruktur eines Textes anzeigen, durch tei:l wird der Wechsel zwischen Prosatext und Lyrik greifbar, tei:note zeigt Metatexte in Form von Anmerkungen, z.B. Marginalien, an, tei:hi repräsentiert Hervorhebungen von Textpassagen gegenüber dem Grundtext. Die Ergebnisse wurden einer qualitativen Beurteilung unterzogen. 
         
         
            Ergebnisse
            Zur Evaluation eines Merkmals war hier nicht allein der Blick auf seine relativen Häufigkeiten in und deren signifikante Unterschiede zwischen den untersuchten Korpora relevant. Die signifikant erhöhte Häufigkeit eines Merkmals kann vielmehr unterschiedliche Gründe haben. So kann sie einerseits zwar durchaus (1) auf die höhere Relevanz des Merkmals im Korpus hindeuten, wie sich am Merkmal der Marginalie zeigt, das signifikant häufig und regelmäßig verteilt im Korpus der Funeralschriften auftritt (Abb. 1). Sie kann andererseits aber auch (2) aufgrund der unausgewogenen Verteilung des Merkmals im Korpus gar nicht aussagekräftig sein, entweder weil (2a) sich das Korpus selbst als in sich unausgewogen und nicht repräsentativ für den zu beschreibenden Gegenstand erweist oder weil (2b) das Merkmal im gegebenen Kontext nicht relevant ist, wie etwa die horizontale Trennlinie zwischen Textteilen, die in allen drei Vergleichskorpora unregelmäßig verteilt war (Abb. 2). Andererseits kann es (3) auch vorkommen, dass die bestehende Ausgewogenheit der Verteilung eines Merkmals in einem Korpus letztlich nicht aussagekräftig für dessen Relevanz ist. 
            
               
               Abb. 1: Relative Häufigkeiten je 1 Mio. Token und deren Verteilung in den drei untersuchten Korpora für das Merkmal „Marginalie“
            
            
               
               Abb. 2: Relative Häufigkeiten je 1 Mio. Token und deren Verteilung in den drei untersuchten Korpora für das Merkmal „Horizontale Trennlinie“
            
            Weiterhin konnten im gegebenen Kontext (4) auch Merkmale mit geringeren Häufigkeiten relevant sein, und schließlich ist nicht zuletzt (5) auch der Ort, an dem ein Merkmal im Dokument auftritt, zu berücksichtigen. Beide Aspekte (4 und 5) zeigen sich z.B. am Merkmal der Liste, die in den erbaulichen Prosawerken erwartungsgemäß selten, aber relativ regelmäßig auftritt, und zwar in Form von Registern am Buchbeginn oder Buchende (in tei:front oder tei:back).
            Methodisch zeigte sich also, dass für eine adäquate Beurteilung der untersuchten Merkmale verschiedene Blickwinkel notwendig sind. Das Poster veranschaulicht anhand der erwähnten und weiterer Beispiele diese genannten methodischen Aspekte.
            Inhaltlich führte die Untersuchung zutage, dass z.B. Merkmale, die den Zugang zum Text erleichtern und Hilfe zur Orientierung im Text geben, für die erbaulichen Textsorten relevant sind (Näheres vgl. Haaf (in Vorber.)). 
         
      
      
         
            
               Bibliographie
               
                  Deutsches Textarchiv. Grundlage für ein Referenzkorpus der neuhochdeutschen Sprache. Herausgegeben von der Berlin-Brandenburgischen Akademie der Wissenschaften, Berlin 2017. http://www.deutschestextarchiv.de [letzter Zugriff: 24.09.2017]
                    
               
                  Haaf, Susanne (i. Vorb.): „Art und Funktion von typographischen Mitteln zur Textgliederung in erbaulichen Textsorten des 17. Jahrhunderts. Automatische Analyse im Korpusvergleich und qualitative Einordnung“, in: Simmler, Franz / Baeva, Galina (Hrsg.): 
                        Textgliederungsprinzipien. Ihre Kennzeichnungsformen und Funktionen vom 8. bis 18. Jahrhundert. Akten zum Internationalen Kongress vom 22. bis 24. Juni 2017 an der Universität St. Petersburg. Berlin: Weidler [2018].
                    
               
                  Haaf, Susanne (2016): “Corpus Analysis based on Structural Phenomena in Texts. Exploiting TEI Encoding for Linguistic Research”, in: Nicoletta Calzolari et al.: 
                        Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), 23.–28. Mai 2016, Portorož (Slovenia). Paris.
                    
               
                  Haaf, Susanne / Geyken, Alexander / Wiegand, Frank (2014/15): “The DTA ‘Base Format’. A TEI Subset for the Compilation of a Large Reference Corpus of Printed Text from Multiple Sources”, in: 
                        Journal of the Text Encoding Initiative 8.
                    
               
                  Hausendorf, Heiko / Kesselheim, Wolfgang (2008): 
                        Textlinguistik fürs Examen. Göttingen: Vandenhoeck & Ruprecht.
                    
               
                  Schöch, Christoph (2016): „Ein digitales Textformat für die Literaturwissenschaften. Die Richtlinien der Text Encoding Initiative und ihr Nutzen für Textedition und Textanalyse“, in: 
                        Romanische Studien 4. 
                    
               
                  Stein, Stephan (2003): 
                        Textgliederung. Einheitenbildung im geschriebenen und gesprochenen Deutsch. Theorie und Empirie, Berlin.
                    
            
         
      
   



      
         
            Ausgangspunkt: Die TEI2016 Abstacts App
            Die Abstracts zur TEI Konferenz 2016 wurden via ConfTool eingereicht und anschließend in Microsoft Word und InDesign ediert, um ein gedrucktes Book of Abstracts herzustellen.
                     Die InDesign-Datei wurde dann als PDF exportiert und online verfügbar gemacht. Diese beiden Fassungen des Book of Abstracts wurden vor der Konferenz fertiggestellt. Nach der Konferenz wurde die InDesign Datei erneut exportiert, diesmal in XML Format. Diese Datei wurde dann teils manuell, teils automatisiert in Einzeldateien (jedes Abstract als eine Datei) zerlegt, bearbeitet und mit umfangreichen Metadaten versehen, um TEI-P5-Dateien herzustellen. Diese wurden auf GitHub veröffentlicht. Die jeweiligen  wurden mit umfassenden Informationen ausgestattet und detailliert codiert; etwa wurden innerhalb der mit Normdaten (VIAF) angereicherten - und -Tags konsequent -, - und -Tags eingesetzt. Das stellte sich für die Weiterverarbeitung, Analyse und Visualisierung der Daten später als vorteilhaft heraus.
                
            Dieses Corpus bildete die Basis für die TEI2016 Abstracts App, in der die Abstracts in der Erstversion über ein Inhaltsverzeichnis, das nach Abstract-Titel oder nach Verfassenden sortiert werden konnte, ansteuerbar waren. Schon in dieser rudimentären Form zeigte sich das Potential der Daten und der Applikation für Auswertungen, die über Struktur und Zusammensetzung der Konferenz und ihrer Beitragenden Auskunft geben würden. Die App wurde daher sowohl im Bereich der Funktionalität als auch auf der Ebene der enthaltenen Daten substantiell weiterentwickelt.
         
         
            Fragestellung und gender-theoretische Überlegungen
            Wir trafen die Entscheidung, der weiteren Entwicklung des Prototyps der App eine Forschungsfrage zugrundezulegen. Wir entschieden uns für die Frage nach der Geschlechterverteilung unter den Beitragenden.
            Aus Perspektive der Gendertheorie (in Butler’scher Tradition) bedarf die Entscheidung, diese Frage zu stellen, an sich eine Rechtfertigung, denn “a performative utterance (or practice) brings into being that of which it speaks.” (Butler 2010, 150f) Das bedeutet, dass die Frage nach der Verteilung der Geschlechter die Unterscheidung der Geschlechter erst hervorbringt, und so die historische Situation, die unser Körper bedeutet, fortsetzt: “As an intentionally organized materiality, the body is always an embodying of possibilities both conditioned and circumscribed by historical convention. In other words, the body is a historical situation, as Beauvoir has claimed, and is a manner of doing, dramatizing, and reproducing a historical situation.” (Butler 1988, 521) Berücksichtigt man außerdem die Konzepte des 
                    Doing bzw. 
                    Undoing Gender (West & Zimmerman 1987; Hirschauer 1994 & 2001), so wird deutlich, dass die oben formulierte Frage deutlich präzisiert werden muss, um die zu erhebenden Daten in einer Weise strukturieren zu können, die eine zeitgenössische Auffassung von Geschlechteridentitäten wiedergibt.
                
            Die Frage nach dem Geschlecht der Konferenzbeitragenden wurde daher wie folgt präzisiert: Da sie aus einem Interesse an der Stellung bzw. Repräsentation der Geschlechter im gegenwärtigen Wissenschaftsbetrieb heraus gestellt wurde, richtet sie sich nicht auf die biologische Beschaffenheit der Körper der Konferenzbeitragenden, sondern auf ihre gesellschaftliche (Selbst-)Wahrnehmung (also auf 
                    gender und nicht 
                    sex). Folgt man Butlers Auffassung, nach der die performative Äußerung das, wovon sie spricht, hervorbringt, wären daher eigene Angaben der Betroffenen zu ihrem Geschlecht die ideale Datenquelle für diese Fragestellung gewesen. Solche Angaben standen allerdings nicht zur Verfügung.
                
         
         
            Datenmodellierung und -anreicherung
            Nachdem eine Personenliste aus dem abstracts-Corpus extrahiert worden war, kamen wir zu dem Schluss, dass das Zuweisen von Geschlechtern an Personen unter Berücksichtigung der oben skizzierten geschlechtertheoretischen Überlegungen kein gangbarer Weg sein konnte. Um die soziodeterministische Neugier, aus der heraus die Ausgangsfrage gestellt worden war, dennoch befriedigen zu können, entwickelten wir ein gendersensibles Workaround für die Modellierung der Daten, das uns dennoch eine Erhebung der Geschlechterverteilung erlaubte: Anstatt den s ein  zuzuweisen, wie es die TEI-Richtlinien vorsehen, entschieden wir uns dafür, dem jeweiligen  einen @type zuzuweisen, der eine Angabe zum (sozialen) Geschlecht enthalten sollte. Diese Vorgehensweise erlaubte es, die Daten nicht den eigentlichen Personen nachzubauen und damit annahmen über deren Selbstwahrnehmung zu treffen, sondern zu erheben, welchem Geschlecht sie von einer allgemeinen Gesellschaft oder Öffentlichkeit aufgrund ihrer Namen aller Wahrscheinlichkeit nach zugeordnet würden.
            Um das Vorgehen weiter zu objektivieren, entschieden wir uns gegen die Zuordnung der Geschlechter zu den Vornamen auf Basis unseres eigenen Weltwissens. Stattdessen machten wir uns auf die Suche nach Namensdatenbanken, die Angaben zum Geschlecht enthalten. Im Natural Language Toolkit (NLTK) etwa ist eine solche Liste enthalten (Kantrowitz 1997), jedoch werden keine Angaben gemacht, wie diese Liste zustande kam und auf welcher Grundlage die Geschlechter zugeordnet wurden. Wir entschieden uns daher für genderize.io, eine Datenbank, die nach eigenen Angaben auf Daten von Social-Media-Plattformen basiert, wo Personen die Informationen zu ihrem Geschlecht selbst bestimmen können. Nach dem Abgleich unserer Personenliste mit genderize.io via deren API konnten wir 102 von 124 Namen ein Geschlecht zuordnen. Da genderize.io nur zwei Geschlechter kennt, setzten wir “male”, “female” und “no-match” als @type-Werte ein. Die Namen, die nach diesem ersten Abgleich “no-match” waren, verglichen wir anschließend manuell mit der genderchecker.com-Datenbank, die ihre Geschlechtsangaben aus den "2001 and 2011 UK Census Data" bezieht, "together with multiple online sources and contributions from our 2m website visitors". Diese kommerzielle Datenbank konnte nicht für einen Gesamtvergleich genutzt werden, da nur die manuelle Verwendung kostenfrei ist. Ein interessanter Aspekt des Datenmodells von genderchecker.com ist die Berücksichtigung der Möglichkeit eines dritten Geschlechts: "If we see just one instance of a name appearing as both male and female, we categorise it as unisex." Aus gendertheoretischer Perspektive ist dieses Modell ein Schritt in die richtige Richtung, wenngleich die Benennung “unisex” diskutabel scheint. In unserer Namensliste kam kein solcher Fall vor - nach dem Abgleich der verbleibenden 22 Namen mit genderchecker.com blieben drei Namen “no-match”.
            Da Daten und Code, aus denen die TEI2016 Abstracts App gebaut ist, unter einer offenen Lizenz auf GitHub frei zugänglich sind, steht es den Beitragenden frei, ihr Geschlecht und damit unsere statistische Auswertung der Geschechterverteilung zu verändern - das Ergebnis bleibt also ein vorläufiges. Im geplanten Vortrag wird es im Rahmen einer live-Demonstration der App präsentiert werden.
         
      
      
         
             Dieser Workflow ist, zugegebenermaßen, einer Konferenz im Bereich der Digital Humanities, und der TEI Konferenz im Besonderen, nicht ganz angemessen; eine Integration etwa des DH-Convalidators hätte den Prozess wesentlich gestrafft. Eingespielte Prozesse und Zeitdruck waren die Hauptgründe für diese Vorgehensweise.
         
         
            
               Bibliographie
               
                  Peter Andorfer, Vanessa Hannesschläger (2017): TEI Abstracts 2016. A little application to publish the abstracts of the TEI conference 2016. 
                        http://tei2016app.acdh.oeaw.ac.at/
                        
               
               
                  Judith Butler (1988): Performative Acts and Gender Constitution: An Essay in Phenomenology and Feminist Theory. In: Theatre Journal 40:4, 519–531.
               
                  Judith Butler (2010): Performative Agency. In: Journal of Cultural Economy 3:2, 147–161.
               
                  Genderchecker: 
                        http://genderchecker.com/
                        
               
               
                  genderize.io: Determine the gender of a first name. 
                        https://genderize.io/
                        
               
               
                  Vanessa Hannesschläger, Daniel Schopper (Hg.) (2017): TEI Conference and Members’ Meeting 2016. Book of Abstracts [XML/TEI files]. 
                        https://github.com/acdh-oeaw/TEI2016abstracts
                        
               
               
                  Stefan Hirschauer (1994): Die soziale Fortpflanzung der Zwei-Geschlechtlichkeit. In: Kölner Zeitschrift für Soziologie und Sozialpsychologie 46:4, 668–692.
               
                  Stefan Hirschauer (2001): Das Vergessen des Geschlechts: Zur Praxeologie einer Kategorie sozialer Ordnung. In: Kölner Zeitschrift für Soziologie und Sozialpsychologie (Sonderheft 41), 208–235.
               
                  Mark Kantrowitz (1997): Name Corpus: List of Male, Female, and Pet names. CMU Artificial Intelligence Repository. 
                        http://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/
                        
               
               
                  Natural Language Toolkit (NLTK). 
                        http://www.nltk.org/
                        
               
               
                  Susanne Oelkers (2003): Naming gender. Empirische Untersuchungen zur phonologischen Struktur von Vornamen im Deutschen. Frankfurt a. M.: Lang.
               
                  TEI Consortium (2017): TEI P5: Guidelines for Electronic Text Encoding and Interchange. 
                        http://www.tei-c.org/release/doc/tei-p5-doc/en/Guidelines.pdf
                        
               
               
                  Virtual International Authority File (VIAF). 
                        https://viaf.org/
                        
               
               
                  Candance West, Don Zimmerman (1987): Doing Gender. In: Gender and Society 1:2, 125–151.
            
         
      
   



      
         Das Deutsche Historische Institut in Washington (DHI) befindet sich in der Entwicklungsphase von " Deutsche Geschichte-
                Digital / German History-
                Digital" (DG-D), einer transatlantischen digitalen Initiative, um die wissenschaftlichen Bedürfnisse von HistorikerInnen im Kontext neuer historiografischer und technologischer Herausforderungen zu bewältigen. DG-D ist eine neue Infrastruktur zur Erleichterung der transnationalen historischen Wissensschöpfung für eine große Wissenschaftsgemeinschaft und eine wachsende Zahl von "Citizen Scholars", die bereits auf digitale Ressourcen des DHI angewiesen sind. Im Poster stellen wir zwei zentrale GH-Digital-Pilotprojekte und deren Integration in die DG-D "Knowledge Creation Environment" vor. Das erste ist "Deutsche Geschichte in Dokumenten und Bildern", unterstützt von der Deutschen Forschungsgemeinschaft (DFG), und das zweite ist Deutsche Geschichte 
                Intersections, unterstützt durch das Europäische Wiederaufbauprogramm (ERP). 
            
         Die Planung für DG-D umfasste die Befragung von mehr als vierhundert WissenschaftlerInnen, die bereits mit digitalen Ressourcen arbeiten, welche vom DHI produziert wurden. Die umfassendste dieser Ressourcen ist die im Jahr 2003 gestartete digitale Quellensammlung "Deutsche Geschichte in Dokumenten und Bildern / German History in Documents and Images" (GHDI), die an deutsch- und englischsprachigen Universitäten weitläufig genutzt wird. Derzeit wird GHDI in Verbindung mit DG-D einem technischen und konzeptionellen Umbau unterzogen. Es enthält Tausende Seiten englischsprachiger Übersetzungen deutscher historischer Texte sowie Bilder und Karten, die von ca. 5.000 Besuchern pro Tag genutzt werden. Unsere Planung für DG-D beinhaltet auch weiterhin Konsultationen und Workshops mit ExpertInnen aus den Geschichtswissenschaften und den digitalen Geisteswissenschaften sowie die Gründung von Partnerschaften mit Institutionen und großen Initiativen, die unser Interesse für die Zukunft der Geschichte im digitalen Zeitalter teilen.
         Die Deutsche Geschichte-
                Digital-Plattform befasst sich mit den Bedürfnissen der digitalen Forschung durch fünf Ziele und integrierte Arbeitspakete, die auf diese Ziele abgestimmt sind: Entdeckung, Analyse, Produktion, Bewahrung und Gemeinschaft. DG-D beinhaltet die Entwicklung eines Peer-Review-Index von wissenschaftlichen digitalen Objekten mit Dublin Core (DC) und CLARINs Component MetaData Infrastructure (CMDI) Standards über einen angepassten Blacklight (http://projectblacklight.org/) Technologie-Stack.
            
         Für WissenschaftlerInnen, die historische digitale Projekte in Nordamerika entwickeln, gibt es keine interinstitutionelle Infrastruktur für die Speicherung ihrer Daten oder die Bereitstellung von Open Access. CLARIN-D, Teil der europäischen Forschungsinfrastruktur CLARIN, berät und unterstützt das DG-D-Projekt zur Erstellung eines Portals für CLARIN in Nordamerika am DHI Washington. Im Mittelpunkt dieses Prozesses steht die Implementierung eines Repository-Systems, das eine nachhaltige Speicherung des Inhalts und die Einbindung in eine digitale Umgebung ermöglicht, um den Zugriff, die Suche und die interoperablen Datenformate zu erleichtern. Unsere Partnerschaft mit CLARIN fördert den freien Zugang, die offene, kooperative Wissenschafts- und Wissenserzeugung im nordamerikanischen Kontext und ist ein wichtiger Bestandteil der gesamten Strategie der digitalen Geisteswissenschaften des DHI. Als Institut der Max Weber Stiftung sind wir auch in Partnerschaft mit DARIAH-DE. DARIAH-DE wird Web-Hosting und langfristige Bewahrung von DHI-Digitalprojekten in ihrer Gesamtheit, einschließlich der ersten Version der Webseite Deutsche Geschichte in Dokumenten und Bildern, zur Verfügung stellen.
         Als kooperative Wissensplattform wird DG-D Redakteure, Forscher und Citizen Scientists bei der Entwicklung weiterer innovativer Online-Projekte zusammenbringen. Drei solcher Pilotprojekte befinden sich derzeit in der Entwicklung und beinhalten TEI und unsere Internationalisierung der Scalar 2.0 Plattform. DG-D verwendet Scalar 2.0 für das Baseline Content Management System, insbesondere aufgrund seiner Schnittstellenfunktionen, Unterstützung für Resource Description Framework (RDF), Konnektivität zu externen Repositorien, Dublin Core (DC) Unterstützung, Hypothes.is Integration und sein Mehrfachpfad-Navigationssystem.
         HistorikerInnen nutzen zunehmend digitale Geisteswissenschaften, um Daten zu analysieren und ihre Forschungsergebnisse darzustellen. Ein weiterer Vorteil der Speicherung von TEI-Digitalobjekten in einem CLARIN-Repository ist, dass eine ganze Palette von korpus-linguistischen analytischen Werkzeugen von WissenschaftlerInnen auf Textinhalte angewendet werden kann. In diesem Zusammenhang werden wir unsere Entwicklung von Scalar Adapters für den Anschluss an deutsche Repositorien und virtuelle Forschungsumgebungen (VREs) diskutieren.
         Die DG-D-Plattform integriert die Blog-Aggregation, ein erweitertes Diskussionssystem, Community-orientierte Tools und Social Media, um miteinander kooperierende Wissensgemeinschaften zu erleichtern und Forschung zu öffnen. Dies ist ein wegweisender Aspekt unseres Projektes, das die Annahme von sozialen und gemeinschaftlichen digitalen Instrumenten durch HistorikerInnen in ihren Forschungsaktivitäten untersuchen wird. Wir wollen hierbei auch die einzigartige Rolle nutzen, die das DHI als Drehscheibe des transatlantischen wissenschaftlichen Dialogs und als eines großen Knotenpunkts in einem internationalen Netzwerk von HistorikerInnen spielt, um die Zusammenhänge zwischen verschiedenen Wissenschaftsgemeinschaften zu erleichtern.
         Deutsche Geschichte-
                Digital Projekte bietet ein Modell für neue, quellenbasierte methodische Ansätze in den Geschichtswissenschaften. Die Initiative zielt darauf ab, durch digitale Instrumente, Standards und Methoden zur argumentbasierten Forschung beizutragen. Es fördert transnationale Ansätze in der historischen Forschung durch das Verfügbarmachen einer transnationalen technischen Plattform, die auf TEI und anderen aufkommenden Standards in den digitalen Geisteswissenschaften wie DC und RDF gründet. 
            
         Es unterstützt narratologische Komplexität in der Geschichtsschreibung und vermeidet redaktionelle Ansätzen, die eine singuläre Erzählung oder „
                Master Narrative“ ergeben. 
            
      
      
         
            
               Bibliographie
               
                  Cohen, Daniel J., and Roy Rosenzweig (2006): 
                        Digital History: A Guide to Gathering, Preserving, and Presenting the Past on the Web. Philadelphia: University of Pennsylvania Press.
                    
               
                  Cohen, Daniel J., Michael Frisch, Patrick Gallagher, Steven Mintz, Kirsten Sword, Amy Murrell Taylor, William G. Thomas III, and William J. Turkel (2008): “Interchange: The Promise of Digital History.” 
                        Journal of American History, volume 95, issue 2, 452-491.
                    
               
                  “Diskussionsforum: Historische Grundwissenschaften und die digitale Herausforderung.” From H-Soz-Kult, November 15, 2015 (
                        http://www.hsozkult.de/text/id/texte-2890).
                    
               
                  Fiedler, Norman and Werthmann, Antonina and Stuehrberg, Maik and Schonefeld, Oliver and Bingel, Joachim and Witt, Andreas (2014): Research infrastructures in non-university research facilities. Research paper. Mannheim: Institute for German Language, 2014. 116 S.
               
                  Hiebert, Matthew, Lässig, Simone and Witt, Andreas (2017): German history-digital: A platform for transnational historical knowledge co-creation. In: Digital Humanities 2017, Conference Abstracts, McGill University & Université de Montréal Montréal, Canada August 8-11, 2017. Montréal: McGill University & Université de Montréal, 2017. p. 269-271
               
                  Hiebert, Matthew, Bowen, W. R., and Siemens, R.G (2015): “Implementing a social knowledge creation environment.” Scholarly and Research Communication, 6(3).
               
                  Mandić, Slobodan (2005): 
                        Computerization and Historiography 1995-2005. Belgrade: Belgrade Historical Society.
                    
               
                  McCullough, Kelly, and James Retallack (2013): “Digital History Anthologies on the Web: German History in Documents and Images.” 
                        Central European History, volume 46, 346-361.
                    
               
                  Ngai, Mae M (2012): “The Future of the Discipline: The Promise and Perils of Transnational History.” 
                        Perspectives on History, December 2012 (
                        https://www.historians.org/publications-and-directories/perspectives-on-history/december-2012/the-future-of-the-discipline/promises-and-perils-of-transnational-history).
                    
               
                  Patel, Kiran Klaus (2010): “Transnational History.” In European History Online (EGO), published by the Institute of European History (IEG). Mainz, 2010 (http://www.ieg-ego.eu/patelk-2010-en).
                    
               
                  Patel, Kiran Klaus (2011): “Zeitgeschichte im digitalen Zeitalter: Neue und alte Herausforderungen.” 
                        Vierteljahrshefte für Zeitgeschichte, volume 59, issue 3, July, 331-51.
                    
               
                  Putnam, Lara (2016): “The Transnational and the Text-Searchable: Digitized Sources and the Shadows They Cast.” 
                        The American Historical Review, volume 121, issue 2, April, 377-402. 
                    
               
                  Sahle, Patrick (2013):
                        Digital Editionsformen. 
                            Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. Volume 3 (Norderstedt).
                    
               
                  Thomas, William G., III (2016): “Renegotiating the Archive: Scholarly Practice in the Digital Age.” 
                        (http://railroads.unl.edu/blog/?p=1195). 
                    
            
         
      
   



      
         Die Aufbereitung und Erkennung von handschriftlichen Dokumenten oder von speziellen Druckschriften ist sowohl für Menschen als auch für Computeralgorithmen eine (technische) Herausforderung. Die Bearbeitung von schriftlichem, insbesondere handschriftlichem Material aber auch früher Drucke wird bislang von spezialisierten Experten durchgeführt, um technisch und qualitativ hochstehende Resultate aus historischen Dokumenten zu erhalten. Zur Erstellung hochwertiger Editionen sind hilfswissenschaftliche Kenntnisse (Paläographie, Editorik), historisches Kontextwissen und technisches Know-how gefragt.
         Im Rahmen des Projekts READ (Recognition and Enrichment of Archival Documents) werden unterschiedliche Aufgaben der Automatisierung (weiter-)entwickelt, um qualitativ gute Ergebnisse mit optimalem Ressourceneinsatz zu erhalten. Ein speziell dafür entwickeltes Tool ist die Software Transkribus und die Transkribus Weboberfläche (öffentliche Vorstellung im November 2017). Beide Ansätze verkoppeln auf unterschiedliche Weise die Arbeit von Expertinnen und maschinelle Erkennleistung. Software und Webservice sind frei verfügbar unter www.transkribus.eu. Im Workshop wird Transkribus vorgestellt und kann durch die Teilnehmenden mit eigenen oder zur Verfügung gestellten Dokumenten getestet werden.
         Transkribus unterstützt alle Prozesse vom Import der Bilder über die Identifikation der Textblöcke und Zeilen, die zu einer detaillierten Verlinkung zwischen Text und Bild führt, sowie die Transkription und Annotation der Handschrift bis zum Export der gewonnen Daten in standardisierten Formaten.
         
            Workflow in Transkribus
            Um Texte zu transkribieren oder zu edieren, müssen digitale Bilder hochgeladen und danach mit Layouterkennungswerkzeugen bearbeitet werden. Die Analyse des Layouts kann automatisiert geschehen, wobei die manuelle Kontrolle und falls nötig die Nachbearbeitung im Moment noch sinnvoll ist.
            Dokumente können entweder automatisch mit bereits bestehenden ATR-Modellen (Automatic Text Recognition) erkannt werden oder die Transkription erfolgt händisch und kann danach zum Training neuer Modelle genutzt werden. Insbesondere für die Bearbeitung großer Dokumentenkorpora, die in ähnlichen Handschriften verfasst wurden, lassen sich bereits heute Effizienzgewinne und Vereinfachungen erzielen.
            Aufbauend auf den Transkriptionen ist es möglich eine Vielzahl von Auszeichnungen und Annotationen innerhalb des Textes, aber auch darüber hinaus für Einzeldokumente und ganze Dokumentenbestände anzulegen. Neben der Anreicherung der Dokumente mit Metadaten (Identifikation von Personen, Orten und Sachwörtern) ist somit auch die Möglichkeit der Herstellung von Bestandsbeschreibungen und der Hinterlegung von Transkriptions- und Editionsvorschriften gegeben.
         
         
            Ausgabeformate
            Für den Export stehen unterschiedliche Formate und Ausgabeformen zur Verfügung. So ist es möglich XML-Dateien zu exportieren, die den Vorgaben der TEI entsprechen. Ausgehend davon können komplexe digitale Editionen erstellt werden, die jedoch im Unterschied zu den meisten herkömmlichen Editionen eine enge Verzahnung mit den verwendeten Bilddateien aufweisen. Dadurch werden Editionen ermöglicht, die den transkribierten Text in der Zusammenschau mit der faksimilierten Vorlage sichtbar machen. Daneben sind auch Ausgaben als Druckdaten (PDF) oder zur Weiterbearbeitung für Textverarbeitungsprogramme (DOCX) implementiert. Schließlich ist auch ein Export im PAGE-Format (zur Anzeige in Viewern für OCR gelesene Dokumente, Pletschacher, 2010) sowie als METS (Metadata Encoding and Transmission) möglich.
         
         
            Zielpublikum
            Die Plattform ist für unterschiedliche Gruppen konzipiert. Einerseits für GeisteswissenschaftlerINNEN, die selbst Transkriptionen und Editionen historischer Dokumente erstellen möchten. Andererseits richtet sich die Plattform an Archive, Bibliotheken und andere Erinnerungsinstitutionen, die handschriftliche Dokumente in ihren Sammlungen aufbewahren und ein Interesse an der Aufbereitung des Materials haben. Angesprochen werden sollen auch Studierende der Geistes-, Archiv- und Bibliothekswissenschaften mit einem Interesse an der Transkription historischer Handschriften.
            Das Ziel, eine robuste und technisch hochstehende Automatisierung von Layout- und Handschriftenerkennung, lässt sich nur durch die enge Zusammenarbeit zwischen GeisteswissenschaftlerINNEn und ComputerspezialistINNen mit unterschiedlichen Voraussetzungen und Ansprüchen an Datenqualität und Herstellung von Transkriptionen erreichen. Die Algorithmen werden somit nicht nur bis zu einem Status als 
                    proof-of-concept erarbeitet, sondern bis zur Praxistauglichkeit verfeinert und in grösseren Forschungs- und Aufbewahrungsumgebungen getestet und verbessert. Die ComputerwissenschaftlerINNEN sind entsprechend ebenfalls ein wichtiges Zielpublikum, wobei bei ihnen weniger die Nutzung der Plattform als das Beisteuern von Software(teilen) anvisiert wird. 
                
            Die Speicherung der Dokumente erfolgt in der Cloud, gehostet auf Servern der Universität Innsbruck. Die importierten Daten bleiben auch während der Bearbeitung unverändert im Dateisystem liegen und werden ergänzt durch METS und PAGE XML. Alle bearbeiteten Dokumente und Daten bleiben somit in den unterschiedlichen Bearbeitungsstadien nicht nur lokal verfügbar, sondern können für andere Transkribusnutzerinnen und -nutzer freigegeben werden. Dank elaboriertem
                     user-management ist die Zuteilung von Rollen möglich. 
                
            Die eingespeisten Dokumente und Daten bleiben privat und vor dem Zugriff Dritter geschützt. Von Projektseite können vorgenommene Arbeitsschritte zwecks besserem Verständnis der ausgeführten Arbeiten und letztlich der Verbesserung der Produkte ausgewertet werden.
            Die Erkennprozesse werden serverseitig durchgeführt, sodass die Ressourcen auf den lokalen Rechnern nicht strapaziert werden. Transkribus ist mit JAVA und SWT programmiert und kann daher plattformunabhängig (Windows, Mac, Linux) genutzt werden.
         
         
            Ein- und Ausblicke im Workshop
            Der Workshop richtet sich sowohl an GeisteswissenschaftlerINNEN als auch an ComputerwissenschaftlerINNEN, wobei vorwiegend die Tools und Möglichkeiten von Transkribus präsentiert werden.
            Zwei zentrale Forschungsaspekte aus READ werden im Rahmen des Workshops durch Experten vorgestellt:
            Einerseits das technische Verfahren des Automatic Text Recognition mit rekurrenten neuronalen Netzen (Leifert et al. 2016). Dabei wird kurz in die Trainings- und Auswertungsmechanismen mit neuronalen Netzen eingeführt und Möglichkeiten der Auswertung demonstriert.
            Andererseits wird die Erkennung von komplexen Layouts, insbesondere Tabellen, erklärt und neueste technische Lösungen vorgestellt.
         
         
            Programm/Ablauf des Workshops
            
               
                  Begrüßung und Informationen zum Projekt READ (Tobias Hodel, Zürich): 20‘
                        Überblick über Ziele und Fortschritte im Rahmen des von der EU geförderten Projekts.
               
                  Machine Learning und automatisierte Text Erkennung (Tobias Strauß, Rostock): 30‘
                        Einführung und Erklärung zum Einsatz neuronaler Netze bei der Texterkennung
                    
               
                  Einführung in Transkribus
                   30‘
                        Aufbau und Funktionieren des Programms, Demonstration des Gebrauchs anhand von Beispielen. Aufzeigen der Möglichkeiten zum Einsatz der Automatisierungen.
                    
               
                  Selbstständiges Arbeiten der Teilnehmenden mit Transkribus:
                   90‘
                    
               Die Möglichkeiten und Grenzen von Transkribus sollen von den Teilnehmenden (falls gewünscht mit eigenen Dokumenten) selbst ausgetestet werden.
               
                  Layout Analyse: Tabellen und andere schwierige Formen
                   (Markus Diem, Wien): 30‘
                        Ein über Transkribus hinausgehender Teil des Projekts beschäftigt sich mit 
                        computer vision. Ziel ist es, auch komplexe Strukturen korrekt als Layout zu erkennen, um die automatisierte Texterkennung überhaupt zu ermöglichen. Tabellen gehören in dem Bereich zu den schwierigsten Formen der Texterkennung.
                    
               
                  Diskussion über Vor- und Nachteile der Software: 30‘
                        Inklusive Evaluation des Tools und der Veranstaltung. Feedbacks werden eingeholt, zur Verbesserung der Software und Webtools (usability, Umfang und Leistung der Automatisierungen etc.).
                    
               Nach Interesse der Teilnehmenden werden während des Workshops Kurzinputs zu folgenden Themen angeboten:
                        
                     Matching von Text und Bild (bspw. aus bestehenden Transkriptionen), 
                            Transkribus Learn (e-Learningumgebung), 
                     Crowdsourcing-Infrastruktur, 
                     ScanTent und DocScan (Fotografieren eigener Dokumente mit Android App).
                  
               
            
         
         Workflow in Transkribus
         Um Texte zu transkribieren oder zu edieren, müssen digitale Bilder hochgeladen und danach mit Layouterkennungswerkzeugen bearbeitet werden. Die Analyse des Layouts kann automatisiert geschehen, wobei die manuelle Kontrolle und falls nötig die Nachbearbeitung im Moment noch sinnvoll ist.
         Dokumente können entweder automatisch mit bereits bestehenden ATR-Modellen (Automatic Text Recognition) erkannt werden oder die Transkription erfolgt händisch und kann danach zum Training neuer Modelle genutzt werden. Insbesondere für die Bearbeitung großer Dokumentenkorpora, die in ähnlichen Handschriften verfasst wurden, lassen sich bereits heute Effizienzgewinne und Vereinfachungen erzielen.
         Aufbauend auf den Transkriptionen ist es möglich eine Vielzahl von Auszeichnungen und Annotationen innerhalb des Textes, aber auch darüber hinaus für Einzeldokumente und ganze Dokumentenbestände anzulegen. Neben der Anreicherung der Dokumente mit Metadaten (Identifikation von Personen, Orten und Sachwörtern) ist somit auch die Möglichkeit der Herstellung von Bestandsbeschreibungen und der Hinterlegung von Transkriptions- und Editionsvorschriften gegeben.
         Ausgabeformate
         Für den Export stehen unterschiedliche Formate und Ausgabeformen zur Verfügung. So ist es möglich XML-Dateien zu exportieren, die den Vorgaben der TEI entsprechen. Ausgehend davon können komplexe digitale Editionen erstellt werden, die jedoch im Unterschied zu den meisten herkömmlichen Editionen eine enge Verzahnung mit den verwendeten Bilddateien aufweisen. Dadurch werden Editionen ermöglicht, die den transkribierten Text in der Zusammenschau mit der faksimilierten Vorlage sichtbar machen. Daneben sind auch Ausgaben als Druckdaten (PDF) oder zur Weiterbearbeitung für Textverarbeitungsprogramme (DOCX) implementiert. Schließlich ist auch ein Export im PAGE-Format (zur Anzeige in Viewern für OCR gelesene Dokumente, Pletschacher, 2010) sowie als METS (Metadata Encoding and Transmission) möglich.
         Zielpublikum
         Die Plattform ist für unterschiedliche Gruppen konzipiert. Einerseits für GeisteswissenschaftlerINNEN, die selbst Transkriptionen und Editionen historischer Dokumente erstellen möchten. Andererseits richtet sich die Plattform an Archive, Bibliotheken und andere Erinnerungsinstitutionen, die handschriftliche Dokumente in ihren Sammlungen aufbewahren und ein Interesse an der Aufbereitung des Materials haben. Angesprochen werden sollen auch Studierende der Geistes-, Archiv- und Bibliothekswissenschaften mit einem Interesse an der Transkription historischer Handschriften.
         Das Ziel, eine robuste und technisch hochstehende Automatisierung von Layout- und Handschriftenerkennung, lässt sich nur durch die enge Zusammenarbeit zwischen GeisteswissenschaftlerINNEn und ComputerspezialistINNen mit unterschiedlichen Voraussetzungen und Ansprüchen an Datenqualität und Herstellung von Transkriptionen erreichen. Die Algorithmen werden somit nicht nur bis zu einem Status als 
                proof-of-concept erarbeitet, sondern bis zur Praxistauglichkeit verfeinert und in grösseren Forschungs- und Aufbewahrungsumgebungen getestet und verbessert. Die ComputerwissenschaftlerINNEN sind entsprechend ebenfalls ein wichtiges Zielpublikum, wobei bei ihnen weniger die Nutzung der Plattform als das Beisteuern von Software(teilen) anvisiert wird. 
            
         Die Speicherung der Dokumente erfolgt in der Cloud, gehostet auf Servern der Universität Innsbruck. Die importierten Daten bleiben auch während der Bearbeitung unverändert im Dateisystem liegen und werden ergänzt durch METS und PAGE XML. Alle bearbeiteten Dokumente und Daten bleiben somit in den unterschiedlichen Bearbeitungsstadien nicht nur lokal verfügbar, sondern können für andere Transkribusnutzerinnen und -nutzer freigegeben werden. Dank elaboriertem
                 user-management ist die Zuteilung von Rollen möglich. 
            
         Die eingespeisten Dokumente und Daten bleiben privat und vor dem Zugriff Dritter geschützt. Von Projektseite können vorgenommene Arbeitsschritte zwecks besserem Verständnis der ausgeführten Arbeiten und letztlich der Verbesserung der Produkte ausgewertet werden.
         Die Erkennprozesse werden serverseitig durchgeführt, sodass die Ressourcen auf den lokalen Rechnern nicht strapaziert werden. Transkribus ist mit JAVA und SWT programmiert und kann daher plattformunabhängig (Windows, Mac, Linux) genutzt werden.
         Ein- und Ausblicke im Workshop
         Der Workshop richtet sich sowohl an GeisteswissenschaftlerINNEN als auch an ComputerwissenschaftlerINNEN, wobei vorwiegend die Tools und Möglichkeiten von Transkribus präsentiert werden.
         Zwei zentrale Forschungsaspekte aus READ werden im Rahmen des Workshops durch Experten vorgestellt:
         Einerseits das technische Verfahren des Automatic Text Recognition mit rekurrenten neuronalen Netzen (Leifert et al. 2016). Dabei wird kurz in die Trainings- und Auswertungsmechanismen mit neuronalen Netzen eingeführt und Möglichkeiten der Auswertung demonstriert.
         Andererseits wird die Erkennung von komplexen Layouts, insbesondere Tabellen, erklärt und neueste technische Lösungen vorgestellt.
         Programm/Ablauf des Workshops
         
            Begrüßung und Informationen zum Projekt READ (Tobias Hodel, Zürich): 20‘
                Überblick über Ziele und Fortschritte im Rahmen des von der EU geförderten Projekts.
                Machine Learning und automatisierte Text Erkennung (Tobias Strauß, Rostock): 30‘
                Einführung und Erklärung zum Einsatz neuronaler Netze bei der Texterkennung
                Einführung in Transkribus
             30‘
                Aufbau und Funktionieren des Programms, Demonstration des Gebrauchs anhand von Beispielen. Aufzeigen der Möglichkeiten zum Einsatz der Automatisierungen.
                Selbstständiges Arbeiten der Teilnehmenden mit Transkribus:
            90‘
            
         Die Möglichkeiten und Grenzen von Transkribus sollen von den Teilnehmenden (falls gewünscht mit eigenen Dokumenten) selbst ausgetestet werden.
         
            Layout Analyse: Tabellen und andere schwierige Formen
             (Markus Diem, Wien): 30‘
                Ein über Transkribus hinausgehender Teil des Projekts beschäftigt sich mit 
                computer vision. Ziel ist es, auch komplexe Strukturen korrekt als Layout zu erkennen, um die automatisierte Texterkennung überhaupt zu ermöglichen. Tabellen gehören in dem Bereich zu den schwierigsten Formen der Texterkennung.
            
         
            Diskussion über Vor- und Nachteile der Software: 30‘
                Inklusive Evaluation des Tools und der Veranstaltung. Feedbacks werden eingeholt, zur Verbesserung der Software und Webtools (usability, Umfang und Leistung der Automatisierungen etc.).
            
         Nach Interesse der Teilnehmenden werden während des Workshops Kurzinputs zu folgenden Themen angeboten:
                - Matching von Text und Bild (bspw. aus bestehenden Transkriptionen), 
                - Transkribus Learn (e-Learningumgebung), 
                - Crowdsourcing-Infrastruktur, 
                - ScanTent und DocScan (Fotografieren eigener Dokumente mit Android App).
            
         Während des gesamten Workshops stehen drei wissenschaftliche Mitarbeitende des Projekts für Fragen und Auskünfte zur Verfügung. 
                Tobias Hodel (
            
               tobias.hodel@ji.zh.ch)
            
             nimmt bereits im Vorfeld gerne Dokumente oder Projektideen an, damit sich die Veranstalter bereits vor dem Workshop Gedanken zu möglichen technischen Umsetzungen machen können.
         
         Das Projekt READ und somit die Weiterentwicklung von Transkribus werden finanziert durch einen Grant der Europäischen Union im Rahmen des Horizon 2020 Forschungs- und Innovationsprogramms (grant agreement No 674943).
         Zahl der möglichen Teilnehmerinnen und Teilnehmer: 30-40 Personen (auch abhängig von der Raumgrösse).
                Benötigte technische Ausstattung: Allgemein: Beamer, evtl. Whiteboard.
                Teilnehmende: Eigener Rechner (wenn möglich Installation von Transkribus; Hilfe zur Installation von Transkribus wird 15 Minuten vor der Veranstaltung angeboten)
            
         Anmeldungen und Rückfragen bitte an tobias.hodel@ji.zh.ch
         Kontaktdaten aller Beitragenden (inkl. Forschungsinteressen)
         Markus Diem, Technische Universität Wien, Institute of Computer Aided Automation Computer Vision Lab, Favoritenstr. 9/183-2, A-1040 Vienna, Österreich; diem@caa.tuwien.ac.at (Computer Vision, Document Analysis, Layout Analysis/Page Segmentation, Cluster Analysis, Automated Flow Cytometry Analysis).
         Tobias Hodel, Staatsarchiv des Kantons Zürich, Winterthurerstrasse 170, CH-8057 Zürich, Schweiz; tobias.hodel@ji.zh.ch (Digital Humanities; Automatic Textrecognition; eArchiving; Information Retrieval).
         Tobias Strauß, Institut für Mathematik, Ulmenstraße 69, Universität Rostock, 18051 Rostock, Deutschland; tobias.strauss@uni-rostock.de; (Deep Learning, Information Retrieval und Natural Language Processing).
      
      
         
            
               Bibliographie
               Leifert, G., Strauß, T., Grüning, T., Wustlich, W., Labahn, R., 2016. Cells in Multidimensional Recurrent Neural Networks. Journal of Machine Learning Research 17, 1-37.
               
                  Leifert, G., Strauß, T., Grüning, T., Wustlich, W., Labahn, R., 2016. Cells in Multidimensional Recurrent Neural Networks. Journal of Machine Learning Research 17, 1-37.
            
         
      
   



      
         
            Einleitung
            Computergestütztes Arbeiten kann geisteswissenschaftliches Forschen auf unterschiedlichste Weise befördern und bereichern. Dennoch müssen wir in unserem Arbeitsalltag und in Gesprächen mit Kolleginnen und Kollegen immer wieder feststellen, dass viele traditioneller arbeitende Geisteswissenschaftler digitalen Methoden noch immer mit Skepsis begegnen. Dies liegt nicht zuletzt daran, dass in den Geisteswissenschaften zahlreiche Methoden zum Einsatz kommen, von denen nur einigen wenigen eine derart formalisierte Arbeitsweise naheliegt, wie sie im Rahmen der Digital Humanities oft verfolgt wird.
                
            Das lässt sich gut am Beispiel der Literaturwissenschaft illustrieren: Digitale Methoden werden bisher vornehmlich von Literaturwissenschaftlern genutzt, die an strukturellen oder anderen formalen Aspekten literarischer Texte interessiert sind (beispielsweise an narrativen Strukturen, Figurennetzwerken etc.).
            
            Ihrem traditionellen Selbstverständnis nach ist die Literaturwissenschaft allerdings zentral an komplexen und innovativen 
                    Interpretationen literarischer Texte interessiert – und wie diese durch digitale Methoden der Textanalyse befördert werden können, ist nicht evident.
                
            Damit digitale Methoden eine breitere Akzeptanz finden, ist es deswegen notwendig, den Nutzen dieser Methoden auch für stärker hermeneutisch ausgerichtete geisteswissenschaftliche Forschungsfragen zu reflektieren. Unserem (weiten) Verständnis von “Hermeneutik” entsprechend handelt es sich bei hermeneutischen Forschungsfragen um Fragen, die auf die (holistische) Auslegung bzw. Deutung von Texten gerichtet sind (vgl. bspw. Spörl 2004: 128). In literaturwissenschaftlichen Zusammenhängen spielen dabei insbesondere Fragen nach Funktion bzw. Wirkung bestimmter Textelemente oder des Gesamttextes eine Rolle, ebenso wie die In-Beziehung-Setzung des Textes mit bestimmten Kontexten. Diese Forschungsfragen sollten exponiert im Zusammenhang mit der Entwicklung von Tools, digitaler Forschungsumgebungen und vor allem didaktischer Konzepte zur Vermittlung von DH-Methoden berücksichtigt werden. Diese Forderungen werden bisher jedoch nicht in zureichendem Maße erfüllt.
            
            Wir möchten in diesem Beitrag das aktuelle Projekt forTEXT (2017–2020) vorstellen, das der Vermittlung, Aufbereitung und Bereitstellung von Mitteln zur computergestützten Textanalyse insbesondere für hermeneutisch arbeitende Geisteswissenschaftler gewidmet ist. Im Folgenden sollen in diesem Zusammenhang zunächst die unterschiedlichen konzeptionellen Dimensionen (Abschnitt 2) sowie anschließend erste inhaltliche Ergebnisse des Projekts präsentiert werden (Abschnitt 3).
         
         
            Dimensionen des forTEXT-Projekts
            
               Paradigmen
               Das Anfang 2017 gestartete DFG-Projekt 
                        forTEXT. Literatur digital erforschen (http://www.fortext.net) hat die Entwicklung einer digitalen Forschungsumgebung zum Ziel, die im Rahmen der qualitativen Analyse und Interpretation von Texten genutzt werden kann. Das Augenmerk bei der Gestaltung dieser Umgebung liegt insbesondere auf zwei Aspekten:
                    
               (a) Orientierung an genuin geisteswissenschaftlichen Arbeitsweisen: Es geht, ganz im Sinne des geisteswissenschaftlichen Selbstverständnisses, um die Unterstützung der genuin 
                        interpretativen Auseinandersetzung mit Texten. In anderen Worten: forTEXTs Fokus liegt 
                        nicht ausschließlich auf der statistischen Auswertung von Texten, wie es sonst im Rahmen von DH-Methoden zur Textanalyse oft der Fall ist.
                         Auf diese Weise soll gewährleistet werden, dass traditioneller arbeitende Geisteswissenschaftler die Umgebung tatsächlich zur digitalen Unterstützung vertrauter Methoden der Textanalyse und -interpretation nutzen können und ihnen keine Hinwendung zur statistischen Textanalyse abverlangt wird.
                    
               (b) Niedrigschwelliger Zugang: Geisteswissenschaftler sollen die digitale Forschungsumgebung 
                        intuitiv und weitgehend ohne technische Vorkenntnisse nutzen können. Hierzu trägt zum einen die Tatsache bei, dass forTEXT ein individualisiertes Empfehlungssystem zur Verfügung stellt, das geisteswissenschaftlichen Nutzern Vorschläge unterbreitet, welche digitalen Ressourcen, Routinen und Tools für ihr Projekt hilfreich sein könnten (siehe auch Abschnitt 2.2). Nutzer werden also nicht einfach mit einem unüberschaubaren digitalen Angebot alleingelassen, dessen potenziellen Nutzen für die eigene Fragestellung sie sich erst noch selbst erschließen müssen. Zum anderen stellt forTEXT leicht verständliche Beschreibungen zu digitalen Methoden und Korpora zur Verfügung, ebenso wie intuitiv bedienbare Benutzeroberflächen für digitale Werkzeuge zur Textanalyse und -interpretation (siehe auch Abschnitt 2.3). Auf diese Weise können DH-Methoden ohne technisches Know-how sowie ohne das aufwändige Studieren von Nutzerhandbüchern eingesetzt werden.
                    
               In den folgenden Unterabschnitten sollen sowohl forTEXTs Empfehlungssystem als auch die drei Komponenten des Informationsrepositoriums (
                        Routinen, Ressourcen und Tools) kurz etwas detaillierter vorgestellt werden.
                    
            
            
               Individualisiertes Empfehlungssystem
               Für Geisteswissenschaftler, die noch nicht wissen, auf welche Weise digitale Methoden der Textanalyse und -interpretation ihre eigene Forschung unterstützen können, bietet forTEXT ein individualisiertes Empfehlungssystem in Form eines digitalen Fragebogens an (siehe Abb. 1).
               
                  
                  Abb. 1: forTEXTs individualisiertes Empfehlungssystem für digitale Textuntersuchung (Ausschnitt des Prototyps)
               
               Hier können die Nutzer beispielsweise angeben, ob sie schon Vorerfahrungen mit digitalen Methoden der Textanalyse gemacht haben, in welchem Zustand sich ihr Textkorpus befindet, unter welcher Fragestellung sie ihre Texte untersuchen wollen und welcher literaturtheoretischen Schule sie sich zuordnen. Als Output erhalten die Forscher, angepasst an die von ihnen gemachten Angaben, eine Liste mit Vorschlägen zu digitalen Korpora, Methoden und Werkzeugen, die zu ihrer Fragestellung und Arbeitsweise passen. Das Empfehlungssystem also eine individualisierte Kompilation aus forTEXTs Inhalten und Verzeichnissen, die im Folgenden kurz vorgestellt werden sollen.
                    
            
            
               Routinen, Ressourcen und Tools
               forTEXTs digitale Forschungsumgebung ist in drei Bereiche gegliedert.
               (a) Routinen: Im Teilbereich 
                        Routinen finden sich zum einen Informationstexte zu digitalen Methoden, die der Analyse und Interpretation von Texten dienen (bspw. zu taxonomiebasiertem Annotieren, zur Textanalyse mittels individualisierter Abfragen auf Text- und Annotationsdaten, zu Topic Modeling etc.), sowie zu vorbereitenden Prozeduren wie der Digitalisierung von Texten. In diesen Informationstexten finden sich darüber hinaus Links zu digitalen Tools (s.u.), mithilfe derer die fraglichen Methoden ausgeführt werden können.
                    
               Zum anderen werden unter 
                        Routinen auch didaktische Texte (d.h. Lerneinheiten und Lehrmodule) zur Verfügung gestellt. Die Lerneinheiten dienen der selbstständigen Aneignung bestimmter digitaler Methoden und Tools, während die Lehrmodule didaktisches Material für Lehrende zur Verfügung stellen, die auf 90-minütige Workshopsituationen zugeschnitten sind (siehe auch Abschnitt 3). Die Entwicklung neuer Lehrmodule wird sich dabei an den im Projektverlauf akquirierten Bedarfen der Nutzer orientieren.
                    
               (b) Ressourcen: Unter 
                        Ressourcen ist ein Verzeichnis digital nutzbarer Korpora zu finden. Hierunter fallen sowohl hochqualitativ digitalisierte Textkorpora als auch inhaltlich annotierte Korpora, die nachgenutzt werden können. Einige annotierte Korpora werden im Rahmen von forTEXT selbst bzw. affiliierten Projekten produziert. Das Verzeichnis enthält darüber hinaus informative Beschreibungen der gelisteten Ressourcen.
                    
               (c) Tools: Im Bereich 
                        Tools findet sich schließlich eine kommentierte Liste digitaler Werkzeug-Suites bzw. Funktionskomponenten, mithilfe derer unterschiedliche textanalytische und -interpretatorische Operationen durchgeführt oder unterstützt werden können. Darüber hinaus sollen in forTEXT auch eigene Funktionskomponenten entwickelt werden. Hierzu gehört vornehmlich die Weiterentwicklung des Textannotations- und Analyseprogramms CATMA (
                        http://www.catma.de) – aber auch die Entwicklung von graphischen Step-by-step-Benutzerschnittstellen für bestehende Tools. Dies soll es technisch weniger versierten geisteswissenschaftlichen Nutzern ermöglichen, hochfunktionale Tools einzusetzen, ohne sich umfangreich einarbeiten zu müssen.
                    
               Alle Verzeichnisse und Einträge aus den drei forTEXT-Bereichen Routinen, Ressourcen und Tools können von Nutzern eigenständig durchsucht und aufgerufen werden – oder es erfolgt ein angeleiteter Zugriff durch die Nutzung des Empfehlungssystems.
               Im verbleibenden Teil dieses Beitrags möchten wir etwas genauer auf einen Lehrmodulentwurf eingehen, das dem forTEXT-Bereich 
                        Routinen zuzuordnen ist. Anhand dieses Moduls soll beispielhaft gezeigt werden, wie in forTEXT die Paradigmen der Orientierung an einer genuin geisteswissenschaftlichen Arbeitsweise und des niedrigschwelligen Zugangs umgesetzt werden.
                    
            
         
         
            Lehrmodule Manuelles Annotieren
            Zwei von forTEXTs 90-minütigen Lehrmodulen sind der Vermittlung der digitalen Methode des 
                    manuellen Annotierens gewidmet. Die Methode des Annotierens stellt einen guten Brückenschlag zur traditionelleren geisteswissenschaftlichen Arbeitsweise dar – schließlich gehört das Anbringen von Notizen in zu interpretierenden literarischen Texten seit jeher zur literaturwissenschaftlichen Arbeitspraxis. In den Lehrmodulen zum 
                    digitalen Annotieren gilt es nun, zum einen an die bereits bekannte Praxis anzuknüpfen und zum anderen deutlich zu machen, inwiefern die digitale Unterstützung es ermöglicht, bekannte Arbeitsprozesse deutlich effektiver durchzuführen, oder gar ganz neue Arbeitsweisen eröffnet, die das jeweilige Forschungsziel befördern.
                
            Um diese Anforderungen umzusetzen, sieht forTEXT zwei Lehreinheiten zum manuellen Annotieren vor, von denen wir die erste im Folgenden kurz vorstellen möchten.
            In der Einheit Taxonomiebasiertes Annotieren wird schrittweise gezeigt, wie mithilfe des Annotations- und Analysetools CATMA freie Kommentare in literarischen Texten angebracht sowie analysiert und systematisiert werden können. Die Systematisierung freier Kommentare kann wiederum als Grundlage dienen, um eine Annotationstaxonomie zu entwickeln, die dann für eine noch feinere und zielgerichtetere Analyse des literarischen Textes genutzt werden kann.
                
            Das verwendete Programm CATMA ist hierbei in zweifacher Hinsicht auf die forTEXT-Paradigmen abgestimmt: Es bietet eine intuitiv bedienbare Benutzeroberfläche und unterstützt den freien, undogmatischen und genuin interpretativen Zugang zu Texten, während es zugleich Optionen stärkerer Formalisierung bereithält (vgl. Abb. 2).
            
            
               
               Abb. 2: Intuitives, nicht-deterministisches Annotieren in CATMA
            
            Die im Rahmen des Lehrmoduls verfolgte didaktische Strategie hat mehrere Vorteile: Der erste Schritt, d.h. das digitale Anbringen freier Kommentare, stellt einen vollkommen explorativen und potenziell unstrukturierten Zugang zu literarischen Texten dar. Er erzwingt also kein formalistisches Umdenken und bildet die traditionellere geisteswissenschaftlich-hermeneutische Arbeitsweise gut ab. Im Vergleich zum analogen Arbeiten birgt er aber dennoch den Vorteil, dass die freien Kommentare durch digitale Unterstützung effektiver 
                    nachgenutzt werden können.
                
            Als zusätzliches Angebot an Literaturwissenschaftler, die für eine etwas stärkere Formalisierung ihres Zugangs offen sind, zeigt das Lehrmodul, welche weiteren Vorteile und Optionen 
                    taxonomiebasiertes Annotieren mit sich bringt (bspw. bessere Vergleichbarkeit, detailliertere und vereinfachte Analyse, ggf. Reproduzierbarkeit etc.) und wie dieses digital umgesetzt werden kann. Diese Herangehensweise kann in der Folge auch die Nützlichkeit noch stärker formalistisch anmutender DH-Techniken plausibilisieren – wie beispielsweise der kollaborativen, guidelinegestützten Annotation oder der automatisierten Annotation bzw. Informationsextraktion.
                
            Im Lehrmodul zum manuellen digitalen Annotieren sind also die Paradigmen umgesetzt, die auch die weitere Arbeit am forTEXT-Projekt bestimmen sollen: Durch stärkere Orientierung an der traditionell-geisteswissenschaftlichen Arbeitsweise und erleichterten Zugang können digitale Methoden einer breiteren Nutzergemeinschaft nähergebracht werden.
         
      
      
         
             Wir werden aufgrund besserer Lesbarkeit im Folgenden nur noch die männliche Form verwenden – unsere Ausführungen beziehen sich aber selbstverständlich dennoch auf alle Geschlechter.
             Auf diese persistente Skepsis verweisen beispielsweise auch Fiedler und Weiß in ihrem Tagungsbericht zur DHd-Konferenz 2015 in Graz (vgl. Fiedler/Weiß 2015).
             Beispiele hierfür sind u.a. die folgenden Projekte aus dem Bereich der digitalen Literaturwissenschaft: “heureCLÉA”, ein Projekt zur automatischen Annotation von Zeitphänomenen in narrativen Texten (
                            
                                http://www.heureclea.de
                            , vgl. Bögel et al. 2015); “Redewiedergabe. Eine literatur- und sprachwissenschaftliche Korpusanalyse”, ein Projekt zur automatisierten Annotation von Figurenrede (
                            
                                http://www.redewiedergabe.de
                            , vgl. auch Brunner 2015); die “Computational Stylistics Group” (
                            
                                https://sites.google.com/site/computationalstylistics
                            , vgl. Rybicki/Eder/Hoover 2016); das “Rhythmicalizer”-Projekt (
                            
                                http://www.rhythmicalizer.net
                            , vgl. Baumann/Meyer-Sickendiek 2016), das ein Tool zur Erkennung von Prosodie in freien Versen entwickelt; ebenso wie Projekte zur Analyse von Figurennetzwerken wie etwa “Digital Literary Network Analysis” (
                            
                                https://dlina.github.io/about
                            , vgl. Fischer et al. 2017).
                        
             So betonen beispielsweise Kindt und Köppe, dass literaturwissenschaftliche Interpretationen weniger auf strukturelle Aspekte oder bloßes Sprachverstehen gerichtet sind als vielmehr auf mannigfaltige, komplexe und oft nicht eindeutig umrissene Verstehensziele (vgl. Kindt/Köppe 2008: 12–14).
             So sind beispielsweise bestehende Verzeichnisse für Tools zur Textanalyse aufgrund der vornehmlich technisch ausgerichteten Beschreibungen für Geisteswissenschaftler ohne DH-Vorwissen äußerst schwer zugänglich (z.B. TAPoR, 
                            
                                http://www.tapor.ca
                            ). Einen vielversprechenden Ansatz zur nutzbaren Aufbereitung und Vermittlung digitaler Methoden – allerdings vornehmlich in schulischen Kontexten – stellt das Projekt “Digitalität in den Fachdidaktiken” dar (
                            
                                http://dhd-blog.org/?p=6812
                            ).
                        
             So basieren zahlreiche DH-Praktiken zur Textanalyse auf der automatischen Verarbeitung von Textoberflächendaten wie Wortfrequenzen, beispielsweise stilometrische Untersuchungen oder Topic Modeling (vgl. Brett 2012).
             Der Fragebogen bietet Nutzern zudem die Option, auf fehlende Antwort- oder sogar Fragemöglichkeiten hinzuweisen. Auf diese Weise kann der Fragebogen – und mit ihm forTEXTs Empfehlungssystem – im Laufe des Projekts weiter optimiert werden.
             Affiliierte Projekte sind beispielsweise “3DH” (
                                
                                    http://www.threedh.net
                                ) und “SANTA: Shared Task on the Analysis of Narrative levels Through Annotation” (
                                
                                    https://sharedtasksinthedh.github.io/
                                ), in deren Rahmen narrative Ebenen in Texten annotiert werden.
                            
             Ein Vorbild hierfür ist der bereits in CATMA integrierte “Query Builder”, mithilfe dessen Nutzer komplexe Abfragen über Text- und Annotationsdaten laufen lassen können, ohne eine Abfragesprache lernen zu müssen.
             Vgl. bspw. Bauer/Zirker 2015: Absatz 1.
             So hat sich CATMA beispielsweise dem Konzept des 
                            hermeneutischen Markups verschrieben – darunter verstehen wir nach Piez Markup das bewusst interpretativ und flexibel ist (vgl. Piez 2010). Auf diese Weise erlaubt CATMA beispielsweise mehrfache und sogar “widersprüchliche” Annotationen derselben Textstelle – dennoch sind die Annotationen im standardisierten TEI-Format exportierbar und somit flexibel nachnutzbar. Diese Markup-Eigenschaften werden im Backend ermöglicht durch Standoff-Markup und die Nutzung von TEI Feature Structures (
                            
                                http://www.tei-c.org/release/doc/tei-p5-doc/de/html/FS.html
                            ).
                        
             Vgl. zu den Vorteilen von Klassifikationssystemen wie Taxonomien und Typologien auch Bailey 1994.
             Zur Methode und zum Nutzen kollaborativen Annotierens, siehe auch Gius/Jacke 2015 und Gius/Jacke 2017.
         
         
            
               Bibliographie
               
                  Bailey, Kenneth D. (1994): 
                        Typologies and Taxonomies. An Introduction to Classification Techniques. Thousand Oaks/London/New Delhi: Sage Publications.
                    
               
                  Bauer, Matthias / Zirker, Angelika (2015): “Whipping Boys Explained. Literary Annotation and Digital Humanities”, in: 
                        MLA Commons. Literary Studies in the Digital Age
                  
                            https://dlsanthology.mla.hcommons.org/whipping-boys-explained-literary-annotation-and-digital-humanities/
                         [letzter Zugriff 06. September 2017].
                    
               
                  Baumann, Timo / Meyer-Sickendiek, Burkhard (2016): “Large-scale Analysis of Spoken Free-verse Poetry”, in:
                         Proceedings of LT4DH-Workshop 2016
                  
                            http://edoc.sub.uni-hamburg.de/informatik/volltexte/2016/228/pdf/baumann_large_scale_analysis.pdf
                         [letzter Zugriff 06. September 2017].
                    
               
                  Bögel, Thomas / Gertz, Michael / Gius, Evelyn / Jacke, Janina / Meister, Jan Christoph / Petris, Marco / Strötgen, Jannik (2015): “Collaborative Text Annotation Meets Machine Learning. heureCLÉA, a Digital Heuristic of Narrative”, in: 
                        DHCommons Journal 1 
                        
                            http://dhcommons.org/journal/issue-1/collaborative-text-annotation-meets-machine-learning-heurecl%C3%A9-digital-heuristic
                         [letzter Zugriff 06. September 2017].
                    
               
                  Brett, Megan R. (2012): “Topic Modeling. A Basic Introduction”, in: 
                        Journal of Digital Humanities 2(1) 
                        
                            http://journalofdigitalhumanities.org/2-1/topic-modeling-a-basic-introduction-by-megan-r-brett/
                         [letzter Zugriff 06. September 2017].
                    
               
                  Brunner, Annelen (2015): 
                        Automatische Erkennung von Redewiedergabe. Berlin / Boston: de Gruyter (= Narratologia Bd. 47).
                    
               
                  Fiedler, Maik / Weiß, Andreas (2015): “Von Daten zu Erkenntnissen. Digitale Geisteswissenschaften als Mittler zwischen Information und Interpretation. DHd-Jahrestagung 2015” 
                        
                            http://www.hsozkult.de/conferencereport/id/tagungsberichte-6059
                         [letzter Zugriff 06. September 2017].
                    
               
                  Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario / Kittel, Christopher / Trilcke, Peer (2017): “Network Dynamics, Plot Analysis. Approaching the Progressive Structuration of Literary Texts”, in: 
                        Digital Humanities 2017. Conference Abstracts  
                        
                            https://dh2017.adho.org/abstracts/DH2017-abstracts.pdf
                         [letzter Zugriff 06. September 2017].
                    
               
                  Gius, Evelyn / Jacke, Janina (2015): “Informatik und Hermeneutik. Zum Mehrwert interdisziplinärer Textanalyse”, in: 
                        Zeitschrift für digitale Geisteswissenschaften, Sonderband 1 
                        
                            http://www.zfdg.de/sb001_006
                         [letzter Zugriff 06. September 2017].
                    
               
                  Gius, Evelyn / Jacke, Janina (2017): “The Hermeneutic Profit of Annotation. On Preventing and Fostering Disagreement in Literary Analysis”, in: 
                        International Journal of Humanities and Arts Computing 11(2) 233–254.
                    
               
                  Kindt, Tom / Köppe, Tilmann (2008): “Einleitung”, in: dies. (eds.): 
                        Moderne Interpretationstheorien. Ein Reader. Göttingen: Vandenhoeck & Ruprecht 7–26.
                    
               
                  Piez, Wendell (2010): “Towards Hermeneutic Markup. An Architectural Outline”, in: 
                        Digital Humanities 2010. Conference Abstracts
                  
                            http://piez.org/wendell/papers/dh2010/
                         [letzter Zugriff 06. September 2017].
                    
               
                  Rybicki, Jan / Eder, Maciej / Hoover, David (2016): “Computational stylistics and text analysis”, in: Crompton, Constance / Lane, Richard J. / Siemens, Ray (eds.): 
                        Doing Digital Humanities. London / New York: Routledge 123–144.
                    
               
                  Spörl, Uwe (2004): 
                        Basislexikon Literaturwissenschaft. 2., durchges. Aufl. Paderborn [u.a.]: Schöningh.
                    
               
                  Websites
               
               
                  3DH (http://www.threedh.net)
                    
               
                  CATMA (http://www.catma.de) 
                    
               
                  Computational Stylistics Group (https://sites.google.com/site/computationalstylistics)
                    
               
                  Digital Literary Network Analysis (https://dlina.github.io/about)
                    
               
                  Digitalität in den Fachdidaktiken. Projektpräsentation im DHd-Blog (
                        
                            http://dhd-blog.org/?p=6812)
                    
               
                  forTEXT (http://www.fortext.net)
                    
               
                  heureCLÉA (http://www.heureclea.de)
                    
               
                  Redewiedergabe (http://www.redewiedergabe.de)
                    
               
                  Rhythmicalizer (http://www.rhythmicalizer.net)
                    
               
                  
                     SANTA: Shared Task on the Analysis of Narrative levels Through Annotation (
                  https://sharedtasksinthedh.github.io/
                  )
               
               
                  TAPoR (http://www.tapor.ca)
                    
               
                  TEI Feature Structures (http://www.tei-c.org/release/doc/tei-p5-doc/de/html/FS.html)
                    
            
         
      
   



      
         
            Die Postkarte als Forschungsobjekt
            Denkt man an Postkarten, denkt man zunächst an Urlaub und Landschaftsmotive aus fernen Ländern. Die aktuelle Nutzung der Postkarte unterscheidet sich jedoch stark von der vor über hundert Jahren. Postkarten waren ein weit verbreitetes Kommunikationsmittel, vergleichbar mit Kurznachrichtendiensten von heute. Sie wurden aber auch gesammelt und getauscht. So vielfältig wie ihr Gebrauch ist auch ihre Nutzung in der geisteswissenschaftlichen Forschung. Postkarten werden seit den 1980er Jahren, als die Ethnologie sie zunächst für Kolonialismusforschungen vermehrt nutzte, als Quelle unterschiedlicher geisteswissenschaftlicher Disziplinen herangezogen. Mittlerweile dient das Massenmedium Postkarte dazu, Fragen aus der Geschichtswissenschaft, Sozial-, wie Kunst- und Fotografiegeschichte, den Literaturwissenschaften oder der Linguistik zu beantworten. Wurde zunächst in erster Linie bildwissenschaftlich gearbeitet, wird in den letzten zwanzig Jahren auch vermehrt auf die Relevanz der Postkarte als Bild-Text-Medium hingewiesen. Doch nicht nur die Motive dieses Hybridmediums, das zugleich Bild und Text beinhaltet, sondern auch die Natur des Massenmediums an sich, die Herstellung, der Vertrieb, die Zirkulation im politisch-gesellschaftlichen Kontext, Drucktechniken oder Auflagen sind für Forschungsfragen relevant (vgl. Holzer 2007, Tropper 2014).
            Insbesondere im Hinblick auf die letzten beiden Aspekte ermöglichen die digitalen Geisteswissenschaften neue Forschungsperspektiven. Aufbauend auf digitalisierte und strukturiert erfasste Konvolute können anhand digitaler Methoden statistische Auswertungen, quantitative Analysen und qualitative Untersuchungen durchgeführt werden.
         
         
            Das Projekt
            Das Projekt “Postkartensammlung GrazMuseum Online” zeigt, welche Wege für die Postkartenforschung denkbar sind, da im Projekt der Quellenwert der Postkarte an sich und die Aufbereitung für unterschiedliche Disziplinen im Mittelpunkt stehen. Zentrales Anliegen ist es, bildbasierte ebenso wie textbasierte Analysen zu gewährleisten und die Materialität der Postkarte - als Objekt, das produziert und gebraucht wurde, ernst zu nehmen. In diesem Sinne werden im Unterschied zu den meisten Postkarten, die online gezeigt werden (vgl. AKON, Bildarchiv ETH Schweiz), nicht nur die Bild-, sondern auch die Textseite präsentiert. Dadurch wird über bildwissenschaftliche Fragestellungen hinaus ein ganzheitlicher Blick auf das Medium und seine Verwendungsweisen ermöglicht. Für eine inhaltliche Recherche sind die Postkarten mit Schlagworten angereichert, wodurch thematische Zugänge - etwa zu einer Geschichte des Tourismus oder des Verkehrs möglich sind. Ebenso erfasst sind die Reproduktionsverfahren, die für Untersuchungen zu Druck- oder Fototechniken herangezogen werden können und auch Rückschlüsse auf Auflagenhöhe erlauben. Daneben sind auch die Produzenten der Karten erfasst. Damit ist erstmals die Möglichkeit gegeben, zielgerichtet nach Herstellern zu suchen und so auch einen Einblick in deren Sortimente und Produktionsweisen zu erhalten. 
            Für jede Postkarte gibt es einen Datensatz, der die zugehörigen Digitalisate und Metadaten beinhaltet und im digitalen Archiv GAMS (GAMS; Steiner/Stigler 2017) langfristig verfügbar gemacht wird.
            Der Zugang zum Bestand erfolgt über klassische Suchmöglichkeiten (Volltext- und Facettensuche), Schlagwortwolken, geographische Karten (DARIAH GeoBrowser) und virtuelle Rundgänge (StoryMapJS). Die digitalen Faksimiles können stufenlos gezoomt und gedreht werden (OpenSeadragon Viewer), wodurch unterschiedliche Leserichtungen oder etwa Details im Bildmotiv betrachtet werden können. Anhand der Faksimile der Text- oder Adressseite werden auch Informationen vermittelt, die bei der Aufnahme der Metadaten im Zuge des Projektes nicht erfasst werden konnten (z.B. Schriftbilder oder Postwertzeichen).
         
         
            
               Herausforderungen in der Datenmodellierung
                
            Bei der Datenmodellierung steht man vor der Entscheidung, aus welcher Richtung man sich der Beschreibung der Postkarte nähern will. Auf der einen Seite steht die Mitteilung, die Text beinhaltet, der transkribiert, annotiert und angereichert werden kann. Auf der anderen Seite steht das Objekt, das unter spezifischen Gesichtspunkten eingeordnet, bewertet und inventarisiert wird. Dafür bieten sich zwei etablierte Standards an: LIDO und TEI. Letzteres eignet sich speziell für den Zugang über textuelle Inhalte.
                     LIDO ist ein Standardformat, das eigens für die Beschreibung von Kulturerbeobjekten, ihre Klassifikation, Identifikation und Einbindung von in Bezug stehenden Ereignissen (Entstehung, Nutzung, Archivierung, ...), entwickelt wurde. Für tiefere Texterschließungen ist dieser Standard nicht geeignet.
                     Da im Projekt die Beschreibung der Postkarte aus museologischer Sicht im Vordergrund steht und sich das Modellierungskonzept der Events (insb. für Postläufe) anbietet, sind die Daten nach LIDO modelliert und annotiert. Für Transkriptionen handschriftlicher Mitteilungen, die im skalierbaren Modell für spätere Bearbeitungen angedacht sind, ist das Schema um TEI Lite erweitert. Sollte künftig Text detaillierter kodiert werden, als TEI Lite es zuläßt, ist die Textedition in einem separaten TEI-Dokument angedacht.
                
         
         
            
               Fazit
                
            Anhand digitaler Methoden eröffnen sich neue Wege für das Hybrid- und Massenmedium “Postkarte”. Will man bei der Datenmodellierung dem ganzheitlichen Blick gerecht werden, erkennt man bald, dass die Einbeziehung mehrerer Standards nötig ist. Denn auch wenn man zunächst dazu tendiert sich an die Restriktionen eines Standards zu halten, bieten genau die digitalen Methoden den enormen Vorteil, dass man sich von mehreren Richtungen dem Objekt nähern kann. Auch wenn spätestens die Europeana zu der Entscheidung IMAGE oder TEXT zwingt.
         
      
      
         
             Zur weiteren Beschreibung von Postkarten mit der TEI s. Correspondance-SIG, Burnard, Lou 2014.
             Obwohl das Element  für die strukturierte Erfassung von Transkriptionen und Beschreibungen von Markierungen oder Texten etc. definiert ist, ist das nur sehr oberflächlich möglich.
         
         
            
               Bibliographie
               
                  AKON - Ansichtskarten Online der Österreichischen Nationalbibliothek, 
                        , [letzter Zugriff 2017-08-29].
                    
               
                  Bildarchiv ETH Schweiz, 
                        http://ba.e-pics.ethz.ch, [letzter Zugriff 2017-08-29].
                    
               
                  Burnard, Lou (2014): TEI ODD workshop - Case Study: designing a TEI customization for a corpus of postcards, 
                        , [letzter Zugriff 2017-09-22].
                    
               
                  GAMS - Geisteswissenschaftliches Asset Management System, 
                        http://gams.uni-graz.at, [letzter Zugriff 2017-08-29].
                    
               
                  Holzer, Anton (2007): Naserümpfend am Postkartenstand. Was Ansichtskarten erzählen können (wenn man sie lässt), in: Hochreiter, Otto / Otti, Margareth (eds.): Hier ist es schön. Grazer Ansichtskarten. Ausstellungskatalog Stadtmuseum Graz, Salzburg: Fotohof (= Fotohof edition 93) 75-87.
               
                  LIDO - Lightweight Information Describing Objects, 
                        , [letzter Zugriff 2017-09-22].ademic press 10-41.
                    
               
                  Postkartensammlung GrazMuseum Online, 
                        , [letzter Zugriff 2017-08-29].
                    
               
                  Steiner, Elisabeth / Stigler, Johannes (2014): GAMS and Cirilo Client: Policies, documentation and tutorial, latest update 2017-04-10, 
                        , [letzter Zugriff 2017-09-22].
                    
               
                  TEI - Text Encoding Initiative, http://www.tei-c.org/index.xml, [letzter Zugriff 2017-08-29].
               
                  Tropper, Eva (2014): Illustrierte Postkarten - ein Format entsteht und verändert sich, in: Tropper, Eva / Starl, Timm (eds.): Format Postkarte - Illustrierte Korrespondenzen, 1900 bis 1936, Wien: new academic press.
            
         
      
   



      
         
         
            Goals of the workshop
            The workshop presents ATHEN
                     (Annotation and 
                    Text 
                    Highlighting 
                    Environment), an extensible desktop-based annotation environment which supports more than just regular annotation. Besides being a general purpose annotation environment, ATHEN supports indexing and querying support of your data as well as the ability to automatically preprocess your data with Meta information. It is especially suited for those who want to extend existing general purpose annotation tools by implementing their own custom features, which cannot be fulfilled by other available annotation environments. On the according gitlab, we provide online tutorials, which demonstrate the use of specific features of ATHEN.
                
         
         
            Related Work
            We compare ATHEN to three web-based and four desktop applications in 12 categories by adapting most criteria defined by Neves and Leser (Neves / Leser 2012) to compare different annotation tools:
            
               Availability and up-to-dateness of the documentation
               Active development at the present time
               Source code for download
               Complexity of system requirements
               Interoperability by supporting certain formats
               Support of different annotation layers
               Support of NLP-preprocessing to speed up manual annotation
               Support of visualization
               Support of self-learning systems to speed up manual annotation
               Support of querying annotated data
               Possibility to do an inter-annotator-agreement, this is important for projects, in which more than one annotator labels the same documents
               Extensibility 
            
            We explicitly do not want to compare subjective features like usability or how the annotations are presented.
            
               
               Table 1: Comparison of three web-based and four desktop applications with ATHEN in twelve categories. No tool excels in every category.
            
            All of the listed tools have an accessible documentation, either web-based or as a PDF, available for download. Besides UAM (O'Donnell 2008), every other application is listed as open source, so at least extensions based on code level can be made. WebAnno (Yimam et al. 2013) is the only application having a tutorial supporting a new developer to make changes in their project. ATHEN stands out in the sense that extensions to its UI can be made at runtime, therefore easing the process of adding functionality to it. WebAnno supports the largest number of formats and comes with a machine learning based automatic annotation, however lacks integrated NLP-preprocessing. CATMA (Meister 2017) is the only project that has a very good visualization component and also supports TEI-XML (Wittern et al 2009), the unspoken standard of text processing. Being a standalone web application, CATMA itself does not support NLP-preprocessing. ATHEN comes with the support of the execution of UIMA analysis engines, accessible from web repositories or a local repository, giving the user a chance to integrate her custom-made annotators. Four tools, ATHEN, UAM, MMAX2 (Müller / Strube 2006) and CATMA feature an integrated query language which helps to analyze existing corpora. Most tools allow the annotation of user-defined annotation schemas earning therefore the title “generic annotation tool”. Alongside UAM, ATHEN supports the annotation based on queries, while UAM defines its own language, ATHEN supports the annotation using Apache UIMA Ruta (Kluegl et al. 2016) rules. Three of the listed tools, MMAX2, Knowtator (Ogren 2006) and WordFreak (Morton / LaCivita 2003) are currently no longer in active development. 
         
         
            Brief technological description of ATHEN
            ATHEN is a Java-based desktop application with the vision to be extensible. Therefore, it makes use of the flexible plugin architecture of the eclipse 
                    Rich-
                    Client-
                    Platform (RCP)
                    . Internally, it is built around Apache UIMA, which means incoming data is automatically converted into the UIMA specific Common Analysis Subject (CAS) architecture. Working with UIMA allows the integration of standalone analysis engines, which can be used to preprocess data and speed up manual annotation. The use of Apache Lucene enables ATHEN to create an index comprising documents, as well as their annotations, which results in queries that can answer questions based on text and meta information in real time. With the ability to execute Apache UIMA Ruta one can even create queries of far higher complexity. On top of that, ATHEN features OWL-Support, which allows the definition of an ontological annotation schema in a machine- readable format. Using Apache UIMA internally allows ATHEN to even address more complicated input. Currently ATHEN supports the annotation of image regions, based on user defined polygons.
                
         
         
            Program of the workshop
            The program is split into four sections:
            
               Introduction to ATHEN and distinguishing from other existing annotation environments.
               Working with ATHEN, which contains the definition of scenarios and annotating sample documents.
               Utility of ATHEN (beyond regular annotation), which addresses the following topics:
                       
                     Defining an annotation schema using OWL
                     Preprocessing texts based on Apache UIMA Analysis Engines
                     Creating and executing queries based on Apache Lucene
                     Annotating images with ATHEN
                  
               
               Extending ATHENs functionalities and adapting it to your needs (developer specific)
            
            The first section is a presentation which shows the main differences between the existing annotation tools. The second section defines an ordinary annotation scenario and it is used to introduce the participants to the general-purpose annotation view of ATHEN. Afterwards, for tasks to which ATHEN has special support (annotating character references and their coreferences, annotating direct speech and their speaker) an introduction to the special purpose views of ATHEN is given.
            The third panel introduces the participants to the functionality of ATHEN beyond regular text annotation. It starts with the definition of an OWL ontology (and its utilization for texts). This is centered on relation detection of character references, as well as an attribution of those references.
            To speed up manual annotation it is helpful to have it preprocessed with existing tools. The task definition is then changed from pure annotation to an application with a consecutive correction of the output of the automatic engines. In this context, Nappi, a submodule of ATHEN is presented and it is shown how to define, execute and integrate custom analysis engines. 
            The next part is dedicated towards extracting knowledge from annotated data, for this purpose, an Apache Lucene Index is created using ATHEN and is queried in a live fashion. This feature allows rapid insight into an existing corpus and enables the user to answer their own hypothesis.
            The tutorial continues with the presentation of how images can be annotated with polygon-based annotations to show, that ATHEN is not only limited to textual resources.
            The last part is directed towards Java developers who are interested in developing their own annotation component.
            Each section starts with a set of slides which introduce the features in focus and presents the participants with one or more tasks that can be fulfilled by using ATHEN.
         
         
            Requirements
            The participants need their own laptops with an active internet connection. The number of participants is limited to 15 to 20. The last section requires knowledge of Java. Data which is necessary for the tutorials will be hosted on our own server and will be made accessible for download.
         
         
            Research projects
            ATHEN is mainly developed in the context of the project Kallimachos at the University of Wuerzburg. Its main purpose was to support the annotation process of DROC (
                    Deutscher 
                    ROman 
                    Corpus). Currently automatic creation of literary interaction networks, automatic genre detection of novels and sentiment analysis in literary novels are in the focus of interest.
                
            An extension to ATHEN was made in the project “Redewiedergabe” to manually annotate different forms of speech, thought and writing representation (STWR). These annotations will then be used to train an automatic recognizer for STWR.
         
      
      
         
            
               https://gitlab2.informatik.uni-wuerzburg.de/kallimachos/Athen
            
            A web version of ATHEN with its major features is available at: 
                            https://webathen.informatik.uni-wuerzburg.de
            
         
         
            
               Bibliographie
               
                  Kluegl, Peter / Toepfer, Martin / Beck, Philip-Daniel / Fette, Georg / Puppe, Frank (2016): “UIMA Ruta: Rapid Development of Rule-based Information Extraction Applications“, in: 
                        Natural Language Engineering 22.1 1-41.
                    
               
                  Meister, Jan Christoph / Gius, Evelyn / Jacke, Janina / Petris, Marco: 
                        CATMA 5.0.
                  http://catma.de/ [Accessed September 22, 2017].
                    
               
                  Morton, Thomas / LaCivita, Jeremy (2003): “WordFreak: an open tool for linguistic annotation“, in: 
                        Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Demonstrations-Volume 4 17-18.
                    
               
                  Müller, Christoph / Strube, Michael (2006): “Multi-level annotation of linguistic data with MMAX“, in
                        : Corpus technology and language pedagogy: New resources, new tools, new methods 3 197-214.
                    
               
                  Neves, Mariana / Leser, Ulf (2012): “A survey on annotation tools for the biomedical literature“, in: 
                        Briefings in Bioinformatics 15.2 327-340.
                    
               
                  O'Donnell, Mick (2008): “Demonstration of the UAM CorpusTool for text and image annotation“, in: 
                        Proceedings of the 46th annual meeting of the Association for computational linguistics on human language technologies: Demo session 13-16.
                    
               
                  Ogren, Philip V. (2006): “Knowtator: a Protégé plug-in for annotated corpus construction“, in: 
                        Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: companion volume: demonstrations 273-275.
                    
               
                  Stenetorp, Pontus / Pyysalo, Sampo / Topić, Goran / Ohta, Tomoko / Ananiadou, Sophia / Tsujii, Jun’ichi (2012): “BRAT: a Web-based Tool for NLP-Assisted Text Annotation“, in : 
                        Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics 102-107.
                    
               
                  Wittern, Christian / Ciula, Arianna / Tuohy, Conal (2009): “The making of TEI P5“, in: 
                        Literary and Linguistic Computing 24.3 281-296.
                    
               
                  Yimam, Seid Muhie / Gurevych, Iryna / de Castilho, Richard Eckart / Biemann, Chris (2013): “WebAnno: A Flexible, Web-based and Visually Supported System for Distributed Annotations“, in: 
                        Proceedings of ACL-2013, demo session, Sofia, Bulgaria 1-6
                        .
               
            
         
      
   



      
         
            Zusammenfassung
            Ziel der hier vorgestellten Studie ist eine Beschreibung der Schnittmenge von Diskursräumen in der Lexikographie bzw. Metalexikographie und den Digital Humanities (DH). Dabei geht es um die Bestimmung von explizit bzw. implizit als Teil der DH aufzufassenden Beiträgen zu lexikographischen Themen und, andersherum, von lexikographierelevanten Themen, die in den DH diskutiert werden. Zur Bestimmung der Diskursräume, von Schnitt- und disjunktiven Mengen, werden Volltexte und Metadaten analysiert, bibliometrische Netzwerke (Autoren- bzw. Zitationsnetzwerke) verglichen und Topic Modelings vorgenommen.
         
         
            Einleitung
            Der Einzug digitaler Methoden und Werkzeuge in die Geistes- und Sozialwissenschaften, genauer: der als „computational turn“ (Berry 2011) bezeichnete methodisch-epistemologische Quantensprung, lässt sich in allen Disziplinen der Humanities beobachten. In der Sprachwissenschaft hat sich dieser Wandel bekanntermaßen besonders deutlich in der Etablierung der Computerlinguistik als eigene Disziplin niedergeschlagen. Neben computerlinguistischen Verfahren der Textanalyse sind eine maschinenlesbare Wissensrepräsentation und -organisation, sind Formate für digitale Editionen und komputationell erstellte Visualisierungen heute in allen textbasierten Disziplinen in Gebrauch.
            Der angesprochene Wandel lässt sich ebenfalls in der Lexikographie feststellen. Die Lexikographie bzw. Metalexikographie, als solche bereits seit geraumer Zeit als Disziplin emanzipiert (Tarp 2008; Wiegand 2013), haben den Übergang zum digitalen Medium inzwischen vollzogen (cf. zum frühen Stand der Dinge De Schryver 2003) und sind beständig dabei, ihr komputationell informiertes methodisches Instrumentarium weiterzuentwickeln (Heid 2013). Als zentrale Aspekte gelten hier der Einzug korpuslinguistischer Verfahren in die Lexikographie (Hanks 2008; Heid 2008), komputationelle Methoden zur Datenrepräsentation (Spohr 2012), speziell auch für die digitale Edition historischer Wörterbücher (Lemnitzer u. a. 2013) und zur Implementierung funktionsgerichteter Benutzerschnittstellen (Heid 2014) sowie zur Wörterbuchbenutzungsforschung (Müller-Spitzer 2014).
            In der hier vorgestellten Studie gehen wir der Frage nach, wie sich der gemeinsame Diskursraum als Schnittmenge von Lexikographie und Digital Humanities mit quantitativen Methoden definieren lässt. Nach einer nicht exhaustiven und von Hand durchgeführten Voruntersuchung folgen wir der Ausgangshypothese, der gemeinsame Diskursraum sei um ein Vielfaches größer als man annehmen könnte, folgte man allein denjenigen Themen, die als lexikographierelevant gelten können und die in Publikationen diskutiert werden, die explizit zum Bereich der DH gehören (vgl. Abb. 1).
            
               
                  
                  Abb. 1: Ausgangshypothese: Explizite und implizite Schnittmengen.
               
            
            Diejenigen Arbeiten, die im DH-Kontext veröffentlicht werden und explizit einem Thema der Lexikographie zugeordnet werden, sind recht leicht über relevante Schlüsselwörter bestimmbar. Dazu tritt die Gruppe jener Publikationen, die zur eingangs skizzierten Schnittmenge zu zählen sind, ohne dass sie sich selbst ausdrücklich den Digital Humanities zuordnen. Es ist das Ziel dieser Untersuchung, zu bestimmen, welche in der Lexikographie diskutierten Themen und welche Autoren zu dieser Gruppe gerechnet werden und also eine Zurechnung zu den Digital Humanities 
                    implizieren können.
                
         
         
            Voruntersuchung und Zwischenergebnis: Explizite Verortung in den DH
            Als Voruntersuchung zum benannten Gegenstand haben wir eine Recherche in den Archiven bedeutender englischsprachiger Zeitschriften der Digital Humanities
                     sowie in den Proceedings der ADHO-Jahreskonferenzen
                     durchgeführt. Über die Suchbegriffe „Lexicography“ und „Dictionary“ finden sich in den genannten Archiven 31 englischsprachige Beiträge, die sich mit lexikographischen Themen befassen, und die sich qua Erscheinen in DH-Medien zu denjenigen Publikationen zählen lassen, die sich explizit in den Digital Humanities verorten.
                
            Eine manuelle Zuordnung lexikographierelevanter Schlüsselwörter zu den genannten 31 Beiträgen ergibt das in Tabelle 1 wiedergegebene Bild; dabei sind mehrfache Zuordnungen möglich. Zunächst lässt sich ohne Verwunderung feststellen, dass in allen Beiträgen die digitale Repräsentation lexikalischer Daten eine Rolle spielt, allerdings mit unterschiedlichen Fragestellungen, Herangehensweisen und Zielsetzungen. Die drei größten Themencluster haben wir hier, in dieser Reihenfolge, mit den Schlagwörtern „e-Wörterbücher / Visualisierung lexikalischer Daten“, „Historische Lexikographie“ und „Korpuslinguistik“ bezeichnet. Ersteres benennt Fragen der Produktion digitaler Wörterbücher einschließlich neuer Methoden der Visualisierung, letzteres die Erstellung und Nutzung elektronischer Textkorpora zu einer Reihe lexikographischer Zwecke. Beide Bereiche sind durch die Heraufkunft digitaler Methoden überhaupt erst möglich geworden und haben die Lexikographie revolutioniert. Die Historische Lexikographie kann als philologische Disziplin gelten, die sich mit der Edition historischer lexikalischer Datensammlungen befasst; die diesem Schlüsselwort zugeordneten Beiträge befassen sich grundsätzlich mit Methoden digitaler Edition, einem Kernbereich der DH.
            
               
                  Schlüsselwort (Topic)
                  Zählung
               
               
                  Digitale Wissensrepräsentation / Formate
                  31
               
               
                  e-Wörterbücher / Visualisierung lexikalischer Daten
                  14
               
               
                  Historische Lexikographie
                  10
               
               
                  Korpuslinguistik
                  9
               
               
                  Wörterbuchnetz
                  4
               
               
                  NLP-Lexicon
                  2
               
               
                  Bilingual Dictionary Drafting
                  2
               
               
                  Autorenwörterbuch
                  2
               
               
                  Dialektologie
                  1
               
            
            Tabelle 1: Schlüsselwörter, manuelles Clustering, manuelle Zählung
            Unter den weniger häufig gewählten Schlüsselwörtern sticht das „Wörterbuchnetz“ hervor, das Strategien zur Vernetzung lexikalischer Ressourcen bezeichnet. Hinzu kommen noch lexikalische Datensammlungen zur Anwendung in der maschinellen Sprachverarbeitung („NLP-Lexicon“), Methoden zum Entwurf zweisprachiger Wörterbuchinhalte („Bilingual Dictionary Drafting“), das „Autorenwö
         
         
            Methode für die Bestimmung implizit in den DH verorteter Arbeiten
            Wir haben ein Textkorpus erstellt, das im Zeitraum 2000 bis zur Gegenwart (2018) erschienene englischsprachige Beiträge aus Zeitschriften, Kongressakten und Handbüchern zu den Digital Humanities (Subkorpus DH) und der Lexikographie (Subkorpus Lexicog) enthält; Tabelle 2 gibt die Titel der verarbeiteten Quellen wieder. Dabei wurden die ausgewählten Zeitschriften bzw. Sammelbände jeweils vollständig berücksichtigt; die Beiträge wurden zusammen mit Metadaten, u. a. Verfasser (Name und Affiliation), Datum, Textsorte, Umfang und Identifier (ISBN, DOI), im Tool Zotero
                     verwaltet. Die Volltexte wurden semiautomatisch bereinigt und zusammen mit Metadatensätzen in das Korpus aufgenommen. Darüber hinaus wurden die in den Volltexten enthaltenen bibliographischen Referenzen extrahiert (GROBID, Lopez 2009).
                
            
               
                  DH / 1.422 (41%)
                  Digital Humanities Quarterly: 
                             / 284
                        
               
               
                  
                  DSH (ex LLC):
                            https://academic.oup.com/dsh / 886
                        
               
               
                  
                  TEI Journal of the Text Encoding Initiative:
                            http://jtei.revues.org / 63
                        
               
               
                  
                  Digital Studies/Le champ numerique: 
                             / 152
                        
               
               
                  
                  Blackwell Companion to DH: Schreibman et al. (ed.) 2004 / 37
               
               
                  Lexikog
                            / 2.056
                            (59%)
                        
                  IJL: 
                            http://ijl.oxfordjournals.org/ 282
                        
               
               
                  
                  Lexikos: 
                             / 376
                        
               
               
                  
                  Dictionaries (Journal of the DSNA): 
                             / 257
                        
               
               
                  
                  Euralex: 
                             / 782
                        
               
               
                  
                  eLex: 
                             / 202
                        
               
               
                  
                  HSK 5/4: Gouws et al. (ed.) 2013 / 110
               
               
                  
                  The Routledge Handbook of Lexicography: Fuertes-Olivera (ed.) 2018 / 47
               
            
            Tabelle 2: Quellen für das DH/Lexikog Textkorpus / Zahl der Volltexte
            
               Topic Modeling 
               Unüberwachtes Topic Modeling (LDA, eingesetztes Tool: MALLET (McCallum 2002)) soll es uns ermöglichen, die in Abb. 1 grob skizzierten Mengen als sich überschneidende Diskursräume zu bestimmen und zu visualisieren. Unsere Ergebnisse zeigen die relative Relevanz in beiden Subkorpora von 50 durch den LDA-Algorithmus bestimmten, jeweils mit einer Reihe von Schlüssel-Tokens repräsentierten Topics. Eine Reihe von Anhaltspunkten spricht für das zuverlässige Funktionieren der Methode: Die Liste der Topics, die besonders DH-relevant seien, wird von den Tokens „digital humanities computing tools“ angeführt, die Liste der Lexikographie-Topics von „dictionary dictionaries english words word learners language“.
               Bei der Ansicht der im Mittelfeld befindlichen Topics, also Themen, die in beiden Subcorpora als relevant bezeichnet sind, stellt sich heraus, dass sich hier nicht nur der digitale Wandel als Thema widerspiegelt, sondern dass darüber hinaus weitere Topics den gemeinsamen Diskursraum von Lexikographie und DH ausmachen. Inmitten von Zeilen, die, quasi erwartungsgemäß, Tokens wie „information model data structure process analysis“ oder „corpus words frequency texts word corpora table“ sowie eine ganze Reihe von Namen natürlicher Sprachen enthalten, ist hier etwa das mit den Tokens „women male female gender woman man people [...] black [...] girl feminist [...]“ repräsentierte Topic auffällig (40 der 100 für dieses Topic relevantesten Beiträge stammen aus dem DH-, 60 aus dem Lexikog-Subkorpus).
               Abbildung 2 zeigt für 50 von MALLET bestimmte Topics (Spalten) die Verteilung der 100 jeweils relevantesten Texte über die Subcorpora (Ausgabe von MALLET; DH-Beiträge sind grün, Lexicog-Beiträge violett unterlegt). Es wird deutlich, dass ein Teil der Topics eindeutig einem der Subkorpora zuzurechnen ist, andere Topics dagegen eine starke Durchmischung aufweisen.
               
                  
                     
                     Abb. 2: Visualisierung des Topic Modeling
                  
               
            
            
               Citation Network
               Für alle Artikel des Korpus (siehe Tabelle 2) untersuchten wir die Anzahl der auf Items innerhalb des Netzwerks gerichteten Zitationen. Es zeigten sich 2.431 Zitationen (31% DH, 69% Lexicog). Die Zitationen aus DH sind nur zu 2% auf items aus Lexicog gerichtet; die Zitationen aus Lexicog zu 1% auf items aus DH.
            
         
         
            Ergebnisse und Schlussfolgerungen
            Die vorgestellten korpuslinguistischen und bibliometrischen Untersuchungen bieten wie beschrieben Aufschluss über Schnitt- und disjunkte Mengen von Themen- und Autorenclustern der Lexikographie und der Digital Humanities. Visualisierungen dieser Cluster und Listen der relevanten Keywords und Autoren werden bereitgestellt. Topic Modeling und Zitationsnetzwerke bilden unterschiedlich große Schnittmengen zwischen beiden Disziplinen ab: Während einerseits deutlich wird, dass eine ganze Reihe von Themen in beiden Disziplinen relevant ist, zitiert man sich vergleichsweise selten gegenseitig.
            Die gezeigten Ergebnisse können zunächst zu einer verbesserten gegenseitigen Wahrnehmung in Lexikographie und Digital Humanities beitragen sowie in der lexikographischen Community das Bewusstsein dafür stärken, ein Gutteil der Disziplin gehöre durch die inhaltliche Überschneidung de facto zum Einflussbereich der Digital Humanities. Dies wiederum kann in der Zukunft zu einer stärkeren expliziten Verortung relevanter lexikographischer Beiträge in den Digital Humanities führen.
            Weiterhin haben wir mit den für diese Studie durchgeführten Arbeiten eine annotierte bibliographische Datensammlung angelegt und die dazugehörigen Volltexte mit korpuslinguistischen Methoden annotiert und analysiert. Wir beabsichtigen, diese Sammlung auch weiterhin zu pflegen und öffentlich zugänglich zu machen.
         
      
      
         
             Digital Humanities Quarterly, DSH, TEI Journal
            
               http://adho.org
            
            
               http://www.zotero.org
            
         
         
            
               Bibliographie
               
                  Berry, David M. (2011): „The Computational Turn: Thinking About the Digital Humanities“. (The Computational Turn). In 
                        Culture Machine 12 (0).
                    
               
                  De Schryver, Gilles-Maurice (2003): „Lexicographers’ Dreams in the Electronic‐Dictionary Age“. In 
                        International Journal of Lexicography 16 (2): 143–199.
                    
               
                  Fuertes-Olivera, Pedro (ed.) (2018): 
                        The Routledge Handbook of Lexicography. London: Routledge.
                    
               
                  Gouws, Rufus / Heid, Ulrich / Schweickard, Wolfgang / Wiegand, Herbert E. (eds.). (2013): 
                        Dictionaries. An International Encyclopedia of Lexicography. HSK 5/4. Berlin / Boston: De Gruyter Mouton.
                    
               
                  Hanks, Patrick (2008). „The Lexicographical Legacy of John Sinclair“. In 
                        International Journal of Lexicography 21 (3): 219–229.
                    
               
                  Heid, Ulrich (2008). „Corpus linguistics and lexicography“. In Anke Lüdeling / Merja Kytö (ed.) 
                        Corpus Linguistics. An international Handbook: 131–153. Berlin: Mouton de Gruyter.
                    
               
                  Heid, Ulrich (2013): „The impact of computational lexicography“. In Gouws et al. 2013: 24–30.
                    
               
                  Heid, Ulrich (2014): „Natural Language Processing Techniques for Improved User-friendliness of Electronic Dictionaries“. In 
                        Proceedings of EURALEX 2012: 47–61.
                    
               
                  Lemnitzer, Lothar / Romary, Laurent / Witt, Andreas (2013): „Representing human and machine dictionaries in markup languages (SGML, XML)“. In Gouws et al. 2013: 1195–1209.
                    
               
                  Lopez, Patrice (2009): „GROBID: Combining Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications“. In 
                        Research and Advanced Technology for Digital Libraries: 473–474. Lecture Notes in Computer Science. Berlin / Heidelberg: Springer
                    
               
                  McCallum, Andrew K. (2002): 
                        MALLET: A Machine Learning for Language Toolkit. http://mallet.cs.umass.edu/.
                    
               
                  Müller-Spitzer, Carolin (2014): „Methoden der Wörterbuchbenutzungsforschung“. In 
                        Lexicographica 30 (1): 112–151.
                    
               
                  Schreibman, Susan / Siemens, Ray / Unsworth, John (2004): 
                        A Companion to Digital Humanities. Oxford: Blackwell
                    
               
                  Spohr, Dennis (2012): 
                        Towards a Multifunctional Lexical Resource, Design and Implementation of a Graph-Based Lexicon Model. Lexicographica Series Maior, 141. Berlin / Boston: De Gruyter.
                    
               
                  Tarp, Sven (2008): 
                        Lexicography in the borderland between knowledge and non-knowledge: general lexicographical theory with particular focus on learner’s lexicography. Lexicographica, Series Mayor 134. Tübingen: Niemeyer.
                    
               
                  Wiegand, Herbert E. (2013): „Lexikographie und Angewandte Linguistik“. In 
                        Zeitschrift für angewandte Linguistik 58 (1): 13–39.
                    
            
         
      
   



      
         
            Zusammenfassung des Vorhabens
            Die AG Datenzentren des DHd möchte ein Panel mit insgesamt fünf Diskussionspartnern und zwei Moderatoren veranstalten, um die Herausbildung fachbezogenen Datenmanagements im deutschsprachigen Raum in den digitalen Geisteswissenschaften weiter zu fördern sowie den Stand und die Perspektiven dieses Forschungsbereichs zu diskutieren. Das Panel reiht sich damit in den fachlich übergreifenden Organisationsprozess zum Datenmanagement ein. Im Mittelpunkt der Diskussion soll hier die Veränderung der Datenkultur in der Geisteswissenschaft stehen und damit die Frage, wie eine Konsolidierung, Vernetzung und fachliche Abrundung von Diensten die Anreize zur wissenschaftlichen Nutzung des Datenmanagements erhöhen können. Dies soll mit zwei Schwerpunktsetzungen erfolgen:
            A. Zunächst möchten wir anhand eines Überblicks zu bestehenden Angeboten von Datenzentren informieren und aufzeigen, welche fachbezogenen Leistungen momentan oder in naher Zukunft von Datenzentren in einem Netzwerk verteilter DH-Strukturen eingebracht werden. Dieser Block macht die Schwerpunktsetzungen, Rollen und herausgehobene fachliche Dienste einzelner Datenzentren sichtbar, die über allgemeine Anforderungen an ein Datenzentrum hinausragen. Daran anschließend kann über die mögliche Ausgestaltung kooperativer Strukturen und die Zusammenarbeit innerhalb eines Netzwerkes der DH-Datenzentren diskutiert werden.
            B. Weiterhin fragt das Panel grundlegend nach zentralen Aufgabenstellungen im Bereich von Standards aus den einzelnen Fachgebieten der Geisteswissenschaften: Welche fachlichen Standards müssen Datenzentren über die zuvor gezeigten Ansätze hinaus kooperativ entwickeln, um passfähige Daten für eine möglichst große Breite von Nachnutzungen interdisziplinär abzusichern? Wo liegen Möglichkeiten und wo Grenzen von fachlicher Standardisierung und Normalisierung über die einzelnen Fachdisziplinen der Geisteswissenschaft hinweg? Hierzu sollen Erkenntnisse und Erfahrungen aus abgeschlossenen bzw. weitgehend realisierten Projekten dazu dienen, abstrahierende Anforderungen aus fachwissenschaftlicher Perspektive zu benennen, die zugleich einen Mehrwert der Realisierung besitzen. Insgesamt geht es darum, die Notwendigkeit eines geisteswissenschaftlichen sowie fachspezifischen Angebots nachdrücklich unter Beweis zu stellen, zu etablierende Standards zu benennen und Desiderate zu erkennen. 
         
         
            Forschungsstand
            Der digitale Wandel, das Entstehen, Archivieren, Erschließen und Nachnutzen großer Datenbestände verändert wissenschaftliche Forschung in ihren Grundlagen, Arbeitsweisen und Methodiken fundamental (OECD 2007; HRK 2014; RfII 2016; Allianz 2010; DFG 2015). Die zahlreichen Aspekte dieses Wandels beschäftigen Fach-Communities und Fachverbände in umfassender Weise mit Fragen des Datenmanagements, der Lizenzen, Rechte und des Datenschutzes, der technischen und inhaltlichen Langzeitarchivierung sowie Nachnutzung von Daten. Die intensiven Diskussionen um das Datenmanagement sind verbunden mit einem grundlegenden Nachdenken über künftige Aufgaben, Strukturen und Workflows bestehender und sich neu formierender Institutionen oder Akteure und führen zu grundsätzlichen Reflexionen über die notwendige Schaffung verteilter nationaler infrastruktureller Angebote (RfII 2016: 39f.; RfII 2017). In der jüngeren Vergangenheit sind bevorzugt technische Aspekte des Datenmanagements diskutiert worden, wie sie etwa die interoperationale Metadatenhaltung (Bibliotheksparadigma), Probleme der Speicherformate, Speichermedien und Dokumentation in der technischen Langzeitarchivierung über einen Rahmen von zehn Jahren hinaus (Archivparadigma) betreffen (Nestor 2016). Solche Lösungen und Angebote können oft in übergreifenden technischen Strategien gefunden werden, die ein zentrales Aufgabengebiet von Bibliotheken und Archiven umfassen. Wie in den Arbeitsbereichen der digitalen Wissenschaft allgegenwärtig zu beobachten ist, lässt sich dabei eine sukzessive, aber dennoch intensive Spezialisierung und Differenzierung digitaler Erfordernisse und Techniken beobachten. War beispielsweise noch das erste große deutschsprachige Überblickswerk der nestor-Arbeitsgruppe zur Langzeitarchivierung fachübergreifend technisch geprägt (Neuroth 2010), offenbaren die neueren Arbeitsergebnisse eine deutliche Hinwendung zur fachbezogenen Darstellung (Neuroth 2012). Allerdings blieben auch in dieser Hinsicht die Parameter der technischen Langzeitarchivierung aus der Sicht der Archive und Bibliotheken bestimmend. 
            Gleiches lässt sich mit Blick auf die Wissenschaft erkennen, wenn man etwa auf die sich konstituierenden Fachgruppen im Rahmen des Verbandes der Digital Humanities im deutschsprachigen Raum
                     oder andere traditionelle Fachverbände mit ihren sich ausdifferenzierenden digitalen Arbeitsweisen/Methoden schaut.
                     Intensiv diskutiert wurden solche Themen der fachbezogenen Binnendifferenzierung auch in der Auftaktveranstaltung zum Förderprogramm "'Mixed Methods' in den Geisteswissenschaften?" der VW-Stiftung im Frühsommer 2017, die eine Vielzahl deutschsprachiger Forschungsprojekte im Bereich der Digital Humanities versammelte.
                    
            
            Um nachhaltig wirksam zu sein, braucht Forschungsdatenmanagement eine enge Anbindung an die Wissenschaft mit ihren spezifischen Problemen und fachlichen Erfordernissen (RfII 2016: 60; RfII 2017: 1f., AG DZ 2017: 3). Nicht in erster Linie der technische Service zur Langzeitarchivierung standardisierter Daten, sondern die fachbezogene Datenkuration und die Tiefenerschließung auf der Basis von Standards sichert die Güte und Qualität von Forschungsdaten und ihre Nachnutzbarkeit für spezifische wissenschaftliche Forschungsprojekte. Qualitätsgerechtes Forschungsdatenmanagement - das auch im analogen Zeitalter den Kern jeder Forschung ausmachte - sichert den wissenschaftlichen Mehrwert von Daten, schafft vielfältige Nutzungsanreize und innerwissenschaftliche Akzeptanz (RfII 2016: 38-40; AG DZ 2017: 2). Im gleichen Maße wie daher die übergreifende Basis technischer Lösungen zur Metadatenhaltung, Lizenzierung und Langzeitarchivierung entwickelt werden muss, sind fachbezogene Standards der Datenkuration, der kontrollierten Vokabularien, der fachbasierten Quellenkritik und Annotation sowie spezialisierte Werkzeuge der fachlichen Datenverarbeitung zur Erzeugung nachhaltiger Datenstrukturen notwendig. Während erstere eher in den Verantwortungsbereich der Bibliotheken und Archive fallen, sind letztere das zentrale Aufgabenfeld der Wissenschaft mit ihren spezifischen Fachgebieten. 
            Hinzu treten die zahlreichen Herausforderungen des geisteswissenschaftlichen Datenmanagements im Unterschied zu natur- und sozialwissenschaftlichen Projekten. Geisteswissenschaftliche Daten bilden in vielen Anwendungsbereichen komplexe Textkorpora mit hochspezialisierten Formen der Transkription, Annotation und der Unterscheidung verschiedener Ebenen von Quellenbegriff, Lemmatisierung, Normalisierung, Standardisierung und Begriffserklärung. Ähnliche komplexe Formen der Erschließung gelten ebenso in eher auf Objekte und Gegenstände bezogene Geisteswissenschaften. Ansätze der einen Diziplin sind (bis heute) nicht ohne weiteres auf eine andere übertragbar bzw. nicht fachübergreifend bekannt. Gleichzeitig ist es Anliegen von Datenzentren übergreifende Standards bzw. Austauschformate zu definieren und zu etablieren, welche die Transparenz und Verschneidung von Daten überhaupt erst einmal ermöglichen.
            Ebenso muss auf der Ebene der Datenzentren intensiv über Kooperationsformen und Vorgehensweisen für neue heterogene Datenbestände mit einem Mix aus Quellen, Daten, Methoden, Medien und Rechten nachgedacht werden (AG DZ 2017: 3). 
         
         
            Fragestellung und Aufbau des Panels
            Die DH ist angetreten, um computergestützte Ansätze und Methoden quer über die geisteswissenschaftlichen Fachdisziplinen zu entwickeln. Was jedoch macht die Summe geisteswissenschaftlicher Bedürfnisse aus, um Daten tatsächlich interdisziplinär und langfristig nachnutzbar zu machen? Die "Kritik der digitalen Vernunft" besteht für ein solches geisteswissenschaftliches Unterfangen darin, gemeinsame Anforderungen und Standards der Datenhaltung aus bestehenden Projekten und Erfahrungen zu formulieren, zusammenzutragen, zu systematisieren und zu abstrahieren. Zudem bedürfen solche Standards der Akzeptanz durch Forschende, sollen sie Wirksamkeit entfalten. Standards müssen daher einen klaren Mehrwert für wissenschaftliche Arbeit bieten.
            1. Viele Datenzentren haben mittlerweile ihre reguläre fachliche Arbeit aufgenommen, projektbasiert Erfahrungen gesammelt und neben der regulären Arbeit oft spezifische Schwerpunkte gebildet. In Form einer Präsentation der wichtigsten Akteure, Dienste und Angebote möchten wir über das bestehende Leistungsangebot innerhalb der AG Datenzentren des DHd-Verbandes informieren, wobei bevorzugt die Schwerpunktsetzungen im Bereich der Standards thematisiert werden. Auf diese Weise können sich Tagungsteilnehmer einen Überblick zum Dienstleistungsspektrum von Datenzentren verschaffen, was angesichts der insgesamt unübersichtlichen (weil sich dynamisch entwickelnden) Forschungslandschaft einen erheblichen Vorteil darstellt. Diese ca. 15-20 Minuten dauernde Präsentation beruht auf einer Umfrage unter den im Verband organisierten Einrichtungen und Akteuren, die von der Organisatorin des Panels in Zusammenarbeit mit der AG Datenzentren vorstrukturiert und präsentiert wird. Bei Bedarf können weitere zentrale fachwissenschaftliche Angebote in den Geisteswissenschaften einbezogen werden. Dieser Leistungskatalog ist einerseits Grundlage der Diskussion mit dem Publikum und andererseits für die Außendarstellung der AG Datenzentren der DHd förderlich.
            2. Nach dieser Präsentation leitet die Moderation in den Diskussionsteil über. Eingeladen werden Vertreter und Vertreterinnen geisteswissenschaftlicher Datenzentren mit unterschiedlichen Schwerpunktsetzungen und Funktionen, die Impulsreferate zu notwendigen Anforderungen fachwissenschaftlicher Standards im Datenmanagement zu geben. Diese Statements werden vor der Tagung allen Diskussionspartnern zur Verfügung gestellt, um eine angeregte Diskussion über fachliche Belange und Standards für das Datenmanagement der Geisteswissenschaften zu ermöglichen und auch Kritik zu formulieren. Die Panelorganisatorin und Moderatoren sichten die Beiträge zuvor redaktionell-moderierend. Hier sollen vor allem Möglichkeiten und Grenzen von bestehenden innerfachlichen Standards (z.B. existierende Ansätze wie GND, XML, TEI, Transkriptionsregeln; neue Entwicklungen wie z.B. "Ontologie historischer Berufe” etc.) benannt und diskutiert werden. An Einzelbeispielen soll kontrovers diskutiert werden, welche Standards die Geisteswissenschaft und das Datenmanagement tatsächlich voranbringen. Sind es eher technische Aspekte und Rahmenbedingungen (Tools, Formate, Annotationswerkzeuge), die formale Aspekte des Datenmanagements vereinheitlichen oder thematisch übergreifende Normansätze, Ontologien oder kontrollierte Vokabulare, die häufig aber sehr voraussetzungsvoll und arbeitsintensiv sind? Zielgerichtet sollen so Ansätze, Möglichkeiten und Grenzen diskutiert werden. Fachwissenschaftliche Aspekte sollen zusätzlich durch die Öffnung der Diskussion für das Publikum einbezogen werden. Gleichzeitig kann das Publikum Hindernisse und Desiderate des Forschungsdatenmanagements formulieren. Insgesamt sollen zudem mögliche Kooperationen und Organisationsstrukturen zwischen verschiedenen Datenzentren angerissen werden (Wer darf Standards entwickeln? Wer muss sie nutzen?). Nach Möglichkeiten werden dazu Beiträge des Publikums bzw. die Use Cases der Diskussion aufgegriffen. Diese Diskussion soll die fachliche Positionierung und Ausdifferenzierung der Datenzentren schärfen.
            Es ist die Aufgabe der Moderation, entsprechend der Schwerpunktsetzungen in der Diskussion abstrahierende Aussagen zur Ansetzung von Standards zu finden, die aber zuvor innerhalb der AG und auch der eingeladenen Diskutanten bereits andiskutiert wurden, um ein konzentriertes und fachbezogen-klares Fazit der Diskussion zu ermöglichen.
         
         
            Zusammensetzung des Panels
            
               Organisation und Präsentation: 
            
            Dr. Katrin Moeller, Historisches Datenzentrum Sachsen-Anhalt (Hist-Data), Martin-Luther-Universität Halle-Wittenberg, Deutschland
            
               Moderation: 
            
            Dr. Ulrike Wuttke, Stellvertretende Vorsitzende der AG Datenzentren des DHd, Fachbereich Informationswissenschaften, Fachhochschule Potsdam, Deutschland
            Dr. Jörg Wettlaufer, Göttingen Centre for Digital Humanities, Georg-August Universität Göttingen, Deutschland
            
               Datenzentren:
            
            Marina Lemaire, M.A., Servicezentrum eSciences, Universität Trier, Deutschland
            
               DI Matej Ďurčo, Austrian Center for Digital Humanities, Österreichische Akademie der Wissenschaften, Österreich
                    
               Dr. Barbara Ebert, Leiterin des Rats für Informationsinfrastrukturen (RfII), Geschäftsstelle Göttingen
                
            Prof. Dr. Lukas Rosenthaler, Data and Service Center for the Humanities DaSCH, Universität Basel (DHLab) und Schweizerische Akademie der Geistes- und Sozialwissenschaften, Schweiz
                    apl. Prof. Dr. Patrick Sahle, Data Center for the Humanities (DCH), Universität zu Köln, Deutschland
                
         
      
      
         
             Siehe die Arbeitsgruppen des Verbands DHd: 
                            https://dig-hum.de/dhd-ags
                            .
                        
             Vergleiche hier etwa das CfP und die daraus resultierenden Ergebnisse der Tagung: Quellen und Methoden der Geschichtswissenschaft im digitalen Zeitalter. Neue Zugänge für eine etablierte Disziplin, DIGIMET 2017, 25./26.09.2017 in Berlin, URL: http://welt-der-kinder.gei.de/wp-content/uploads/2017/05/CfP-DH-Abschlusstagung-WdK-02.05.2017.pdf.
             Dabei nahmen zahlreiche Vertreter der DH-Community in den Geisteswissenschaften teil, die auch über das eigentliche Förderprogramm "Mixed Methods" in den Geisteswissenschaften Projekte durchführen: https://www.volkswagenstiftung.de/fileadmin/downloads/publikationen/20161221_Mixed_Methods_Volkswagen
                Stiftung_Bewilligungen.pdf.
         
         
            
               Bibliographie
               
                  AG Datenzentren des DHd (Hg.) (2017): Stellungnahme der DHd AG Datenzentren und des DHd-Verbands zur Nationalen Forschungsdateninfrastruktur (NFDI), URL: 
                        
                     http://dig-hum.de/sites/dig-hum.de/files/DHd_NFDI_Stellungnahme_2017-07-31.pdf
                   [letzter Zugriff 22. September 2017].
                    
               
                  Allianz der deutschen Wissenschaftsorganisationen (Hg.) (2010): Grundsätze zum Umgang mit Forschungsdaten, URL: 
                        
                     http://www.allianzinitiative.de/fileadmin/user_upload/www.allianzinitiative.de/Grundsaetze_Forschungsdaten_2010.pdf
                   [letzter Zugriff 22. September 2017].
                    
               
                  DFG (Hg.) (2015): DFG-Leitlinien zum Umgang mit Forschungsdaten, Bonn 2015, 
                        
                     http://www.dfg.de/foerderung/antragstellung_begutachtung_entscheidung/antragstellende/antragstellung/nachnutzung_forschungsdaten/index.html
                   [letzter Zugriff 22. September 2017].
                    
               
                  Nestor (Hg.) (2016): Standardisierung, 2016, URL: 
                        
                     https://wiki.dnb.de/display/NESTOR/Standardisierung
                   [letzter Zugriff 22. September 2017].
                    
               
                  Neuroth, Heike / Oßwald, Achim / Scheffel, Regine / Strathmann, Stefan / Huth, Karsten (Hg.): nestor Handbuch. Eine kleine Enzyklopädie der digitalen Langzeitarchivierung. Version 2.3, Göttingen 2010, URL: 
                        
                     http://nestor.sub.uni-goettingen.de/handbuch/nestor-handbuch_23.pdf
                   [letzter Zugriff 22. September 2017].
                    
               
                  Neuroth, Heike / Strathmann, Stefan / Oßwald, Achim / Scheffel, Regine / Klump, Jens / Ludwig, Jens (Hg.) (2012): Langzeitarchivierung von Forschungsdaten. Eine Bestandsaufnahme. Boizenburg / Göttingen.
                    
               
                  OECD (Hg.) (2007): OECD Principles and Guidelines for Access to Research Data from Public Funding, Paris 2007, URL: 
                        
                     http://www.oecd.org/sti/sci-tech/38500813.pdf
                   [letzter Zugriff 22. September 2017].
                        OECD (Hg.) (2016):, "Research Ethics and New Forms of Data for Social and Economic Research", 
                        OECD Science, Technology and Industry Policy Papers, No. 34, OECD Publishing, Paris, URL: 
                        
                     http://dx.doi.org/10.1787/5jln7vnpxs32-en 
                  [letzter Zugriff 22. September 2017]
                        .
                  
                  Hochschulrektorenkonferenz (HRK) (Hg.) (2014): Management von Forschungsdaten als strategische Aufgabe der Hochschulleitungen, 14. Mai 2014, URL: 
                        
                     https://www.hrk.de/positionen/beschluss/detail/management-von-forschungsdaten-eine-zentrale-strategische-herausforderung-fuer-hochschulleitungen/
                   [letzter Zugriff 22. September 2017].
                    
               
                  Rat für Informationsinfrastrukturen (RfII) (Hg.) (2016): Leistung aus Vielfalt, Empfehlungen zu Strukturen, Prozessen und Finanzierung des Forschungsdatenmanagements in Deutschland, Göttingen, URL: 
                        
                     http://www.rfii.de/de/category/dokumente/
                   [letzter Zugriff 22. September 2017].
                    
               
                  Rat für Informationsinfrastrukturen (RfII) (Hg.) (2017): Schritt für Schritt - oder: Was bringt wer mit? Ein Diskussionsimpuls zur Zielstellung und Voraussetzungen für den Einstieg in die Nationale Forschungsdateninfrastruktur (NFDI), URL: 
                        
                     http://www.rfii.de/de/category/dokumente/
                   [letzter Zugriff 22. September 2017].
                    
            
         
      
   



      
         Kontinuierlich steigt die Menge der erzeugten und prozessierten Information und damit der Bedarf an technologisch assistierter bishin zu autonomer Datenverarbeitung. Textuell repräsentierte Inhalte mittels strukturgebender Methoden aufzubereiten ist meist aufwendiger als deren Erzeugung, insofern gewinnt automatische Textprozessierung zunehmend an Bedeutung (Bernstein et al.: 2016).
          
         Mithilfe der formalen Grammatiken RBNF (Rich Backus-Naur Form)
                 und ABNF (Augmented Backus-Naus Form) können Produktionssysteme im Sinne domänenspezifischer, formaler Sprachen (engl. Domain-Specific Languages) modelliert und auf textuelle Daten angewandt werden: Syndred
                 nutzt als Parser konkretisierte Grammatiken zur Abbildung von Texten auf Strukturbäume (engl. Parse Trees), die in vielfältiger Form (z. B. als TEI-Dokumente) persistiert, mittels Treewalkern (Parr 2013: 17ff) für weitere Prozessierung transformiert oder beispielhaft zur Überprüfung formaler Korrektheit unter Berücksichtigung von Merkmalen wie bspw. Schriftgröße, -stil und -farbe eingesetzt werden.
            
          
         Wegweisend für die Entwicklung des hier beschriebenen Systems sind die Arbeiten von Wirth und Gutknecht (Gutknecht 1985 bzw. Wirth / Gutknecht 1992: 78ff). Entgegen der Funktionsweise eines Compilers jedoch, der die Analyse des zu übersetzenden Programms meist direkt in Programmcode vornimmt, definiert Syndred textuelle Strukturen anhand domänenspezifischer Grammatiken und bildet sie auf Syntaxgraphen ab.
          
         Der syntaxkontrollierte Editor (engl. Syntax-Driven Editor, kurz: Syndred) soll als flexibles Werkzeug in unterschiedlichsten Projekten (z. B. Lexikoneditoren und denzentraler Korrektur) dienen, kollaboratives Arbeiten durch effiziente formale Kontrolle unterstützen und damit die allgemeine Produktivität durch die Erstellung domänenspezifischer Sprachen erhöhen (Fowler 2011: xxi).
          
         Syndred wartet mit einem Split-View Design auf, welches einerseits das Erstellen und Bearbeiten von Grammatiken und andererseits syntaxkontrolliertes Editieren textueller Inhalte in direkten, visueller Bezug zueinander bringt. Kernkompetenzen der Benutzerschnittstelle sind die Herleitungen von Grammatiken aus textuellen Bestandsdaten oder Rückmeldungen formaler Produktionsregeln zu textuellen Inhalten.
          
         Dezentrales Arbeiten erfordert eine netzwerkbasierte Architektur, idealerweise eine Web-Applikation mit clientseitigem Editor und serverseitigem Parser; Syndred nutzt das auf ReactJS
                 basierende Editor-Framework DraftJS
                 und kommuniziert im JSON-Format über WebSockets mit einer zentralen, auf Basis des Spring-Frameworks
                 implementierten Java-Applikation. Neben dem Vorteil der Plattformunabhängigkeit ermöglichen die eingesetzten Web-Technologien die zentralisierung der Hardwareleistung wie auch des Speicherbedarfs und sichern gegen Datenverlust ab. Auch garantiert die Bidirektionalität der Client-Server-Verbindung die Kohärenz und Persistenz der kollaborativen Instanzen. Zeitnahe Verfügbarkeit aller für die kollaborative Arbeit mit Syndred notwendigen Ressourcen wird durch einen serverseitigen Cached Thread Pool sichergestellt; aktiven Instanzen wird bei Bedarf ein Parser-Threads zur Verfügung gestellt, der nach Verwendung beendet wird.
            
          
         Syndred ist somit ein Werkzeug zur intuitiven Entwicklung domänenspezifische Sprachen und Überprüfung textueller Inhalte anhand dieser formalen Grammatiken, realisiert als kollaborative Web- Applikation; zusammengenommen ein Meilenstein in der Entwicklung dieser Art von Programmiersprachen.
      
      
         
             RBNF ist eine EBNF (Extended Backus-Naus Form) Erweiterung um Rich-Text Auszeichnungen.
            
               https://github.com/spinfo/syndred
            
            
               https://facebook.github.io/react
            
            
               https://draftjs.org
            
            
               https://projects.spring.io/spring-framework
            
         
         
            
               Bibliographie
               
                  Bernstein, Abraham / Hendler, James / Noy, Natalya (2016): “A New Look at the Semantic Web.” in: 
                        Communications of the ACM. 59:9. 35-37.
                    
               
                  Fowler, Martin (2011): 
                        Domain-Specific Languages. Addison-Wesley: Upper Saddle River, New Jersey.
                    
               
                  Gutknecht, Jürg (1985): “Concepts of the Text Editor Lara.” in: 
                        Communications of the ACM. 28:9. 942-960.
                    
               
                  Parr, Terence (2013):  
                        The Definitive ANTLR 4 Reference. Pragmatic Bookshelf: Dallas Texas.
                    
               
                  Wirth, Niklaus (19864): 
                        Compilerbau. Teubner: Stuttgart.
                    
               
                  Wirth, Niklaus (20005): 
                        Algorithmen und Datenstrukturen. Teubner: Stuttgart.
                    
               
                  Wirth, Niklaus / Gutknecht, Jürg (1992):  
                        Project Oberon: Design of an Operating System and Compiler. Addison-Wesley Longman: Amsterdam. 
                    
            
         
      
   



      
         
            Forschungsstand:
            Berufsbezeichnungen sind eine der häufigsten Angaben von individualspezifischen Quellen. Besonders in den Sozial- und Politikwissenschaften, den Geisteswissenschaften und einigen naturwissenschaftlichen Disziplinen (Sozialtopografie, Medizin, Arbeitsmedizin, Epidemiologie etc.) bieten Berufsbezeichnungen einen wichtigen Bezugspunkt sozialstruktureller Analysen. Dazu kommen verschiedene Formen von Berufsklassifikationen zum Einsatz. Während für die Berufswelten des 20./21. Jahrhunderts verschiedene normierte Klassifikationsmodelle als Standard sowohl auf nationaler wie internationaler Ebene existieren, kann die interdisziplinäre Forschung für deutschsprachige, historische Berufe nicht auf ein solches Normsystem zurückgreifen. Insgesamt gibt es bisher keinen gültigen Standard. Zu den renommiertesten und qualitativ hochwertigsten historischen Klassifikationsmodellen gehören bisher die Historical International Standard Classification of Occupations (HISCO) (van Leeuwen et al. 2002), das PST-System der Cambridge Group um Wrigley und Davies (Wrigley 2010). Sie zeichnen sich durch eine theoretisch nachvollziehbare Konzeption der Tätigkeit aus, beinhalten bisher aber kaum deutschsprachige Berufsnamen. Berufsklassifikationssysteme wie zu Altona 1803 (Brandenburg et al.1991) und das Berufsklassifikationsmodell zur Analyse von Bürgerlichkeit von Schüren und Hettling (Schüren 1989, Hettling 1999) liefern Ansätze für deutschsprachige Berufe. Daneben existieren etliche andere, im Zuge der Städteforschung entstandene Systeme, die jedoch eher als Teilaspekt einer Arbeit entstanden sind (bspw. François 1982; Kill 2001; Rödel 1985; Sachse 1987). 
            Aufgrund des hohen personellen Aufwands werden Systematiken häufig induktiv entwickelt und die Einordnung der Berufe (Abklären des genauen historischen Tätigkeitsprofils; Zuordnung zur richtigen Beschreibungsform der Berufsklassifizierung) gar nicht durchgeführt bzw. nicht dokumentiert. Daher entwickeln Forschungsprojekte jeweils neue Klassifizierungsmodelle, was eine Vielzahl nicht vergleichbarer Ergebnisse und für das einzelne Projekt ein enorm zeitaufwendiges Verfahren produziert. Dies schmälert den wissenschaftlichen Mehrwert, da die Ergebnisse von sozialstrukturellen Aussagen und Analysen letztlich nicht reproduzierbar sind. Zudem unterbleibt bei solchen Vorhaben sowohl eine konzeptionelle als auch eine direkte Anbindung an moderne Klassifikationsmodelle, da diese keine unmittelbare „Schnittstelle“ zu historischer Beruflichkeit bieten.
            Mit der Ontologie deutschsprachiger Berufs- und Amtsbezeichnungen möchten wir diese Lücke schließen und eine Berufssystematik für historische, deutschsprachige Berufe entwickeln, die sowohl an die (modernen) Klassifikationssysteme anknüpfen (Klassifikation der Berufe 2010: Wiemer et al. 2011) als auch die deutschen Berufsbezeichnungen für internationale, historische Klassifikationssysteme (HISCO, PST) anschlussfähig macht und die sowohl als maschinenlesbare Ontologie als auch mittels eines Webservice für einen individuellen Zugriff genutzt werden kann. 
         
         
            Ziel des Projekts
            Ziel des Projektes ist es einen nationalen Standard zur Normierung und multiperspektivischen Klassifizierung von deutschsprachigen, historischen Berufsbezeichnungen zu entwickeln und als webbasierte, offene Ressource für die Nachnutzung in Forschungsprojekten und für Infrastrukturen des Forschungsdatenmanagements zur Verfügung stellen. Der hohe Mehrwert für die wissenschaftliche Analyse und die enorme Arbeitserleichterung sollte erheblich dazu beitragen, auf positive Weise tatsächlich einen anreizbildenden Standard zu etablieren, zumal dieser dann auch international anschlussfähig sein wird. Da sich die meisten Klassifizierungsmodelle auf Berufssysteme des 19./20. Jahrhundert konzentrieren, möchten wir diese Systematiken um Beruflichkeit der Frühen Neuzeit und des Spätmittelalters erweitern. Im Einzelnen umfasst dieser Service:
            
               Einen Standard zur Normierung von deutschsprachigen Berufsbezeichnungen. Dazu stehen ca. 200.000 Varianten von historischen Berufsschreibungen zur Verfügung, die sich in ca. 20.000 normierte Einheitsbezeichnungen zusammenfassen lassen. Dieser enorme Korpus ist Ergebnis jahrelanger eigener Datenproduktionen und Kooperationen mit qualitativ hochrangigen Projekten. Im Gegensatz zu vielen Handbüchern, Lexika und anderen gedruckten Kompendien (Ebener 2015; Gerholz 2005; Haemmerle 1933; Molle 1975; Palla 2014; Puchner / Stadler 1935; Reith 1991; Ulm-Sanford 1975) werden die Originalschreibungen der Varianten erhalten. Dies ermöglicht einen hohen Mehrwert für interdisziplinäre Zugriffe z.B. linguistische und etymologische Analysen, aber auch für den technischen Vorgang der Normierung und Zuweisung der Kodierung in einer Vielzahl von Erschließungsprojekten. Damit bietet das Werkzeug eine außerordentliche gute Grundlage, zur Erschließung (Lemmatisierung und Kodierung) von textuellen Quellen jeder Art.
               Um die Klassifizierung zu erleichtern, wie auch einen Informationsverlust durch die Klassifizierung zu verhindern, werden alle Berufsbezeichnungen zunächst in kategorial geordnete semantische Einheiten aufgetrennt. Dies erleichtert erheblich die Nutzbarkeit des Angebots für unterschiedlichste Anforderungen. Diese semantische Trennung und Normierung der Namenseinheiten ermöglicht später eine individuell flexible inhaltliche Auswahl verschiedener logischer Einheiten (Topic Modeling) nach eigenen Prinzipien. In unserem Vortrag möchten wir die Logik dieses Modells erläutern und dem Fachpublikum zur Diskussion stellen, um Anregungen für eine interdisziplinär optimal zu nutzende Methode zu erhalten. Diese Vorgehensweise unterscheidet das Projekt von allen anderen rein auf ID-Identifikatoren zugeschnittenen Verfahren und macht es wesentlich flexibler und breiter für verschiedenste Bedürfnisse geistes- und sozialwissenschaftlicher Forschung anwendbar. Bei herkömmlichen Verfahren kann lediglich über eine fest zugewiesene ID-Nummer auf den verschiedenen Ebenen des Kodierungssystems klassifiziert und sortiert werden. In dem hier vorgeschlagenen Modell erfolgt zusätzlich auch auf der semantischen Ebene die Möglichkeit zum intuitiven Systematisieren, Anordnen und Auswählen. Dies gilt auch für alle weiteren Informationen (Geschlecht, Herrschaft, Rechtsbeziehungen, Ruhestand, Karriere etc.), die mit typisch frühneuzeitlichen, quellenbasierten Berufsbezeichnungen einhergehen und in moderne Berufssystematiken meist keinen Eingang finden. Moderne Klassifikationssysteme für frühneuzeitliche Berufsnamen sind daher tendenziell mit einem sehr hohen Informationsdefizit belastet und können unter Umständen in ahistorischen Analysen enden.
            
            
               
            
            
               Zur Klassifizierung von Berufen werden aus zeittypischen Lexika des 17./18. Jahrhunderts Tätigkeitsbeschreibungen abstrahiert, welche die Anforderungsprofile und Kompetenzen einzelner Berufe definieren. Sie dienen dem Abgleich mit modernen Tätigkeitsprofilen und erlauben erstmals überhaupt eine tätigkeitsgenaue Zuordnung historischer Berufe. Im Vortrag möchten wir zeigen, welche Möglichkeiten zur Systematisierung und Kategorisierung Lexika der frühen Neuzeit bieten und welche inhaltlichen Anknüpfungspunkte sie für moderne Klassifikationssysteme liefern. Dieser Aspekt bietet zudem hervorragende Möglichkeiten zur fachwissenschaftlichen Analyse, werden auf diese Weise doch Definitionskriterien frühneuzeitlicher Beruflichkeit überhaupt ermittelt und mittels einer logischen Auszeichnung auffindbar und auswertbar (xml/TEI). Wir möchten zeigen, welche Probleme dabei auftreten und welche Lösungsansätze wir hierfür entwickelt haben.
            
            
               
            
            Weitere Ziel des Projektes (aber nicht unmittelbar unseres Vortrages, auch wenn wir auf die einzelnen Punkte sicherlich hinweisen) sind zudem folgende Punkte:
            
               Die Tätigkeitsprofile ermöglichen die nachvollziehbare, transparente Einordnung von Berufsbezeichnungen zum bereits existierenden englischsprachigen Berufsklassifizierungssystems HISCO. Momentan gibt es in HISCO lediglich 1.306 deutschsprachige Berufsbezeichnungen. Die Daten wurden nicht von einem Muttersprachler kodiert, weshalb viele Zuordnungen korrigiert werden müssten. Die Kodierung von HISCO ermöglicht das Mapping weiterer historischer Klassifikationsmodelle wie HISCLASS oder PST. Da HISCO moderne Beruflichkeit misst, ist eine Erweiterung und Präzisierung für die Berufe der Frühen Neuzeit unerlässlich (van Leeuwen et al. 2002; HISCO Tree Of Occupational Groups [http://historyofwork.iisg.nl/major.php]).
               Zudem werden die Einheitsbezeichnungen der KldB 2010 zugeordnet. Sie bieten nicht nur einen Zugriff auf (weitere) moderne Berufsklassifikationsmodelle, sondern durch den theoretischen Perspektivwechsel auch eine Systematisierung nach Anforderungsprofilen (Wiemer 2011).
               Die offenen Daten ermöglichen es Nutzern, transkribierte, quellenbasierte Originalberufsbezeichnungen zu normalisieren und automatisiert in ein ausgewähltes Kodierungssystem zu überführen. Ein Usecase wäre bspw. ein Forschungsprojekt, welches eine Netzwerkanalyse durch Heiratsverbindungen vornimmt. Der Beruf bietet hier häufig den einzigen Hinweis auf die sozialstrukturelle Dimension von Gruppen. Über das Werkzeug können die Daten in einem gewünschten Klassifizierungsmodell ausgegeben werden, indem die Originalschreibungen der Berufe in das Tool geladen und automatisiert kodiert werden. 
               Varianten die keine automatisierte Kodierung beinhalten, können anschließend recherchiert und kodiert und in das Kompendium übertragen werden. Das Werkzeug ist damit beliebig erweiterbar. Das Projekt entwickelt ein Geschäftsmodell, wie solche Daten als Teil eines Serviceangebots nachkodiert und integriert werden können. Als Teil der langfristigen Infrastruktureinrichtung DARIAH soll die Ontologie der Berufe durch die Niedersächsische Staats- und Universitätsbibliothek auf Dauer angeboten werden.
            
         
         
            Zwischenergebnisse
            
               Im Bereich der Berufssegmentierung wurde ein Prototyp eines Datenschemas entwickelt, um die Berufe in adäquate Informationseinheiten aufzutrennen. 
               Der Informationsgehalt der Lexika bezogen auf berufskundliche Informationen wurde tiefergehend untersucht und erste Aussagen können darüber getroffen werden. 
            
            
               
            
            Des Weiteren ist ein erstes Netzwerk von Autorenschaften und Informationsflüssen nachweisbar.
            
               
            
            
               Die Taxonomie der Klassifikationssysteme von HISCO und der KldB 2010 wurden ausführlich untersucht und erste Ansätze einer eigenen Systematik können vorgestellt werden.
               Mehrere Workflowalternativen von der Quelle bis zum Ergebnis der Inhaltsanalyse wurden erprobt, bewertet und können mit ihren Vor- und Nachteilen präsentiert werden. An dieser Stelle platziert sich am deutlichsten die „Kritik an der digitalen Vernunft“, vor allem in Bezug auf die 
                        
                     Arbeit mit OCR-Erkennung (am Beispiel der OCR-Programmsammlung „ocropy“) 
                     dem Pre- und Postprocessing von Quellendigitalisaten (im Sinne von Ordnerstrukturen und Bildaufarbeitung) 
                     XML vs. QDA-gestützten hermeneutischen Verfahren der Inhaltsanalyse (zur Implementierung und Weiterverarbeitung der Ergebnisse und Daten) und
                     der Nutzung von Objekt- oder relationalen Datenbanken zur Datenverwaltung und Verarbeitung. 
                  
               
            
            Hierbei wird über die Vermeidung von epistemischen Fallstricken durch informationstechnische Automatisierung reflektiert und die Effizienzsteigerung digitaler Werkzeuge kritisch betrachtet, aber auch über die neuen Möglichkeiten zur Bewältigung von Big Data und dem dazugehörigen Erkenntnisgewinn referiert.
         
      
      
         
            
               Bibliographie
               
                  
                  Brandenburg, Hajo / Gehrmann, Rolf / Krüger, Kersten / Künne, Andreas / Rüffer, Jörn (1991): Berufe in Altona. Berufssystematik für eine präindustrielle Stadtgesellschaft anhand der Volkszählung. Kiel: Arbeitskreis für Wirtschafts- und Sozialgeschichte Schleswig-Holsteins.
                    
               
                  Brückner, Carola / Möhle, Sylvia / Pröve, Ralf / Roschmann, Joachim (1988): Vom Fremden zum Bürger: Zuwanderer in Göttingen 1700-1755. In: Hermann Wellenreuther (Hg.): Göttingen 1690-1755. Studien zur Sozialgeschichte einer Stadt. Göttingen (Göttinger Universitätsschriften. Serie A, Schriften, Bd. 9), S. 88–174.
                    
               
                  Bundesanstalt für Arbeit (1988): Klassifizierung der Berufe. Systematisches und alphabetisches Verzeichnis der Berufsbenennungen. Nürnberg. Online verfügbar unter http://statistik.arbeitsagentur.de/Statischer-Content/Grundlagen/Klassifikation-der-Berufe/KldB1975-1992/Generische-Publikationen/KldB1988-Systematischer-Teil.pdf, zuletzt geprüft am 07.06.2016.
                    
               
                  Bundesanstalt für Arbeit (2011): Klassifikation der Berufe 2010. Systematischer und alphabetischer Teil mit Erläuterungen. 2 Bände. Nürnberg (1). Online verfügbar unter https://statistik.arbeitsagentur.de/Statischer-Content/Grundlagen/Klassifikation-der-Berufe/KldB2010/Printausgabe-KldB-2010/Generische-Publikationen/KldB2010-Printversion-Band1.pdf, zuletzt geprüft am 07.06.2016.
                    
               
                  
                  Ebeling, Dietrich (1987): Bürgertum und Pöbel. Wirtschaft und Gesellschaft Kölns im 18. Jahrhundert. Köln (Städteforschung. Reihe A, Darstellungen, Bd. 26).
                    
               
                  Ebner, Jakob (2015) Wörterbuch historischer Berufsbezeichnungen. Berlin u.a.
                    
               
                  
                  Fischer, Volker (2000): Stadt und Bürgertum in Kurhessen. Kommunalreform und Wandel der städtischen Gesellschaft 1814-1848. Kassel (Hessische Forschungen zur geschichtlichen Landes- und Volkskunde, 35).
                    
               
                  
                  François, Etienne (1982): Koblenz im 18. Jahrhundert. Zur Sozial- und Bevölkerungsstruktur einer deutschen Residenzstadt. Göttingen (Veröffentlichungen des Max-Planck-Instituts für Geschichte, Bd. 72).
                    
               
                  
                  Gerber, Roland (2001): Gott ist Burger zu Bern. Eine spätmittelalterliche Stadtgesellschaft zwischen Herrschaftsbildung und sozialem Ausgleich. Weimar (Forschungen zur mittelalterlichen Geschichte, 39).
                    
               
                  Gerholz, Heinrich (2005): Gerholz-Kartei, Eine Sammlung alter Berufsbezeichnungen, Lübeck.
                    
               
                  Haemmerle, Albert (1966): Alphabetisches Verzeichnis der Berufs- und Standesbezeichnungen vom ausgehenden Mittelalter bis zur neueren Zeit, (Reprografischer Nachdruck der Ausgabe München 1933), Hildesheim.
                    
               
                  
                  Hahn, Hans-Werner (1991): Altständisches Bürgertum zwischen Beharrung und Wandel. Wetzlar, 1689-1870. München (Stadt und Bürgertum, Bd. 2).
                    
               
                  
                  Hettling, Manfred (1999): Politische Bürgerlichkeit. Der Bürger zwischen Individualität und Vergesellschaftung in Deutschland und der Schweiz von 1860 bis 1918. Göttingen.
                    
               
                  
                  ILO (1958): International Standard Classification of Occupations. Geneva. Online verfügbar unter http://www.ilo.org/public/libdoc/ilo/1958/58B09_81_engl.pdf, zuletzt geprüft am 31.05.2016.
                    
               
                  
                  ILO (1969): International Standard Classification of Occupations. Revised Edition 1968. Geneva.
                    
               
                  
                  ILO (2004): ISCO-88. Main Objectives. Online verfügbar unter http://www.ilo.org/public/english/bureau/stat/isco/isco88/publ1.htm, zuletzt geprüft am 06.07.2016.
                    
               
                  
                  ILO (2008): Resolution Concerning Updating ISCO 1 Resolution Concerning Updating the International Standard Classification of Occupations. Online verfügbar unter http://www.ilo.org/public/english/bureau/stat/isco/docs/resol08.pdf, zuletzt geprüft am 31.05.2016.
                    
               
                  
                  Jägers, Regine (2001): Duisburg im 18. Jahrhundert. Sozialstruktur und Bevölkerungsbewegung einer niederrheinischen Kleinstadt im Ancien Regime (1713-1814). Köln (Rheinisches Archiv, 143).
                    
               
                  
                  Kill, Susanne (2001): Das Bürgertum in Münster 1770-1870. bürgerliche Selbstbestimmung im Spannungsfeld von Kirche und Staat. München.
                    
               
                  
                  Kroll, Stefan (1997): Stadtgesellschaft und Krieg. Sozialstruktur, Bevölkerung und Wirtschaft in Stralsund und Stade 1700 bis 1715. Göttingen (Göttinger Beiträge zur Wirtschafts- und Sozialgeschichte, Bd. 18).
                    
               
                  
                  Krüger, Kersten (1986): Sozialstruktur der Stadt Oldenburg 1630 und 1678. Analysen in historischer Finanzsoziologie anhand staatlicher Steuerregister. Oldenburg.
                    
               
                  Laufer, Wolfgang (1973): Die Sozialstruktur der Stadt Trier in der frühen Neuzeit. Bonn (Rheinisches Archiv, 86).
                    
               
                  
                  Lundgreen, Margret Kraul; Ditt, Karl (1988): Bildungschancen und soziale Mobilität in der städtischen Gesellschaft des 19. Jahrhunderts. Göttingen.
                    
               
                  
                  Manke, Matthias (2000): Rostock zwischen Revolution und Biedermeier. Alltag und Sozialstruktur. Rostock (Rostocker Studien zur Regionalgeschichte, Bd. 1).
                    
               
                  Molle, Fritz (1975): Wörterbuch der Berufs- und Berufstätigkeitsbezeichnungen, Wolfenbüttel .
                    
               
                  
                  Müller, Christina (1992): Karlsruhe im 18. Jahrhundert. Zur Genese und zur sozialen Schichtung einer residenzstädtischen Bevölkerung. Karlsruhe (Forschungen und Quellen zur Stadtgeschichte, 1).
                    
               
                  Palla, Rudi (2014): Verschwundene Arbeit. Das Buch der untergegangenen Berufe. Wien.
                    
               
                  
                  Paulus, Wiebke / Matthes, Britta (2013): Klassifikation der Berufe. Struktur, Codierung und Umsteigeschlüssel. Online verfügbar unter http://doku.iab.de/fdz/reporte/2013/MR_08-13.pdf, zuletzt geprüft am 11.06.2016.
                    
               
                  Puchner, Karl / Stadler, Josef Klemens (1935): Lateinische Berufsbezeichnungen in Pfarrmatrikeln und sonstigen orts- und familiengeschichtlichen Quellen, Hirschenhausen (Obby).
                    
               
                  
                  Raschke, Helga (2001): Bevölkerung und Handwerk einer thüringischen Residenzstadt. Gotha zwischen 1640 und 1740. 1. Aufl. Jena (Palmbaum Texte. Kulturgeschichte, Bd. 9).
                    
               
                  Reith, Reinhold (1991): Lexikon des alten Handwerks. Vom späten Mittelalter bis ins 20. Jahrhundert. München.
                    
               
                  
                  Rödel, Walter Gerd (1985): Mainz und seine Bevölkerung im 17. und 18. Jahrhundert. Demographische Entwicklung, Lebensverhältnisse und soziale Strukturen in einer geistlichen Residenzstadt. Stuttgart (Geschichtliche Landeskunde, Bd. 28).
                    
               
                  
                  Sachse, Wieland (1987): Göttingen im 18. und 19. Jahrhundert. Zur Bevölkerungs- und Sozialstruktur einer deutschen Universitätsstadt. Göttingen (Studien zur Geschichte der Stadt Göttingen, Bd. 15).
                    
               
                  
                  Schüren, Reinhard (1989): Soziale Mobilität. Muster, Veränderungen und Bedingungen im 19. und 20. Jahrhundert. St. Katharinen.
                    
               
                  Schüren, Reinhard (1989): Soziale Mobilität: Muster, Veränderungen und Bedingungen im 19. und 20. Jahrhundert. St. Katharinen.
                    
               
                  
                  Statistisches Bundesamt (1992): Klassifikation der Berufe 1992 (KldB 92). Gliederungsstruktur bis zur 4 Steller-Ebene. Stuttgart: Metzler-Poeschel. Online verfügbar unter https://www.destatis.de/DE/Methoden/Klassifikationen/Berufe/klassifikationkldb92_4st.pdf?__blob=publicationFile, zuletzt geprüft am 06.06.2016.
                    
               
                  
                  Straubel, Rolf (1995): Frankfurt (Oder) und Potsdam am Ende des Alten Reiches. Studien zur städtischen Wirtschafts- und Sozialstruktur. 1. Aufl. Potsdam (Quellen und Studien zur Geschichte und Kultur Brandenburg--Preussens und des Alten Reiches, Bd. 2).
                    
               
                  Ulm-Sanford, Gerlinde (1975): Wörterbuch von Berufsbezeichnungen aus dem siebzehnten Jahrhundert, gesammelt aus den Wiener Totenprotokollen der Jahre 1648 - 1668 und einigen weiteren Quellen, Bern 1975.
                    
               
                  
                  van Leeuwen, Marco H.D. / Maas, Ineke / Miles, Andrew (2002): Historical international standard classification of occupations. Leuven.
                    
               
                  
                  Weichel, Thomas (1993a): Die Berufsstruktur der Städte - erste Ergebnisse und Vergleiche. In: Lothar Gall (Hg.): Stadt und Bürgertum im Übergang von der traditionalen zur modernen Gesellschaft. München (Historische Zeitschrift. Beihefte, n.F., Bd. 16), S. 51–74.
                    
               
                  
                  Weichel, Thomas (1993b): Die Bürger in ihrer beruflichen und sozialen Stellung. In: Lothar Gall (Hg.): Stadt und Bürgertum im Übergang von der traditionalen zur modernen Gesellschaft. München (Historische Zeitschrift. Beihefte, n.F., Bd. 16), S. 93–103.
                    
               
                  Weichel, Thomas (1997): Die Bürger von Wiesbaden. von der Landstadt zur "Weltkurstadt", 1780 1914. München.
                    
            
         
      
   



      
         Eine Kritik der digitalen Vernunft muss auch eine Kritik der digitalen Ressourcen umfassen.
                 Eine traditionelle Form der Kritik ist die wissenschaftliche Besprechung oder Rezension. Die kritische Instanz eines Rezensionswesens fehlt den Digital Humanities bisher fast gänzlich. Digitale Ressourcen wie etwa wissenschaftliche Editionen, Textkorpora, Bilddatenbanken oder auch Software werden selten umfassend rezensiert.
                 Das Gleiche gilt für das “Primärmaterial” dieser Ressourcen, also für Datensätze.
            
         Im Gegensatz zu traditionellen Forschungsergebnissen der Geisteswissenschaften sind digitale Ressourcen nicht statisch, sondern wandelbar, oft prozesshaft und nicht abgeschlossen. Die Kritik der Ressourcen muss die besonderen Bedingungen, Eigenschaften und Folgephänomene digitaler Daten berücksichtigen und eine eigene Form finden.
                 Wie sich die Ressourcen wandeln, so muss sich auch die Kritik wandeln, denn panta rhei - “alles fließt”, sagt Heraklit.
                
         
         Vor diesem Hintergrund widmet sich das Panel u. a. folgenden Fragen: Wie können traditionelle Rezensionsorgane die Besprechung digitaler Ressourcen fördern? Brauchen die Digital Humanities ein eigenes Rezensionswesen, um den Besonderheiten digitaler Ressourcen gerecht zu werden? Wenn ja, wie muss die Rezension als ,Momentaufnahme’ einer digitalen Ressource methodisch und technisch konzipiert sein, um nicht der ,Schnelllebigkeit’ der digitalen Welt zu erliegen? Welche Herausforderungen stellen Publikationen von Daten oder Algorithmen an die RezensentInnen? Inwiefern steigen mit dem Zuwachs an technischen Möglichkeiten auch die Ansprüche an digitale Ressourcen und wie lassen sich diese als Standards und Evaluationskriterien verhandeln, dokumentieren und weiterentwickeln? Wie kann man die Digitalität des Rezensionswesens nutzen, um die Prozesshaftigkeit der zu rezensierenden Objekte zu berücksichtigen, z. B. auch durch dynamischere Formen der Kritik jenseits der traditionellen Rezension?
          Das Panel wird mit einer Moderation und sechs Impulsbeiträgen einen Überblick über das Themenfeld “Ressourcen und Rezensionen in den Digital Humanities” geben. Die ReferentInnen berichten von Erfahrungen aus der herausgeberischen Praxis, diskutieren Problemfelder und setzen theoretische Impulse, um im Anschluss rasch in eine offene Diskussion mit dem Publikum überzugehen.
         
            
               Rüdiger Hohls:
                     Die Rezension analoger und digitaler Medien bei H-Soz-Kult
                
            Die Tradition der wissenschaftlichen Rezensionen wird vor allem in den Geistes-, Kunst-, Kultur-, Politik- und Sozialwissenschaften gepflegt, überwiegend Fächer, die unter einer Überproduktion von Texten leiden und deshalb für eine Sichtung und für ein „Marketing“ über Rezensionen besonders empfänglich sind. 
                
            Zu den Pflichten einer Rezension zählt laut Georg Jäger (2001) die Berichtspflicht über Zielsetzung, Gliederung, Argumentationslinien und Ergebnisse, kritische Reflexion des methodischen Vorgehens und der ausgewählten Quellen, weiterhin die Wertung und Einbettung in den Kontext der einschlägigen Fachdiskussion. Zur Kür bei digital vertriebenen Rezensionen zählt darüber hinaus ein „feuilletonistischer Ton“. Außerdem sei es notwendig, die „Ethik wissenschaftlicher Kommunikation“ neu zu justieren. Viele dieser Aspekte gelten auch für die Rezension digitaler Medien, und natürlich haben wir bei H-Soz-Kult unsere formatspezifischen Hinweise für RezensentInnen regelmäßig an neue technische Entwicklungen angepasst. 
            H-Soz-Kult hat seit 1996 über 15.200 Buchrezensionen, mehr als 220 Ausstellungsrezensionen, knapp 160 Rezensionen zu Publikationen auf digitalen Trägermedien und lediglich 33 sogenannte Webrezensionen veröffentlicht. Die Zahlen sprechen nicht nur für die Relevanz der Genres bei H-Soz-Kult, sondern unterstreichen auch die Bedeutung des „Buchs“ in den Geschichtswissenschaften, egal ob analog oder digital publiziert. Es stellt sich aber u. a. die Frage, warum es trotz aller Bemühungen bei H-Soz-Kult nur vergleichsweise wenige Besprechungen von fachwissenschaftlichen digitalen Ressourcen gibt.
         
         
            
               Frederike Neuber:
                     RIDE und die Herausforderungen der Digitalität
                
            Seit 2014 gibt das IDE die digitale Rezensionszeitschrift RIDE (
                    A review journal for digital editions and resources) heraus. Bis Dezember 2017 wurden in sieben Ausgaben 30 wissenschaftliche digitale Editionen und 10 digitale Textsammlungen rezensiert. Zum methodischen Rahmenprogramm RIDEs zählen Kriterienkataloge für die Besprechung des jeweiligen Ressourcentyps (Henny u. Neuber 2017, Sahle 2014), die Erhebung von Daten in einem Questionnaire und ein externes Peer Reviewing der Beiträge.
                
            Ein “markantes Kennzeichen digitaler Texte ist deren Veränderbarkeit und prinzipielle Offenheit” (cf. Working Paper der DHd-Arbeitsgruppe “Digitales Publizieren” 2016), das gilt auch für die Rezensionsobjekte in RIDE: Etwa kann sich schon während des Rezensierens und/oder nach der Publikation einer Rezension die Datengrundlage einer besprochenen Ressource verändern; im Zuge eines Relaunches kann ein User Interface ersetzt werden. Um derlei Fällen vorzubeugen empfehlen die RIDE-
                    Reviewing Guidelines
               
                     etwa die Erstellung von Screenshots und die Archivierung von Websites. Damit ist der Status Quo der Ressourcen zum jeweiligen Rezensionszeitpunkt zumindest teilweise dokumentiert.
                
            Wie die Rezensionsobjekte, so sind auch die digitalen Rezensionstexte grundsätzlich offener und veränderlicher als das bei gedruckten Rezensionen der Fall ist. Vereinzelt wurden in RIDE bereits nachträgliche Änderungen (z. B. auf Wunsch der BetreiberInnen der rezensierten Ressource) in Rezensionstexte integriert, was in der XML/TEI-Version der jeweiligen Ressource dokumentiert ist.
            Grundsätzlich steht die “Aktualität” digitaler Rezensionen von digitalen Ressourcen trotz den in RIDE bestehenden und oben geschilderten Dokumentationsverfahren auf wackeligen Beinen. Es stellt sich daher die Frage, ob eine Rezension mehr sein kann bzw. muss als die “Momentaufnahme” eines bestimmten Entwicklungsstandes einer digitalen Ressource. Und wenn sich eine Ressource nach der Rezension signifikant weiterentwickelt, liegt es dann in der Verantwortung der RIDE-HerausgeberInnen, eine erneute Rezension anzustoßen? Oder sollte man den RessourcenbetreiberInnen selbst die Möglichkeit geben, in RIDE über Updates zu berichten, wenn Rezension und aktueller Ressourcenstand zu sehr divergieren? 
         
         
            
               Anne Baillot:
                     Daten als Rezensionsobjekte
                
            In den Geisteswissenschaften werden Forschungsfragen verfolgt und in Publikationen beantwortet. Diese sind traditionell Gegenstand von Rezensionen. Im Digitalen kommt mit den Forschungsdaten eine “neue” Publikationsform hinzu, die in das Rezensionswesen einbezogen werden muss. Der Status des "Primärmaterials" ist im digitalen Rezensionswesen zu klären und kann sich unter Umständen am Modell der Naturwissenschaften orientieren. Data Papers und Data Journals sind neue Formen der Dokumentation und Publikation, die sich Struktur, Kohärenz und Vollständigkeit von Daten widmen. Damit rückt auch Datenmodellierung als eine zu evaluierende wissenschaftliche Tätigkeit mehr in den Vordergrund.
            Während naturwissenschaftliche Zeitschriften, die Data Papers veröffentlichen oder rezensieren, solche Datensätze in speziellen, selten öffentlich zugänglichen Repositorien verfügbar machen, entwickelt sich in den Geisteswissenschaften die Tendenz, die Primärdaten eher zusammen mit ihren Auswertungen oder auf allgemeinen Plattformen offen und langzeitverfügbar zugänglich zu machen. Wie stabil ist diese offene Bereitstellung auf lange Sicht? Wie komplex wird dadurch bei einer immer wieder aktualisierten digitalen Ressource die Bezugnahme auf jene Version, die in einer Rezension begutachtet wird? Darüber hinaus stellt sich die grundsätzliche Frage: Stoßen Rezensionen von Data Papers und den Daten selbst in einem Kontext der Informationsflut, in der kaum die Auswertungen wahrgenommen werden, überhaupt auf Interesse? Welche Funktion hätten sie dann in einem geisteswissenschaftlichen Rezensionswesen in der digitalen Welt? Werden sie zur Rettung oder zum endgültigen Sturz des Rezensionswesens in die Bedeutungslosigkeit beitragen?
         
         
            
               Christof Schöch:
                     Im Spannungsfeld von Projektzielen und Best Practice-Anforderungen
                
            Bei der Rezension digitaler Ressourcen treffen zwei Perspektiven aufeinander: Einerseits ist das der Rahmen der jeweils spezifischen Projektziele, die für die Erstellung einer Ressource handlungsleitend waren. Hierzu gehören auch praktische Aspekte der Machbarkeit wie Zeitrahmen, Budget und Personal. Nicht immer ist diese Perspektive für die Rezensierenden transparent. Andererseits ist das die Perspektive der theoretischen Anforderungen und der mehr oder weniger gut etablierten Best Practices, die in Form von Qualitätskriterien an eine digitale Ressource herangetragen werden können. Auch hier sind die entsprechenden Maßstäbe nicht immer geteilt und explizit. 
            Dieses Spannungsfeld und die unter Umständen asymmetrische Verfügbarkeit von Informationen wirft die Frage auf, wie die Rezension einer digitalen Ressource sowohl fair als auch anspruchsvoll sein kann, wenn unterschiedliche Maßstäbe zugleich anzulegen sind. Es stellt sich auch die Frage, inwiefern die Rezension digitaler Ressourcen hier grundsätzlich anders funktioniert als die Rezension von Monographien oder Sammelbänden. Inwieweit sollten bei der Rezension digitaler Ressourcen die äußeren Bedingungen ihrer Erstellung berücksichtigt werden? Braucht es womöglich verschiedene Ebenen von Anforderungen, wenn z. B. Ergebnisse der Individualforschung mit denjenigen großer Projekte verglichen werden? Inwiefern muss beim Anlegen von Maßstäben berücksichtigt werden, ob eine digitale Ressource weitestgehend abgeschlossen oder noch in der Bearbeitung ist?
         
         
            
               Jürgen Hermes:
                     Die Rezension als Prozess
                
            Ein Leitsatz, der die Open Source-Softwareentwicklung stark geprägt hat, und der sich auch im Paradigma der agilen Entwicklung wiederfindet, lautet “Release early, release often” (Raymond 2001).
            Ist dieses Paradigma auch auf andere Arten digitaler Publikationen wie digitale Monographien, Aufsätze, Editionen, Text- und Datensammlungen anwendbar? Eine Veröffentlichung auf digitalen Plattformen bietet völlig neue Möglichkeiten des Austauschs zwischen ErstellerInnen und KonsumentInnen von digitalen Ressourcen ‒ Errata können kurz nach ihrer Entdeckung korrigiert, Verbesserungsvorschläge zum Zeitpunkt ihrer Einreichung aufgenommen werden. Das Zusammenspiel zwischen Veröffentlichung und kritischer Würdigung wird so granularer, als dies beim althergebrachten Zusammenspiel zwischen Monographie und Rezension der Fall war. 
            Derartige, sich im Fluss befindliche Ressourcen schaffen aber dort Probleme, wo auf verlässliche Quellen verwiesen werden soll. Ebenso wird die etablierte Kultur des wissenschaftlichen Diskurses in Frage gestellt, der auf der Grundlage fixierter Objekte stattfindet. Derlei Aspekte müssen ernst genommen, können aber womöglich durch Entwicklungen in den Bereichen der Kontrolle, Referenzierung und Kommentierung von Text- und Daten-Versionen aufgefangen werden. Des Vorteils, wissenschaftliche Daten, Methoden und Ergebnisse zeitnaher, barrierefreier und intensiver zu diskutieren, sollten sich auch GeisteswissenschaftlerInnen nicht berauben lassen. 
         
         
            
               Patrick Sahle:
                     Prinzipien der Digitalität
                
            Unsere digitale Informationsumwelt ist, wie die vorhergehende analoge Welt, von wenigen Grundprinzipien geprägt, die zunächst technischer Natur sind, dann aber Folgen in allen möglichen Bereichen erzeugen: Technik, Methode, Inhalte, Form, Soziologie, Politik von Wissen, Wissensproduktion und Wissenspräsentation in der Forschung. Im Bereich digitaler Ressourcen und Rezensionen lassen sich hier vielfältige Phänomene durch verallgemeinernde Schlagworte andeuten: Multimedialisierung, Vernetzung, Aufhebung von Mengenbegrenzungen, Trennung von Daten, Auswertung und Präsentation, Generativität und Prozesshaftigkeit von Publikationen, Dynamik und Interaktion, Zunahme der Komplexität, Kollaborativität, Interdisziplinarität, unmittelbare Diskursivität etc. Damit sind neue Herausforderungen markiert, deren Lösungen jetzt noch nicht absehbar oder erst thesenhaft formulierbar sind. Zu nennen wären hier unmittelbare Sichtbarkeit, “Unfassbarkeit” von Ressourcen (hinsichtlich Inhalt, Status, Version, Verantwortung), Probleme der Kreditierung von Leistungen, Interdisziplinarität als Kompetenzproblem, unformierte Wissenschaftskommunikation, Halbwertszeit von Kritik, eingefrorene versus dynamische Kritikdiskurse, Standardisierung versus Innovation etc. Auch die digitale Rezension digitaler Ressourcen steht damit vor ganz anderen Aufgaben als die eingeführte Form der Besprechung.
            Vielleicht kann man es aber auch so zusammenfassen: Die Rezension in der Buchkultur ist die Grabrede auf ein in den Brunnen gefallenes Kind ‒ die Rezension in einer digitalen Kultur ist ein Verbesserungsvorschlag, der mit seiner Annahme hinfällig wird. Wie so vieles wird in der digitalen Welt auch die Kritik komplexer, die ihre neue Form und Position noch nicht gefunden hat. Vielleicht muss sie aber auch grundsätzlich anders gedacht werden, denn als Fortsetzung des traditionellen Objekts “Rezension”?
         
      
      
         
            Unter “digitale Ressource” verstehen wir publizierte digitale Objekte, in Anlehnung an die Definitionen der DNB im Papier “Digitale Publikation” (DNB 2017).
            Werden digitale Ressourcen rezensiert, dann meist aus der Fachperspektive. Einige Beispiele für verstreute Rezensionen aus DH-Perspektive sind Schöch (2017); Schelbert (2017); Assmann und Sahle (2008) mit Rezension zur Rezension (Just 2009); erste Ansätze für systematische Kritik im Sinne gebündelter Aktivitäten zur Bewertung bestimmter Typen digitaler Ressourcen (wie Projekte, Editionen, Textsammlungen, Datensätze), mit Begleitmaterialien und eingebunden in einen kritischen Diskurs, finden sich im DH Commons Journal, dem jTEI und RIDE.
            Vgl. zu diesem Problemfeld u.a. die Sektion “Evaluating Digital Scholarship” in Profession, herausgegeben von Schreibman et al. 2011; das Working Paper der DHd-Arbeitsgruppe “Digitales Publizieren” 2016; der Band “Closing the Evaluation Gap” des JDH von Cohen / Fragaszy Trojano 2012; zu digitalen Editionen insbesondere Pierazzo 2014, Yates 2008 und Henny, erscheint demnächst.
            Ein Phänomen dieses Wandels sind diverse Kataloge für Best Practices und Bewertungskriterien digitaler Ressourcen (AHA 2015, Jannidis 1999, MLA 2012, Presner 2012, Rockwell 2012, Sahle et al. 2014, Henny und Neuber 2017).
            Humboldt-Universität zu Berlin, Redaktionsmitglied von Clio online bzw. H-Soz-Kult (Projektleitung).
            Berlin-Brandenburgische Akademie der Wissenschaften, Mit-Herausgeberin des Rezensionsjournals RIDE.
            Siehe Institut für Dokumentologie und Editorik e.V. (2014 ff.), 
                            .
                        
            Le Mans Université, Managing Editor des Journals der Text Encoding Initiative und Redaktionsmitglied der Zeitschrift des französischsprachigen DH-Verbandes Humanistica sowie bei DHCommons.
            Universität Trier, Redaktionsmitglied bei Romanistik.de.
            Universität zu Köln, Institut für Digital Humanities.
            Universität zu Köln, CCeH, Mit-Herausgeber des Rezensionsjournals RIDE.
         
         
            
               Bibliographie
               
                  
                  American Historical Association (AHA, ed., 2015): “Guidelines for the Evaluation of Digital Scholarship in History”. 
                         [letzter Zugriff 14. Januar 2018]
                    
               
                  Assmann, Bernhard / Sahle, Patrick (2008): 
                        Digital ist besser. Die Monumenta Germaniae Historica mit den dMGH auf dem Weg in die Zukunft – eine Momentaufnahme. Norderstedt: Books on Demand.
                    
               
                  Cohen, Daniel J. / Fragaszy Trojano, Joan (eds., 2012): “Closing the Evaluation Gap”, in: 
                        Journal of the Digital Humanities 1, 4. 
                         [letzter Zugriff 14. Januar 2018]
                    
               
                  Deutsche Nationalbibliothek (ed., 2017): Definition des Begriffs “Digitale Publikation” und aktuelle Verwendung der Terminologie in der Deutschen Nationalbibliothek. Ergänzende Ausführungen im Rahmen der Diskussion “Zum Sammelauftrag der Deutschen Nationalbibliothek”. 
                         [letzter Zugriff 14. Januar 2018]
                    
               
                  Digital Humanities im deutschsprachigen Raum (DHd, ed., 2016): “Digitales Publizieren”. Wolfenbüttel: HAB. Stand: 01.03.2016. DOI: 10.15499/dhd-wp.001 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Henny, Ulrike / Neuber, Frederike / unter Mitarbeit von den Mitgliedern des IDE (2017):. 
                        Criteria for Reviewing Digital Text Collections, version 1.0. 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Henny, Ulrike [erscheint demnächst]: “Reviewing von digitalen Editionen im Kontext der Evaluation digitaler Forschungsergebnisse.” In: 
                        Zeitschrift für digitale Geisteswissenschaften (ZfdG). 
                        Sonderband 2: Digitale Metamorphose. Digital Humanities und Editionswissenschaft. Hrsg. von Roland S. Kamzelak und Timo Steyer.
                    
               
                  Hohls, Rüdiger (eds., 1996ff.): 
                        H-Soz-Kult. Kommunikation und Fachinformation für die Geisteswissenschaften. Angeboten von Clio-online - Historisches Fachinformationssystem e.V. Berlin.
                   [letzter Zugriff 14. Januar 2018].
                    
               
                  Institut für Dokumentologie und Editorik e.V. (eds. 2014ff.): 
                        RIDE. A Review Journal for Digital Editions and Resources. Köln. 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Jäger, Georg (2001): “Von Pflicht und Kür im Rezensionswesen.” In: 
                        IASLonline Diskussionsforum. Wissenschaftliche Kommunikation in der Kontroverse. Bayreuth / München, 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Jannidis, Fotis (1999): “Bewertungskriterien für elektronische Editionen”, in: 
                        IASL Diskussionsforum online. 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Just, Thomas (2009): “Rezension von: Digital ist besser”, in: 
                        sehepunkte 9, Nr. 3 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Modern Language Association (MLA, ed., 2012): “Guidelines for Evaluating Work in Digital Humanities and Digital Media” 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Pierazzo, Elena (2014): “Trusting Digital Editions? Peer Review and Evaluation of Digital Scholarship”, in: dies. 
                        Digital Scholarly Editing: Theories, Models and Methods, 182-205. Hal Id: hal-01182162. 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Presner, Todd (2012): “How to Evaluate Digital Scholarship”, in: 
                        Journal of Digital Humanities 1, 4 (2012).
                    
               
                  Raymond, Eric S. (2001): 
                        The Cathedral & the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary. Sebastopol: O’Reilly, 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Rockwell, Geoffrey (2012): “Short Guide To Evaluation Of Digital Work”, in: 
                        Journal of Digital Humanities 1, 4 (2012).
                    
               
                  Sahle, Patrick / unter Mitarbeit von Georg Vogeler und den Mitgliedern des IDE (2014): 
                        Kriterienkatalog für die Besprechung digitaler Editionen, Version 1.1. 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Schelbert, Georg (2017): “Digital kann mehr! Das neue Graphikportal”, in: 
                        blog.arthistoricum.net 
                   [letzter Zugriff 14. Januar 2018].
                    
               
                  Schöch, Christof (2017):. “Poston / Niles, eds., Folger Digital Texts”, in: 
                        Variants 11, 2014, pp. 16-20. DOI: 
                         [letzter Zugriff 14. Januar 2018].
                    
               
                  Schreibman, Susan / Mandell, Laura / Olsen, Stephen (eds., 2011): “Evaluating Digital Scholarship”, in: 
                        Profession. DOI: 10.1632/prof.2011.2011.1.123
                    
               
                  Yates, Kimberly (2008): “Creating a Prize for the Best Digital Editions / Online Archives.”, in: 
                        Scroll – Essays on the Design of Electronic Texts 1, 1.
                    
            
         
      
   



      
         
            Einleitung
            Das hier beschriebene Poster thematisiert den technischen Entwurf und den funktionalen Aufbau von 
                    VedaWeb, einer webbasierten Plattform für die sprachwissenschaftliche Erforschung altindischer Texte (siehe 
                    http://vedaweb.uni-koeln.de). Das 2017 begonnene Vorhaben wird als Kooperationsprojekt an der Universität zu Köln durchgeführt, in enger Zusammenarbeit zwischen Fachwissenschaftlern des Instituts für Linguistik (Allgemeine Sprachwissenschaft, Historisch-Vergleichende Sprachwissenschaft und Sprachliche Informationsverarbeitung) sowie des Cologne Center for eHumanities (CCeH). Das Projekt wird von der Deutschen Forschungsgemeinschaft (DFG) in der LIS-Förderlinie „eResearch-Technologien“ gefördert. 
                
            Über die 
                    VedaWeb-Plattform werden altindische, in Sanskrit verfasste Texte morphologisch und metrisch annotiert, sowie nach lexikographischen und korpuslinguistischen Kriterien durchsuchbar zur Verfügung gestellt. Als Pilottext dient zunächst der 
                    Rigveda, einer der ältesten und wichtigsten Texte der indogermanischen Sprachfamilie. Der 
                    Rigveda ist in der ältesten Sprachform des Altindischen, dem Vedischen, verfasst. Seine Entstehung kann auf das späte zweite Jahrtausend v. Chr. datiert werden. Mit einem Umfang größer als Homers 
                    Ilias und 
                    Odyssee zusammen stellt er eine überaus reiche Datengrundlage dar. Perspektivisch sollen auch weitere Texte wie etwa der 
                    Atharvaveda, 
                    Yajurveda und vedische Prosatexte in die 
                    VedaWeb-Plattform integriert werden. Das Projekt wird Forschungen in allen Bereichen des Vedischen erleichtern und voranbringen, beispielsweise in Bezug auf Fragestellungen der Syntax (siehe z.B. zu referentiellen Null-Objekten Keydana/Luraghi 2012, zu Nicht-Konfigurationalität Reinöhl 2016), der Morphologie (siehe z.B. zum vedischen vr̥kī-Typus Widmer 2007, zu ya-Präsentien Kulikov, 2012) oder der Wortbildung (siehe z.B. zu Komposita Scarlata/Widmer 2015). Es wird angestrebt, die 
                    VedaWeb-Plattform längerfristig zur zentralen Anlaufstelle für die internationale Fachgemeinschaft, die mit altindischen Primärtexten arbeitet, auszubauen, um den in Köln bestehenden Schwerpunkt auf südasiatische Sprachen weiter zu stärken. 
                
         
         
            Projektziele
            Ausgangspunkt und Grundlage des Projekts ist eine vollständige morphologische (d.h. wortstrukturelle) Annotation des 
                    Rigveda, die im Vorfeld an der Universität Zürich durchgeführt und dem Projekt zur Verfügung gestellt wurde. Hinzu kommen metrische Informationen (Kevin Ryan, Harvard University, siehe 
                    ) sowie perspektivisch auch syntaktische Informationen aus verschiedenen abgeschlossenen und andauernden Forschungsprojekten als weitere Annotationsebenen. Anhand dieser Annotationen werden im Projekt verschiedene Recherche- und Analysewerkzeuge entwickelt und sukzessive in die 
                    VedaWeb-Plattform integriert. Hierzu gehören eine kombinierte Suchfunktion nach linguistischen Parametern (u.a. Lemmata, Wortformen, morphologische und metrische Informationen), die Verknüpfung mit dem Standardwörterbuch zum 
                    Rigveda von Hermann Grassmann (1873), die Anzeige von Übersetzungen (u.a. Grassmann 1876, Geldner 2003, Griffith 1896) und von Kommentaren (Oldenberg 1909/1912), sowie die Möglichkeit des Exports von annotierten Textabschnitten nach vom Nutzer gewählten Kriterien. 
                
            Von zentraler Bedeutung ist die Verknüpfung des 
                    Rigveda mit der am CCeH angesiedelten Portalseite für Sanskritwörterbücher (
                    Cologne Digital Sanskrit Dictionaries, siehe 
                    http://www.sanskrit-lexicon.uni-koeln.de), einer zentralen Anlaufstelle für die internationale Sanskritforschung. Auf Basis einer Modellierung der Daten in TEI werden die Wortformen über die jeweiligen Lemmata mit dem digital erfassten Wörterbuch von Grassmann verknüpft, so dass sowohl vom Text auf das Wörterbuch verwiesen wird als auch umgekehrt vom Wörterbuch aus Textstellen aufgesucht werden können. Auf diese Weise kann über das Lemma gleichzeitig auch eine Verknüpfung zu den weiteren Sanskrit-Wörterbüchern hergestellt werden, etwa um vergleichende, wörterbuchübergreifende Recherchen zu ermöglichen. In der direkten Verknüpfung von Text und Wörterbuch und der damit verbundenen Erweiterung der Analysemöglichkeiten besteht das wesentliche Alleinstellungsmerkmal des Projekts gegenüber bestehenden Ressourcen des Altindischen wie z.B. dem 
                    Thesaurus Indogermanischer Text- und Sprachmaterialien (TITUS, siehe 
                    http://titus.uni-frankfurt.de).
                
         
         
            Technische Aspekte
            Der Schwerpunkt des Posters liegt auf der Präsentation des Systementwurfs der 
                    VedaWeb-Plattform sowie der dort eingesetzten Technologien. Dies umfasst zum einen eine Beschreibung der funktionalen Elemente der Nutzerschnittstelle, die dem Anwender als Forschungsumgebung dient, indem sie verschiedene Werkzeuge bereitstellt (z.B. Suche, Verlinkung, Export in TEI-Format). Zum anderen werden die Systemarchitektur und die für deren Umsetzung verwendeten Technologien thematisiert. Die 
                    VedaWeb-Plattform wird als Webanwendung auf Basis des 
                    Spring-Frameworks umgesetzt (siehe 
                    https://spring.io). Durch dessen weite Verbreitung und den großen Funktionsumfang sind sowohl die langfristige Wartbarkeit als auch zahlreiche Möglichkeiten zur späteren Erweiterung der Plattform gewährleistet. Für die Umsetzung der Suchlogik wurde zunächst der Einsatz von etablierten, auf 
                    Lucene (siehe 
                    http://lucene.apache.org) aufbauenden Such-Servern wie 
                    Solr oder 
                    Elasticsearch geprüft, jedoch wieder verworfen, da ihre Stärken v.a. im Durchsuchen sehr umfangreicher, dabei aber verhältnismäßig einfach strukturierter Textsammlungen liegen. Die 
                    VedaWeb-Suche hingegen soll mittels komplexer, kombinierbarer Suchkriterien über die reine Volltextsuche hinaus auch linguistisch motivierte Suchanfragen auf Grundlage der verschiedenen Annotationsebenen zulassen. Anstelle der genannten Such-Server wird deshalb eine eigene, auf den gegebenen Anwendungsfall zugeschnittene Suchlogik direkt mit 
                    Lucene implementiert. Das Frontend wird unter Verwendung einer Template Engine (
                    Thymeleaf, siehe 
                    http://www.thymeleaf.org) und Client-seitigen JavaScript-Technologien umgesetzt, um eine möglichst flexible und effiziente Arbeitsweise zu ermöglichen. Mit der Fokussierung auf den Systementwurf möchten wir mit dem Poster vor allem einen kompakten Überblick über die Nutzungsmöglichkeiten sowie über die technische Funktionsweise der VedaWeb-Plattform geben.
                
         
      
      
         
            
               Bibliographie
               
                  Geldner, Karl Friedrich (2003) [1951-57]: 
                        Der Rig-Veda. Aus dem Sanskrit ins Deutsche übersetzt und mit einem laufenden Kommentar versehen von Karl Friedrich Geldner. Cambridge (Mass.): Harvard University Press.
                    
               
                  Grassmann, Hermann (1873): 
                        Wörterbuch zum Rig-veda. Wiesbaden, O. Harrassowitz.
                    
               
                  Grassmann, Hermann (1876): 
                        Rig-veda. Übersetzt und mit kritischen und erläuternden Anmerkungen versehen von Hermann Grassmann. Leipzig: F.A. Brockhaus.
                    
               
                  Griffith, Ralph T. H. (1896): 
                        The Hymns of the Rigveda. Benares: Lazarus.
                    
               
                  Keydana, Götz / Luraghi, Silvia (2012): 
                        Definite referential null objects in Vedic Sanskrit and Ancient Greek. Acta Linguistica Hafniensia 44 (2):116–28. 
                        .
                    
               
                  Kulikov, Leonid (2012): 
                        The Vedic -ya-Presents: Passives and Intransitivity in Old Indo-Aryan. Amsterdam, Netherlands: Rodopi.
                    
               
                  Oldenberg, Hermann (1909/1912): 
                        R̥gveda. Textkritische und exegetische Noten. Berlin: Weidmann.
                    
               
                  Reinöhl, Uta (2016): 
                        Grammaticalization and the Rise of Configurationality in Indo-Aryan. Oxford: Oxford University Press
                    
               
                  Scarlata, Salvatore / Widmer, Paul (2015): 
                        Vedische exozentrische Komposita mit drei Relationen. Indo-Iranian Journal, 58(1):26-47.
                    
               
                  Widmer, Paul (2007): 
                        Der altindische vrkī́-Typus und hethitisch nakki-: Der indogermanische Instrumental zwischen Syntax und Morphologie. Zeitschrift für Sprachwissenschaft, (1-2):190-208.
                    
            
         
      
   



      
         Historische Korpora (Gippert und Gehrke 2015; Claridge 2008; Rissanen 2008) dienen in vielen geisteswissenschaftlichen Disziplinen als Analysegrundlage und können sehr unterschiedlich aufbereitet sein. Mit korpusbasierten Studien können qualitative und quantitative Analysen, die für die Überprüfung von Hypothesen über ein bestimmtes Phänomen notwendig sind, durchgeführt werden. Dem gegenüber steht methodisch die korpusgetriebene Studie, die das Korpus selbst nutzt, um Hypothesen über ein Phänomen zu generieren (vgl. McEnery und Hardie 2012; Lüdeling und Zeldes 2007). Neben diesen zwei Studientypen können mit Hilfe von Korpora auch einzelne Belege und Kontexte für die Beantwortung verschiedenster Forschungsfragen ermittelt werden. 
         Eine andere methodische Unterscheidung wird mit dem 
                close reading und dem 
                distant reading gemacht (vgl. Moretti 2016; Federico 2015; Gooding et al. 2013; Simanowski 2011). Wobei 
                close reading hermeneutische, nicht zwingend digitale Methoden umfassen und eine methodische Nähe zu den korpusinformierten Belegstudien sowie zu qualitativen korpusbasierten Studien aufweisen kann. 
                Distant reading ist hingegen vergleichbar mit überwiegend quantitativen, korpusgetriebenen Studien. Ein zusätzlicher Aspekt dieser Methoden ist auch die Visualisierung der Daten für 
                distant reading oder des Textes für
                 close reading. Verschiedene Visualisierungen der Annotationen werden für die korpusbasierten, -getriebenen und -informierten Studien eingesetzt und können so verschiedene Analysen unterstützen oder auch erst ermöglichen.
            
         In den digitalen Geisteswissenschaften müssen daher für die jeweiligen Methoden und Forschungsdaten Analyse- und Visualisierungswerkzeuge entwickelt werden, die es den Forscherinnen und Forschern ermöglichen, für ihren jeweiligen Forschungskontext aus einem breiten methodischen Spektrum wählen zu können (vgl. für einen Überblick z.B. Kupietz und Geyken 2016). Ein solches Werkzeug ist ANNIS (Krause und Zeldes 2016), das Such- und Visualisierungstool für Annotationen, das wir in unserem Workshop den Forscherinnen und Forschern aus den Digital Humanities vorstellen möchten. ANNIS erlaubt das Durchsuchen von Korpora, die unterschiedliche Arten von Annotationen, die möglicherweise durch unterschiedliche Forschergruppen unter verschiedenen Gesichtspunkten annotiert worden, in einem Korpus vereinen. Diese Flexibilität erlaubt es, annotierte Phänomene in der Suche zu kombinieren und damit komplexere Strukturen zu finden.
         Neben der Unterstützung der vielfältigen Analysemethoden ist eine weitere Herausforderung für die Analysewerkzeuge, dass historische Korpora je nach Forschungskontext und -frage unterschiedlich erstellt und aufbereitet werden (Lüdeling 2011). Dies zeigt sich unter anderen in den vielfältigen Transkriptions- und Normalisierungsverfahren (vgl. z.B. Odebrecht et al. 2016; Krasselt et al. 2015; Archer et al. 2015; Bollmann et al. 2012; Jurish 2010) und Annotationsguidelines (für z.B. Annotation von Wortarten für historisches Deutsch Coniglio et al. 2016; Dipper et al. 2013) sowie verschiedenen Formaten (z.B. Romary et al. 2015; Schmidt und Wörner 2009; Burnard und Baumann 2008; Wittenburg et al. 2006; Dipper 2005), die allein für die Erstellung von historischen Korpora eingesetzt werden. 
         Damit historische Korpora mit verschiedenen Methoden analysiert werden können, muss deren Wiederverwendung ermöglicht werden. Die Wiederverwendung von historischen Korpora wird durch u.a. deren freie Veröffentlichung und umfassende Dokumentation möglich (Odebrecht 2014; Borgmann 2012; Büttner et al. 2011). Weiterhin erhöht eine Wiederverwendung ihre Sichtbarkeit und stellt eine Chance zur engeren Vernetzung und Zusammenarbeit in den digitalen Geisteswissenschaften dar. So können auch historische Korpora in unterschiedlichen Wiederverwendungsszenarien gedacht werden (vgl. Simons und Bird 2008) und als empirische Grundlage für die verschiedenen Analysemethoden dienen. 
         Dieser Workshop möchte ausgehend von diesen Themenkomplex mit den Teilnehmerinnen und Teilnehmern folgende Fragen diskutieren: Wie können Analysewerkzeuge den Forscherinnen und Forschern vielfältige Analysemethoden und Visualisierungsmethoden für verschiedene historische Korpora ermöglichen? Wie kann ANNIS die verschiedenen Analysemethoden bislang unterstützen? Wie kann es gelingen, auch die Vielfältigkeit der Forschungsdaten als solche zu berücksichtigen und deren Wiederverwendung zu ermöglichen? Wie können Werkzeuge spezifisch genug entwickelt werden, um genaue und für den Forschungskontext und die Forschungsdaten angepasste Analysen zu ermöglichen? 
         Der Workshop hat das Ziel, anhand mehrerer historischer Korpora des Deutschen das generische Such- und Visualisierungstool ANNIS (Krause und Zeldes 2016) für den Einsatz in den Digital Humanities zu diskutieren und anzuwenden, da es bislang überwiegend für korpusbasierte und korpusgetriebene Studien sowie für das Auffinden von sprachlichen Belegen eingesetzt wird. 
         ANNIS wird seit 2009 als ein generisches webbasiertes Such- und Visualisierungstool für verschiedene Korpustypen und Annotationskonzepte in verschiedenen Kooperationen mit der Humboldt-Universität zu Berlin und der Georgetown University und in mehreren Projekten entwickelt. Der Quellcode von ANNIS ist frei zugänglich veröffentlicht und bietet gleichzeitig eine Desktop- sowie Server-Installation. In ANNIS können Korpora mit Token-, Spannen-, Baum- und Pointingannotationen unabhängig von den einzelnen, jeweils korpusspezifischen Annotationsguidelines in ANNIS analysiert werden. ANNIS bietet weiterhin den Korpuserstellerinnen und -erstellern annotations- oder fachspezifische Visualisierungen für Korpora. Mit einer wiederum generischen und mächtigen Anfragesprache (ANNIS Query Language – AQL) können alle Korpora in ANNIS nach Annotationen und Kombinationen von Annotationen durchsucht werden. Weiterhin können die Suchergebnisse für bspw. weitere statistische Auswertungen exportiert werden. Jedes Korpus, jede Suchanfrage und jeder Beleg kann über einen permanenten Link stabil referenziert werden. Mit dem Konverterframework Pepper (Zipser und Romary 2010) werden Korpora, die in verschiedenen Formaten vorliegen können, in das ANNIS-Format überführt.
         Repositorien wie das LAUDATIO-Repository (Odebrecht et al. 2015) ermöglichen einen Open Access Zugang zu verschiedensten historischen Korpora und stellen eine umfassende Korpusdokumentation (Odebrecht 2014) zur Verfügung, die eine Erschließung dieser heterogenen Datengrundlage unabhängig von den Korpuserstellerinnen und -erstellern ermöglicht. Damit wird eine Voraussetzung für die Wiederverwendung der historischen Korpora erfüllt. Für den Workshop werden aus LAUDATIO beispielhaft die Korpora „Referenzkorpus Altdeutsch“ (Donhauser 2015) und „RIDGES Herbology Korpus“ (Odebrecht et al. 2016) verwendet.
         Das Referenzkorpus Altdeutsch ist ein historisches Mehrebenenkorpus der ganzen Sprachperiode des Althochdeutschen mit ca. 650.000 Wörtern (von den ersten Überlieferungen bis Mitte des 11. Jahrhunderts). Als Grundlage für die diplomatischen Transkription sind Editionen der jeweiligen Handschriften, die mit weiteren Annotationen zur Textstruktur sowie mit komplexen Wortartenannotation (Dipper et al. 2013), Annotation zu Flexionsklassen und Lemmatisierung versehen sind. Das RIDGES Korpus ist ein tief annotiertes Korpus mit Auszügen aus gedruckten Kräuterbüchern aus der Zeit zwischen 1487 und 1910, anhand derer die Entwicklung der deutschen Wissenschaftssprache auf vielen Ebenen untersucht wird. Die Drucke sind diplomatisch transkribiert (wo möglich, nach vorher digitalisierten oder durch OCR-Verfahren erstellte Vorlagen, vgl. Springmann und Lüdeling 2017). Die Daten sind mehrfach normalisiert und auf vielen Ebenen annotiert (unter anderem mit Wortart, Lemma, Informationen zu Kompositionstypen (Perlitz 2014), Dependenzsyntax, Informationen zur graphischen Struktur nach den TEI Guidelines). Dabei werden automatische und manuelle Annotationsverfahren und Prüfverfahren genutzt.
         Um die eingangs formulierten Fragen adressieren zu können, wird der Workshop zwei Schwerpunkte enthalten. Der erste Schwerpunkt wird die Einführung in die Funktionen und Suchanfragesprache von ANNIS sowie die damit verbundene Vorstellung der zwei historischen Beispielkorpora umfassen. Wir wollen den Teilnehmerinnen und Teilnehmern die verschiedenen Analyse- und Visualisierungsmöglichkeiten online und hands-on vorstellen. Über die Vorstellung zweier historischer Korpora mit dem generischen ANNIS können bereits die Herausforderungen der heterogenen Datengrundlage in den digitalen Geisteswissenschaften für Analysetools herausgearbeitet werden.
         Der zweite Schwerpunkt soll Raum für eine Diskussion mit den Teilnehmerinnen und Teilnehmern sowie auch die Möglichkeit geben, weitere Korpora in ANNIS – geleitet von den Forschungsinteressen der Teilnehmerinnen und Teilnehmern – zu durchsuchen. Mit diesem Workshop wollen wir uns gemeinsam mit den Teilnehmerinnen und Teilnehmern kritisch mit den Anforderungen an ein Analysetool für verschiedene Methoden zur Analyse und Visualisierung von historischen Korpora auseinandersetzen und prüfen, in wie weit ANNIS bereits einige dieser Anforderungen erfüllen kann. So wollen wir ANNIS in einem neuen Forschungskontext der Digital Humanities diskutieren und dabei neue Nutzerszenarien für die weitere Entwicklung erarbeiten. 
         Zeitplan:
            
            1,5 Stunden Online Hands-on-Einführung in das Such- und Visualisierungstool, der Anfragesprache mit dem Referenzkorpus Altdeutsch und RIDGES Korpus
            1,5 Stunden Teilnehmergeleitete Anfragen, weitere Korpora und Diskussion der Anforderungen
         
         Technische Anforderungen:
            
            Für den Workshop werden ein Raum mit Beamer und Zugang zu eduroam, ggf. einzelne WLAN-Zugänge für die Teilnehmerinnen und Teilnehmer benötigt.
            Alle Teilnehmerinnen und Teilnehmer benötigen eigenes Notebook.
         
         Teilnehmeranzahl: 
            
            max. 30
         
      
      
         
            
               Bibliographie
               
                  Moretti, Franco (2016): 
                        Distant Reading. Konstanz: Konstanz University Press.
                    
               
                  Romary, Laurent / Zeldes, Amir / Zipser, Florian (2015): ". Serialising the ISO SynAF syntactic object model", in: 
                        Language Resources and Evaluation 49 (1), S. 1–18. 10.1007/s10579-014-9288-x.
                    
               
                  Archer, Dawn / Kytö, Merja / Baron, Alistair / Rayson, Paul (2015): "Guidelines for normalising Early Modern English corpora. Decisions and justifications", in: 
                        ICAME Journal (39), 5–24.
                    
               
                  Bollmann, Marcel / Dipper, Stefanie / Krasselt, Julia / Petran, Florian (2012): "Manual and semi-automatic normalization of historical spelling. Case studies from Early New High German", in: 
                        Proceedings of KONVENS 2012 342–350. http://www.oegai.at/konvens2012/proceedings/51_bollmann12w/ [letzter Zugriff am 24.08.2016].
                    
               
                  Borgmann, Christine L. (2012): "The conundrum of sharing research data", in: 
                        Journal of the American Society for Information Science and Technology 63 (6), 1059–1087. 10.2139/ssrn.1869155.
                    
               
                  Burnard, Lou / Baumann, Sid (eds.) (2008): 
                        TEI P5. Guidelines for Electronic Text Encoding and Interchange. Oxford. http://www.tei-c.org/Guidelines/P5/ [zuletzt geprüft am 11.11.2015].
                    
               
                  Büttner, Stephan / Hobohm, Hans-Christoph / Müller, Lars (2011): "Research Data Management", in: Büttner, Stephan / Hobohm, Hans-Christoph / Müller, Lars (eds.): 
                        Handbuch Forschungsdatenmanagement. Bad Honnef: Bock + Herchen, 13–23.
                    
               
                  Claridge, Claudia (2008): "Historical Corpora", in: Lüdeling, Anke / Kytö, Merja (eds.): 
                        Corpus Linguistics. An International Handbook, Bd. 1. 2. Berlin: De Gruyter (1), 242–259.
                    
               
                  Coniglio, Marco / Donhauser, Karin / Schlachter, Eva / Rasskazova, Oxana / Odebrecht, Carolin / Wirth, Matthias / Miltenberger, Anke (2016): 
                        Historisches Predigtenkorpus zum Nachfeld (HIPKON Version 1.0). Technischer Bericht, Humboldt-Universität zu Berlin. 10.18452/13681.
                    
               
                  Dipper, Stefanie (2005): "XML-based Stand-off Representation and Exploitation of Multi-Level Linguistic Annotation", in: 
                        Proceedings of Berliner XML Tage 2005 (BXML 2005). Berlin, 39–50.
                    
               
                  Dipper, Stefanie / Donhauser, Karin / Klein, Thomas / Linde, Sonja / Müller, Stefan / Wegera, Klaus-Peter (2013): "HiTS. Ein Tagset für historische Sprachstufen des Deutschen“, in: Zinsmeister, Heike / Heid, Ulrich / Beck, Kathrin (eds.): Das Stuttgart-Tübingen Wortarten-Tagset. Stand und Perspektiven", in: 
                        Journal for Language Technology and Computational Linguistics, 28(1), 85–137.
                    
               
                  Donhauser, Karin (2015): "Das Referenzkorpus Altdeutsch. Das Konzept, die Realisierung und die neuen Möglichkeiten", in: Gippert, Jost / Gehrke, Ralf (eds.): 
                        Historical Corpora. Tübingen: Narr (Korpuslinguistik und interdisziplinäre Perspektiven auf Sprache, 5), 35–49.
                    
               
                  Federico, Annette (2015): 
                        Engagements with Literature. Engagements with Close Reading. Florence: Routledge.
                    
               
                  Gippert, Jost / Gehrke, Ralf (eds.) (2015): 
                        Historical Corpora. Tübingen: Narr (Korpuslinguistik und interdisziplinäre Perspektiven auf Sprache, 5).
                    
               
                  Gooding, Paul / Terras, Melissa / Warwick, Claire (2013): “The myth of the new. Mass digitization, distant reading, and the future of the book”, in: 
                        Literary and Linguistic Computing 28 (4), 629–639. 10.1093/llc/fqt051.
                    
               
                  Jurish, Bryan (2010): “More than Words: Using Token Context to Improve Canonicalization of Historical German”, in: 
                        Journal for Language Technology and Computational Linguistics 25 (1), 23–40.
                    
               
                  Krasselt, Julia / Bollmann, Marcel / Dipper, Stefanie / Petran, Florian (2015): 
                        Guidelines für die Normalisierung historischer deutscher Texte. Bochumer Linguistische Arbeitsberichte, 15. urn:nbn:de:hebis:30:3-419680.
                    
               
                  Krause, Thomas / Zeldes, Amir (2016): ANNIS3. “A new architecture for generic corpus query and visualization”, in: 
                        Digital Scholarship in the Humanities 31 (1), 118–139. 10.1093/llc/fqu057.
                    
               
                  Kupietz, Marc / Geyken, Alexander (eds.) (2016): “Corpus Linguistic Software Tools”, in: 
                        Journal for Language Technology and Computational Linguistics 31(1).
                    
               
                  Lüdeling, Anke (2011): “Corpora in Linguistics. Sampling and Annotation”, in: Grandin, Karl (ed.): 
                        Going Digital. Evolutionary and Revolutionary Aspects of Digitization. New York: Science History Publications (Nobel Symposium, 147), 220–243.
                    
               
                  Lüdeling, Anke / Zeldes, Amir (2007): “Three Views on Corpora. Corpus Linguistics, Literary Computing, and Computational Linguistics”, in: 
                        Jahrbuch für Computerphilologie (9), 149–178.
                    
               
                  McEnery, Tony / Hardie, Andrew (2012): 
                        Corpus Linguistics. Method, Theory and Practice. Cambride [u.a.]: Cambridge University Press (Cambridge Textbooks in Linguistics).
                    
               
                  Odebrecht, Carolin (2014): "Modeling Linguistic Research Data for a Repository for Historical Corpora", in: 
                        Digital Humanities Conference Abstracts. Lausanne 284–285.
                    
               
                  Odebrecht, Carolin / Belz, Malte / Zeldes, Amir / Lüdeling, Anke / Krause, Thomas (2017): „RIDGES Herbology. Designing a Diachronic Multi-Layer Corpus”, in: 
                        Language Resources and Evaluation 51 (2) First Online 2016, 695-725
                        . 10.1007/s10579-016-9374-3.
                    
               
                  Odebrecht, Carolin / Krause, Thomas / Lüdeling, Anke (2015): "Austausch von historischen Texten verschiedener Sprachen über das LAUDATIO-Repository", 
                        37. Jahrestagung der Deutschen Gesellschaft für Sprachwissenschaft, DGfS-CL Poster Session, Leipzig. http://conference.uni-leipzig.de/dgfs2015/fileadmin/zusatzdokumente/dgfs-tagung-2015-final.pdf [letzter Zugriff am 17.08.2017].
                    
               
                  Perlitz, Laura (2014): 
                        Konkurrenz zwischen Wortbildung und Syntax. Historische Entwicklung von Benennung. Bachelorarbeit. Humboldt-Universität zu Berlin, Berlin. 10.18452/14232
                    
               
                  Rissanen, Matti (2008): "Corpus Linguistics and Historical Linguistics", in: Lüdeling, Anke / Kytö, Merja (eds.): 
                        Corpus Linguistics. An International Handbook. 2 Bände. Berlin: De Gruyter (1), 53–68.
                    
               
                  Schmidt, Thomas / Wörner, Kai (2009): "EXMARaLDA. Creating, analysing and sharing spoken language corpora for pragmatic research", in: 
                        Pragmatics 19 (4), 565–582.
                    
               
                  Simanowski, Roberto (2011): 
                        Digital Art and Meaning. Reading Kinetic Poetry, Text Machines, Mapping Art, and Interactive Installations (Electronic Mediations). Minnesota: University of Minnesota Press.
                    
               
                  Simons, Gary / Bird, Steven (2008): "Toward a global infrastructure for the sustainability of language resources", in: 
                        Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation. Cebu City,  87–100.
                    
               
                  Springmann, Uwe / Lüdeling, Anke (2017): "OCR of historical printings with an application to building diachronic corpora. A case study using the RIDGES herbal corpus", in: 
                        Digital Humanities Quarterly 11 (2). http://www.digitalhumanities.org/dhq/vol/11/2/000288/000288.html [letzter Zugriff am 12.09.2017].
                    
               
                  Wittenburg, Peter / Brugmann, Hennie / Russel, Albert / Klassmann, Alex / Sloetjes, Han (2006): "ELAN. A Professional Framework for Multimodality Research", in: 
                        Proceeedings of LREC. Language Resources and Evaluation Conference. Genoa 1556–1559. http://www.lrec-conf.org/proceedings/lrec2006/ [letzter Zugriff am 23.12.2016].
                    
               
                  Zipser, Florian / Romary, Laurent (2010): "A model oriented approach to the mapping of annotation formats using standards", in: 
                        Proceedings of the Workshop on Language Resource and Language Technology Standards, LREC 2010. http://hal.archives-ouvertes.fr/inria-00527799/en/ [letzter Zugriff am 12.11.2014].
                    
            
         
      
   



      
         
            Einleitung
            Das interdisziplinäre Forschungsprojekt 
                    Digital Plato
                untersucht die Rezeption und Nachwirkung des platonischen Werkes in der griechischen Literatur bis in die Spätantike mit einem Fokus auf nicht-wörtlichen Referenzen. Der folgende Beitrag und das dazugehörige Poster geben einen Überblick über die wichtigsten Zwischenergebnisse, die während der zurückliegenden ersten Hälfte der Projektlaufzeit erzielt wurden. Neben der Organisation des Korpus sowie dessen linguistischer Anreicherung, dem Aufbau eines Wortnetzes und die Einbettung in einen Vektorraum, zählen dazu die Erfassung und Kategorisierung bekannter Referenzstellen, die Adaption der CTS zur wortgenauen Referenzierung, die theoretische Annäherung und schließlich die semi-automatische Suche nach neuen Paraphrasen.
                
         
         
            Teilbereiche
            
               Textgrundlage
               Die Textgrundlage des Projekts ist der Thesaurus Linguae Graecae (TLG), der zunächst in ein XML-Format nach TEI-Standard überführtwurde. Da das Textkorpus als statisch anzusehen ist, wir im Laufe des Projektes aber mehrfach zusätzliche Annotationen hinterlegen und aktualisieren möchten, haben wir in einem zweiten Schritt Text und Annotationen nach dem Single-Source Prinzip voneinander getrennt. Dazu wurde der eigentliche Inhalt des Werks als unveränderliche Quelle (Single-Source) in Form einer einfachen fortlaufenden Textdatei angelegt, auf welche wiederum Dateien mit zusätzlichen Informationen (Standoff-Markup) referenzieren.Eine Referenz gibt dabei an, am wievielten Zeichen der Single-Source die Annotation startet und endet. Auch Texthervorhebungen und strukturelle Auszeichnungen des ursprünglichen TLG-Formats wurden so erfasst.
               Für jede Form der Annotation wird eine eigene Datei mit Standoff-Markup angelegt, sodass eine Bearbeitung dieser keinen Einfluss auf die übrigen Auszeichnungen hat und sogar überlappende Annotationen ermöglicht werden.
            
            
               Annotationen
               Der TLG enthält hauptsächlich strukturelle Annotationen. Über eine Kombination verschiedener bestehender linguistischer Werkzeuge, wie Morpheus (Crane 1991) und Mate Tagger (Bohnet und Nivre 2012), werden dem Korpus nachträglich Lemmata und morphologische Informationen in Form von Standoff-Markup hinzugefügt. Ferner sollen auf dieser Basis auch die Nominalphrasen automatisch erkannt und ausgezeichnet werden. Solche Angaben helfen bei der Suche nach für das Projekt relevanten Textstellen.
            
            
               Helleninet
               Über die Verknüpfung diverser Wörterbücher wurde im Rahmen des Projekts ein Wortnetz für das Altgriechischegeneriert. Die Helleninet getaufte Struktur stellt ein weiteres wichtiges Standbein für die Einordnung der Relationen zwischen Wörtern und Textstellen dar.
            
            
               Worteinbettung
               Die Wörter eines Korpus können mit Hilfe statistischer Verfahren in einen Vektorraum eingebettet werden, sodass dieser semantische Beziehungen zwischen den Wörtern abbildet. Vorteilhaft hierbei ist, dass lediglich ein hinreichend großes Korpus benötigt wird, da das Verfahren auf den Kontexten der Wörter und nicht auf Vorwissen zur Sprache aufbaut. Das genutzte Verfahren word2vec (Mikolov et al. 2013) erlaubte uns dank seiner Performanz die Durchführung einer umfangreichen Evaluation, um eine für das Projekt möglichst optimale Einbettung zu finden.
            
            
               Semi-automatische Rezeptionserkennung
               Um weitere Referenzen auf Platon im Korpus aufzuspüren, verfolgen wir verschiedene (semi-)automatische Ansätze, die über die aus der Literatur bekannten Ansätze hinausgehen. Beim auf der DHd 2017 vorgestellten 'Rütteln' (Kath et al. 2017) handelt es sich um ein exploratives Verfahren, bei dem interaktiv von einer Textstelle Platons ausgehend einzelne Worte mit sinnverwandten Wörtern (bspw. Synonyme oder Übersetzungen) ersetzt werden und anschießend nach der modifizierten Textstelle im Korpus gesucht wird.
               Ein ähnliches, aber systematisches Vorgehen stellt die n-Gramm-Suche dar. Nach einer umfangreichen Normalisierungwerden die n-Gramme verschiedener Längen für das gesamte Korpus indiziert. Anschließend können alle übereinstimmenden n-Gramme effizient ermittelt werden. 
               Ein drittes Verfahren basiert auf der Word Mover's Distance (Kusner et al. 2015), einem Distanzmaß für zwei Wortgruppen auf Grundlage einer Worteinbettung. Ausgehend von einer Textstelle wird das Korpus hierbei nach Textstellen mit möglichst geringer Distanz durchsucht(Pöckelmann et al. 2017).Die systematische Evaluation an Hand des im Projekt erstellten Goldstandard zeigt, dass dieses Verfahren zu sehr guten Ergebnissen führt.
            
            
               Referenzierungssystem
               Zur wortgenauen Referenzierung von Textstellen im Korpus wurden CTS-URNs adaptiert, d.h. 
                        Uniform Resource Names nach der Notation der 
                        Canonical Text Services (Blackwell und Smith 2014). Im Unterschied zum Standard werden die Wörter einer Zeile ebenfalls durchnummeriert, sodass in einer Subreferenz nicht das Wort selbst, sondern dessen Position genutzt werden kann. Um Probleme mit durch Zeilenumbruch getrennten Wörtern zu vermeiden, werden beide Teile in ihrer jeweiligen Zeile mitgezählt. Ein entsprechender Konverter bildet die CTS-URN auf Positionen in den Single-Sources sowie umgekehrt ab.
                    
            
            
               Goldstandard und Referenzannotierer
               Mit Hilfe des im Projekt entwickelten, graphischen Werkzeugs ‑ des Referenzannotierers ‑ wurde eine zuvor erstellte Sammlung bekannter Rezeptionen mit Annotationen verschiedener Kategorien versehen. Der Goldstandard erlaubt durch diese umfassende Kategorisierung eine statistische Auswertung und hilft bei der Begriffsbildung. Zudem bildet er die Grundlage zur systematischen Evaluation der automatischen Suchverfahren und für einen umfassenden Thesaurus.
            
            
               Begriffsbildung
               Das Projekt arbeitet an einer theoretischen Ausdifferenzierung des Paraphrasenbegriffs: als konstitutiv werden hierbei Ähnlichkeiten von Textstellen zueinander angesehen, die sich auf der Wortebene abbilden. Hierbei nehmen wir wie anderen Ansätze die Notwendigkeit eines ‘Dritten’ an, um die Relation von Texten zueinander zu charakterisieren, nur verorten wir dies weniger stark im Bereich der Semantik, der schwer operationalisierbar ist. Vielmehr zielen wir auf eine fruchtbare Synthese dieser Theorie, die Paraphrasen ohne Annahme von Autorenintentionen oder der Bestimmung von Abhängigkeitsverhältnissen zwischen Texten beschreibbar macht, mit bestehenden Ansätzen zur Bestimmung von Ähnlichkeit zwischen Texten aus den DH.
            
         
      
      
         
            Gefördert durch die VolkswagenStiftung. Weitere Informationen auf der Projektseite unter: 
                            
            
         
         
            
               Bibliographie
               
                  Blackwell, Christopher / Smith, Neel (2014): "The Canonical Text Service (CTS)" 
                         [letzter Zugriff 18. September 2017].
                    
               
                  Bohnet, Bernd / Nivre, Joakim (2012): "A Transition-Based System for Joint Part-of-Speech Tagging and Labeled Non-Projective Dependency Parsing" in: 
                        Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning: 1455-1465.
                    
               
                  Crane, Gregory (1991): "Generating and parsing classical greek" in: 
                        Literary and Linguistic Computing 6(4):243–245.
                    
               
                  Kath, Roxana / Keilholz, Franz / Klinker, Fabian / Pöckelmann, Marcus / Rücker, Michaela / Švitek, Mihael / Wöckener-Gade, Eva / Yu, Xiaozhou (2017): "Paraphrasenerkennung im Projekt Digital Plato" in: 
                        Tagungsband der 4. Jahrestagung der Digital Humanities im deutschsprachigen Raum: 266-270.
                    
               
                  Kusner, Matt J. / Sun, Yu / Kolkin, Nicholas I. / Weinberger, Kilian Q. (2015): "From Word Embeddings To Document Distances" in: 
                        Proceedings of the 32. International Conference on Machine Learning: 957–966.
                    
               
                  Mikolov, Tomas / Sutskever, Ilya / Chen, Kai / Corrado, Greg S. / Dean, Jeff (2013): "Distributed representations of words and phrases and their compositionality" in: 
                        Advances in Neural Information Processing Systems 26: 3111–3119.
                    
               
                  Pöckelmann, Marcus / Ritter, Jörg / Wöckener-Gade, Eva / Schubert, Charlotte (2017): "Paraphrasensuche mittels word2vec und der Word Mover’s Distance im Altgriechischen"in: Digital Classics Online, Band 3, Ausgabe 3, S. 24-36.
            
         
      
   



      
         Daten werden laut der Vision der ‘High Level Expert Group on Scientiﬁc Data’ in der Zukunft einen Grad an Ausdrucksstärke und Formen der Selbstbeschreibung erhalten, dass sie in die Lage versetzt werden, ihre eigene Infrastruktur zu stellen (Neuroth, Heike / et al. 2012). Auch die Idee des Semantic Web verspricht eine Zukunft in der Maschinen selbständig mit Daten agieren können (Berners-Lee, Tim 2000). Die praktische Realität - gerade für die digitalen Geisteswissenschaften - ist noch eine Andere. Dennoch steckt in den Methoden des Semantic Webs ein Potenzial, das es mit vernünftiger Kritik zu nutzen gilt. 
                Das Projekt MEDEA (Modeling semantically Enriched Digital Edition of Accounts) versucht dies zu verwirklichen, indem an einem kollektiven Standard zu semantischen Anreicherung digitaler Editionen von historischen Rechnungsbüchern gearbeitet wird. Es wird der Frage nachgegangen, inwieweit Methoden des Semantic Webs bei der Erschließung, Analyse und Darstellung historischer Rechnungsbücher helfen können (MEDEA, 2017).
                Ein zentrales Anliegen jedes wissenschaftlichen Projektes ist es, über qualitative und konsistente Daten zu verfügen. Dateninkonsistenz und mangelnde Datenqualität bergen die Gefahr falscher wissenschaftlicher Interpretationen und kritischer Fehlerquellen für die technische Verarbeitung der Daten. Da unterschiedliche TEI-Kodierungen von unikalen Quellen zusammengeführt werden, ist es eine besondere Herausforderung im MEDEA Projekt, Workflows zu etablieren, die dieser Herausforderung begegnen. Die Entwicklung einer domänenspezifischen Ontologie zur Formalisierung von historischen Prozessen des Rechnungswesens kann als eine solche potenzielle Lösung betrachtet werden. Die (Bookkeeping Ontologie 2017) formalisiert in ihrem jetzigen Zeitpunkt eine grundlegende Wissensstruktur, um Einträge in Rechnungsbüchern, ihre Transaktionen von Gütern, Dienstleistungen oder Geldbeträgen von einem Akteur oder Konto zu einem anderen, standardisiert beschreiben zu können.
                Aus jeder Transaktion, die mittels des Attributes @ana in einem TEI kodierten Text annotiert wurde, wird ein XML/RDF Datensatz erzeugt, der auf Konzepte der in OWL serialisierten Bookkeeping-Ontologie referenziert (Vogeler 2016). Der Ontologie Editor Protégé erlaubt es, eine Ontologie und die darin enthaltenen Daten (Individuals) einem Reasoning - dem Abarbeiten alle Vorhanden Regeln in einer Ontologie auf Basis der Description Logic - zu unterziehen (Musen 2015). Das Reasoning gilt als ein essentieller Baustein im Design, der Entwicklung, der Wartung und in der praktischen Anwendung einer Ontologie. Das Ergebnis davon sind Inferenzen. Inferenzen sind neu hergeleitete Schlussfolgerungen auf Basis des Reasoning Prozesses (Dentler / et al. 2011). Die Überprüfung strukturierter Daten mittels logischen Schlussfolgerungen kann dazu dienen, größere Datenmengen auf ihre Konsistenz und somit auch auf ihre Qualität hin zu prüfen, da logische Inkonsistenzen als Fehlermeldung angezeigt werden. Die Überprüfung und Zusammenführung der TEI-Kodierungen wird im MEDEA Projekt auf Basis dieser Ontologie durchgeführt. Use Cases für die Bookkeeping-Ontologie im Projekt umfassen:
            
         
            Formalisierung und Systematisierung von Rechnungsbüchern in einer maschinenverständlichen Wissensbasis
            Überprüfung der Datenkonsistenz und inhaltliche Zusammenführung der Daten
            Schaffung eines Definitionskonsenses
            Grundlage für semantische Retrieval und Discovery Strategien
            Wiederverwendbarkeit und Erweiterbarkeit
            Interoperables und offenes, sowie transparentes Verteilen von Daten
         
         
            
            Dieser Zugang kann mit der Arbeit von (Steffen, Henniche / et al. 2015) verglichen werden, in der die Anwendung von Semantic Web Methoden, im Speziellen des Reasoning, auf geisteswissenschaftliche Daten angewandt wird. 
                So verlockend die Möglichkeiten einer Ontologie sein können, so kritisch sind diese auch zu betrachten. Ein grundlegendes Problem ist bereits durch den Widerspruch der hermeneutischen Arbeit der Historikerin und der Entscheidbarkeit von OWL gegeben. Sind Ontologien in OWL ausdrucksstark genug, um geisteswissenschaftliche Daten so beschreiben zu können, dass ein logisches Schlussfolgern Ergebnisse erzielt, das die Konsistenz und die Qualität der Daten abbildet?
            
      
      
         
            
               Bibliographie
               
                  Berners-Lee, Tim. (2000): Weaving the Web: The Past, Present and Future of the World Wide Web by Its Inventor. London.
               
                  Bookkeeping Ontologie, 
                        http://glossa.uni-graz.at/o:medea.1951/ONTOLOGY [letzter Zugriff 21.09.2017].
                    
               
                  Dentler, Kathrin / et al. (2011): Comparison of reasoners for large ontologies in the OWL 2 EL profile. Semantic Web 2.2, 71-87.
               
                  MEDEA, 
                        https://medea.hypotheses.org [letzter Zugriff 21.09.2017].
                    
               
                  Musen, M.A. (2015): The Protégé project: A look back and a look forward. AI Matters. Association of Computing Machinery Specific Interest Group in Artificial Intelligence, DOI: 10.1145/2557001.25757003. 
               
                  Neuroth, Heike / et al. (2012): Langzeitarchivierung von Forschungsdaten. Eine Bestandsaufnahme. Hülsbusch.
               
                  Steffen, Henniche / et al. (2015): Reasoning with Reasoning. Using Faceted Browsers to Find Meaning in Linked Data. Berlin, 1-61, 
                        https://lirias.kuleuven.be/handle/123456789/485851
               
               
                  Vogeler, Georg (2016): The Content of Accounts and Registers in their Digital Edition. XML/TEI, Spreadsheets, and Semantic Web Technologies, in: SARNOWSKY, Jürgen (Hg.): Konzeptionelle Überlegungen zur Edition von Rechnungen und Amtsbüchern des späten Mittelalters. Göttingen, 13-41.
            
         
      
   



      
         Die digitale Erschließung historischer Zeitungen lag vor wenigen Jahrzehnten noch außerhalb des Vorstellbaren. Inzwischen ist die Zeitung als Forschungsgegenstand etabliert und damit für viele verschiedene Disziplinen ins Zentrum gerückt: NutzerInnen sehen Nachrichtenblätter nicht nur als Quelle, die punktuell und komplementär zu anderen Texten befragt wird, sondern haben auch ein besonderes Interesse an der diachronen Entwicklung von historischen Zeitungen und ihren Themen, von Textsorten und natürlich von Sprache ganz im Allgemeinen. Die Transformation historischer Zeitungen in ein digitales Format, die von Bibliotheken, Archiven und Forschungseinrichtungen in den letzten Jahren stark vorangetrieben worden ist, befördert Fragestellungen dieser Art und überlässt den geistes- und sozialwissenschaftlichen Disziplinen einen reichen Schatz an Daten. 
         Im Zuge des wachsenden Interesses an historischem Zeitungsmaterial in digitaler Form haben Bibliotheken eigene Portale mit Bild-Digitalisaten und Kalenderübersichten eröffnet (vgl. etwa die Staats- und Universitätsbibliothek Bremen
                , die Staatsbibliothek zu Berlin
                , die Universität Bonn
                 oder die Österreichische Nationalbibliothek
                ) oder stellen ihre Daten Europeana
                 zur Verfügung. Um historische Zeitungen besser durchsuchbar zu machen, wird vereinzelt bereits an der sorgfältigen Volltexterschließung besonderer historischer Zeitungen gearbeitet, vgl. etwa das Projekt „Volltextdigitalisierung der Staats- und Gelehrte[n] Zeitung des Hamburgischen Unpartheyischen Correspondenten und ihrer Vorläufer (1712-1848)“
                 (Schuster, Wille 2016: 7-29) oder das „Mannheimer Korpus für Historische Zeitungen und Zeitschriften in COSMAS II“
                . Internationale Vernetzungsinitiativen
                 machen einerseits auf die Herausforderungen bei der Erschließung historischer Zeitungen aufmerksam (enorme Textmenge, fehlerhafte OCR-Ergebnisse, wenig Trainingsdaten) und lassen andererseits die Entwicklung gemeinsamer Strategien und Standards bei der Aufbereitung erkennen. 
            
         Die AutorInnen des vorliegenden Beitrags gehen davon aus, dass historische Zeitungen von Institutionen zwar „digital verfügbar“ gemacht werden, aber bislang kaum an die Erkenntnisinteressen potentieller NutzerInnen angepasst sind - auch, aber nicht nur aufgrund der oben genannten Herausforderungen und des erheblichen Aufwandes, der mit der Erschließung einer historischen Zeitung verbunden ist. Bevor ein aufwändiges Projekt startet, ist daher zu überlegen, wie eine Erschließung geplant sein muss, sodass sie idealerweise für mehrere Disziplinen von Nutzen ist. Bei der Konzeption ist darauf zu achten, dass keine (für den überwiegenden Teil der antizipierten Disziplinen) relevanten Informationen vernachlässigt werden oder verloren gehen: Die Entscheidungen, die zu treffen sind, beginnen (1) bei der Auswahl der Ausgaben, setzen sich (2) bei den Transkriptionsrichtlinien fort und lassen sich (3) bis zu den Annotationskonzepten weiterführen. 
         Anhand eines laufenden Projektes, das sich der Volltextdigitalisierung ausgewählter Nummern des „Wien[n]erischen Diariums“
                 aus dem 18. Jahrhundert widmet, sollen all diese Aspekte kritisch hinterfragt und anhand von Beispielen, Erfahrungswerten und Zwischenergebnissen dargestellt werden. Das methodische Konzept, auf dem dieses konkrete Vorhaben beruht, wurde ausgehend von der Überzeugung gestaltet, dass insbesondere bei einer vielseitig nutzbaren Ressource wie dem “Wiennerischen Diarium” die einzelnen Fachdisziplinen und NutzerInnen bereits möglichst früh in den interdisziplinären Erschließungsprozess einzubeziehen sind. Im Vortrag soll darüber berichtet werden, welche Maßnahmen bereits gesetzt wurden, um die Fachwissenschaften zu vernetzen, welche Tools im Projekt entwickelt wurden, um Personen außerhalb des Kernteams zu involvieren, und durch welche digitalen Angebote diese Kollaboration ermöglicht wird und gelingen kann. 
            
         Auswahl der Ausgaben: Um ein ausgewogenes Korpus von mehreren hundert Ausgaben verteilt über das 18. Jahrhundert zu erstellen, waren sowohl ExpertInnen mehrerer geisteswissenschaftlicher Disziplinen als auch LeserInnen der heutigen Wiener Zeitung dazu aufgefordert, jene Nummer(n) online zu nominieren, die sie als besonders relevant einstufen würden. Bei der Auswertung dieses Calls hat sich erneut bestätigt, wie breit das Themenspektrum und damit die individuellen Erkenntnisinteressen sind: Nominiert und zur Volltextdigitalisierung empfohlen wurden Ausgaben mit Geburten, Taufen und Sterbefällen bekannter Persönlichkeiten, Geburts- und Namenstage, Krönungen und Erbhuldigungen, kirchliche und weltliche Feste, Ankündigungen, Eröffnungen und Einweihungen sowie die Besuche prominenter Gäste in der Residenzstadt. Errungenschaften im weitesten Sinn – wie die Erklärung der Menschenrechte oder der Beginn der Luftfahrt – waren ebenso in der Auswahl wie das medienimmanente Thema der Herausgeberschaft. Die Ergänzungen, die das Projektteam letztlich vorgenommen hat, betrafen daher weniger die Vielfalt der angesprochenen Themen, sondern waren darauf ausgerichtet, zeitliche Lücken zu füllen. Um das Wien[n]erische Diarium als Korpus in einer kontinuierlich chronologischen Jahresabfolge plausibel dokumentieren zu können, wurden die Nominierungen dahingehend komplettiert, dass sich die Umbrüche und Wendungen in einer Periode sich verändernder politischer, sozialer, wissenschaftlicher und künstlerischer Bedingungen im 18. Jahrhundert, idealerweise auch korpusgestützt nachvollziehen lassen.
         Texterstellung und -transkription: Ausgangspunkt für die Weiterverarbeitung sind jene Image-Digitalisate, welche die Österreichische Nationalbibliothek in ANNO (Austrian Newspapers Online) zur Verfügung stellt.
                 Die Erstellung des digitalen Textes für die ausgewählten Nummern erfolgt mit Transkribus
                , genauer mittels der Handwritten Text Recognition (HTR). Anhand einer kleinen Zahl an Ausgaben, die im Rahmen von projektbezogenen Vorstudien (vgl. Resch et al. 2016) bearbeitet wurden, konnte ein erstes Modell für das Diarium erstellt werden. Je nach Beschaffenheit der Digitalisate liegt die Genauigkeit hiermit zwischen 70% und 95%. Um die Qualität zu steigern, werden mehrere Tranchen von 40 bis 50 Ausgaben von zwei unterschiedlichen externen Dienstleistern nach den für dieses Projekt erstellten Transkriptionsrichtlinien erfasst, sodass das Modell weiter trainiert und die Erfassungsgenauigkeit erhöht werden kann. 
            
         
            
         
         Ein „Reporting-Tool“
                , das direkt auf die Transkribus-Plattform zugreift, dokumentiert den Bearbeitungsstatus der ausgewählten Einzelnummern, gibt Auskunft über deren Umfang (Zählung von Regionen, Zeilen und Wörtern) und informiert die Fachgemeinschaft tagesaktuell über den Fortschritt des Projekts. 
            
         Erprobung von Annotationskonzepten: Parallel dazu arbeitet das Kernteam an einer digitalen Arbeits- und Annotationsoberfläche: Für die Präsentation der Texte wird nach derzeitigem Projektstand eine auf eXist basierende Umgebung genutzt, die auch in anderen Projekten des Instituts und in anderen Institutionen Anwendung findet. In die HTML-Präsentation integriert ist die Möglichkeit zur Annotation des Textes. Dabei kann entweder der Text korrigiert, eine Entität ausgezeichnet und/oder identifiziert oder eine Volltextanmerkung geschrieben werden. Die Möglichkeiten der Annotation, die Modellierung der annotierten Entitäten, ihre Verwaltung und Darstellung sollen im Rahmen eines „Annote-a-thons“ in Zusammenarbeit mit den NutzerInnen weiterentwickelt werden. Für das Projektteam ist es etwa wichtig zu erfragen, welche unterschiedlichen Sichtweisen es seitens der verschiedenen Disziplinen auf den Text gibt, welche Aspekte bei der Erschließung von besonderer Relevanz sind oder ob es so etwas wie einen “kleinsten, gemeinsamen Nenner” aller Annotationskonzepte geben kann. Die Einbeziehung von ExpertInnenen im Annotationsprozess ist dem Projektteam ein besonderes Anliegen: Es begreift das Annotieren als höchst anspruchsvolle Forschungsleistung, die ein profundes historisch-kulturelles Wissen bei der Beurteilung erfordert und für eine Quelle wie dem „Wiennerischen Diarium“ bei genauerer Betrachtung eigentlich nur gemeinsam erbracht werden kann. Ein kollaboratives Annotieren technisch vorzusehen und zu ermöglichen, ist hierbei die besondere Herausforderung. Die dafür entstehende benutzerfreundliche Präsentations- wie auch die Annotationsumgebung wird unter einer freien Lizenz zur Nachnutzung zur Verfügung gestellt werden.
      
      
         
             Vgl. 
                        
                  https://www.suub.uni-bremen.de/ueber-uns/projekte/alte-zeitungen/
                [letzter Zugriff 14. Januar 2018]
                    
             Vgl. 
                        
                  http://zefys.staatsbibliothek-berlin.de/
                [letzter Zugriff 14. Januar 2018]
                    
             Vgl. 
                        
                  http://digitale-sammlungen.ulb.uni-bonn.de/ulbbnz/date/list/229854
                [letzter Zugriff 14. Januar 2018]
                    
             Vgl. 
                        
                  http://anno.onb.ac.at/
                [letzter Zugriff 14. Januar 2018]
                    
             Vgl. 
                        
                  http://www.europeana-newspapers.eu/
                [letzter Zugriff 14. Januar 2018]
                    
             Vgl. 
                        
                  https://kw.uni-paderborn.de/institut-fuer-germanistik-und-vergleichende-literaturwissenschaft/germanistische-und-allgemeine-sprachwissenschaft/schuster/forschung/projekte/der-hamburgische-unpartheyische-correspondent-volltextdigitalisierung/
                [letzter Zugriff 14. Januar 2018]
                    
             Vgl. 
                        
                  http://repos.ids-mannheim.de/mkhz-beschreibung.html
                [letzter Zugriff 14.01.2018]
                    
             Vgl. etwa die Einrichtung einer “Special Interest Group Newspapers” bei der TEI-Konferenz 2016 in Wien, das CLARIN-Vernetzungstreffen “Working with Digital Collections of Newspapers” 2016 in Leuven oder das “Transatlantic Digitised Newspaper Symposion” 2017 in London. 
             Das „Wien[n]erische Diarium“ ist am 8. August 1703 erstmals erschienen, als „Wiener Zeitung“ bis heute erhältlich und somit die älteste, noch existierende Tageszeitung der Welt. Insbesondere im 18. Jahrhundert lässt sich die Entwicklung der Zeitung von den Anfängen des modernen Journalismus in einer spannenden Zeit und unter sich verändernden politischen, sozialen und künstlerischen Bedingungen gut nachverfolgen und mitvollziehen. 
             Vgl. 
                        http://anno.onb.ac.at/cgi-content/anno?aid=wrz [letzter Zugriff 14. Januar 2018]
                    
             Vgl. 
                        https://transkribus.eu/Transkribus/ [letzter Zugriff 14. Januar 2018]
                    
             Vgl. 
                        https://www.oeaw.ac.at/acdh/projects/wienerisches-diarium-digital/ [letzter Zugriff 14. Januar 2018]
                    
         
         
            
               Bibliographie
               
                  ANNO – AustriaN Newspapers Online. 
                  
                     http://anno.onb.ac.at/
                   [letzter Zugriff 14. Januar 2018]
                    
               
                  Harald Burger / Luginbühl, Martin (2015): Mediensprache , Berlin / Boston, 2015, 39-45.
               
                  Reisner, Andrea / Schiemer, Alfred (2016): „Das Wien(n)erische Diarium und die Entstehung der periodischen Presse“ in: 
                        Österreichische Mediengeschichte 1: 87-112.
                    
               
                  Digitalisierung der vollständigen deutschsprachigen Zeitungsbestände des 17. Jahrhunderts der Staats- und Universitätsbilbiothek Bremen: 
                        https://www.suub.uni-bremen.de/ueber-uns/projekte/alte-zeitungen/ [letzter Zugriff 14. Januar 2018]
                    
               
                  Europeana Newspapers: 
                        
                     http://www.europeana-newspapers.eu/
                   [letzter Zugriff 14. Januar 2018]
                    
               
                  Mannheimer Korpus für Historische Zeitungen und Zeitschriften in COSMAS II: 
                        
                     http://repos.ids-mannheim.de/mkhz-beschreibung.html
                   [letzter Zugriff 14. Januar 2018]
                    
               
                  Newspapers Sammlung der Universitäts- und Landesbibliothek Bonn: 
                        
                     http://digitale-sammlungen.ulb.uni-bonn.de/ulbbnz/date/list/229854
                   [letzter Zugriff 14. Januar 2018]
                    
               
                  Projekt „Volltextdigitalisierung der Staats- und Gelehrte[n] Zeitung des Hamburgischen Unpartheyischen Correspondenten und ihrer Vorläufer (1712-1848): 
                        
                     https://kw.uni-paderborn.de/institut-fuer-germanistik-und-vergleichende-literaturwissenschaft/germanistische-und-allgemeine-sprachwissenschaft/schuster/forschung/projekte/der-hamburgische-unpartheyische-correspondent-volltextdigitalisierung/
                   [letzter Zugriff 14. Januar 2018]
                    
               
                  Resch, Claudia / Schopper, Daniel / Hannesschläger, Vanessa / Wohlfarter, Eva / Mader, Anna / Fischer, Nora (2016): Wienerisches Diarium Digital: Unlocking a historic newspaper for interdisciplinary studies with the TEI Guidelines. 
                        
                     https://goo.gl/a9SmLk
                  
               
               
                  Schuster, Britt-Marie / Wille, Manuel (2016): Von der Kanzlei- zur Bügersprache? Textsortengeschichtliche Betrachtungen zur „Staats- und gelehrten Zeitung des Hamburgischen unpartheyischen Correspondenten“ im 18. Jahrhundert in: Jahrbuch für Kommunikationsgeschichte 17 Stuttgart: Franz Steiner Verlag 7-29.
               
                  ZEFYS Das Zeitungsinformationssystem der Staatsbibliothek zu Berlin: 
                        http://zefys.staatsbibliothek-berlin.de/ [letzter Zugriff 14. Januar 2018]
                    
            
         
      
   



      
         
            1. Thema
            Dieser Beitrag stellt anhand eines konkreten Fallbeispiels eine Methode zur inhaltlichen Analyse von heterogenen Textkorpora vor. Das Verfahren entstand im Rahmen einer Dissertation im Fach Musikwissenschaft und operiert einerseits mit X‑Technologien, andererseits bezieht es geisteswissenschaftliche Ansätze mit ein. Der Bezug zum Tagungsthema besteht vor allem in dem Unterfangen, eine Brücke zwischen digitalem Material und hermeneutischen Analyseansätzen zu schlagen.
            Das Vorhaben befasste sich mit der 
                    Messa da Requiem des italienischen Komponisten Giuseppe Verdi (1813–1901). Dieses geistliche Werk für Chor, großes Orchester und vier Solisten stellt eine Ausnahme in dem Schaffen des Opernkomponisten dar. Gewidmet ist es dem italienischen Schriftsteller Alessandro Manzoni, der ein Zeitgenosse und Freund Verdis war (vgl. Schweikert 2013) und 1873 verstarb. Das deshalb so genannte "Manzoni-Requiem" verbreitete sich schnell und mit großem Erfolg in ganz Europa, vor allem in Frankreich, Österreich und (kurze Zeit später) auch im Deutschen Reich.
                
            Der Fokus der Analyse liegt auf der deutschsprachigen Musikkritik im Zeitraum der Erstaufführungen (1874–1878). Mit Hinweis auf den oft emotionalen und bildhaften Charakter der Musik wurde das Werk vor allem im Deutschen Reich abschätzig beurteilt: Ein Kritiker aus Köln meinte beispielsweise, das Werk sei "kein Requiem nach deutscher Art" (August Guckeisen, Kölnische Zeitung, 12.12.1875). Diese dichotomische Abgrenzung einer "deutschen Art" gegen eine "italienische Art" ist nicht nur als Reflex von Wagnerismus und reichsdeutschem Kulturkampf zu interpretieren, sondern auch im Kontext romantischer Kirchenmusikästhetik und des zeitgenössischen Realismusbegriffs zu verstehen (vgl. Kirsch 2013). Darüber hinaus regten sich aber gerade auch in Österreich solche Stimmen, die dem geforderten "deutschen Ernst" nur wenig abgewinnen konnten und für mehr Emotionalität in der geistlichen Musik plädierten. Auch gegenwärtig führt die Ambivalenz des Werkes, das zwischen spiritueller und szenischer Musik changiert, immer wieder zu Diskussionen hinsichtlich der "richtigen" Interpretation. Das Nachdenken über historische Auffassungen und Begriffsbildungen gibt uns die Möglichkeit, unsere aktuellen Positionen und deren Ursprünge zu hinterfragen. Dies bildet den Ausgangspunkt der Untersuchung.
         
         
            2. Material
            Für die Grundlage der Analyse wurden Artikel aus Tageszeitungen und Musikfachblättern aus den Jahren 1874–1878 zu einem Textkorpus zusammengestellt. Auswahlkriterien waren erstens eine Erwähnung der 
                    Messa da Requiem und zweitens die Sprache Deutsch. Durch diese Offenheit entstand ein Korpus aus Texten, die hinsichtlich des Umfangs, der Gattung und der inhaltlichen Qualität stark variieren: Die Textsorten decken ein Spektrum von lapidaren Konzertanzeigen bis hin zu seitenlangen Werkbesprechungen mit zahlreichen Notenbeispielen ab. Für die Forschungsarbeit erfüllen die Texte unterschiedliche Zwecke: Die knapperen Texte beschränken sich meist auf Hinweise zu Aufführungen, Mitwirkenden oder Veranstaltungsorten und dienten deshalb vorrangig der Rekonstruktion einer Aufführungsgeschichte. Dabei gaben präzise Datumsangaben oft einen Anhaltspunkt für die Recherche nach ausführlicheren Quellen in lokalen Tageszeitungen. Für die Untersuchung der Rezeptionsgeschichte sind die längeren Texte weitaus interessanter und ergiebiger. Dabei bestand eine Herausforderung darin, dass sehr häufig (in ca. 70% der Fälle) nichts über den Hintergrund des Autors bekannt ist, so dass der Entstehungskontext bei der Analyse nicht einbezogen werden konnte. Bisherige Analysen berücksichtigten deshalb vor allem namhafte Autoren, deren (wie beispielsweise Eduard Hanslick, Heinrich Adolf Köstlin und Emil Naumann; vgl. z. B. Kreuzer 2005). Digitale Auswertungsverfahren sollten nun dabei helfen, auch die Äußerungen unbekannter Autoren in einem breiteren Spektrum einordnen und bewerten zu können.
                
            Die Texte wurden dazu nach Digitalisaten oder Fotokopien (20% der Fälle) transkribiert und die daraus resultierenden 950.000 Zeichen Reintext mit TEI-Markup versehen. Dabei wurde der Schwerpunkt, im Sinne von "smart data" (vgl. Schöch 2013), auf die Erschließung semantischer Einheiten gelegt. Dazu wurde ein halbautomatisches Verfahren eingesetzt, das mit Listen von bereits bekannten Eigennamen operierte. Das automatische Tagging wurde auf Richtigkeit überprüft und Sonderfälle (z.B. Pseudonyme oder Abkürzungen) manuell verzeichnet. Im Korpus konnten insgesamt 8.000 Entitäten identifiziert und mit Normdaten versehen werden. Die vier Register beinhalten 530 Personen, 142 Aufführungen, 96 Ortschaften und 135 Werke der Musik.
         
         
            3. Auswertung
            
               3.1. Metadaten
               Die Analyse der Metadaten mithilfe geographischer und einfacher statistischer Visualisierungstools (z. B. OpenLayers/MapWarper und Google Chart API) gab Aufschluss über die geographische und chronologische Verteilung der Texte. Die Presseresonanz ist in den Monaten der Erstaufführungen an den jeweiligen Orten wie erwartet am stärksten. Dabei fiel auf, dass die Verteilung im Deutschen Reich großflächiger ist als in dem stark auf Wien zentrierten Österreich (vgl. Abbildung 1). Die Anzahl der Berichte an einem Ort ist in Österreich durchschnittlich höher als im Deutschen Reich, da dort durch das Digitalisierungsprojekt ANNO erheblich mehr Periodika digitalisiert wurden und relevante Texte durch Suche im OCR-Volltext viel leichter aufzufinden sind als in realen Zeitungsbänden. Im Deutschen Reich überwiegt deshalb auch der Anteil an Texten aus den bereits umfassend digitalisierten Musikfachzeitschriften (vor allem aus Leipzig) um so mehr.
               
                  
                     
                     (Abbildung 1: Übersicht aller Spielorte, die im deutschsprachigen Raum erwähnt wurden)
                  
               
            
            
               3.2. Textanalyse
               Die Textanalyse knüpfte zunächst an mehrere Vorarbeiten zur Dichotomieanalyse und Rezeptionsgeschichte an (vgl. Sponheuer 2001) und entwickelte diese anhand des digitalen Materials weiter. Es wurde das Experiment unternommen, mithilfe eines vorgefertigten Sachvokabulars die relevanten Diskurskategorien zu erschließen und dichotomische Muster zu identifizieren. Dies musste im Rahmen des damaligen Vorhabens verworfen werden, da es in vielen Einzelfällen zu einer deutlichen Verzerrung der Textbedeutung kam, die durch den relativ geringen Umfang des Textkorpus leider nicht ausgeglichen werden konnte.
               Deshalb wurde der Fokus in einem zweiten Versuch auf die "Named Entities" gesetzt, also die bereits durch das Tagging identifizierten sachlichen Einheiten. Eine solche Einheit konnte beispielsweise ein Werkteil wie das "Kyrie" oder das "Agnus Dei" sein, zu dem dann aus allen Texten die unmittelbaren Kontexte ausgezogen und zu einem Spektrum zusammengestellt wurden. Die Kontexte zur stark umstrittenen Nummer "Mors stupebit" aus dem zweiten Teil des Werkes zeigen sowohl eine grundsätzliche Ablehnung und eine starke Grenzziehung zwischen "nordischen" und "italienischen" Stereotypen auf.
               
                  
                     
                     (Abbildung 2: Kontexte für "Mors stupebit")
                  
               
               Aus geisteswissenschaftlicher Perspektive genügt bereits diese Aufstellung, die ich hier "semantische Sichtachse" nennen möchte, um ein Spektrum von unterschiedlichen Positionen aufzuzeigen, in das auch die Haltungen der nicht näher bekannten Autoren mit einfließen. Vergleicht man die hier gezeigte Sichtachse zum "Mors stupebit" mit der Sichtachse des fast überall bewunderten "Agnus Dei", kommt ein Kontrast zwischen einer idealisierenden und einer realistischen Repräsentation des Todes deutlich zum Vorschein. Weitere aufschlussreiche Sichtachsen können beispielsweise zu Vergleichswerken (etwa die Requiem-Kompositionen von Mozart oder Cherubini), Komponistennamen (im Sinne einer stilistischen Referenz) und auch zu Schlagwörtern wie z. B. "Realismus" erzeugt werden.
               Der Zweck dieses Verfahrens ist es also, mithilfe der Textdatenbank vergleichbare Kontexte innerhalb des Korpus zu finden und diese gegenüberzustellen. Die komparative Analyse der Einzelkontexte eröffnet die Möglichkeit, übergreifende Rezeptionsmuster zu identifizieren. Ich bezeichne das hier rein metaphorisch als "horizontales Lesen" (in vager Anlehnung an den Begriff des "distant reading", vgl. Moretti 2016), da es die Texte gleichwertig nebeneinander anordnet und weniger den einzelnen Text, sondern mehr gemeinsame Bezugspunkte im gesamten Korpus betrachtet. Auf der Grundlage dieser vergleichbaren Extrakte könnte übrigens der erste, zunächst verworfene Ansatz wieder ins Spiel kommen – dies ist jedoch Zukunftsmusik.
               Ich füge noch einige Ergebnisse aus musikwissenschaftlicher Sicht an: Beispielsweise wurde das sehr beliebte "Agnus Dei", welches in über 60 Texten erwähnt wurde, auffallend oft als Abklatsch verschiedener Opernnummern gedeutet, um die schöpferische Qualität herabzusetzen. Hinsichtlich des Fugenstils im "Sanctus" verhielten sich die Berliner und Dresdner Kritiker auffallend empfindlich, während die Kritiker in Wien und auch in Köln in dieser Hinsicht liberaler waren. Eine ähnliche Aufteilung ist auch hinsichtlich des Einsatzes von musikalischen Effekten und Affekten festzustellen. Dabei wurden sowohl allgemeine Unterschiede zwischen dem Deutschen Reich und Österreich sowie dem katholischen und dem protestantischen Raum deutlich. Hinzu treten mehrere lokale Besonderheiten: So kam am Hamburger Stadt-Theater ein Bühnenbild zum Einsatz, welches das Innere einer katholischen Kirche zeigte, was die Hamburger Kritik durchaus faszinierte. In Salzburg schlossen sich mehrere Musikvereine zusammen, um das Werk für einen karitativen Zweck aufzuführen, weshalb die Kritik wohlwollender war als andernorts.
               Die insgesamt sehr unterschiedlichen Kritiken können deshalb erst aus den jeweiligen Kontexten heraus angemessen bewertet werden. Durch einen flächendeckenden Vergleich ist es möglich, allgemeine Tendenzen der Rezeption zu identifizieren und zu verorten, auch wenn die individuellen Hintergründe der Autoren nicht immer bekannt sind. Die Texte von unbekannten Autoren sind auf diese Weise leichter zu kontextualisieren, zu bewerten und in das Gesamtbild einzuordnen.
            
         
         
            4. Ausblick
            Das Textmaterial ist für das ursprüngliche musikwissenschaftliche Vorhaben zunächst ausgeschöpft, bietet aber Potenzial für weitere Analysen mit anderen Methoden. Wärenbeispielsweise positive und negative Kritiken mithilfe stilometrischer Verfahren voneinander unterscheidbar? Geben Netzwerkvisualisierungen von Vergleichswerken in den Texten möglicherweise einen Hinweis auf Strukturen kanonischer Bezugssysteme? Deckt sich dies mit den bisherigen Beobachtungen? Wäre es für die Präsentation denkbar, eine in MEI kodierte Partitur takt- oder abschnittsweise mit den diversen Kommentaren der Kritiker zu versehen? Es ist zu hoffen, dass in Zukunft ähnliche Projekte entstehen, um das digital aufbereitete Datenmaterial für weitere Forschung nutzen können – etwa mit dem Fokus auf der Rezeption des Werkes in anderen Sprachgebieten oder Zeiträumen, oder auch auf anderen Requiem-Vertonungen. Zu diesem Zweck wird das Material digital publiziert, offen lizenziert sowie in Hackathons und THATcamps eingebracht. Die Publikationsplattform wird zum Konferenztermin bekanntgegeben.
         
      
      
         
            
               Bibliographie
               
                  Kirsch, Winfried (2013): "Kirchenmusikreform, Cäcilianismus und Palestrina-Renaissance", in: Wolfgang Hochstein und Christoph Krummacher (eds.): 
                    Geschichte der Kirchenmusik, Bd. 3, Laaber: Laaber 56–71.
                    
               
                  Kreuzer, Gundula (2005): "Oper im Kirchengewande"? Verdi’s Requiem and the Anxieties of the Young German Empire, in: Journal of the American Musicological Society 58,2: 399–450.
               
                  Moretti, Franco (2016): 
                    Distant Reading, Konstanz: Konstanz University Press.
                    
               
                  Schöch, Christof (2013): "Big? Smart? Clean? Messy? Data in the Humanities", in: 
                    Journal of Digital Humanities 2, No. 3: 2–13.
                    
               
                  Schweikert, Uwe (2013): Messa da Requiem, in: Anselm Gerhard and Uwe Schweikert (eds.): 
                    Verdi-Handbuch, Kassel: Bärenreiter; Stuttgart/Weimar: Metzler (2., überarb. und erw. Auflage) 557–565.
                    
               
                  Sponheuer, Bernd (2001): "Über das ›Deutsche‹ in der Musik. Versuch einer idealtypischen Rekonstruktion", in: Hermann Danuser / Herfried Münkler (eds.): 
                        Deutsche Meister – böse Geister? Nationale Selbstfindung in der Musik, Schliengen: Edition Argus 123–150.
                    
            
         
      
   



      
         
            Introduction
            This poster presents a corpus of 19th-century sonnets in Spanish in XML-TEI (685 authors, 2677 sonnets). It includes well-known authors, but also less canonized authors. Texts and authors are enriched with identifiers and metadata. See 
                    
                  https://github.com/pruizf/disco
               
            
            A fundamental difficulty for Digital Humanities studies on Spanish literature is a scarcity of digital resources (Agenjo, 2015). 
            Some resources do however exist. BiDTEA (Gago Jover et al, 2015), ADMYTE (Marcos Marin and Faulhaber, 1992), ReMetCa (González-Blanco and Rodríguez, 2014) and PoeMetCa (Escribano et al, 2016) have digitized Spanish Medieval texts. Navarro-Colorado et al. (2015) presented the Corpus of Spanish Golden-Age Sonnets.
            Regarding 19th-century Spanish literature, available collections covering different genres are Textbox (Schöch et al., 2015), BETTE (Santa María Fernández, 2017), Aracne (Álvarez-Mellado and Martín-Fuertes, 2015), and Revistas Culturales 2.0 (Ehrlicher and Rißler-Pipka, 2015). Nevertheless, none of these projects are working on poetry. 
            DISCO complements this growing ecosystem by adding a meaningful representation of 19th-century sonnets, with more periods under validation, to be published shortly.
         
         
            Corpus description
            
               What is DISCO
               Our corpus collects 2677 sonnets in Spanish from the 19th century, by 685 authors (Spain or Latin America). It intends to provide a wide sample, inspired by distant reading approaches (Moretti, 2005). The raw texts were extracted from Biblioteca Virtual Miguel de Cervantes (1999), which contains an anthology (Garcia, 2006) of 19th century sonnets covering both well-known and non-canonical authors. We used an anthology in order to have external scholarly criteria for the literary relevance of the corpus texts. 
               The texts have been encoded in XML-TEI P5, given this standard’s benefits in terms of reuse, storage and retrieval. Author metadata have been extracted or inferred from unstructured content in the sources, and placed in the TEIheader (year, place of birth and death, and gender). Two versions of the texts are available: one collecting every sonnet per author, the other encoding a single sonnet per file. For corpus preparation, we closely followed the TEI guidelines and RIDE’s criteria for Digital Text Collections (Henny and Neuber, 2017).
               Additionally, authors have been assigned VIAF identifiers. This gives the corpus an entry-point to the linked open data cloud, enhancing its findability. The corpus is available on GitHub and saved in Zenodo, adopting good practices for data use, reuse, and conservation.
               The metadata we added allow users to create subcorpora, such as “female authors from Cuba born in the first half of the 19th century”.
               We have also obtained sonnets from other centuries, since the 15th century to the present. These are under validation and will be published shortly within the DISCO corpus, which intends to give a wide perspective on the sonnet in Spanish diachronically.
            
            
               Why sonnets
               The sonnet has had great importance in European poetry; the relevance of the corpus for literary scholarship is guaranteed. It is a "manageable" form to treat computationally, obeying clear restrictions. Variability stays within bounds, making meaningful comparison across poems easier, regarding scansion or rhyme types. Besides, some digital collections of sonnets already exist (with different features to ours, as discussed below) as well as automatic analyses of this form. The sonnet has received attention from the computational linguistics community (Navarro-Colorado et al, 2015, 2016, 2017; Agirrezabal, 2017) including the ADSO project (Navarro-Colorado 2017). The DISCO corpus will also be useful for that audience. For these reasons, a new sonnet corpus allows us to engage in a dialogue with earlier work in traditional literary studies, in digital corpus development, and in computational poetry analyses. 
               Concerning digitally available sonnet corpora, Sonnet-Archiv (Elf Edition) is organized as a forum, and its coverage is less wide than ours. The “Sonnet Library” (Biblioteca Virtual Miguel de Cervantes, 2007) is organized alphabetically, rather than using meaningful criteria for literary scholarship, like periods. Both are traditional websites. Finally, the Corpus of Spanish Golden Age Sonnets (Navarro-Colorado et al., 2015) covers major authors from the 15th to the 17th century, with an automatic metrical annotation. Author metadata in these corpora are very limited and unavailable in a machine-readable format (see Calvo Tello, 2017, for discussion of related issues). With the DISCO corpus, we are offering a wider period and author range, from major to minor authors, encoded in XML-TEI, available as repository, with richer structured and standard metadata.
            
         
         
            Conclusion
            With the DISCO corpus, while focusing on sonnets, we intend to increase available digital resources in Spanish poetry, by addressing additional periods, covering minor as well as canonical authors, and including materials from several Spanish-speaking countries. Choosing the sonnet complements existing work on this form, in traditional and computational literary studies. TEI was adopted in order to serve the large community using this format. The corpus can be made available as linked open data as it includes VIAF IDs. It is published at 
                    https://github.com/pruizf/disco and 
                    https://doi.org/10.5281/zenodo.1012567.
                
         
         
            Acknowledgements
            The work was supported by Starting Grant ERC-2015-STG-679528 
                    POSTDATA, PI Elena González-Blanco. We also thank Helena Bermúdez from the LINHD-UNED lab for later contributions to the corpus.
                
         
      
      
         
            
               Bibliography
               
                  Agenjo, Xavier (2015): “Las bibliotecas virtuales españolas y el tratamiento textual de los recursos bibliográficos”, in Ínsula: revista de letras y ciencias humanas 822: 12–15.
                    
               
                  Agirrezabal, Manex (2017): Automatic Scansion of Poetry. PhD Thesis. University of the Basque Country.
                    
               
                  Álvarez Mellado, Elena / Martín-Fuertes, Leticia (2015): Aracne Project [online]. Available at: http://www.fundeu.es/aracne/ [Accessed 22 Sep. 2017].
                    
               
                  Biblioteca Virtual Miguel de Cervantes (1999): Biblioteca Virtual Miguel de Cervantes [online]. Available at: 
                        http://www.cervantesvirtual.com/ [Accessed 22 Sep. 2017].
                    
               
                  Biblioteca Virtual Miguel de Cervantes (2007): Biblioteca del Soneto [Sonnet Library] [online]. Available at: 
                        http://www.cervantesvirtual.com/bib/portal/bibliotecasoneto/ [Accessed 22 Sep. 2017].
                    
               
                  Calvo Tello, José. (2017). Review of Corpus of Spanish Golden Age Sonnets by Borja Navarro Colorado, María Ribes Lafoz and Noelia Sánchez (ed.), in RIDE, 6. Institut für Dokumentologie und Editorik, Köln. [Online]. Available at: http://ride.i-d-e.de/issue-6/corpus-of-spanish-golden-age-sonnets/ [Accessed 22 Sep. 2017].
                    
               
                  Ehrlicher, Hanno / Rißler-Pipka, Nanette (2015). Revistas Culturales 2.0. Augsburg: Universität Augsburg. [Online]. Available at: 
                        https://www.revistas-culturales.de/es [Accessed 22 Sep. 2017].
                    
               
                  Elf Edition: Sonett-Archiv [online]. Available at: http://sonett-archiv.com [Accessed 22 Sep. 2017].
                    
               
                  Escribano, Juanjo / González-Blanco, Elena / Río Riande, Gimena del (2016). PoeMetCa—Recursos digitales para el estudio de la Poesía Medieval Castellana. [Online]. Available at: 
                        http://poemteca.linhd.es [Accessed 22 Sep. 2017].
                    
               
                  Gago Jover, Francisco (2015): “La biblioteca digital de textos del español antiguo (BiDTEA), in Scriptum Digital 4: 5–36.
                    
               
                  García González, Ramón (2006): Sonetos del siglo XIX. Biblioteca Virtual Miguel de Cervantes, Alicante. [Online]. Available at: 
                        http://www.cervantesvirtual.com/obra-visor/sonetos-del-siglo-xix--0/html/ [Accessed 26 Nov. 2017
                        ]
               
               
                  González-Blanco, Elena / Rodríguez, José Luis (2014): “ReMetCa: A Proposal for Integrating RDBMS and TEI-Verse”, in Journal of the Text Encoding Initiative 8 [online]. Available at: 
                        https://jtei.revues.org/1274 [Accessed 22 Sep. 2017], doi:10.4000/jtei.1274.
                    
               
                  Henny, Ulrike / Neuber, Frederike (2017): “Criteria for Reviewing Digital Text Collections, version 1.0”. IDE, Institut for Dokumentologie und Editorik, [online]. Available at: 
                        https://www.i-d-e.de/publikationen/weitereschriften/criteria-text-collections-version-1-0/ [Accessed 22 Sep. 2017].
                    
               
                  Marcos Marín, Francisco / Faulhaber, Charles B. (coord.) (1992): ADMYTE. Archivo Digital de Manuscritos y Textos Españoles, in 
                        http://www.admyte.com/admyteonline/contenido.htm [Accessed 22 Sep. 2017].
                    
               
                  Moretti, Franco (2005): Graphs, Maps, Trees: Abstract Models for a Literary History. London and New York: Verso.
                    
               
                  Navarro-Colorado, Borja (2015): A computational linguistic approach to Spanish Golden Age Sonnets: metrical and semantic aspects. In ACL Workshop on Computational Linguistics for Literature 105.
                    
               
                  Navarro-Colorado, Borja (2017): ADSO project – Análisis distante del soneto castellano de los Siglos de Oro [Distant analysis of the Spanish Golden Age sonnet] [online]. Available at: 
                        http://adso.gplsi.es/index.php/es/proyecto-adso [Accessed 22 Sep. 2017].
                    
               
                  Navarro-Colorado, Borja / Ribes Lafoz, María / Sánchez, Noelia (2015): Corpus of Spanish Golden-Age Sonnets. Alicante: University of Alicante [online]. Available at: 
                        https://github.com/bncolorado/CorpusSonetosSigloDeOro [Accessed 22 Sep. 2017].
                    
               
                  Navarro-Colorado, Borja / Ribes Lafoz, María, / Sánchez, Noelia (2016): “Metrical Annotation of a Large Corpus of Spanish Sonnets: Representation, Scansion and Evaluation”, in Proceedings of the Language Resources and Evaluation Conference [online]. Available at: 
                        http://www.lrec-conf.org/proceedings/lrec2016/pdf/453_Paper.pdf [Accessed 22 Sep. 2017]
                    
               
                  Navarro-Colorado, Borja (2017): “A metrical scansion system for fixed-metre Spanish poetry”, in Digital Scholarship in the Humanities. 
                        https://doi.org/10.1093/llc/fqx009 [Accessed 22 Sep. 2017]
                    
               
                  Santa María Fernández, María Teresa / Jiménez Fernández, Concepción María (2017): Biblioteca Electrónica Textual Del Teatro Español, 1868-1936. Universidad Internacional de la Rioja, Spain.
                    
               
                  Schöch, Christof / Henny, Ulrike / Calvo Tello, José / Popp, Stefanie (2015): The CLiGS Textbox. Würzburg: University of Würzburg. [Online]. Available at: 
                        https://github.com/cligs/textbox [Accessed 22 Sep. 2017]
                    
            
         
      
   



      
         In der Beschreibung der DHd-Konferenz 2018 „Kritik der digitalen Vernunft“ wird die These formuliert, dass die Digital Humanities„häufig als digital transformierte Bearbeitung von Fragestellungen aus den verschiedenen beteiligten Fächern beschrieben“ werden und „in weiten Teilen eine daten-, algorithmen- und werkzeuggetriebene Wissenschaft sei[en], die von ihren unmittelbaren Möglichkeiten und ihren Praktiken dominiert“ werden, welche den Prinzipien kritischer Wissenschaftlichkeit möglicherweise nicht genüge. Am Beispiel der Entwicklung der Guidelines sowie der Analyse einzelner konkreter Anwendungsfälle der Text Encoding Initiative (TEI; Synonym auch für die Richtlinien der Initiative, den „Guidelines for (Electronic) Text Encoding and Interchange“) soll untersucht werden, welche Wechselwirkungen es hierbei zwischen Theorie, Methode und Tool-Entwicklung gegeben hat. 
            
         
            Der Gegenstand
            Der Beobachtungsgegenstand dieser Untersuchung sind die Guidelines der TEI selbst, die daraus resultierenden Schemata sowie deren Anwendung in ausgewählten Anwendungsfeldern der DH. Die TEI dokumentiert die Entwicklung der Guidelines seit der Einführung 1990 mit der Version P1 in unterschiedlicher Tiefe. Während für die älteren Versionen meist nur deren Endprodukt in Form der Document Type Definition (DTD) bzw. des Textes der Guidelines vorliegt, kann man die Entwicklung des de-facto-Standards seit der Version P4 und dann ab 2007 in listenförmiger Dokumentation der vorgenommenen Änderungen nachvollziehen. Die TEI dokumentiert außerdem die Diskussionen, die in ihren Gremien sowie in der Community über Mailingliste geführt werden, in je eigenen Archiven. Damit lässt sich ein nahezu lückenloses Bild der Entwicklungsschritte hin zu der geltenden Version nachvollziehen. Diese Materialien können vor dem Hintergrund der oben genannten Thesen untersucht und mit den traditionellen Wissenschaften in Kontext gesetzt werden, um Aufschlüsse über die Wechselwirkungen zu erhalten.
                
            Ein weiterer Gegenstand besteht in der konkreten Anwendung der Guidelines bzw. Schemata der TEI in Projekten. Hier sollen beispielhaft Editionsprojekte sowie die Verwendung der TEI zur Speicherung von Metadaten betrachtet werden, um die Wechselwirkungen zu analysieren. Es soll damit der Frage nachgegangen werden, inwieweit die Definition einer Markup-Sprache, die Anwendung der Sprache bei der Erstellung konkreter Dokumente und die Validierung der Ergebnisse durch technische Rahmenbedingungen vorgegeben oder durch außerhalb der DH liegenden Theoriebildung beeinflusst werden oder die Theoriebildung und Methodenentwicklung selbst vorantreiben.
         
         
            Theoriebildung
            Bereits 1994 wies Sperberg-McQueen auf die theoretischen Implikationen der Textauszeichnung, analog dazu: des Gebrauchs einer Wissenschaftssprache, hin: „Like any notation, the TEI Guidelines inevitably make it easy to express certain kinds of ideas, and concomitantly harder to express other kinds of ideas, about the texts we encode in electronic form. Any notation carries with it the danger that it must favor certain habits of thought --- in the TEI's case, certain approaches to text --- at the expense of others. No one should use TEI markup without being aware of this danger --- any more than we should use the English language, or any other, without realizing that it favors the expression of certain kinds of ideas, and discourages the expression, and even the conception, of other ideas.“ (Sperberg-McQueen 1994) Die TEI-Community versucht allerdings dennoch, diesen Gefahren mit der offenen Diskussion über die semantischen Dimensionen des Markups, mit Best-Practice-Beispielen für die Anwendung entgegen zu wirken. Die Community beweist immer wieder die Fähigkeit, den etablierten Standard im Fall erweiterter Anforderungen an neue Aufgaben anzupassen. Paradigmenwechsel in der Editionsphilologie können dadurch adaptiert, in der Auszeichnungspraxis angewendet und somit wissenschaftlich nutzbar gemacht werden, ohne deshalb vorhandene Dokumente ungültig werden zu lassen oder die bisherige Anwendung des Standards zu konterkarieren. Beispiele hierfür sind die Inkorporation von Markup-Subsystemen für genetische Editionen, zur Beschreibung von Handschriften oder auch zur Dokumentation von Kommunikationsvorgängen, etwa in Briefen, mit der Einführung von . Das System der TEI wurde dabei prinzipiell erweitert als grundlegend verändert. Die Abschaffung von Elementen oder Attributen, die etwa die Falsifikation nach Popper gleich zu setzen wären, ist daher sehr viel seltener als der Proof of Concept, mit welchem zunächst die Tauglichkeit von Markup zur Repräsentation bestimmter Phänomene getestet wird, bevor die Aufnahme in den Standard erfolgt und die Angebote durch die Community weiter genutzt werden können.
                
            Gleichzeitig bestimmen die Anforderungen der inhaltlichen bzw. theoretischen Weiterentwicklung eines Faches die technische und theoretische Weiterentwicklung der TEI mit. Ein rezentes Beispiel hierfür ist die theoretische Ausrichtung des 
                    Material Turns in der Editionsphilologie und den historischen Wissenschaften. Die Hinwendung zum Objekt wird in der TEI mit der Einführung von 
                     und dem Modul für genetische Editionen gespiegelt. Auch Überlegungen zur Verallgemeinerung des Beschreibungsstandards von Dokumenten und Objekten mittels der Strukturen von  wären hier zu nennen.
                
         
         
            Methodenbildung
            Ein Beispiel für die Anwendung neuer Methoden der DH ist die Anreicherung von Texten um bestimmte Kontexte. Wenn in einem Text benannte Entitäten wie Personen, Orte, Objekte oder Ereignisse ausgezeichnet werden, dann entspricht dies zunächst der Registerarbeit traditioneller Publikationsverfahren. Wenn diese Entitäten allerdings mit Normdaten angereichert und diese somit zu nachnutzbaren Bestandteilen im Sinne der Linked Open Data (LOD) werden, darf man wohl von der Anwendung einer neuen Methode sprechen. Die entstandenen Dokumente verändern ihren Charakter von traditionell erstellten und händisch ausgewerteten Objekten zu konzeptionell gestalteten und automatisiert nutzbaren. Die Dokumente werden in die Lage versetzt, auch außerhalb bestimmter, inhaltlich vorgegebener Auswertungskontexte mit anderen Methoden ausgewertet zu werden.
                
            Die Konformität zu den TEI-Richtlinien spielt in der Anwendung der neuen Methode eine wesentliche Rolle. Im System der TEI ist die 
                    Customisation als Normalfall vorgesehen. Unter
                    Customisation wird die kontextabhängige Auswahl von Modulen, Elementen und Attributen oder die Vorgabe von Wertemengen für Attribute verstanden. Diese Vorgaben können in der TEI in sogenannten „One Document Does it all“-Dateien (ODD) definiert und dokumentiert werden, um dann anschließend daraus eigene Datenstrukturdefinitionen in Form von Schema-Dokumenten zu generieren. Zunehmend werden hierbei neben den klassischen Schemasprachen zur Validierung von Datenstrukturen (DTD, RelaxNG, XML Schema) auch Regelwerke definiert, welche eine Validierung des Inhalts eines Dokumentes ermöglichen. (Schematron) ODD sind durch die Dokumentationsleistung und eine normierte Beschreibungssprache ein starkes Hilfsmittel, um die Verwissenschaftlichung von textuellem Markup und dem unterliegenden Textverständnis zu erreichen. Wie aber der Grad der TEI-Konformität zu messen und zu beschreiben sei, ist in der TEI bereits seit Längerem Gegenstand von Diskussionen.
                
         
         
            Tool-Entwicklung
            Wo nun mehr Texte in TEI-konformer Auszeichnung vorliegen, umso wichtiger wird die Existenz von Tools, um diese Daten zu nutzen. Wo Text- und Image-Digitalisierung Quellen einfacher zugänglich macht, werden diese Materialien potentiell als Gegenstand der Digital Humanities nützlich. Ähnlich wie bei der Theorie- oder Methodenbildung befruchten sich zur Verfügung stehende Materialien und daran anzulegende Fragestellungen gegenseitig. Die Kritik der digitalen Methoden wird hier ansetzen müssen, wo das Markup vom Ende her zu denken ist, weil bestimmte Funktionalitäten gewünscht sind: Dokumente sollen nach bestimmten Kriterien sortiert werden? Was bedeutet dies für die Aufbereitung der Daten? – Dokumente sollen im Rahmen des LOD genutzt werden? Welche Eigenschaften müssen sie hierfür haben?
                
         
         
            DH, eine Wissenschaft wie jede andere?
            Die in einer Wissenschaft zur Verfügung stehenden Quellen und deren Verarbeitungsmechanismen haben sich immer schon gegenseitig sowie die Theorie- und Methodenbildung beeinflusst. An den Beispielen der Untersuchung soll gezeigt werden, dass sich dies in den DH nicht anders ausnimmt.
         
      
      
         
            
               Bibliographie
               
                  Lou Burnard / Sebastian Rahtz
                         (2004): "RelaxNG with Son of ODD", in:
                        Proceedings of Extreme Markup Languages 2004.
                        
               
               
                  Lou Burnard / C. Michael Sperberg-McQueen
                         (1995): "The Design of the TEI Encoding Scheme", in: 
                        Computers and the Humanities 
                        29 (1) p. 17–39. 10.1007/BF01830314
                    
               
                  Guidelines for Electronic Text Encoding and Interchange, edited by TEI Consortium. 1990-, P1--P5.
                        , 
                        
               
               
                  Patrick Sahle
                         (2013):
                        Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels.
                         3 Bde. Norderstedt: BoD.
                        
               
               
                  C. Michael Sperberg-McQueen 
                        (1994):
                        Textual Criticism and the Text Encoding Initiative
                        . (First draft of a paper presented at MLA, San Diego, 1994)
                        
               
               
                  C. Michael Sperberg-McQueen / Henry Thompson 
                        (2000):
                        XML-Schema.
                        
               
               
                  Jörg Wettlaufer 
                        (2016): "Neue Erkenntnisse durch digitalisierte Geschichtswissenschaft(en)? Zur hermeneutischen Reichweite aktueller digitaler Methoden in informationszentrierten Fächern", in:
                        Zeitschrift für digitale Geisteswissenschaften. 
                        DOI: 10.17175/2016_011.
                    
               
                  RelaxNG
                         (2008):
                        ISO/IEC 19757-2:2008. Information technology -- Document Schema Definition Language (DSDL) -- Part 2: Regular-grammar-based validation -- RELAX NG. 
                  
               
               
                  Schematron
                         (2016):
                        ISO/IEC 19757-3:2016. Information technology -- Document Schema Definition Languages (DSDL) -- Part 3: Rule-based validation – Schematron.
                  
               
            
         
      
   



      
         
            Kontext und Zielsetzung
            Die Modellierung textueller und transbiblionomer Relationen mithilfe von Semantic Web-Technologien bildet mittlerweile eines der zentralen Forschungsfelder der Digital Humanities. Die Struktur und Funktionsweise literarischer Texte erfordern in Bezug auf die formale Beschreibung, Erklärung und Kategorisierung von semantischen Strukturen ein besonders differenziertes Vorgehen: Interne und externe textuelle Beziehungen bestehen in Form komplexer, häufig ambiger Zeichenrelationen, die plurale, sich auf verschiedenen Ebenen überlagernde Bedeutungsangebote stiften. Letztere können zudem nicht auf verortbare Ereignisse, stabile Relationen zwischen (bibliografischen, historischen) Artefakten oder ein konkretes argumentatives Ziel bezogen werden. Die Kategorisierung literarischer ‚Daten‘ erfolgt deshalb systematisch im Spannungsfeld zwischen dem Text als linguistisch-materieller Zeichenformation und deren interpretierender Auffassung. Die Aktualisierung bestimmter Codes hängt hierbei immer auch von kulturellen und historischen Faktoren, methodologischen Vorannahmen sowie der Kontingenz interpretativer Schlussfolgerung ab. Die daraus resultierende Bedingtheit und potentielle Vielfalt semantischer Zuschreibungen muss daher in einer Modellierung transparent abbildbar sein.
                     Dieser Herausforderung begegnet das Projekt, welches in dem Beitrag vorgestellt wird, indem auf der Basis eines situationstheoretischen Formalismus (Barwise/Perry 1983, Devlin 1990) ein mehrstufiges Modell zur Abbildung und Beschreibung komplexer intertextueller Relationen zwischen literarischen Texten entwickelt wird.
                
            Das Projekt möchte damit in zweierlei Hinsicht einen Beitrag zur methodologischen Reflexion leisten: Zum einen streben wir mit dieser zunächst auf das genauere Verständnis intertextueller Phänomene gerichteten stufenweisen Modellierung
                     einen literaturtheoretisch reflektierten Einsatz digitaler Methoden an. Zum anderen trägt das Vorgehen bei der Modellierung ebenso zu einer Schärfung der literaturwissenschaftlichen Perspektive auf Intertextualität und zur fundierten Beschreibung hierbei wirksamer Faktoren bei. Ziel des Projekts ist eine maschinenlesbare Systematisierung intertextueller „Schreibweisen“ (Verweyen/Wittig, S. 38)
                     sowie der Kriterien zur Isolierung und Charakterisierung der Schreibweisen. Diese soll eine computergestützte Erschließung literarischer Intertextualität ermöglichen.
                
            Der Beitrag diskutiert zum einen konkrete Probleme, welche sich im Spannungsfeld zwischen literarischer Polysemie, der Literaturwissenschaft inhärenter Perspektivenvielfalt und technischer Normierung ergeben, denn das Projekt dient nicht zuletzt auch der Reflexion der Möglichkeiten zur Formalisierung literaturwissenschaftlicher Erkenntnisse sowie dem Ausloten der Grenzen für den Einsatz formaler Beschreibungssprachen im Hinblick auf literaturwissenschaftliche Forschungsfragen.
            Zum anderen wird dargestellt, wie durch die spezifische Anlage der Modellierung auf verschiedenen Ebenen Desideraten bisheriger Ansätze begegnet und gleichzeitig ein Beitrag zum literaturtheoretisch fundierten Einsatz von DH-Methoden geleistet werden kann.
            Erste Ergebnisse werden in dem vorgeschlagenen Beitrag anhand konkreter Beispiele wie etwa des intertextuellen Netzes um Matthias Claudius’ 
                    Rheinweinlied präsentiert, welches aufgrund der dem Netz inhärenten Vielzahl intertextueller Phänomene bei gleichzeitig relativ kurzen Texten hierfür besonders geeignet erscheint.
                
         
         
            Stand der Forschung und Abgrenzung
            In vielen bisherigen Projekten zur Erfassung literarischer Beziehungen schränkt die Konzentration auf automatisierbare Analysevorgänge das heuristische Potential digitaler Modellierung für die literaturwissenschaftliche Forschung in verschiedener Hinsicht ein:
            Erstens werden Modelle zur Beschreibung (innertextueller) literarischer Strukturen an stark Plot-lastigen Texten entwickelt,
                     was die Herausforderung der vielfältigen Bedeutungsebenen komplexerer literarischer Texte deutlich reduziert. Die Modelle erscheinen deshalb kaum auf den Großteil der literaturwissenschaftlich relevanten Beispiele übertragbar.
                
            Zweitens erfolgt eine Modellierung intertextueller Beziehungen anhand einer „historische[n] Positivtät von Kontext-Dokumenten“ (Wagner/Mehler/Biber 2016, S. 90 mit Bezug auf das Projekt Wikidition), deren Verknüpfungen auf linguistischer Ebene modelliert werden. Auf diese Weise werden zwar viele Probleme im Hinblick auf die Intersubjektivierbarkeit der Modellierung und die Differenzen zwischen verschiedenen Intertextualitätskonzepten vermieden, gleichzeitig wird aber aus literaturwissenschaftlicher Sicht die Aussagekraft der Ergebnisse stark eingeschränkt, indem der literaturwissenschaftlich relevante Fokus auf die Kategorisierung, Funktion und Wirkung von Intertextualität und die hierbei produktiven Schreibweisen und Markierungen (vgl. Kocher 2007, 179) zugunsten einer eher enzyklopädischen Perspektive verloren geht.
                    
            
            Unser Projekt richtet sich hingegen weder auf die automatisierte Textanalyse noch beschränkt es sich auf die linguistische Ebene konkreter Wortäquivalenz. Vielmehr steht die Entwicklung eines formalisierten Vokabulars zur semantischen Repräsentation literarischer Intertextualität im Zentrum des Forschungsinteresses. Die Isolierung und formale Beschreibung intertextueller Phänomene dient der Beobachtung und Darstellung des Zusammenwirkens jener Schreibweisen und Markierungen bei der Erzeugung von Intertextualität. Die intertextuellen Beziehungen werden dabei also nicht deduktiv im Sinne einer Qualifizierung als Parodie, Kontrafaktur, Nachahmung, Hommage etc. modelliert, da derartige ‚Gattungszuschreibungen‘ in systematischen literaturwissenschaftlichen Untersuchungen zur Typisierung von Intertextualität oftmals den Blick für die spezifischen, bei der Erzeugung von Intertextualität wirksamen Faktoren verstellen. Die angeführten literarischen Beispiele dienen dann eher der selektiven Untermauerung der jeweils präfigurierten Typologie (vgl. einschlägig Broich/Pfister 1985; Genette 1993). Im Gegensatz dazu bildet die Modellierung von Beziehungen zwischen konkreten Schreibweisen den Ausgangspunkt unseres Projekts, welcher im Anschluss Schlussfolgerungen über die Relationen der verschiedenen Ebenen, auf denen intertextuelle Verknüpfungen stattfinden, sowie über die jeweils erzeugten Wirkungen ermöglichen soll.
         
         
            Methodik
            Für das angestrebte Forschungsziel stellt die mehrstufige Modellierung einen expliziten heuristischen Gewinn gegenüber bisherigen Ansätzen der Systematisierung dar, indem die umfassende induktive Erfassung und Beschreibung sowie die anschließend abgeleiteten strukturellen Konstanten nicht unverbunden nebeneinander stehen, sondern Mikro- und Makrostrukturen durch das Modell in ihrem Zusammenhang beobachtbar gemacht werden (s. Abbildung 1).
            
               
               Abbildung 1: Konzeptuelle Darstellung des Modells
            
            Indem zunächst Einzeltextphänomene modelliert, darauf aufbauend deren Funktionen im Sinne ihres Beitrags zur „Bedeutungskonstitution“ (Hempfer 1991, S. 19) erfasst und daraus übergreifende Kategorien abgeleitet werden, können zwei bislang getrennt voneinander verhandelte Bereiche der Untersuchung von Intertextualität in einer ganzheitlichen Modellierung verbunden werden: die ‚Entwirrung‘ des intertextuellen Gefüges eines einzelnen Textes (vgl. hierfür exemplarisch Bauer Lucca 2001; Dudzik 2017) sowie die übergeordnete Suche nach gemeinsamen Strukturen und Funktionsweisen intertextueller Verweise. Das vorgestellte Modell verknüpft also die in der Literaturwissenschaft seit den 1980er Jahren unternommenen Bestrebungen zur Typologisierung intertextueller Strukturen und Funktionsweisen mit einer umfassenden Detailuntersuchung literarischer Texte. Die Differenzierung in Phänomenbeschreibung und ‑bewertung, welche der eingesetzte Formalismus unterstützt (s. u.), sieht explizit die Modellierung funktionaler Überlagerungen und alternativer Forschungsmeinungen vor, sodass im Rahmen der Formalisierung sowohl der Multifunktionalität intertextueller Schreibweisen (vgl. Kocher 2010, S. 179) als auch der maßgeblich auf produktivem Dissens basierenden Dynamik des literaturwissenschaftlichen Diskurses Rechnung getragen wird.
            Als Ausgangspunkt zur formalen Beschreibung intertextueller Phänomene dient ein situationstheoretischer Ansatz, welcher die Brücke zwischen literaturwissenschaftlicher Analyse und technischer Modellierung darstellt. Die Situationstheorie bietet sich an, da sie im Sinne der angestrebten Beschreibung der Intertextualität einen mehrstufigen Formalismus zur Verfügung stellt, welcher Informationen und Informationsflüsse in Kontextabhängigkeit beschreibt: Basale Phänomene werden durch basale Informationseinheiten (sog. „Infone“) beschrieben, welche wiederum „Situationen“ als Phänomene einer höheren Ordnung aus der Perspektive eines oder mehrerer „Agenten“ zu bilden erlauben.
            Somit kann formal unterschieden werden zwischen der Modellierung konkreter (sprachlicher, inhaltlicher, stilistischer) Texteigenschaften und -relationen (beschrieben als Infone) und der Klassifizierung der modellierten Informationseinheiten im Sinne ihrer Funktion sowie einer durch sie indizierten, Kontext-abhängigen Wirkung (beschrieben als Situationen). Im Gegensatz zu technischen Beschreibungssprachen (wie etwa RDF oder OWL) liefert die Situationstheorie einen Formalismus, welcher zunächst frei von umsetzungsspezifischen Einschränkungen (wie etwa festgelegten Datentypen oder Objekthierarchien) ist, die sich ungewollt perspektivierend auf die Modellierung auswirken können.
                     Für die Beschreibung literarischer Texte erweist sich der situationstheoretische Formalismus also als besonders geeignet, da er Modellierungsfreiheit mit der für die technische Umsetzung notwendigen formalen Strenge vereint.
                
         
         
            Ausblick
            Das Modell wird sukzessive unter Einbezug literarischer Texte verschiedener Textsorten und Publikationszeiträume getestet und weiterentwickelt. An diese sukzessive formale Strukturierung anknüpfend wird geprüft, inwieweit etablierte Beschreibungssprachen bei einer technischen Umsetzung des Modells Anwendung finden können. Insbesondere etablierte Sprachen aus dem Umfeld elektronischer Publikation sollen auf ihre Anwendbarkeit bzw. Möglichkeiten der Erweiterung hin betrachtet werden. Dies sind im Rahmen der durch das W3C beschriebene Standards für Verweisstrukturen Sprachen wie XPath, XLink oder XPointer (vgl. einschlägig Wilde/Lowe 2003), für semantische Auszeichnungen die Sprachen des Semantic Web wie RDF oder OWL. Dies schließt – unabhängig von der konkreten Sprache – die Berücksichtigung unterschiedlicher Auszeichnungskonzepte wie bspw. Standoff- in Abgrenzung zu Inline-Markup ein (vgl. Banski 2010).
                    
            
         
      
      
         
             Meister 2012, S. 112 betont aufgrund der Dynamik und historisch-kulturellen Dependenz von Sprache zurecht, dass „[t]he problems posed by the interpretation of literary texts are thus not an exceptional, but rather an exemplary case“. Dennoch erfolgt literarische (bzw. allgemein künstlerische) Kommunikation im Rahmen einer spezifischen „Kommunikationslogik“, die sich von der alltagssprachlicher Kommunikationssituationen unterscheidet (vgl. Spoerhase 2007, S. 414–418).
             Vgl. zur Unterscheidung zwischen „modeling for understanding“ und „modeling for production“ Eide 2014.
            „Schreibweisen“ ist hierbei nicht intentionalistisch, sondern im Sinne von Textstrukturen mit Verweisfunktion zu verstehen.
             Vgl. hierzu u. a. die Ausführungen zur visuellen Analyse in John u. a. 2016. Einen komplexeren, narratologisch ausgerichteten Ansatz verfolgt das Projekt heureCLÉA (vgl. hierzu Gius/Jacke 2015). Das Projekt ist daher in seiner Orientierung an konkreten literaturwissenschaftlichen Methoden beispielhaft, verfolgt aber mit seiner Ausrichtung auf innertextuelle Strukturen ein grundsätzlich anderes Ziel als unser Projekt.
             Wagner/Mehler/Biber 2016, S. 90 verstehen ihr Projekt daher auch eher im Sinne einer Vorstufe für die „Erschließung des intertextuellen Potentials eines je gegebenen literarischen Texts“. Ihr methodischer Ansatz „zielt nicht auf die 
                            Implementierung literarischer Intertextualität“.
                        
             Im Gegensatz dazu ist bspw. der in Heßbrüggen-Walter 2015 dargestellte Ansatz unmittelbar RDF-basiert gedacht.
             Zahlreiche rechnergestützte, aber proprietäre Anwendungen wurden im Verlauf der Entwicklung der Situationstheorie vorgestellt (vgl. einschlägig etwa Tin/Akman 1994). Neuere Ansätze schlagen die Verwendung etablierter Sprachen, wie insb. durch das Semantic Web gegeben, vor (Kokar/Matheus/Baclawski 2009).
         
         
            
               Bibliographie
               
                  Banski, Pjotr (2010): “Why TEI stand-off annotation doesn't quite work: and why you might want to use it nevertheless”, in: 
                        Proceedings of Balisage: The Markup Conference 2010.
                    
               
                  Barwise, Jon / Perry, John (1983): 
                        Situations and Attitudes. Cambridge: Bradford Book, MIT Press.
                    
               
                  Bauer Lucca, Eva (2001): 
                        Versteckte Spuren: eine intertextuelle Annäherung an Thomas Manns Roman “Doktor Faustus”. Wiesbaden: Deutscher Universitätsverlag.
                    
               
                  Broich, Ulrich / Pfister, Manfred (eds.) (1985): 
                        Intertextualität. Formen, Funktionen, anglistische Fallstudien. Tübingen: Niemeyer.
                    
               
                  Devlin, Keith J. (1990): Logic and Information. Cambridge: Cambridge University Press.
                    
               
                  Dudzik, Yvonne (2017): 
                        Geschichten bereichern die Geschichte: Intertextualität als Untersuchungskategorie in Uwe Johnsons “Jahrestage”. Göttingen: V&R unipress.
                    
               
                  Eide, Øvind (2014): “Ontologies, Data, and TEI”, in: 
                        Journal of the Text Encoding Initiative 8: 1–22.
                    
               
                  Genette, Gérard (1993): 
                        Palimpseste. Die Literatur auf zweiter Stufe. Frankfurt a. M.: Suhrkamp.
                    
               
                  Gius, Evelyn / Jacke, Janina (2015): “Informatik und Hermeneutik. Zum Mehrwert interdisziplinärer Textanalyse”, in: Baum, Constanze / Stäcker, Thomas (eds.): 
                        Grenzen und Möglichkeiten der Digital Humanities. (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 1) 10.17175/sb001_006.
                    
               
                  Hempfer, Klaus W. (1991): „Intertextualität. Systemreferenz und Strukturwandel: Die Pluralisierung des erotischen Diskurses in der italienischen und französischen Renaissance-Lyrik (Ariost, Bembo, Du Bellay, Ronsard)“, in: Titzmann, Michael (ed.): 
                        Modelle des literarischen Strukturwandels. Tübingen: Niemeyer, 7–43.
                    
               
                  Heßbrüggen-Walter, Stefan (2015): “What People Said: The Theoretical Foundations of a Minimal Doxographical Ontology and Its Use in the History of Philosophy”, in: Baum, Constanze / Stäcker, Thomas (eds.): 
                        Grenzen und Möglichkeiten der Digital Humanities. (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 1). DOI: 10.17175/sb001_001 
                    
               
                  John, Markus / Lohmann, Steffen / Koch, Steffen / Wörner, Michael / Ertl, Thomas (2016): „Visual Analytics for Narrative Texts. Visualizing Characters and their Relationships as Extracted from Novels”, in: 
                        Proceedings of the 7th International Conference on Information Visualization Theory and Applications. Rom, Italien: SciTePress [Preprint: 
                        http://www.visualdataweb.org/publications/2016_IVAPP_VA-for-Narrative_preprint.pdf].
                    
               
                  Kocher, Ursula (2007): “Im Gewirr der Fäden: Intertextualitätstheorie und Edition”, in: Falk, Rainer / Mattenklott, Gert (eds.): 
                        Ästhetische Erfahrung und Edition. Tübingen: Max Niemeyer, 175–185.
                    
               
                  Kokar, Mieczyslaw M. / Matheus, Christopher J. / Baclawski, Kenneth (2009): „Ontology-based situation awareness“, in: 
                        Information Fusion 10, 1. St. Louis, MO, USA: Elsevier, 83–98.
                    
               
                  Tin, Erkan / Akman, Varol (1994): „Computational Situation Theory”, in: Hoebel, Louis J. / Powers, David (eds.): 
                        SIGART Bulletin 5, 4. New York, NY, USA: ACM, 4–17.
                    
               
                  Meister, Jan-Christoph (2012): „Crowdsourcing ‚True Meaning‘: A Collaborative Markup Approach to Textual Interpretation“, in: Deegan, Marilyn (ed.): 
                        Collaborative Research in the Digital Humanities. A Volume in Honour of Harold Short, on the Occasion of his 65th Birthday and his Retirement. Farnham: Ashgate 2012, 106–122.
                    
               
                  Spoerhase, Carlos (2007): 
                        Autorschaft und Interpretation. Methodische Grundlagen einer philologischen Hermeneutik. Berlin/New York: De Gruyter.
                    
               
                  Verweyen, Theodor / Wittig, Gunther (2010): 
                        Einfache Formen der Intertextualität: Theoretische Überlegungen und historische Untersuchungen. Paderborn: mentis.
                    
               
                  Wagner, Benno / Mehler, Alexander / Biber, Hanno (2016): “Transbiblionome Daten in der Literaturwissenschaft. Texttechnologische Erschließung und digitale Visualisierng intertextueller Beziehungen digitaler Korpora“, in: 
                        Konferenzabstracts DHd 2016 Modellierung, Vernetzung, Visualisierung. Die Digital Humanties als fächerübergreifendes Forschungsparadigma
                  http://dhd2016.de/boa.pdf, 88–94.
                    
               
                  Wilde, Erik / Lowe, David (2003): 
                        XPath, XLink, XPointer, and XML: A Practical Guide to Web Hyperlinking and Transclusion. Boston, MA: Addison-Wesley.
                    
            
         
      
   



      
         
            Einleitung
            Der vorliegende Beitrag enthält methodische Überlegungen und Experimente zu “Zeta”, einem von John Burrows (2007) vorgeschlagenen Maß für die Distinktivität oder “keyness” von textuellen Merkmalen (Wortformen, Lemmata, etc.). Mit solchen Maßen werden Merkmale ermittelt, die für eine bestimmte Gruppe von Texten gegenüber einer Vergleichsgruppe charakteristisch sind.
            Das Exposé gibt einen Überblick zu solchen Maßen, bevor die Funktionsweise von Zeta erläutert wird. Aufbauend auf einer Neu-Implementierung in Python (“pyzeta”, https://github.com/cligs/pyzeta) und Vorarbeiten (Schöch im Druck) liegt der spezifische Forschungsbeitrag dann in den folgenden Schritten: erstens werden mehrere Varianten von Zeta vorgeschlagen und implementiert; zweitens werden Verfahren zum Vergleich und der Evaluation der Ergebnisse erprobt. Ziel ist es, Zeta in seiner Funktionsweise und in seiner Beziehung zu vergleichbaren Maßen besser zu verstehen und vorhandene Nachteile des Maßes durch gezielte Modifikationen zu beheben.
         
         
            Überblick und Stand der Forschung
            Die vergleichende, kontrastierende Analyse zweier Gruppen von Texten ist ein in den Sprach- und Literaturwissenschaften weit verbreitetes Verfahren. Entsprechend wurden zahlreiche Maße der Distinktivität oder “keyness” von Merkmalen entwickelt und für vielfältige Fragestellungen eingesetzt. Die grundlegende Annahme solcher Maße ist, dass ein Merkmal nicht schon durch seine reine Häufigkeit in einer Textgruppe für diese charakteristisch ist, sondern dass dies auch davon abhängt, wie häufig das Merkmal in einer Vergleichsgruppe ist. Diejenigen Merkmale bekommen einen besonders hohen Wert zugewiesen, die in der einen Gruppe sehr häufig sind und zugleich in der Vergleichsgruppe sehr selten sind (Scott 1997, 236). Man kann vier Arten von Verfahren unterscheiden: 
            
               Verfahren, welche erwartete und beobachtete Werte vergleichen (wie “log-likelihood-ratio”; siehe Rayson und Garside 2000);
               Verfahren, die eine Gewichtung der Häufigkeiten vornehmen (wie “tf-idf”, “term frequency / inverse document frequency”; siehe Robertson 2004);
               Statistische Hypothesentests, die Verteilungseigenschaften vergleichen (wie “Welch’s t-Test”; siehe Bortz und Schuster 2010); 
               Dispersionsmaße, die nicht die Häufigkeit, sondern den Grad der konsistenten Verwendung von Merkmalen in Beziehung setzen (wie “deviation of proportions”; Gries 2008). 
            
            Die praktische Bedeutung von Distinktivitätsmaßen ist daran erkennbar, dass Korpusanalyse-Software meist eine entsprechende Funktion anbietet, so “keyness” in WordCruncher (Scott 1997) oder “spécificity” in TXM (Heiden et al. 2012). Kilgariff 2004 und Lijfijt et al. 2014 sind wichtige Arbeiten zur Evaluation von Distinktivitätsmaßen. 
         
         
            Was ist Zeta?
            Das von John Burrows (2007) vorgeschlagene “Zeta” beruht auf einem Dispersionsmaß. Vor der Berechnung werden die Texte in kleinere Segmente gesplittet, wobei die Segmentlänge ein wichtiger Parameter ist. Dann wird für jedes Merkmal der Anteile der Segmente erhoben, in denen das Merkmal mindestens einmal vorkommt (die “document proportion”). Von diesem Anteil in der untersuchten Gruppe wird der entsprechende Anteil in der Vergleichsgruppe subtrahiert, woraus sich ein Zeta-Wert zwischen -1 und 1 ergibt.
            Ein Effekt dieser Berechnungsweise ist, dass Zeta Inhaltswörter als distinktive Wörter favorisiert, Funktionswörter sowie Eigennamen hingegen penalisiert. Daraus ergibt sich eine hohe Interpretierbarkeit der Ergebnisse, die Zeta im Vergleich zu anderen Maßen für die (digitalen) Literaturwissenschaften besonders attraktiv macht. Ein Nachteil ist, dass Merkmale durch die Subtraktion niemals einen Zeta-Wert bekommen können, der höher ist als ihre “document proportion” in der untersuchten Textgruppe, selbst wenn sie gegenüber der Vergleichsgruppe deutlich überrepräsentiert sind (Abbildung 1, Wörter in den roten Rahmen; Schöch im Druck).
            
               
                  
                  Abbildung 1: Scatterplot der Wörter in zwei Textgruppen (französische Komödien und Tragödien): “document proportions” der Wörter in zwei Textgruppen (x- und y-Achse) und resultierende Zeta-Werte (Distanz von der Diagonale).
               
               Eine bekannte Implementierung von Zeta existiert im stylo-Paket für Rin der Funktion "oppose()" (Eder et al. 2016). Abbildung 2 zeigt für ein Beispiel die Ergebnisdarstellung in der hier verwendeten “pyzeta”-Implementierung. 
            
            
               
                  
                  Abbildung 2: Positive und negative Keywords für französische Komödien (rechts) im Vergleich mit Tragödien (links). Zeta-Werte auf der horizontalen Achse.
               
            
            Anwendungsbeispiele von Zeta gibt es in der Shakespeare-Forschung (Craig und Kinney 2009), der modernen englischsprachigen Literatur (Hoover 2010; Weidman und O’Sullivan 2017) und der Romanistik (Schöch im Druck). In der zuletzt genannten Arbeit zum französischen Theater der Klassik und Aufklärung konnte nicht nur die erwartbare, klare Differenzierung von Komödien und Tragödien gezeigt werden. Vielmehr wurde auch die spezifische Verortung der Tragikomödien deutlich, die nicht als Mischform zwischen Komödien und Tragödien zu verstehen sind, sondern eine besondere Affinität zur Tragödie aufweisen (Abbildung 3). 
            
               
               Abbildung 3: Hauptkomponentenanalyse auf Grundlage der 50 Wörter, die für Komödien und Tragödien die höchsten Zeta-Werte erhalten. Komödien in rot, Tragödien in blau, Tragikomödien in grün. Quelle: Schöch im Druck.
            
         
         
            Varianten von Zeta
            Ausgehend von der ursprünglichen Formulierung von Zeta durch Burrows als Subtraktion der “document proportions” lassen sich mehrere Faktoren identifizieren, die zur Formulierung von Varianten von Zeta geeignet erscheinen: 
            
               Statt “document proportions” werden relative Häufigkeiten verwendet; 
               Statt der Subtraktion erfolgt eine Division;
               Statt nicht-transformierter Werte wird eine log2-Transformation der Werte vorgenommen.
            
            Die Kombination dieser Faktoren ergibt 8 Varianten von Zeta (Tabelle 1). 
         
         
            
               
               document proportions
               relative Häufigkeiten
            
            
               
               keine Transformation
               log2-Transformation
               keine Transformation
               log2-Transformation
            
            
               Subtraktion
               sd0
               sd2
               sr0
               sr2
            
            
               Division
               dd0
               dd2
               dr0
               dr2
            
         
         Tabelle 1: Übersicht über die getesteten Varianten von Zeta. Die Variante mit Label „sd0“ entspricht Burrows‘ Zeta. 
         Einige der Varianten sind mathematisch gut motivierbar und versprechen, den oben genannten Nachteil der begrenzten Werte für bestimmte Wörter auszugleichen und damit Zeta zu verbessern, es wurden aber alle implementiert und auf zwei Datensätzen evaluiert.
         
            Datensätze
            Es wurden zwei unterschiedliche Korpora verwendet. Erstens ein Korpus aus der textbox-Sammlung (Schöch et al. 2017), das Romane enthält, die zwischen 1880 und 1940 veröffentlicht wurden: jeweils 24 Texte aus Spanien und aus Lateinamerika (ca. 2,8 Millionen Tokens). Zweitens, ein Teil der Sammlung 
                    Théâtre classique (Fièvre 2007-2017) mit französischen Dramen: 134 Tragödien und 158 Komödien aus Klassik und Aufklärung (ca. 4,9 Millionen Tokens).
                
         
         
            Evaluation
            Die 8 Varianten führen zu unterschiedlichen Wortlisten, geordnet nach absteigenden Zeta-Werten. Vergleicht man den Beginn der Wortlisten für zwei Varianten, fällt auf, dass es wie erwartet zu Verschiebungen im Rang der distinktivsten Wörter kommt. 
            
               Ähnlichkeit der Varianten
               Um den Grad der Abweichung der Ergebnisse für alle Varianten zueinander auf der Grundlage längerer Wortlisten zu erheben, ist ein quantifizierendes Verfahren unerlässlich. Ein Ansatz ist, ein Clustering der Maße auf Basis der Zeta-Werte ihrer Wörter vorzunehmen (Abbildung 4). 
               
                  
                     
                     Abbildung 4: Dendrogramm auf Grundlage einer Cluster Analyse der Zeta-Werte für die 8 Zeta-Varianten (Théâtre-classique-Datensatz; 500 distinktive Wörter; Ward-Verfahren).
                  
                  Abbildung 4 zeigt, dass der wichtigste Faktor für die Unterschiedlichkeit der Varianten ist, ob subtrahiert oder dividiert wird (zwei Haupt-Cluster). Die beiden anderen Variablen spielen eine viel kleinere Rolle. (Die Ergebnisse weiterer Analysen, u.a. auf Basis der RBO-Ähnlichkeit (“ranked biased order”, Webber et al. 2010), werden aus Platzgründen hier nicht diskutiert.)
               
            
            
               Evaluation mit Klassifikationstask
               Unabhängig von den Beziehungen der Varianten zueinander stellt sich die Frage, welche der Varianten von Zeta besonders gut distinktive Wörter identifiziert. Dabei kann zur Evaluation nicht auf einen Goldstandard zurückgegriffen werden: eine händische Annotation der Wörter nach dem Grad ihrer Distinktivität ist nicht möglich, weil niemand das zugrunde liegende Korpus überblicken kann. Die Qualität eines Distinktivitätsmaßes kann aber evaluiert werden, indem es als Merkmalsselektor für einen Klassifikationstask verwendet wird.
               Wenn die durch Zeta am höchsten bewerteten Wörter als Features für einen Klassifikator verwendet werden, sollte dieser Klassifikator eine höhere Genauigkeit erreichen als bei einfacher Verwendung der häufigsten Wörter. Tatsächlich lässt sich dieser Effekt auf dem Korpus der spanisch-sprachigen Romane nachweisen (Tabelle 2). Zur Ermittlung einer Baseline wurde für die Klassifikation in spanische und lateinamerikanische Romane ein linearer SVM-Classifier auf den häufigsten 80, nach TF-IDF gewichteten Wörtern (ohne Stoppwörter) trainiert. Dieser Classifier erreichte lediglich eine Klassifikationsgüte (F1-Score) von 0.49, ist also nicht vom Zufall zu unterscheiden.
               Trainiert man stattdessen auf den 40 distinktivsten Wörtern nach Zeta (oder einer der Varianten), lassen sich Genauigkeiten von deutlich über 90% erzielen. Diese Genauigkeit kann nicht als tatsächliches Klassifikationsergebnis gesehen werden, da die distinktivsten Merkmale auf dem gesamten Korpus extrahiert wurden, ohne Aufteilung in Trainings- und Testdaten. Dennoch zeigt das Ergebnis, dass die von Zeta selektierten Merkmale tatsächlich sehr nützlich für eine Klassifikation sind. Zudem zeigen sich deutliche Unterschiede in der Performanz je nach verwendeter Variante: während mit “sd0” (=Burrows Zeta) 81% Genauigkeit erreicht wird, erhöht sich dieser Wert bei der Variante mit log2-Transformation, “sd2”, auf 98%. 
               
                  
                     baseline
                     sd0
                     sd2
                     sr0
                     sr2
                     dd0
                     dd2
                     dr0
                     dr2
                  
                  
                     0.49
                     0.81
                     0.98
                     0.48
                     0.83
                     0.79
                     0.85
                     0.75
                     0.79
                  
               
               
                  Tabelle 2: Klassifikationsergebnisse bei Verwendung einer linearen SVM, trainiert auf den 40 am höchsten gerankten Wörtern verschiedener Maße im Vergleich zur Baseline. Alle Werte sind der Durchschnitt einer dreifachen Kreuzvalidierung.
               
            
         
         
            Fazit
            Wichtigste Ergebnisse dieses Beitrags sind ein differenziertes Verständnis davon, wie Zeta im Kontext anderer Distinktivitätsmaße einzuordnen ist und wie bestimmte mathematischen Parameter sich auf die Ergebnislisten auswirken: als ein auf dem Grad der Dispersion der Merkmale beruhendes Maß, dessen entscheidende Eigenschaft die Subtraktion der Werte ist. Ein weiteres wesentliches Ergebnis sind die beiden vorgeschlagenen Strategien zum Vergleich und der Evaluation von Distinktivitätsmaßen, wenn eine direkte Evaluation auf Goldstandard-Daten nicht möglich ist.
            Nächste Schritte: Wir möchten als weitere Evaluationsstrategie künstliche Texte generieren, in denen wir kontrolliert einzelne Wörter mit unterschiedlich stark abweichender Verteilung einfügen. So können verschieden Zeta-Varianten direkt dahingehend evaluiert werden, wie gut sie diese Wörter korrekt identifizieren. Zudem möchten wir neben der “document proportion” von Zeta ein weiteres Dispersionsmaß, die von Gries (2008) vorgeschlagene “deviation of proportions” als Grundlage für eine weitere Zeta-Variante verwenden. Schließlich möchten wir untersuchen, ob die hohe Interpretierbarkeit des Original-Zeta bei den Varianten mit noch höherer Klassifikationsgüte erhalten bleibt.
            Eine separate Untersuchung ist in Vorbereitung zu zwei eng zusammenhängenden Fragen: wie sich unterschiedliche Segmentlängen einerseits auf die Ergebnisse auswirken, und wie sich die Ergebnisse verändern, wenn unterschiedlich lange Texte nicht mit allen Segmenten in die Berechnung eingehen, sondern aus jedem Einzeltext zufällig eine identische Anzahl von Segmenten gesampelt wird. 
            Übergeordnetes Ziel all dieser Arbeiten zu Zeta ist es letztlich weniger, ein perfektes Distinktivitätsmaß zu identifizieren, als ein justierbares Maß vorzuschlagen, bei dem in Abhängigkeit von Daten und Forschungsfragen dynamisch Parameter verändert und die resultierenden Verschiebungen in den Ergebnissen visualisiert werden können. 
         
      
      
         
            
               Bibliographie
               
                  Bortz, Jürgen, and Christof Schuster (2010). 
                        Statistik für Human- und Sozialwissenschaftler. 7. Auflage. Berlin: Springer.
                    
               
                  Burrows, John (2007). “All the Way Through: Testing for Authorship in Different Frequency Strata.” 
                        Literary and Linguistic Computing 22, no. 1: 27–47. doi:10.1093/llc/fqi067.
                    
               
                  Craig, Hugh, and Arthur F. Kinney, eds. (2009). 
                        Shakespeare, Computers, and the Mystery of Authorship. 1st ed. Cambridge University Press.
                    
               
                  Eder, Maciej, Mike Kestemont, and Jan Rybicki(2016). “Stylometry with R: A Package for Computational Text Analysis.” 
                        The R Journal 16, no. 1: 1–15.
                    
               
                  Fièvre, Paul, ed. (2007-2013). “Théâtre classique.” Paris: Université Paris-IV Sorbonne. http://www.theatre-classique.fr.
                    
               
                  Gries, Stefan Th. (2008). “Dispersions and Adjusted Frequencies in Corpora.” 
                        International Journal of Corpus Linguistics 13, no. 4: 403–37. doi:10.1075/ijcl.13.4.02gri.
                    
               
                  Heiden, Serge (2010). “The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme.” In 
                        24th Pacific Asia Conference on Language, Information and Computation - PACLIC24, edited by Ryo Otoguro, Kiyoshi Ishikawa, Hiroshi Umemoto, Kei Yoshimoto, and Yasunari Harada, 389–98. Sendai: Waseda University. https://halshs.archives-ouvertes.fr/halshs-00549764/en.
                    
               
                  Hoover, David L. (2010). “Teasing out Authorship and Style with T-Tests and Zeta.” In 
                        Digital Humanities Conference. London: ADHO. http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-658.html.
                    
               
                  Kilgarriff, Adam (2001). “Comparing Corpora.” 
                        International Journal of Corpus Linguistics 6, no. 1: 97–133. doi:10.1075/ijcl.6.1.05kil.
                    
               
                  Lijffijt, Jefrey, Terttu Nevalainen, Tanja Säily, Panagiotis Papapetrou, Kai Puolamäki, and Heikki Mannila (2014). “Significance Testing of Word Frequencies in Corpora.” 
                        Digital Scholarship in the Humanities 31, no. 2: 374–97. doi:10.1093/llc/fqu064.
                    
               
                  Rayson, Paul, and R. Garside (2000). “Comparing Corpora Using Frequency Profiling.” In 
                        Proceedings of the Workshop on Comparing Corpora, 1–6. Hong Kong: ACM.
                    
               
                  Robertson, Stephen (2004). “Understanding Inverse Document Frequency: On Theoretical Arguments for IDF.” 
                        Journal of Documentation 60, no. 5 : 503–20.
                    
               
                  Schöch, Christof (im Druck). “Zeta für die kontrastive Analyse literarischer Texte. Theorie, Implementierung, Fallstudie.” In 
                        Quantitative Verfahren in der Literaturwissenschaft. Von einer Scientia Quantitatis zu den Digital Humanities, edited by Andrea Albrecht, Sandra Richter, Marcel Lepper, Marcus Willand, and Toni Bernhart. Berlin: de Gruyter. https://cligs.hypotheses.org/files/2017/09/Schoech_2017-preprint_Zeta-fuer-die-kontrastive-Analyse.pdf.
                    
               
                  Schöch, Christof, José Calvo Tello, Ulrike Henny-Krahmer, and Stefanie Popp (angenommen). “The CLiGS Textbox: Building and Using Collections of Literary Texts in Romance Languages Encoded in XML-TEI.” 
                        Journal of the Text Encoding Initiative http://cligs.hypotheses.org/files/2017/09/Schoech-et-al_2017_Textbox.pdf.
                    
               
                  Scott, Mike (1997). “PC Analysis of Key Words and Key Key Words.” 
                        System 25, no. 2: 233–45.
                    
               
                  Webber, William, Alistair Moffat, and Justin Zobel (2010). “A Similarity Measure for Indefinite Rankings.” 
                        ACM Trans. Inf. Syst. 28, no. 4: 20:1–20:38. doi:10.1145/1852102.1852106.
                    
            
         
      
   



      
         
            I. Motivation 
            In den textbasierenden Geisteswissenschaften ist eine optimale Beschaffenheit der zu Grunde liegenden Texte eine wesentliche Bedingung für wissenschaftliches Arbeiten. Wie sehr sich die Bedeutung eines Textes schon durch scheinbar minimale Unterschiede wie die der Interpunktion verschieben kann, hat sich einer breiteren Öffentlichkeit zuletzt in der Diskussion über einen Punkt in einer Abschrift der amerikanischen Unabhängigkeitserklärung gezeigt (The Atlantic 7/2014).
            Während die historische Diskussion über digitale Editionen vielfach mit dokumentarischen Editionstypen bzw. der Frage nach dem Dokumentcharakter der Grundlagen einer Edition (Manuskripte etc.) und der Frage nach der Essenz des Textbegriffes beschäftigt ist (P. Sahle 2013), ergibt sich insbesondere für diejenigen historischen und altertumswissenschaftlichen Disziplinen, die weiterhin auf die klassische Form kritischer Texteditionen angewiesen sind, ein anderes Problem: Da sie traditionell in hohem Maße mit Texten beschäftigt sind, die notwendig als Interpretationen und Rekonstruktionen gelten müssen (West 1973,32), ist hier vor allem die Frage nach der Beschaffenheit und Begründung der zum Teil massiven editorischen Eingriffe nicht nur in die Lesart, sondern auch in den Umfang und die Zuschreibung von Texten, etwa im Falle sogenannter „Fragmente“, von vorrangiger Bedeutung. Der Workshop soll in diese allgemeine Problematik einführen und sich im Hinblick auf das Thema der Konferenz dem Bereich Kritik der digitalen Geisteswissenschaften (traditionelle Fächer und DH) zuordnen. Den Teilnehmenden soll dies praktisch anhand der Anwendung der Software eCOMPARATIO vermittelt werden. eCOMPARATIO bietet eine einfache Möglichkeit der Kollationierung verschiedener Varianten eines Textes und ermöglicht eine digitale, auf dem automatischen Textvergleich beruhende Form des kritischen Apparates.
            Dazu sind einzelne Use Cases ausgearbeitet worden (anhand der Fragmentsammlung der Vorsokratiker von Diels/Kranz [griechisch], der Res Gestae des Augustus [lateinisch], des Genfer Gelöbnisses [deutsch], der Gettysburg Address von Abraham Lincoln [englisch]), anhand derer den Teilnehmern die Funktionalitäten demonstriert werden.
         
         
            II. Die Software
            Im Projekt eCOMPARATIO, das von 2014 bis 2016 von der DFG gefördert wurde und in Leipzig am Lehrstuhl für Alte Geschichte in Kooperation mit dem Center of E-Humanities in History and Social Sciences (ICE) am Max-Weber-Kolleg für kultur- und sozialwissenschaftliche Studien durchgeführt wurde, ist vor dem Hintergrund dieser Problematik ein einfach zu bedienendes Tool für den Vergleich prinzipiell beliebig vieler und beliebig langer digitalisierter Editionen vorrangig griechischer und lateinischer, technisch gesehen aber auch sämtlicher anderer in einem UNICODE-Format zugänglicher Texte entstanden. Der Textvergleich arbeitet auf der Basis der Identifikation von Ungleichheiten. Hierbei reicht die Spanne der programmiertechnisch unterscheidbaren Differenzen von Ungleichheiten innerhalb von Wörtern und Buchstabenfolgen bis hin zu vertauschten Passagen. Im Unterschied zu anderen Vergleichsprogrammen benötigen Anwender weder eine Installation von Python, eine Levenshtein Bibliothek oder ein Java-Plugin, sondern erhalten eine für die Anwender ohne Vorkenntnisse sofort im Browser nutzbare und mit Copy-and-Paste einfach zu bedienende Oberfläche. Auch Fallbeispiele, eine Textdokumentation sowie Videoanleitungen stehen zur Verfügung. In zwei derzeit (2016-17) von der DFG und der Andrew W. Mellon Foundation geförderten Projekten an der Universität Leipzig in Kooperation mit Christopher Blackwell von der Furman University in Greenville, SC/ USA, erfolgt eine Einbindung des Vergleichstools in ein Interface (zu dem Protokoll Canonical Text Services (CTS) als Teil der CITE Architecture: http://cite-architecture.github.io/cts/), dessen Ziel nicht nur die Bereitstellung möglichst vieler digitalisierter Editionen einzelner Texte, sondern damit auch erweiterte Möglichkeiten des Textvergleiches, der Suche nach Parallelstellen und weiterer Formen des sog. „Text-Mining“ ist.
            
               Die Texteingabemaske
               Die eigenständige Version der Vergleichssoftware (unter http://ecomparatio.net/~khk/instanzen/ecompp/ ist ein Beispiel frei zum Gebrauch bereitgestellt) ermöglicht weiterhin eine browserbasierte oder auch offline verwendbare Anwendung für Texte nach dem Copy-and-Paste-Prinzip: Hier können beliebig viele Versionen eines Textes eingefügt werden.
            
            
               Die Darstellung
               Das Tool ermöglicht verschiedene Ausgaben des Vergleiches:
            
            
               
               Abbildung 1, Detail-Vergleich
            
            
               
               Abbildung 2, Synopse
            
            
               
               Abbildung 3, Buch-Darstellung mit digitalem Apparat zum Textvergleich
            
            
               Ausgabe/Export der Ergebnisse
               Die Ausgabe der Ergebnisse kann zur weiteren Integration in den Arbeitsprozess als TEI XML oder LaTeX Code erfolgen. Will man die Vergleichsdaten abfragen, so steht ein JSON Interface zur Verfügung, über das weitere Software angebunden werden kann. 
            
         
         
            III. Ziele und Zielgruppe
            Ziel des Workshops ist eine Einführung in die Anwendung der eCOMPARATIO Vergleichssoftware, auch in Abgrenzung zu bereits verfügbarer Software (collateX, Juxta) mit ähnlichen Anwendungsbereichen. Dazu soll zunächst eine Einführung in die Relevanz der Frage nach scheinbar marginalen textlichen Unterschieden anhand prägnanter Beispiele aus verschiedenen historischen Epochen und in verschiedenen wissenschaftlichen Diskurssprachen gegeben werden (anhand griechischer, lateinischer, englischer und deutscher Texte).
            Darüber hinaus soll den Teilnehmenden die Möglichkeit gegeben werden, auch Texte aus den eigenen Disziplinen oder beliebige im Internet verfügbare Versionen von Texten selbst miteinander zu vergleichen und sich so mit den Funktionen der Software vertraut zu machen.
            Zielgruppe des Workshops sind prinzipiell alle Interessierten aus dem Bereich der textbasierten Geisteswissenschaften. Dabei sind diejenigen, die, wie oben angesprochen, vor allem mit der Vielzahl digitalisierter Editionen arbeiten, ebenso angesprochen wie solche, die selbst an der Erstellung von Editionen etc. arbeiten, und für die eCOMPARATIO ein hilfreiches Mittel bei der Sichtung und Kollationierung von Textzeugnissen sein kann.
            Besondere technische Kenntnisse sind nicht erforderlich, da sich eCOMPARATIO bewusst an Anwender richtet, die entweder selbst an einer kritischen Edition arbeiten oder auf der Grundlage mehrerer kritischer Editionen wissenschaftliche Fragestellungen verfolgen.
         
         
            IV. Ablauf und Teilnehmerzahl
            Im Workshop soll die browsergestützte Version zur Anwendung kommen, ein Download der Software wird aber auch möglich sein. Neben der bereits beschriebenen Einführung in die wissenschaftliche Grundproblematik erfolgt zunächst eine kurze Präsentation eigener Ergebnisse im Zuge der Forschung mit eCOMPARATIO.
            Hauptsächlich soll im praktischen Teil den Teilnehmenden die Möglichkeit gegeben werden, auf ihren eigenen Rechnern in der browsergestützten Version selbst Vergleiche der für sie relevanten Texte vorzunehmen, die mitgebracht werden sollten oder vor Ort aus Onlinedatenbanken heruntergeladen werden können.
            Während des Workshops soll dann von Seiten der Organisatoren auf eventuelle individuelle Probleme und Schwierigkeiten eingegangen werden. Die Organisatoren des Workshops erhoffen sich hiervon einen eigenen Erkenntnisgewinn auch im Hinblick auf die Anwendungsmöglichkeiten und notwendige Ergänzungen im Hinblick auf die Arbeit gerade mit nicht-lateinischen oder nicht-griechischen Texten.
         
         
            V. Ablauf
            Die TeilnehmerInnen erhalten vorab eine detaillierte Anleitung (Handbuch eCOMPARATIO).
            Im eigentlichen Workshop werden die jeweiligen Arbeitsschritte von einem der Organisatoren live vorgeführt (dafür wird ein leistungsstarker Beamer benötigt). Die konkreten Inhalte orientieren sich dabei an den bisher von den Organisatoren ausgearbeiteten Use Cases (s.o.), die von den Organisatoren präsentiert werden.
            Während des Workshops werden wir bei auftretenden Fragen und Problemen den Teilnehmenden helfend zur Seite stehen, da sie auch die Möglichkeit haben sollen, anhand eigener Texte zu arbeiten. Um eine möglichst gute Betreuung der TeilnehmerInnen gewährleisten zu können, sollte die Teilnehmerzahl 25-30 nicht überschreiten.
         
         
            VI. Organisatoren
            Charlotte Schubert ist Althistorikerin, hat zu Themen der Mentalitätsgeschichte, Medizin- und Wissenschaftsgeschichte sowie zu verschiedenen Bereichen der griechischen Geschichte gearbeitet; seit 2006 verantwortliche Koordinatorin in verschiedenen DH-Projekten, die vom BMBF (eAQUA, eXChange), der DFG (eCOMPARATIO, CTS, Etablierung eines Open Access Online eJournals: Digital Classics Online), der VolkswagenStiftung (Digital Plato) und der Andrew W. Mellon Foundation (CTS) gefördert wurden und werden.
            Hannes Kahl ist Informatiker, Berufserfahrung aus diversen DH-Projekten, Entwickler von eCOMPARATIO, arbeitet an der Weiterentwicklung von CTS und an einer Dissertation zu dem Thema „Form und Formalisierung – mit Anwendung innerhalb automatischer Ermittlung von Buchstabenwerten aus digitalen Abbildungen griechischer Lettern innerhalb wissenschaftlicher Editionen sowie deren digitaler Formatierung“ (Betreuer: Prof. Ch. Schubert, Alte Geschichte/Universität Leipzig/ Prof. O. Arnold, Informatik/FH Erfurt).
            Friedrich Meins ist Althistoriker und hat eine Dissertation zum Thema „Literarische Kritik, rhetorische Theorie und historische Methode bei Dionysios von Halikarnassos“ geschrieben. Im Bereich der eHumanities hat er in den Projekten „eAQUA“ und „eAQUA Dissemination“ an der Uni Leipzig sowie im Projekt „eCOMPARATIO“ am ICE der Universität Erfurt mitgearbeitet. Derzeit arbeitet er im von der Andrew W. Mellon Foundation geförderten Kooperationsprojekt der Universität Leipzig und der Furman University (Greenville, SC/ USA) „Annotating and Editing With Canonical Text Services (CTS)“.
            Oliver Bräckel ist Althistoriker und arbeitet an einer Dissertation zu Politischen Flüchtlingen im Römischen Reich. Im Bereich der eHumanities hat er im Projekt „eCOMPARATIO“ am ICE der Universität Erfurt sowie im Projekt eXChange an der Uni Leipzig mitgearbeitet. Derzeit arbeitet er im von der Andrew W. Mellon Foundation geförderten Kooperationsprojekt der Universität Leipzig und der Furman University (Greenville, SC/ USA) „Annotating and Editing With Canonical Text Services (CTS)“.
         
      
      
         
            
               Bibliographie
               
                  A. Olheiser, Have We Been Reading the Declaration of Independence All Wrong?, 
                        
                            https://www.theatlantic.com/entertainment/archive/2014/07/typo-could-mean-weve- been-reading-the-declaration-of-independence-all-wrong/373915/
                         (letzter Zugang 7.7.2017).
                    
               
                  P. Sahle, Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels, Norderstedt 2013, 3 Bde. (Schriften des Instituts für Dokumentologie und Editorik Bd. 7).
               
                  M.L. West, Textual Criticism and Editorial Technique, Stuttgart 1973.
               
                  Links:
               
               The Atlantic: https://www.theatlantic.com/entertain
                        ment/archive/2014/07/typo-could-mean-weve-been-reading-the-declaration-of-independence-all-wrong/373915/
                        (letzter Zugang 7.7.2017)
                    
               http://www.eaqua.net
               
                  http://www.ecomparatio.net/
               
               
                  http://www.ecomparatio.net/~khk/instanzen/ecompp/ (letzter Zugang17.9.2017)
               http://digital-plato.org
               http://digital-classics-online.eu
               
                  http://cite-architecture.github.io/cts/
                    
            
         
      
   



      
         
            
               Ein DH-Lehr-Lernprojekt zu den Gründungsurkunden der Jesuitenuniversität Paderborn
               Die „digitale Revolution“ greift zunehmend in komplexer Weise auf alle Lebensbereiche über. Im „digitalen Zeitalter“ ist auch ein Wandel in der Geschichtsschreibung zu beobachten, der schon bei der Unterscheidung analoger von digitalen Quellen sichtbar wird (Pfanzelter 2016: 85). Die Arbeitsweise von HistorikerInnen wird durch die Verfügbarkeit von Quellen in digitaler und digitalisierter Art verändert (Bernsen 2017: 295, Kelly 2013). Daraus ergibt sich nicht nur das Desiderat nach entsprechenden Kompetenzen im Umgang mit digitalen sowie digitalisierten Quellen, sondern auch nach einer „der digitalen Welt angepassten, technikgestützten Quellenkritik“ (Pfanzelter 2016: 93). Herausforderungen hinsichtlich des Umgangs mit Quellen lassen sich für alle Ebenen der Gesellschaft ableiten – von einer Schülerschaft, die zur Partizipation in der Gesellschaft befähigt werden soll, über (Lehramts-) Studierende, die dieses ermöglichen sollen, bis zu den FachwissenschaftlerInnen und Lehrenden an den Universitäten. Dementsprechend hoch ist die Relevanz, das im folgenden vorgestellte Projekt im Kontext der „Kritik der digitalen Vernunft“ zu diskutieren und einen besonderen Fokus darauf zu legen, welche und wie stark ausgeprägte digitale Kompetenzen HistorikerInnen benötigen, um dem „digitalen Zeitalter“ gerecht zu werden und den kritischen Anforderungen der Geisteswissenschaften zu genügen. Dies gilt in besonderem Maße mit Blick auf zukünftige GeschichtslehrerInnen, die in ihrer Position als gesellschaftliche Multiplikatoren einer adäquaten Ausbildung bedürfen.
               Das Lehrkonzept des Forschenden Lernens eröffnet die Möglichkeit, Studierende mit Projekten, Methoden und Werkzeugen der Digital Humanities kritisch und reflektiert vertraut zu machen. Die aus (Lehramts-)StudentInnen und DozentInnen des Historischen Instituts der Universität Paderborn zusammengesetzte Projektgruppe ‚TEI-Editionswerkstatt‘ will zugleich wichtige Kompetenzen der Digital Humanities wie auch Fachwissen vermitteln. Dies geschieht außerhalb des Curriculums auf freiwilliger Basis, sodass die Motivation aller Beteiligten sehr hoch ist. Das gemeinsame Ziel ist es, vier Urkunden über die Gründung der Jesuitenuniversität Paderborn um 1600 (Meyer zu Schlochtern 2014) in einer digitalen Quellenedition der Forschung online zur Verfügung zu stellen (Sahle 2013). 
               Organisiert ist die Arbeitsgruppe in vier Teilgruppen, die jeweils eine Urkunde bearbeiten, woraus einerseits eine zeitökonomische Arbeitsweise für die heterogene Projektgruppe resultiert und andererseits sichergestellt ist, dass jedes Gruppenmitglied sämtliche Arbeitsschritte auf dem Weg zur digitalen Quellenedition selbstständig durchführt. Diese Vorgehensweise ist einer effektiveren Arbeitsweise dienlich und fördert im Sinne eines sekundären Erkenntnisinteresses die Kompetenzen der Arbeitsgruppe hinsichtlich des Umgangs mit den entsprechenden Tools und Methoden sowie einen elaborierten Erkenntnisgewinn hinsichtlich der erarbeiteten Inhalte. Die Ausdifferenzierung der Vorgehensweise und die gruppeninterne Organisation der Arbeitsschritte erfolgen bei regelmäßigen Treffen, bei denen die Ziele immer wieder dem aktuellen Stand des Projekts angepasst sowie Fragen und Ideen diskutiert werden.
               Die Arbeitsschritte im Detail:
               
                  Transkription: Es wurde eine eigenständige Transkription der Originaldokumente in ein digitales Format angefertigt.
                  TEI-Encoding: Die Quelle wurde in XML nach den Richtlinien der TEI ausgezeichnet. Hier liegt eine der Hauptaufgaben der Arbeitsgruppe.
                  TEI-Schema: Um sicher zu stellen, dass die gleichen Standards eingehalten werden, wurde ein für das Projekt maßgeschneidertes TEI-Schema erarbeitet. 
                  Übersetzung: Da die Urkunden in lateinischer Sprache vorliegen, soll eine deutsche Übersetzung angeboten werden, die eigens angefertigt werden muss. 
                  Historische Einleitung: Der Edition soll eine Einleitung vorangestellt sein, in welcher Informationen (u.a. zur Überlieferung) enthalten sind, die dem Leser eine Quellenkritik erleichtern und den historischen Kontext präsentieren. 
                  HTML-Darstellung: Die Urkunden-Edition soll online zugänglich gemacht werden. Der Benutzer soll die Möglichkeit erhalten, bestimmte Versionen (Original, TEI-Edition, Übersetzung) der Quelle zu vergleichen.
               
               Es wird in unserem Lehr-Lernkontext bewusst TEI – und nicht das textsortenspezifischere CEI – als Auszeichnungssprache eingesetzt, um den Projektmitgliedern möglichst generische Methoden der Textauszeichnung zu vermitteln und ein größtmögliches Spektrum an Anwendungsbereichen hinsichtlich der erlernten Fähigkeiten zu ermöglichen. Für die kollaborativen Arbeiten an den Dokumenten, am TEI-Schema und den Transformationsskripten ist bei GitHub eine entsprechende Gruppe inkl. eines öffentlichen Repositories (https://github.com/gedigiupb/urkunden_upb) eingerichtet worden, wodurch sichergestellt wird, dass alle Beteiligten mit der aktuellen Dateiversion arbeiten. Die TEI-Auszeichnung findet mithilfe des XML-Editors Oxygen statt.
               Im Kontext von „Kritik der digitalen Vernunft“ ist das Projekt als praktisches Beispiel im Rahmen der digitalen Angebote, Projekte und Werkzeuge zu verorten. Dabei liegt nicht nur die Erstellung einer „zeitgemäßen“ Quellenedition im Fokus der Arbeitsgruppe. Darüberhinausgehend wird im Rahmen des Projektes der digitale Horizont der (angehenden) HistorikerInnen erweitert sowie bereits vorhandene Kompetenzen im Sinne historischen Denkens und wissenschaftlichen Arbeitens gefördert. Dem kritischen Anspruch der Geisteswissenschaften wird insofern Rechnung getragen, als dass die permanente Reflexion und Ausdifferenzierung der Vorgehensweise und das kritische Hinterfragen der Sinnhaftigkeit des Einsatzes angewandter Tools eine reflektierte Vereinbarkeit der „daten-, algorithmen- und werkzeuggetriebenen“ Wissenschaft mit geisteswissenschaftlichen Ansprüchen generieren. Das praktische Beispiel der „Editionswerkstatt“ ermöglicht die Diskussion gesellschaftlicher Dimensionen der konkret in diesem Kontext wirksamen Digitalisierungsprozesse besonders unter Berücksichtigung heterogener Begrifflichkeiten wie Interaktionsformen, Partizipation und Bildung – es postuliert geradezu die Diskussion ihrer Konsequenzen in Wissenschaft und Gesellschaft.
            
         
      
      
         
            
               Bibliographie
               
                  Bernsen, Daniel (2017): 
                        “Arbeiten mit digitalen Quellen im Geschichtsunterricht“, in: Bernsen, Daniel / Kerber, Ulf (eds.): Praxishandbuch Historisches Lernen und Medienbildung im digitalen Zeitalter, Opladen/ Berlin/ Toronto: Verlag Barbara Budrich 295-303.
               
               
                  Kelly, T. Mills (2013): 
                        “Teaching History in the Digital Age“, Ann Arbor: MI: University of Michigan Press, 
                  
                     http://dx.doi.org/10.39
                     
                     
                     98/dh.12146032.0001.001
                  
                   [letzter Zugriff: 08. September 2017].
               
               
                  Meyer zu Schlochtern, Josef (2014): 
                        “Die Academia Theodoriana. Von der Jesuitenuniversität zur Theologischen Fakultät Paderborn 1614-2014“, Paderborn: Schöningh.
               
               
                  Pfanzelter, Eva (2017): 
                        “Analoge vs. digitale Quellen: eine Standortbestimmung“, in: Bernsen, Daniel / Kerber, Ulf (eds.): Praxishandbuch Historisches Lernen und Medienbildung im digitalen Zeitalter, Opladen/ Berlin/ Toronto: Verlag Barbara Budrich 85-94.
               
               
                  Sahle, Patrick (2013): 
                        “Digitale Editionsformen. Teil I: Das typografische Erbe, Teil II: Befunde, Theorie und Methodik, Teil III: Textbegriffe und Recodierung.“ Norderstedt: Schriften des Instituts für Dokumentologie und Editorik 7-9, 
                  
                     urn:nbn:de:hbz:38-53510
                  
                  , 
                  
                     urn:nbn:de:hbz:38-53523
                  
                  , 
                  
                     urn:nbn:de:hbz:38-53534
                  
                   [letzter Zugriff: 08. September 2017].
               
            
         
      
   



      
         Mittlerweile versprechen zahlreiche Tools eine mehr oder minder problemlose Lemmatisierung und Annotierung mit Part-of-Speech-Tags von Texten; viele sollen auch für historische (oder andere nicht-standardisierte) Sprachdaten nutzbar sein.
                 Dabei birgt die Verarbeitung historischer Sprachdaten des Deutschen zahlreiche Probleme aufgrund des hohen Grads an Variation, insbesondere auf den Ebenen Phonologie und Graphematik, aber auch in den Bereichen der Morphologie, Syntax und Lexik. Bei einer automatischen Verarbeitung solcher Daten stellen vor allem die Variationen in Phonologie, Graphematik und Morphologie ein besonderes Hindernis dar.
            
         Das Poster stellt verschiedene Werkzeuge überblicksartig vor und befasst sich genauer mit zwei Tools, deren Anwendung auf Texte nicht-standardisierter Sprachstufen exemplarisch anhand zweier Textsorten aufgrund bestimmter Kriterien verglichen wurde. Zum einen handelt es sich um gedruckte deutschsprachige Rechtstexte der Frühen Neuzeit, also aus der Rezeptionszeit des römischen Rechts in Deutschland, deren Sprache in einem Projekt erforscht werden soll, zum anderen um Fürstinnen-Briefe aus einem an der Friedrich-Schiller-Universität Jena erstellten digitalen Korpus.
                 In beiden Fällen weisen die Quellen frühneuhochdeutschen Sprachstand auf. Anhand ausgewählter Beispiele aus den vorliegenden Texten sollen zwei gängige elektronische Werkzeuge miteinander verglichen werden – EXMARaLDA und LAKomp.
            
         EXMARaLDA wurde für das computergestützte Arbeiten mit überwiegend mündlichen Korpora entwickelt, wird aber regelmäßig auch für schriftliche Sprachdaten verwendet, so auch bei den 
                Frühneuhochdeutschen Fürstinnenkorrespondenzen im mitteldeutschen Raum. Das Tool besteht im Wesentlichen aus einem Transkriptions- und Annotationseditor, einem Werkzeug zum Verwalten von Korpora und einem Such- und Analysetool.
                
         
         Das Werkzeug LAKomp
                 wurde im Projekt SaDA (Semiautomatische Differenzanalyse von komplexen Textvarianten)
                 entwickelt und dient der Aufbereitung eines historischen Korpus. Nach der Transkription können die Texte hier lemmatisiert und annotiert werden. Aufgrund der Besonderheiten bei frühneuhochdeutschen Handschriften und Drucken wird der Lemmatisierungs- und Annotationsvorgang komplett manuell durchgeführt. Dabei ist dem Benutzer mit LAKomp ein Werkzeug an die Hand gegeben, das ihn sehr schnell und präzise große Textmengen bearbeiten lässt und damit den Mehraufwand händischer Annotation nahezu ausgleicht. 
            
         Damit wurden zwei Werkzeuge ausgewählt, bei denen manuell annotiert werden muss, die aber dennoch bestimmte Unterschiede aufweisen, die für Nutzerinnen und Nutzer, die mit nicht-standardisierten Sprachdaten arbeiten, je nach Arbeitsziel vor- oder nachteilig sein können. So sind etwa bei LAKomp die halbautomatische Annotation auf der Grundlage des DWB und die Ausgabefunktion besonders gelungen, leider kann hier aber bisher nur lemmabasiert annotiert werden; bei EXMARaLDA ermöglichen die flexiblen Annotationskriterien eine besondere Breite von möglichen Annotationen, eine automatische oder halbautomatische Annotation des frühneuhochdeutschen Textmaterials ist jedoch bislang auch mit Hilfsmitteln wie dem Treetagger
                 nicht ohne weiteres möglich. 
            
         Die beiden genannten Tools werden auf dem Poster hinsichtlich ihrer Anwendbarkeit auf Texte historischer Sprachstufen anhand folgender Kriterien verglichen: Funktionalitäten, Nutzerfreundlichkeit (Technischer Support, Qualität des Handbuchs, Verständlichkeit der Benutzeroberfläche, Verfügbarkeit eines Editors, Umgang mit Metadaten, Exportmöglichkeiten …) und Nachnutzbarkeit. Das Poster wird diesen Tool-Vergleich anhand ausgewählter Beispiele aus einem Rechtsbuch sowie einem Fürstinnenbrief aus der Mitte des 16. Jahrhundert präsentieren und stellt somit Überblick und Evaluation der Werkzeuge gleichermaßen dar.
      
      
         
             In Auswahl: CATMA, CorA, EXMARaLDA, GATE, LAKomp, WebAnno.
            
               https://archive.thulb.uni-jena.de/hisbest/content/below/index.xml?XSL.DisplayComponentBrowse=true; 
                        http://www.laudatio-repository.org/repository/corpus/LAUDATIO%3AFuerstinnenkorrespondenz/TEI-header_version4_Schema7_2017-03-06T08%3A38%3A
                            26%3A247Z
            
             Für genauere Informationen vgl. 
                        http://exmaralda.org/de/ueber-exmaralda/ und 
                        http://exmaralda.org/de/exmaralda-nutzer/. 
                    
             LAKomp steht für 
                        Lemmatisierung, 
                        Annotation, 
                        Komparation.
                    
             http://www.informatik.uni-halle.de/ti/forschung/ehumanities/sada/
             http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/
         
         
            
               Bibliographie
               
                  B. Aehnlich / S. Kösser (2016): „Das Tool LAKomp und seine Anwendung auf Texte nichtstandardisierter Sprachstufen“. In: DHd 2016. Konferenzabstracts, Leipzig, S. 263–264.
               
                  V. Faßhauer (2017): „Compilation, Transcription, Multi-Level Annotation and Gender-oriented Analysis of a Historical Text Corpus:Early Modern Ducal Correspondences in Central Germany“. In: Advances in Digital Scholarly Editing: Papers presented at the DiXiT conferences in The Hague, Cologne, and Antwerp, hg. v. Peter Boot, Anna Cappellotto, Wout Dillen, Franz Fischer, Aodhán Kelly, Andreas Mertgens, Anna-Maria Sichani, Elena Spadini & Dirk van Hulle. Leiden, S. 269–274.
               
                  A. Medek (*Gießler) / M. Pöckelmann / T. Bremer / H.-J. Solms / P. Molitor / J. Ritter (2015): „Differenzanalyse komplexer Textvarianten - Diskussion und Werkzeuge“. In: Informationsmanagement für Digital Humanities, hg. v. G. Heyer und A. Henrich. In: Datenbank-Spektrum 2015. http://dx.doi.org/10.1007/s13222-014-0173-y.
               
                  A. Leipold / J. Ritter / H.-J. Solms: „Neue Wege zu Textzeugenvergleich und Edition am Beispiel der Wundarznei des Heinrich von Pfalzpaint“. In: Jahrbuch für Germanistische Sprachgeschichte 2014, Band 5, Heft 1, S. 335-358.
               
                  D. Prutscher / H. Seidel (2012): „Mehrebenenannotation frühneuzeitlicher Fürstinnenkorrespondenzen“. In: G. Brandt (Hg.): Bausteine weiblichen Sprachgebrauchs. X. Texte – Zeugnisse des produktiven Sprachhandelns von Frauen in privaten, halböffentlichen und öffentlichen Diskursen vom Mittelalter bis in die Gegenwart. Stuttgart, S. 109–124.
            
         
      
   



      
         Der Aufbau von digitalen (Musik)-Editionen und den entsprechenden Online-Publikationen hat sich im wissenschaftlichen Umfeld etabliert und damit Fakten und Muster geschaffen, die nicht notwendigerweise mit den zeitlich parallel entwickelten Theorien kongruent gehen. Das mag zum einen daran liegen, dass die zeitgenössische Theoriebildung selbst nicht einheitlich ist – man vergleiche nur die Idee einer „sozialen Edition“ (Siemens 2011) mit der Debatte um „documentary editing“ (Robinson 2013; Gabler 2010; Pierazzo 2011) oder mit einem „multiplen Textbegriff“ als Grundlage der Edition (Sahle 2013) – zum anderen aber auch an den ganz praktischen Rahmenbedingungen der allermeist als drittmittelgeförderten, zeitlich begrenzten Projekte. 
         Es gilt daher, kritisch rückzublicken und zu reflektieren, in welcher Weise die Art der digitalen Erschließung die Erkenntnismöglichkeiten des Nutzers steuert, und ob sich die mit solchen Webpublikationen häufig verbundenen Ideen von „Offenheit“, „Erweiterbarkeit“, „Vernetzung“ und „Nutzerbeteiligung“ in der täglichen Arbeit der Projekte überhaupt realisieren lassen – oder ob diese Ideen nicht sogar teilweise auf zu sehr simplifizierten Voraussetzungen beruhen? 
         Innerhalb der Musikwissenschaft decken die digitalen Projekte des Detmold/Paderborner Virtuellen Forschungsverbunds Edirom und des Zentrums Musik – Edition – Medien sowie die damit assoziierten Vorhaben einen weiten Bereich der digitalen Aktivitäten im Fach ab. Es handelt sich einerseits um Projekte im Bereich der Musikedition (u.a. 
                Weber-Gesamtausgabe,
            Freischütz Digital, 
                Bargheer-Fiedellieder, 
                Beethovens Werkstatt), andererseits um Methoden- und Softwareentwicklung (
                Edirom, 
                ZenMEM, 
                VideApp, 
                WeGA-WebApp), sowie um neuartige Erschließungskonzepte (
                Detmolder Hoftheater) bzw. Beiträge zur Entwicklung von Codierungsstandards (TEI, MEI). Vor allem in Rückbindung an TEI und MEI sind dabei Publikationen entstanden, die weit über eine schlichte Digitalisierung hinausgehen: Metadaten, die wissenschaftlichen Ansprüchen gerecht werden, extensive Verknüpfungen und inhaltliche Auszeichnungen der Dokumente sind selbstverständlich. Ferner sind unterschiedliche Annotationspraktiken erprobt.
            
         Das Bemühen der bisherigen Arbeiten war vor allem darauf gerichtet, die wissenschaftlichen Standards der „klassischen“ Edition und Informationsbereitstellung zu halten und „digitale“ Möglichkeiten wie Verknüpfungen zu externen Quellen und die Vereinheitlichung von Angaben durch Normdaten zu integrieren.
         Aber genügt eine solche „Aufbereitung“ der gesammelten Daten und wie kommt dabei der „Nutzer“ ins Spiel bzw. wie kann seinen Erwartungen entsprochen werden? Mit Blick auf solche Fragen stellte Jeffrey T. Schnapp 2013 fest:
         „Herein resides the challenge that I am referring to as storied collections and that I associate with the need to give rise to a humanistic culture of critical engagement with data and data architectures themselves as well as with the tools that analyze and translate them into argumentative or narrative forms.“ (Schnapp 2013)
         D. h. es ist zu fragen:
         
            Wie bestimmen die verwendeten Schemata unsere Erschließung?
            Was verstehen wir unter und wie ermöglichen wir Partizipation der Nutzer?
            Wie verhält sich Partizipation zu unserem wissenschaftlichem Anspruch? 
            Welche technischen Probleme stehen Partizipation (noch?) entgegen?
            Wie erreichen wir einen kritischen Umgang mit den bereitgestellten Daten?
            Wie erreichen wir durch bloße Informationsbereitstellung kritisches Wissen?
            Kann der Nutzer ohne Vorwissen solche Portale / Editionen effektiv nutzen?
            Inwiefern nehmen wir überhaupt auf verschiedene Erkenntnisinteressen Rücksicht?
         
         Wenn dies auch Fragen sind, die z. T. alle Geisteswissenschaften betreffen, so sollen sie doch in dem vorgeschlagenen Panel aus Sicht der speziellen Anforderung der Musikwissenschaft betrachtet werden. Hierzu werden drei verschiedene musikwissenschaftliche digitale Projekte (Weber-Gesamtausgabe, Hoftheater-Projekt und Beethovens Werkstatt) ihren bisherigen Umgang mit den Standards und den digitalen Möglichkeiten kritisch erläutern. Ergänzt werden diese Überlegungen durch die kritische Reflexion der technischen Bedingungen von Partizipation und Konsistenz der Daten (Zentrum Musik – Edition – Medien). 
         
            Weber-Gesamtausgabe
            Die digitale Edition der Schriften, Tagebücher und Schriften Carl Maria von Webers wurde 2011 der Öffentlichkeit vorgestellt und seitdem – sowohl in der TEI-Auszeichnung als auch in der HTML-Darstellung – kontinuierlich weiterentwickelt und angepasst. Erst im letzten Jahr z.B. wurden dabei „Themenkommentare“ ergänzt als Versuch, die inzwischen über 27.000 verfügbaren Dokumente stärker narrativ einzubetten bzw. zu verknüpfen. Grundsätzlich bleibt aber das Dilemma, dass ein starker Fokus auf der Standardisierung und Normalisierung der Auszeichnung liegt – das ermöglicht zwar auf einer globalen Ebene das Vernetzen mit anderen Repositorien z.B. durch GND-Beacon oder correspSearch und demonstriert somit die Möglichkeiten und Anschlussfähigkeiten digitaler Editionen, aus dem Blick geraten dabei aber oft die Besonderheiten (und Unsicherheiten) lokaler Phänomene.
         
         
            Hoftheater-Projekt
            Das sog. Hoftheater-Projekt („Entwicklung eines MEI- und TEI-basierten Modells kontextueller Tiefenerschließung von Musikalienbeständen am Beispiel des Detmolder Hoftheaters im 19. Jahrhundert (1825–1875)“) stellt einerseits in traditioneller Weise Informationen zu sehr heterogenen Beständen bereit und verknüpft diese andererseits untereinander in einer Form, die erst durch digitale Mittel möglich ist. D. h. neben der Präsentation von Digitalisaten, Metadaten, Incipits und Textübertragungen, werden die einzelnen Objekte durch die Auszeichnung nicht nur mit Elementen, sondern mit key-Attributen (und wenn möglich mit Normdaten) für Personen, Werke und Rollen miteinander in Verbindung gesetzt.
            Es entsteht so ein Informationsnetz
                    , das unterschiedliche Forschungsinteressen zulässt. Neben den „traditionellen“ Informationen zu den Quellen können Angaben zur Organisation des Theaterbetriebs, zur finanziellen Situation einzelner Personen, zur Theatersituation in den verschiedenen Spielorten etc. abgefragt werden. Die Daten können aber auch Basis soziologischer/historischer Studien werden, indem z. B. die Gehälter am Theater mit denen anderer Berufsgruppen in Beziehung gesetzt werden.
                
            Es ergeben sich u. a. folgende Fragen für einen kritischen Umgang mit diesen Daten:
            
               die bisher verwendeten Auszeichnungselemente sind fachspezifisch gewählt
               Zweifel in der Übertragung werden ausgezeichnet [aber nicht angezeigt]. Bei den Auszeichnungen wird hingegen auf diese Angabe verzichtet bzw. bleiben Lücken, da das grundsätzliche Problem, wie inhaltliche Argumente „dargestellt“ werden können, noch nicht gelöst ist
               die XML-Dateien stehen innerhalb der Anwendung zur Verfügung, können aber nicht frei heruntergeladen werden
            
         
         
            
               Beethovens Werkstatt
                
            Die Zielsetzung der in „Beethovens Werkstatt“ entwickelten VideApp ist es, die Erkenntnisse und Beobachtungen des Projekts zur Textgenese ausgewählter Werke Beethovens möglichst direkt sichtbar zu machen, also eine Vermittlungsform zu finden, die jenseits verbaler Erläuterungen einen möglichst unmittelbaren und nachvollziehbaren Zugang zu den Inhalten bietet. Dabei zeigt sich, dass die Menge der zu treffenden Aussagen, verbunden mit der Neuartigkeit dieser Vermittlungsformen, leicht zu Orientierungsschwierigkeiten des Benutzers führt: Nicht immer erschließen sich gut gemeinte Funktionen so schnell wie erhofft, und besonders spannende Beobachtungen gehen in der Fülle an Details unter. Eine vor Jahren im Kontext des Edirom-Projekts entstandene, aber nie umgesetzte Idee aufgreifend versucht das Projekt daher inzwischen, dem Benutzer besonders relevante Aspekte der Editionen über geführte „Touren“ nahezubringen, ohne dessen eigenständige vertiefende Auseinandersetzung mit den Materialien einzuschränken.
         
         
            ZenMEM
            Im Verbundprojekt „Zentrum Musik – Edition – Medien“ (ZenMEM) beschäftigen sich Wissenschaftler/-innen der Universität Paderborn, der Hochschule für Musik Detmold und der Hochschule Ostwestfalen-Lippe mit den Veränderungen und den neuen Möglichkeiten beim Übergang von analogen zu digitalen Musik- und Medieneditionen. 
            Unbestritten sind an dieser Stelle die vielen Vorteile und Mehrwerte einer digitalen Edition gegenüber der klassischen, analogen Edition in Buchform. Die Digitalisierung von Musikeditionen schafft aber gleichzeitig auch ganz neue Problemstellungen und Herausforderungen. Im Projekt durchgeführte problemzentrierte Leitfadeninterviews mit Editoren zeigten bspw. deutlich das Spannungsfeld zwischen Offenheit und Abgeschlossenheit insbesondere bei der (Nach-)Nutzung:
            
               Wie erreicht man eine Form der Abgeschlossenheit bei der Publikation digitaler Musikeditionen, welche die ‚Wertigkeit‘ einer gedruckten Edition besitzt?
               Wie bringt man die gewünschte offene (Nach-)Nutzung einer digitalen Musikedition durch ein breites Publikum (neben Editoren auch Dirigenten, interessierte Laien, Studierende) in Einklang mit dem Wunsch nach Abgeschlossenheit?
               Wie gestaltet sich die Wertschöpfung im Bereich (offener) digitaler Musikeditionen und wie ist das Verhältnis von Editoren, Verlagen, Forschungs- und Gedächtnisinstitutionen zueinander?
               Wie stellt man eine dauerhafte Verfügbarkeit und Referenzierbarkeit (insbesondere im Hinblick auf eine offene, sich weiterentwickelnde) digitale Musikedition sicher?
               Wie geht man mit Autorschaft in einer gemeinschaftlich erarbeiteten Edition um? Bereits auf Annotationsebene?
               Wie geht man mit den Rechten an (externem) Quellenmaterial um?
               Wie kann man Nachhaltigkeit gewährleisten?
            
            Die genannten Problemfelder ergeben sich zum Teil zwar schon direkt oder indirekt aus dem Übergang von analogen zu digitalen Musikeditionen und haben bereits Auswirkungen auf den Prozess des Edierens selbst, doch eine breitgefächerte (Nach)Nutzung muss frühzeitig mitbetrachtet werden, da zusätzliche Akteure mit unterschiedlichen Interessen in Einklang gebracht werden müssen.
            Zusätzlich implizieren viele der Probleme auch sehr technische Herausforderungen, wie bspw. eine Revisionssicherheit (insbesondere: Berechtigungen, Schutz vor Veränderung und Verfälschung, Sicherung vor Verlust, Dokumentation von Prozessen und Nachvollziehbarkeit) sowie Versionierung und Referenzierung von Arbeits- und Publikationsständen. Hier gilt es nun den nächsten Paradigmenwechsel von einzelnen digitalen Musikeditionen hin zu Editionsinfrastrukturen forschungsbegleitend zu vollziehen, um die genannten Herausforderungen überhaupt adäquat adressieren zu können.
         
      
      
         
             Siehe das Modell auf der Website: 
                        .
                    
         
         
            
               Bibliographie
               
                  
                  Gabler, Hans Walter (2010): „Theorizing the Digital Scholarly Edition“, in: Literature Compass 7/2 (2010): 43–56
                    
               
                  Pierazzo, Elena (2011): „A Rationale of Digital Documentary editions“. Literary and Linguistic Computing 26/4 (2011): 463–77
                    
               
                  Robinson, Peter (2013): „Towards a Theory of Digital Editions“, in: Variants 10 (2013): 105–131
                    
               
                  Sahle, Patrick (2013): „Digitale Editionsformen, Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels“, Norderstedt 2013
                    
               
                  Schnapp, Jeffrey T. (2003): „Knowledge Design. Incubating new knowledge forms / genres / spaces in the laboratory of the digital humanities.“ Keynote delivered at the Herrenhausen Conference „Digital Humanities Revisited – Challenges and Opportunities in the Digital Age“ (Dez. 2013)
                    
               
                  Siemens, Ray / Timney, Meagan / Leitch, Cara / Koolen, Corina / Garnett, Alex (2011): „Toward Modelling the 
                        Social Edition: An Approach to Understanding the Electronic Scholarly Edition in the Context of New and Emerging Social Media“, in: Literary and linguistic computing 27/4 (2012): 445–461
                    
               
                  Wiering, Frans / Crawford, Tim / Lewis, David(2006): „Digital Critical Editions of Music. A Multidimensional Model“, Methods Network Expert Seminar „Modern Methods for Musicology“, online unter 
                        
               
            
         
      
   



      
         
            Ausgangslage
            Virtuelle Rundgänge, virtuelle Ausstellungen oder virtuelle Touren: Die Begriffe bezeichnen oft ähnliche Vorgehensweisen. Zusätzlich zu strukturierten Suchzugängen werden sie auf den unterschiedlichsten Portalen für digitalisiertes Kulturerbe in der Regel als Ergänzung angeboten. Dazu gehören prominente Beispiele wie die Europeana
                    , die Deutsche Digitale Bibliothek
                     oder zahlreiche Regionalportale im deutschsprachigen Raum (stellvertretend sei Bavarikon
                     oder Kulturerbe Niedersachsen
                     genannt). Doch auch kommerziellen Anbietern ist das Konzept nicht fremd, wie Google zeigt.
                     Am häufigsten wird die Bezeichnung (Virtuelle/Digitale) Ausstellung ((
                    virtual/digital) exhibition) verwendet.
                
            Unabhängig von der Begrifflichkeit teilen sich die Umsetzungen meist folgende Merkmale (vgl. auch INDICATE 2012, 17):
            
               es handelt sich um Sammlungen von Informationen und Quellen/Objekten zu einem bestimmten Thema, einer Periode oder einer Person (oft auch anlassbezogen zu Jubiläen, z.B. Reformation
                        , Weltkrieg
                        , etc.)
                    
               die Quellen werden von Fachleuten kontextualisiert und beschrieben
               verschiedene Inhalte (Text, Bild, Audio, Video) werden miteinander verbunden
               die Objekte können aus einem oder mehreren digitalisierten Sammlungen oder Beständen kommen
               meist gibt es einen vorgegebenen (zumindest bevorzugten) Ablauf der Informationen
            
            So vielfältig wie die möglichen Themen der Rundgänge sind die technischen Umsetzungen und Visualisierungen. Zahlreiche Werkzeuge sind für die Erstellung und Darstellung von Virtuellen Ausstellungen verfügbar.
                     Bei der Auswahl des Werkzeuges sind die gewünschte Funktionalität, Kompatibilität mit bestehender Infrastruktur, Unabhängigkeit von äußeren Quellen wie auch eine möglichst geringe Einarbeitungszeit wichtige Kriterien. Eine Wahrung der Prinzipien der Digital Humanities (vor allem mit Hinblick auf open source Software) ist ebenso wünschenswert.
                
            Ein Vorbildprojekt ist AthenaPlus, das nicht nur theoretische und methodische Vorarbeit im Bereich Virtuelle Ausstellungen geleistet hat, sondern mit Movio auch ein open source Tool entwickelt hat.
         
         
            Umsetzung im Portal „Kultur- und Wissenschaftserbe“ Steiermark
            Auch im Webportal „Kultur- und Wissenschaftserbe Steiermark“, einem Ergebnis des Projektes „Repositorium Steirisches Wissenschaftserbe“, ergänzen Virtuelle Rundgänge den strukturierten Suchzugang. Das Konzept zu den Touren wurde gemeinsam mit den Datenlieferanten erarbeitet. Die kuratierten Rundgänge erlauben eine spielerische Auseinandersetzung mit den Objekten und ein „Virtuelles Schlendern“ durch Bestände.
            Für die Umsetzung wurde die JavaScript-Bibliothek StoryMapJS verwendet, die in Form eines eigenen Objektmodells in die dem Webportal zu Grunde liegende Infrastruktur GAMS integriert wurde. Die Wahl fiel deswegen auf StoryMapJS, weil es sich aus den zahlreich verfügbaren Werkzeugen am besten in das bestehende System implementieren ließ. Viele Werkzeuge bieten ein Rundumpaket von Content Management System bis Präsentation, für die Integration in GAMS war ein reines Disseminationswerkzeug zu bevorzugen. Das Objektmodell kapselt den Inhalt der Ausstellung (Text, Medienreferenzen, Ablauf) und übergibt die Daten an die Javascript-Bibliothek, die die Darstellung übernimmt. So sind alle verwendeten Daten unabhängig von der Anwendung Teil des digitalen Archivs und können somit dauerhaft adressiert sowie gemeinsam verwaltet und (langzeit)archiviert werden. Eine persistente Identifikation der Ausstellung ist auf Basis von Handle möglich.
            Derzeit wird der für StoryMapJS verlangte JSON Input eins zu eins aus einer XML Datei konvertiert. Das bringt eine teilweise Vermischung von Form und Inhalt mit sich. Um dem entgegen zu wirken, wäre ein spezielles Set zur Beschreibung der Meta(Daten) der Virtuellen Ausstellung sinnvoll. Teilweise wird das durch DEMES (
                    Digital Exhibition Metadata Element Set) ermöglicht, allerdings werden hier stets die Metadaten der gesamten Ausstellung beschrieben, eine Metabeschreibung der einzelnen Elemente, des Inhalts wie auch von Ablauf und Zusammenhang ist hier nicht möglich.
                
         
         
            Fazit
            Die gewählte Bibliothek StoryMapJS erfüllt den Zweck der Visualisierung der Rundgänge für dieses Projekt gut. Ein Nachteil liegt in der relativ eingeschränkten Reihenfolge der einzelnen Stationen; NutzerInnen können nur begrenzt eigene Wege durch das Material finden. Die Einarbeitungszeit ist gering, die Integration in die eigene Infrastruktur funktionierte problemlos.
            Ein Desiderat ist jedoch die bessere Strukturierung und Standardisierung der Daten und Metadaten. Hier wäre eine einheitliche Beschreibung nicht nur der gesamten Ausstellung (wie durch DEMES ermöglicht, oder vielleicht begrenzt auch mit Dublin Core oder TEI umsetzbar), sondern der einzelnen Stationen und deren Zusammenhang sinnvoll. Zusätzlich könnten durch Anreicherung Anknüpfungspunkte für die Vernetzung der erzeugten Daten entstehen, beispielsweise GeoNames-Referenzen, wenn moderne Karten als Hintergrund verwendet werden. Die vollständige Trennung von Darstellung und Inhalt ist auch für die Weiternutzung und Archivierung zentral.
            Insgesamt wird die wissenschaftliche Auseinandersetzung mit Virtuellen/Digitalen Ausstellungen/Rundgängen oft vernachlässigt, weil sie in erster Linie als Verschönerung bzw. nachrangiges Angebot von Suchportalen verstanden werden. Ganz im Gegenteil sind diese Einstiegspunkte aber für das Publikum oft leichter verständlich und können einen besseren Überblick über die zu vermittelnde Thematik geben als hochstrukturierte Interfaces und rein textuelle Beschreibungen.
         
      
      
         
            
               http://www.europeana.eu/portal/en/exhibitions/foyer (2017-08-09)
                        
            
               https://www.deutsche-digitale-bibliothek.de/content/ausstellungen (2017-08-09)
                        
            
               https://www.bavarikon.de/topics (2017-08-09)
                        
            
               http://kulturerbe.niedersachsen.de/viewer/kontexte/ (2017-08-09)
                        
             https://www.google.com/culturalinstitute (2017-08-17)
            
               https://bavarikon.de/object/bav:BSB-CMS-0000000000001151 (2017-08-09)
                            
            
               http://wk1.staatsarchiv.at/ (2017-08-09)
                            
             Siehe bspw. 
                            http://oedb.org/ilibrarian/5-free-and-open-source-tools-for-creating-digital-exhibitions/ (2017-08-09)
                        
         
         
            
               Bibliographie
               
                  AthenaPlus (2015): Digital storytelling and cultural heritage: stakes and opportunities. 
                        http://www.athenaplus.eu/index.php?en/207/digital-storytelling [letzter Zugriff 22. August 2017]
                    
               
                  AthenaPlus (2016): Metadata for the description of digital exhibitions: the DEMES element set. Version 1.0. 
                        http://www.athenaplus.eu/index.php?en/206/demes [letzter Zugriff 22. August 2017]
                    
               
                  AthenaPlus: Movio. 
                        http://www.athenaplus.eu/index.php?en/211/movio) 
                    
               
                  Geisteswissenschaftliches Asset Management System – GAMS. 
                        http://gams.uni-graz.at [letzter Zugriff 22. August 2017]
                    
               
                  INDICATE (2012): Handbook on virtual exhibitions and virtual performances. Version 1.0. 
                        http://www.indicate-project.org/getFile.php?id=412 [letzter Zugriff 22. August 2017]
                    
               
                  Kultur- und Wissenschaftserbe Steiermark. 
                        http://www.kulturerbe-stmk.at [letzter Zugriff 22. August 2017]
                    
               
                  Repositorium Steirisches Wissenschaftserbe. 
                        https://wissenschaftserbe.uni-graz.at/ [letzter Zugriff 22. August 2017]
                    
               
                  StoryMapJS. 
                        https://storymap.knightlab.com/ [letzter Zugriff 22. August 2017]
                    
            
         
      
   



      
         Ausgehend von thesenförmigen Impulsreferaten (jeweils 10 Min., die PPT-Folien werden nach der Tagung publiziert) sollen editionstheoretische und -technische Fragen diskutiert werden.
            
         
            Block 1 Impulse (90 Minuten) 14:00 – 15:30 Uhr
            
               Begrüßung und Einführung (Thomas Bürger und Jochen Strobel)
               Offenheit und institutionelle Schließung (Patrick Sahle)
               Kommentierung – ein Auslaufmodell? (Anne Bohnenkamp)
               Versionierung/Zitation (Joachim Veit)
               Hemmnisse und Katalysatoren digitaler Brief-Infrastrukturen (Thomas Stäcker)
               Schnelle Wege zu den Briefen (Stefan Dumont)
               DARIAH-Services für Briefeditionen (Mirjam Blümm)
               Akteure und Rollen (Jochen Strobel)
            
         
         
            Block 2 Diskussion, Fazit, Ausblick (90 Minuten) 16:00 – 17:30 Uhr
            Diskussion der Impulsreferate und der fachlichen und förderpolitischen Schlussfolgerungen (Moderation: Thomas Bürger, Jochen Strobel)
            
               Block 1: 14:00 – 15:30 Uhr
            
            
               1. Einführung (Thomas Bürger, Jochen Strobel)
            
            Ob es nun zutrifft, dass die Edition zu den Kerngeschäften der Geisteswissenschaften gehört und sogar die Königsdisziplin der Digital Humanities sei, bleibe dahingestellt – nutzerseitig handelt es sich bei den Forschungsergebnissen dieser Disziplin um diejenigen mit der längsten Halbwertszeit. Die Aussicht, dass gut gemachte Editionen zu den unverzichtbaren Grundlagen der wissenschaftlichen Praxis gehören, setzt die an Editionen Beteiligten aber auch unter Druck. Editionen sind langwierig und teuer, sie müssen im fachlichen und technischen Sinne zuverlässige Daten vorhalten, sie müssen dauerhaft verfügbar und doch auf der Höhe der Zeit sein. Dazu bedarf es fachlich und technisch versierter Bearbeiter. Tagungen und Publikationen zur Digitalen Editionverdeutlichen das Bedürfnis nach Orientierung, Standardisierung, Weiterentwicklung und transparenterer Vernetzung. Eine pragmatische Ausrichtung schließt diese Ziele ein:
            
               die Orientierung der Usability an Forschungsfragen der Nutzer*innen, insbesondere eine Optimierung der Durchsuch- und Findbarkeit und eine gute Lesbarkeit der Texte
               die zeitgemäße Einbettung der Edition in stabile virtuelle Forschungsumgebungen 
               die Anschlussfähigkeit in Infrastrukturen 
               Sparten und Institutionen übergreifende Projektvernetzung.
            
            Die Briefedition ist ein prominentes Paradigma: In analoger Form ist sie besonders zeitraubend und kostspielig. Als Zeugnis historischer Netzwerke und Kommunikations-strukturen und wegen der vergleichsweise fortgeschrittenen Standardisierung bietet sie sich als Avantgarde digitaler Projektvernetzung und Daten-Aggregierung geradezu an. 
            Patrick Sahle hat für Editionen plausibel zwischen der Repräsentation der eigentlichen Daten („Inhalt“) und der Präsentation („Form“) unterschieden. Unumstrittene Geltung besitzen Kriterien, wie sie auch durch Roland Kamzelak benannt wurden, also z. B. XML-Standard, Open Source-Software und Open Access, Nachnutzbarkeit, persistente URL, Langzeitarchivierung. Ein faktischer Standard besteht mit den von der DFG herausgegebenen Förderkriterien für Wissenschaftliche Editionen. Komplementär hierzu und um ein Vielfaches differenzierter bietet der vom IDE bereitgestellte Kriterienkatalog für die Besprechung digitaler Editionen Anhaltspunkte. Mit RIDE besteht ein Rezensionsjournal für Editionen. Thomas Stäcker, Thomas Burch u.a. verweisen auf innovative multiple Analyse- und Darstellungsmöglichkeiten (Distant Reading, Mapping the Republic of Letters), die neue Fragestellungen und Vermittlungsformen erlauben und zu neuen Verständigungen (z.B. über Remediatisierung, über innovative Vermittlungswege der wissenschaftlichen und kulturellen Überlieferung, das Potential graphorientierter Datemodelle) anregen. 
            
               2. Offenheit und institutionelle Schließung (Patrick Sahle)
            
            Die Digitale Edition verheißt ewige Unabgeschlossenheit und technische wie fachliche Offenheit – was aber bedeutet dies: Paradies oder Inferno? Bei allen Vorzügen der Korrigier- und Ergänzbarkeit durch das Editorenteam oder auch durch User: editorische Daten müssen zuverlässig sein – wir wollen nicht ewig auf Baustellen leben. Was bedeutet dies im Hinblick auf mögliche Schließungsregeln für ein offenes Medium (s. u.: Zitation/Versionierung)? Was folgt etwa in puncto Nachnutzbarkeit hieraus? Ist zu unterscheiden zwischen gesicherten Forschungsdaten und solchen, die als Forschungsergebnisse zwar intersubjektiver Nachprüfbarkeit unterliegen, in ihrer Thesenhaftigkeit wie auch Kontextabhängigkeit aber zugleich diskussionswürdig und revidierbar sind, wie etwa Interpretamente?
            In der Praxis sind es institutionelle und finanzielle Zwänge, die die Begrenzung und Schließung von an sich offenen Projekten erforderlich machen. Hieraus ergeben sich grundlegende Fragen nach der gebündelten Kuratierung abgeschlossener Projekte.
            
               3. Kommentierung – ein Auslaufmodell? (Anne Bohnenkamp)
            
            Ist der „Stellenkommentar“ überflüssig, ein in der Editionsphilologie zwischen Redundanz- und Interpretationsverdikt immer schon umstrittenes Element, das gleichwohl der Profilierung der Editor*innen wie vor allem der Usability diente? Ist über Tagging und Verlinkung hinaus ein editionsspezifisches kommentierendes Angebot sinnvoll – und wie sollte es sich zu den zuhauf existenten externen Informationsressourcen technisch und sachlich verhalten? Ist die Kommentierung vielleicht sogar jenseits der scheinbar mechanischen und objektiven digitalen Repräsentation jener letzte Ort, an dem das Fachwissen und die interpretatorische Leistung die Unverzichtbarkeit des Editors als eigentlichem Experten und bestem Kenner der Materie belegt?
            
               4. Versionierung/Zitation (Joachim Veit)
            
            Die Herstellung von beliebig vielen, zitationsfähigen Versionen eines edierten Texts oder Textabschnitts ist technisch kein Problem, die Zuweisung persistenter Identifier ebenso wenig. Wie sollten Forschungsdaten jenseits der Text(präsentations)ebene zitiert werden? Was sind die (Haupt-?)Bestandteile der Editionen, wo liegen ihre Grenzen? Nach der verbindlichen Veröffentlichung eines Textes müssen editorische Veränderungen als unterschiedliche Versionen markiert werden. Hierzu dienlich ist ein Versionierungssystem. Präzise Zitierfähigkeit ist ein unverzichtbares Merkmal von Editionen – doch wie bleibt sie unter digitalen Auspizien praktikabel? Hierzu zählt z. B. die nicht triviale Frage nach der Länge des Zitatnachweises, der ganz oder teilweise zugleich Link in einer Online-Publikation ist. 
            
               5. Hemmnisse und Katalysatoren digitaler Brief-Infrastrukturen (Thomas Stäcker)
            
            Es besteht seit langem Einigkeit darüber, dass die Verwendung normierter Metadaten sowie die Codierung gemäß den Regeln des TEI-Konsortiums eine Garantie für Qualität und Nachhaltigkeit bietet. GND, VIAF – inzwischen verwenden Digitale Editionen bibliothekarische Normdaten zur Referenzierung von Personen, Orten u.a. Die Standardisierung von Werktiteln schreitet eher langsam voran. In der Praxis ist aber ebenso bekannt, dass von Projekt zu Projekt kleinere oder größere Abweichungen zu verzeichnen sind. Welche Folgerungen sind aus dieser Inkongruenz zu ziehen, wie ist mit einer unterschiedlichen Nutzung von Standards umzugehen?
            Mit dem Verlassen der Zweidimensionalität des gedruckten Textes findet nicht nur durch das Verschwinden des Substrates ein Verlust an Stabilität statt, sondern es treten durchaus andere stabile Konstituentien an dessen Stelle. Welche Entwicklungen sind zu erwarten bzw. zu organisieren?
            
               6. Schnelle Wege zu den Briefen (Stefan Dumont)
            
            CorrespSearch will verbesserte Voraussetzungen für die Vernetzung von Briefeditionen schaffen. Damit sollen Briefmetadaten über das bisherige Basis-Set hinaus verarbeitet und auch die Erstellung von digitalen Briefverzeichnissen aus gedruckten Publikationen unterstützt werden. Über Schnittstellen zu anderen Diensten ist der Webservice in die existierende und zu entwickelnde Infrastrukturlandschaft tiefer einzubetten. Wie lassen sich nutzer- und forschungsfreundlich die Zugänge zu Briefen und insbesondere zu den Volltexten vereinfachen? In welchem Verhältnis steht CorrepSearch zu den vielen anderen Angeboten (Deutsche Digitale Bibliothek, Europeana, Kalliope u.a.)?
            
               7. DARIAH-Services für Briefeditionen (Mirjam Blümm)
            
            DARIAH.DE ist eine Informations- und Technikinfrastruktur für Lehre, Forschung, Forschungsdaten und technische Werkzeuge. Welchen Stand hat die Infrastruktur zur Einbindung digitaler Editionen erreicht und welche kollaborativen Strukturen und Ziele sollten angestrebt werden? 
            
               8. Akteure und Rollen (Jochen Strobel)
            
            Die Digital Humanities fördern kollaboratives Arbeiten und generell eine Diversifizierung und Verflüssigung von Rollen und Verantwortlichkeiten. Zu den Rollen einer digitalen wissenschaftlichen Autor- und Beiträgerschaft zählen u.a. Hauptherausgeber*in, Kurator*in, Programmierer*in, Kodierer*in, Tagger*in, wissenschaftliche Hilfskraft, Crowdsourcer*in. Einige dieser und weitere Rollen sind so neu nicht, manche dürften vom bisherigen Urheberrecht nicht hinreichend abgedeckt sein. Editionen sind seit langem kollaborative Projekte, die Rollen der Beteiligten sind dementsprechend vielfach ausdifferenziert. Welche Folgerungen ergeben sich für die digitale Edition einerseits und für die akademische Karriereplanung zwischen Forschung und Informationsinfrastruktur andererseits?
            
               Block 2: 16:00 – 17:30 Uhr
            
            
               9. Diskussion, Fazit, Ausblick (Moderation: Thomas Bürger, Jochen Strobel)
            
            In der Schlussdiskussion sollen die Impulsreferate diskutiert und ein Fazit gezogen werden. Dabei sind auch forschungspolitische Schlussfolgerungen zu thematisieren. Editionen und insbesondere Briefeditionen sind in der Regel langfristige Projekte und deshalb vor allem an Akademien angesiedelt. Durch die sich verbreiternde Retrodigitalisierung und verfügbare digitale Werkzeuge können Projektlaufzeiten perspektivisch weiter verkürzt und Projektfortschritte transparent in Forschung und Lehre eingebunden werden. Wie groß ist der Bedarf an digitalen Editionen und wie können sie Teil einer Nationalen Forschungsdateninfrastruktur werden? Welche drängenden Fragen sind zeitnah in weiteren Workshops zu besprechen?
         
      
      
         
            
               Bibliographie
               
                  Roland Kamzelak: Empfehlungen zum Umgang mit Editionen im Digitalen Zeitalter, in editio 26 (2012), S. 202–209.
               
                  Patrick Sahle: Digitale Editionsformen. 3 Bände. Norderstedt 2013.
               
                  http://www.mww-forschung.de/blog/blogdetail/wie-sieht-die-digitale-edition-der-zukunft-aus-herr-kamzelak/?tx_news_pi1%5Bcontroller%5D=News&tx_news_pi1%5Baction%5D=detail (30.8.2017)
                    
               
                  http://dhd-wp.hab.de/?q=ag-text#abschnitt4 (30.8.2017)
                    
               
                  http://www.dfg.de/download/pdf/foerderung/grundlagen_dfg_foerderung/informationen_fachwissenschaften/geisteswissenschaften/foerderkriterien_editionen_literaturwissenschaft.pdf (30.8.2017)
                    
               
                  https://www.i-d-e.de/publikationen/weitereschriften/kriterien-version-1-1/ (30.8.2017)
                    
               
                  http://ride.i-d-e.de/ (30.08.2017)
                    
            
         
      
   



      
         
            Das Korpus
            Das ausgewählte Korpus besteht aus zwei Hauptwerken Dimitrie Cantemirs, eines Universalgelehrten des 17. Jahrhunderts und Mitglied der „Kurfürstlich –Brandenburgischen Societät der Wissenschaften“. Die zwei Werke wurden ursprünglich auf Lateinisch verfasst, die Originale sind aber verloren aber Kopien davon wurden im späten 20. Jh. wiederentdeckt. Im Umlauf waren lange Zeit nur Übersetzungen ins Englische, Deutsche (Cantemir 1771), (Cantemir 1745), und Französische, die mindestens bis Mitte des 19. Jh. Referenzwerke für die Geschichte des osmanischen Reichs und der historischen Provinz Moldawien waren. Durch seinen langen Aufenthalt in Istanbul, hatte Cantemir Zugang zu vielen Quellen die er zitiert. Daneben zitiert er auch Sagen und Legenden und versucht immer durch geschickte sprachliche Redewendungen zu vermitteln, was seiner Meinung nach historisch gesichert ist. Daher ist das Korpus besonders illustrativ für das Problem der Vagheitsannotation.
         
         
            Erste Schritte zur Vagheitsannotation
            Das Projekt HerCore versucht durch gezielte Annotation von Vagheit drei geisteswissenschaftliche Fragestellungen in Bezug auf die Cantemir-Forschung zu lösen:
            
               Der erstmalige Vergleich aller historischen Übersetzungen, da seit geraumer Zeit die Vermutung formuliert wurde, dass diese relativ stark von den Originalen abweichen.
               Die Untersuchung der Zuverlässigkeit von Äußerungen Cantemirs. Hierbei werden vor allem Quellen von turkologischen Fachwissenschaftlern einbezogen.
               Die Konsistenz von Cantemir über dieselben Personen und Ereignisse in den zwei Werken. 
            
            Die Annotation von Vagheit wird auf drei Ebenen untersucht:
            
               Linguistisch,
               In Metadaten und Editorik,
               im Fachwissen.
            
            
               Für die Linguistische Ebene wurde als Startpunkt die Klassifizierung von (Pinkal 1981)
            
            benutzt. Für die Laufzeit des Projekts haben wir aus dem o.g. Schema wegen besonderer Angemessenheit für das zu analysierende Korpus folgende mögliche Vagheitsindikatoren ausgewählt: 
            Auf lexikalischer Ebene: Non-Intersectives, Adjektive, Hecken, inexakte Maße, Modalverben (Attitudes), Komplexe Quantoren, Zitiereinleitungen, zeitliche Ausdrücke.
            Auf syntaktische Ebene: Subjunktiv-Konstruktionen 
            Zusätzlich werden Named Entities untersucht: Personen, Zeitangaben, Orte etc. und mit einem entsprechenden Vagheitsgrad versehen ("Konstantinopel" ist nur wahr zwischen 337 und 1930).
            Als Vorbereitung wird das Korpus zuerst einer linguistischen Ananlyse unterzogen, um Lemmas und Wortarten, sowie die Textstruktur (Sätze, Paragraphen) zu markieren. Diese wird dann die Basis für die semi-automatische Annotation von Vagheitsausdrücken (Vertan et al 2017).
            Die Annotation von vager Information wird dann in einem ersten Schritt manuell von Fachwissenschaftlern in einem Korpus-Ausschnitt vorgenommen. In einem zweiten Schritt wird versucht diese Annotation automatisch im Korpus zu propagieren. Ein dritter Schritt soll die Ergebnisse von Inferenzen zwischen vagen Ausdrücken erzeugen, um sich nicht dem Vorwurf auszusetzen man schreibe mit einem spezifischen Erkenntnis leitendem Interesse zunächst Annotationen in den Text um sie dann nur wörtlich wieder auszulesen.
         
         
            Zusammenfassung
            Der Beitrag wird die gesamte Systemarchitektur, sowie die einzelnen Schritte zur Annotation von Linguistischer Vagheit illustrieren.
            Um dem Wissenschaftler am Ende eine hermeneutische Interpretation zu erlauben, muss ihm zu jedem annotierten Objekt ein Vagheits-Profil sowie Metadaten über Autoren, Genres und Inferenzergebnisse gezeigt werden können.
            Hierzu eine Erweiterung von TEI und ein entsprechendes GUI zu entwickeln, sind wichtige Ziele des Projekts. 
            Außerdem muss die multilinguale Struktur der Cantemir-Texte mit Zitaten aus dem Griechischen, Lateinische und teilweise dem Türkischen annotiert und weiter erforscht werden.
            Das Projekt soll zeigen, dass die Einbeziehung von Vagheit und Unschärfe in die Annotierung, in die Inferenz-Komponente und die hermeneutische Interpretationen durch den Wissenschaftler, einen erheblichen Gewinn an Funktionen und Glaubwürdigkeit für die DH bringt.
         
      
      
         
            
               Bibliographie
               
                  Cantemir, Dimitrie, (1771) Beschreibung der Moldau, Faksimiledruck der Originalausgabe von 1771, Frankfurt und Leipzig 
                    
               
                  Cantemir, Dimitrie, (1745) Geschichte des osmanischen Reichs nach seinem Anwachse und Abnehmen, 1745, Herold, Hamburg
                    
               
                  Pinkal, Manfred,(1981) Semantische Vagheit: Phänomene und Theorien, Teil I/II. In: Linguistische Berichte Nr. 7/72, Wiesbaden 1980/1981.
                    
               
                  Vertan, Cristina / von Hahn, Walther / Dinu, Anca (2017) On the annotation of vague expressions: a case study on Romanian historical texts, Proceedings of the first Workshop on Language Technology for Digital Humanities in Central and (South-) Eastern Europe, in association with RANL 2017, Varna
                    
            
         
      
   



      
         In diesem Posterbeitrag beschreiben wir die Erstellung eines Thesaurus für das österreichische Strafrecht des 18. Jahrhunderts und gehen auf die damit verbundenen Herausforderungen ein, sowohl seitens des Themengebietes aber auch seitens der Quellen.
         Als Quellen dienen kaum erforschte Flugblätter zur Bekanntmachung von Hinrichtungen im 18. Jahrhundert (vgl. Ammerer/Adomeit 2010), die derzeit mit einem modernen Methodeninventar als XML-Dokumente nach den Richtlinien der TEI annotiert und auf ihre digitale Verfügbarkeit im Netz vorbereitet werden. Die Darstellung der Sachverhalte, die zur Hinrichtung führen, sind zwar medial überformt (vgl. Peil 2002, Kosenina 2005, Wiltenburg 2009, Dainat 2009, Härter 2010), beruhen allerdings auf den Entscheidungen des damaligen Stadtgerichts. 
         Bei der Erschließung zu berücksichtigen ist, dass im Laufe des 18. Jahrhunderts unterschiedliche Strafrechtsbestimmungen in Kraft waren. Vor 1768 gab es in den Ländern Österreichs und Böhmens kein einheitliches Straf- und Strafprozessrecht, jedoch galten die “Constitutio Criminalis Carolina Peinliche Gerichts- oder Peinliche Halsgerichtsordnung Kaiser Karls V.” und daneben unterschiedliche Landgerichtsordnungen, etwa die “Land-Gerichts-Ordnung. Deß Ertz-Hertzogthumbs Oesterreich unter der Ennß”. Erst im Jahre 1768 wurde durch die Constitutio Criminalis Theresiana, auch “Theresiana” genannt, ein einheitliches Straf- und Strafprozessgesetz eingeführt, welches aber bereits 1787 vom Allgemeines Gesetzbuch über Verbrechen und derselben Bestrafung, dem sogenannten “Strafgesetzbuch Josephs II” oder “Josephina” abgelöst wurde.
         All diesen Strafrechtsbestimmungen ist gemein, dass sie unterschiedliche Tatbestände definieren, die mit der Todesstrafe belegt sind. Im Laufe der Zeit wurden aber zum Teil neue Tatbestände ergänzt und mit neuen Definitionen versehen und stattdessen andere Delikte abgeschafft. Diese Delikte werden auch in den zu erschließenden Quellen beschrieben: Wie erwähnt, handelt es sich dabei nicht eigentlich um Rechtstexte, sondern um Flugblätter, die über öffentliche Hinrichtungen im 18. Jahrhundert berichten. Die Quellen werden zwar als “Todesurteile” bezeichnet, aber da sich die Flugblätter an ein breites Publikum wenden, werden die Delikte allgemeinverständlich beschrieben, kaum aber unter Verwendung der damals zeitgenössischen Rechtsterminologie. DelinquentInnen werden als 
                UrphedsbrecherInnen oder 
                DiebInnen bezeichnet, ohne aber den eigentlichen Tatestand zu nennen - eine Referenz auf die jeweilige Gesetzesstelle fehlt oft gänzlich. Im gesamten Quellenmaterial konnte nur ein einziger Beleg gefunden werden, bei dem direkt auf den Gesetzestext referenziert wurde. 
            
         Ein weitere Herausforderung für die Erschließung der Quellen stellen die Schreibvarianten dar, vgl. etwa 
                Diebstahl vs. 
                Diebstall oder 
                Urfehde vs. 
                Urphed, 
                Vrphed oder 
                Vrphedt. Aus diesen Gründen ist die toolgestützte Analyse und die Zuordnung der Flugblätter zu den einzelnen Straftatbeständen erschwert. In ähnlich gelagerten Projekten, wie z.B. den “Proceedings of the old Bailey online”, die eine Zeitspanne von ca. 240 Jahren abdecken, wurden die Delikte für die Erschließung des Materials bewusst nicht nach den gesetzlichen Bestimmungen definiert (vgl. Hitchcock, Tim et al.), sondern allgemeine Definitionen erarbeitet. In dem vorliegenden Projekt haben wir uns für eine quellennahe Definition entschieden. Aus diesem Grund ist es für den Thesaurus auch essentiell, nicht nur die Definitionen bereitzustellen, sondern auch die Angabe der Quelle, aus der die Definition stammt, sowie die Erfassung von Varianten. Als Framework für die Erstellung und mögliche Darstellung des Thesaurus im Semantic Web wird als formale Sprache auf SKOS (Simple Knowledge Organisation System) mit der Erweiterung SKOS-XL (SKOS Simple Knowledge Organization System eXtension for Labels) zurückgegriffen. 
            
         Um eine verallgemeinernde Typologisierung aller in den Quellen vorkommenden Delikte zu vermeiden und eine quellennahe und differenzierte Zuordnung zu ermöglichen, möchten wir den von uns erstellten Thesaurus präsentieren, der auf der Web-Applikation eine von mehreren Möglichkeiten des Zugriffs darstellen wird (andere Zugriffsmöglichkeiten ergeben sich aus biographischen Angaben zu den DelinquentInnen wie Alter, Geschlecht, Familienstand, Religionszugehörigkeit, Herkunft bzw. der Volltextsuche im Text). 
         Anhand des Posterbeitrags wollen wir erörtern, wie die einzelnen Tatbestände aus den unterschiedlichen Rechtsnormen miteinander in Relation gesetzt werden können, wie weiters eine Verbindung zu den heutigen Tatbeständen hergestellt werden wird und worin letztlich der Mehrwert für zukünftige UserInnen innerhalb und außerhalb der Academia besteht. Schließlich sollen auch mögliche Nachnutzungsszenarien für den Thesaurus thematisiert werden. 
      
      
         
            
               Bibliographie
               
                  Adebayo, Kolawole John / Di Caro, Luigi / Boella Guido (2016): “Annotating Legal Documents with Ontology Concepts, Conference” in: Jusletter IT 25. Februar 2016, Proceedings IRIS 2016.
                        https://jusletter-it.weblaw.ch/services/login.html?targetPage=http://jusletter-it.weblaw.ch/issues/2016/IRIS/annotating-legal-doc_77f5c9f8b8.html__ONCE&handle=http://jusletter-it.weblaw.ch/issues/2016/IRIS/annotating-legal-doc_77f5c9f8b8.html__ONCE [letzterZugriff 22. September 2017]
                    
               
                  Ammerer, Gerhard / Adomeit, Friedrich (2010): „Armesünderblätter“ in: Härter, Karl / Sälter, Gerhard / Wiebel, Eva (eds.): Repräsentation von Kriminalität und öffentlicher Sicherheit. Bilder, Vorstellungen und Diskurse vom 16. bis zum 20. Jahrhundert. Frankfurt am Main: Klostermann 271-308.
               
                  Dainat, Holger (2009): „Räuber im Oktavformat: Über die printmediale Aufbereitung von Kriminalität im 18. Jahrhundert“ in: Habermas, Rebekka / Schwerhoff, Gerd (Hrsg.): Verbrechen im Blick. Perspektiven der neuzeitlichen Kriminalitätsgeschichte. Frankfurt/New York: Campus Verlag 339-366.
               
                  Dirschl, Christian (2016): „Thesaurus Generation and Usage at Wolters Kluwer Deutschland GmbH“ in: Jusletter IT 25. Februar 2016, Proceedings IRIS 2016.
                        https://jusletter-it.weblaw.ch/services/login.html?targetPage=http://jusletter-it.weblaw.ch/issues/2016/IRIS/thesaurus-generation_da052418b5.html__ONCE&handle=http://jusletter-it.weblaw.ch/issues/2016/IRIS/thesaurus-generation_da052418b5.html__ONCE [letzterZugriff 22. September 2017]
                    
               
                  Härter, Karl (2010): „Criminalbilder: Verbrechen, Justiz und Strafe in illustrierten Einblattdrucken der Frühen Neuzeit“ in: Härter, Karl / Sälter, Gerhard / Wiebel, Eva (Hrsg.): Repräsentation von Kriminalität und öffentlicher Sicherheit. Bilder, Vorstellungen und Diskurse vom 16. bis zum 20. Jahrhundert. Frankfurt am Main: Klostermann 25-88.
                    
               
                  Hitchcock, Tim / Shoemaker, Robert / Emsley, Clive / Howard, Sharon / McLaughin, Jamie et al. (2012) (eds.) The Old Bailey Proceedings Online, 1674-1913. http://
                        www.oldbaileyonline.org/ [letzter Zugriff: 22. September 2017]
                    
               
                  Košenina, Alexander (2015): „Recht – gefällig. Frühneuzeitliche Verbrechensdarstellung zwischen Dokumentation und Unterhaltung“ in: Zeitschrift für Germanistik. Neue Folge Band 15, Nummer 1 (2005) 28-47.
               
                  Peil, Dietmar (2002): „Strafe und Ritual. Zur Darstellung von Straftaten und Bestrafungen im illustrierten Flugblatt.“ in: Harms, Wolfgang / Peil, Dietmar (Hrsg.): Wahrnehmungsgeschichte und Wissenschaftsdiskurs im illustrierten Flugblatt der Frühen Neuzeit (1450-1700). Basel 265-486.
               
                  Wiltenburg, Joy (2009) „Formen des Sensationalismus in frühneuzeitlichen Kriminalberichten“ in: Habermas, Rebekka / Schwerhoff, Gerd (Hrsg.): Verbrechen im Blick. Perspektiven der neuzeitlichen Kriminalitätsgeschichte. Frankfurt/New York: Campus Verlag 323-338.
               
                  Wersig, Gernot (2016): Thesaurus-Leitfaden: Eine Einführung in das Thesaurus-Prinzip in Theorie und Praxis. Berlin / Boston: de Gruyter Saur Reprint.
               
                  W3C: SKOS Simple Knowledge Organization System Reference. W3C Recommendation 18 August 2009 
                        https://www.w3.org/TR/2009/REC-skos-reference-20090818/) [letzter Zugriff: 22.09.2017]
                    
            
         
      
   



      
         Dank 
                machine learning und 
                computer vision ist seit wenigen Jahren die automatisierte Handschriftenerkennung möglich. Obwohl aktuell einzelne Handschriften bzw. sehr ähnliche Handschriftentypen noch trainiert werden müssen, wird es in absehbarer Zeit allgemeine Modelle geben, die Rohtranskriptionen mit einer Fehlerquote unter 10% ausgeben. Paläographische Kenntnisse werden vor allem zur Korrektur und kritischen Begutachtung der Technik nötig sein.
            
         Im Rahmen des Projekts READ (Recognition and Enrichment of Archival Documents) werden unterschiedliche Aufgaben der Automatisierung (weiter-)entwickelt, um qualitativ gute Ergebnisse mit optimalem Ressourceneinsatz zu erhalten. Ein speziell dafür entwickeltes Tool ist die Software Transkribus und die Transkribus Weboberfläche (für Transkription, Tagging/Annotation und Korrektur in der Layouterkennung). Beide Ansätze verkoppeln auf unterschiedliche Weise die Arbeit von Expertinnen und maschinelle Erkennleistung. Software und Webservice sind frei verfügbar unter 
                www.transkribus.eu. Darüber hinaus wurden im Rahmen von READ weitere Extraktions- und Annotationsmöglichkeiten entwickelt, die im Workshop zusammen mit Transkribus vorgestellt und durch die Teilnehmenden mit eigenen oder zur Verfügung gestellten Dokumenten getestet werden 
                können.
         
         Transkribus unterstützt alle Prozesse vom Import der Bilder über die Identifikation der Textblöcke und Zeilen, die zu einer detaillierten Verlinkung zwischen Text und Bild führt, sowie die Transkription und Annotation der Handschrift bis zum Export der gewonnen Daten in standardisierten Formaten. Darüber hinaus wurden aber noch weitere Tools und Algorithmen entwickelt, die zur Erkennung von graphischen Features genutzt werden können und Tabellen als solche aufbereiten.
         Transkribus als Arbeitsumgebung 
         Die Erkennung von Texten bedingt den Upload digitaler Bilder und Prozessierung mit Layouterkennungswerkzeugen. Upload und Layoutanalyse können grosse Batches verarbeiten. Die Nachkorrektur von Layoutanalyse ist nur noch in wenigen Fällen nötig.
         Je nach Einsatzzweck könne Dokumente entweder automatisch mit bereits bestehenden ATR-Modellen (Automatic Text Recognition) erkannt oder händisch Transkriptionen erstellt werden. Um im erkannten Text und Variantenlesungen (sog. Keywordspotting) zu suchen reicht in den meisten Fällen die Anwendung von bestehenden Modellen.
         Einzig im Umgang mit Tabellen sind weiterhin diverse manuelle Schritte möglich, um eine hochwertige Identifikation zu gewährleisten. Der Workshop wird einen Fokus auf die Bearbeitung und Erkennung von Tabellenstrukturen legen, die halbautomatisch erfolgen kann.
         Korrekter Text – entweder durch Transkription oder Korrektur von erkanntem Text entstanden – kann danach zum Training von Handschriftenmodellen verwendet werden. Im Rahmen des Workshops wird das Trainieren von Handschriftenmodellen demonstriert und kann durch die Teilnehmenden selbst ausgetestet werden.
         Aufbauend auf den Transkriptionen ist es möglich Entitäten (Personen, Orte, Verweise) auszuzeichnen und textuelle Annotationen (Titel, Marginalien, Fussnoten) innerhalb des Textes, aber auch darüber hinaus für Einzeldokumente und ganze Dokumentenbestände anzulegen. Visuelle Features wie Seitenzahlen, Titel oder Marginalien lassen sich nach der Auszeichnung als Strukturmodelle trainieren und können für die Erkennung von grösseren Dokumentenmassen verwendet werden. Die Vorgehensweise wird im Rahmen des Workshops vorgeführt und kann selbst nachvollzogen werden. Daneben ist auch die Anreicherung der Dokumente mit 
                named entities (Personen, Orten und Organisationen) möglich, sodass simple digitale Editionen grösstenteils in Transkribus erstellt werden können.
            
         Ausgabeformate 
         Für den Export stehen unterschiedliche Formate und Ausgabeformen zur Verfügung. So ist es möglich XML-Dateien zu exportieren, die den Vorgaben der TEI entsprechen (auch ist es möglich die Standardumformung abzuändern und den eigenen Bedürfnissen anzupassen). Weiter sind auch Ausgaben als Druckdaten (PDF) oder zur Weiterbearbeitung für Textverarbeitungsprogramme (DOCX, TXT) implementiert. Schließlich ist auch ein Export im PAGE-Format (zur Anzeige in Viewern für OCR gelesene Dokumente, Pletschacher, 2010) sowie als METS (Metadata Encoding and Transmission) möglich. 
         Zielpublikum 
         Die Plattform Transkribus ist für unterschiedliche Gruppen konzipiert. Einerseits für Geisteswissenschaftler*innen, die selbst Transkriptionen und Editionen historischer Dokumente erstellen möchten. Andererseits richtet sich die Plattform an Archive, Bibliotheken und andere Erinnerungsinstitutionen, die handschriftliche Dokumente in ihren Sammlungen aufbewahren und ein Interesse an der Suchbarmachung des Materials haben. Angesprochen werden sollen auch Studierende der Geistes-, Archiv- und Bibliothekswissenschaften mit einem Interesse an der Transkription historischer Handschriften. 
         Das Ziel, eine robuste und technisch hochstehende Automatisierung von Layout- und Handschriftenerkennung, lässt sich nur durch die enge Zusammenarbeit zwischen Geisteswissenschaftler*innen und Informatiker*innen sowie anderen Computerspezialist*innen mit unterschiedlichen Voraussetzungen und Ansprüchen an Datenqualität und Herstellung von Transkriptionen erreichen. Die Algorithmen werden somit nicht nur bis zu einem Status als 
                proof-of-concept erarbeitet, sondern bis zur Praxistauglichkeit verfeinert und in größeren Forschungs- und Aufbewahrungsumgebungen getestet und verbessert. Die Informatiker*innen sowie Personen aus angrenzenden Fächern sind entsprechend ebenfalls ein wichtiges Zielpublikum, wobei bei ihnen weniger die Nutzung der Plattform als das Beisteuern von Software(teilen) anvisiert wird. 
            
         Die Speicherung der Dokumente erfolgt in der Cloud, gehostet auf Servern der Universität Innsbruck. Die importierten Daten bleiben auch während der Bearbeitung unverändert im Dateisystem liegen und werden ergänzt durch METS und PAGE XML. Alle bearbeiteten Dokumente und Daten bleiben somit in den unterschiedlichen Bearbeitungsstadien nicht nur lokal verfügbar, sondern können für andere Transkribusnutzerinnen und -nutzer freigegeben werden. Dank elaboriertem 
                user-management ist die Zuteilung von Rollen möglich. 
            
         Die eingespeisten Dokumente und Daten bleiben privat und vor dem Zugriff Dritter geschützt. Von Projektseite können vorgenommene Arbeitsschritte zwecks besserem Verständnis der ausgeführten Arbeiten und letztlich der Verbesserung der Produkte ausgewertet werden.
         Die Erkennprozesse werden serverseitig durchgeführt, sodass die Ressourcen auf den lokalen Rechnern nicht strapaziert werden. Transkribus ist mit JAVA und SWT programmiert und kann daher plattformunabhängig (Windows, Mac, Linux) genutzt werden. 
         Ein- und Ausblicke im Workshop 
         Der Workshop richtet sich sowohl an Geisteswissenschaftler*innen als auch an Computerwissenschaftler*innen, wobei vorwiegend die Tools und Möglichkeiten von Transkribus präsentiert werden. 
         Drei zentrale Forschungsaspekte aus READ können im Rahmen des Workshops neben Transkribus 
                hands-on ausgetestet werden:
            
         
            Max Weidemann: Das Training von Handschriftenmodellen (HTR+);
            Sofia Ares Oliveira 
                (in English): Identifikation von visuellen Features mit dh-segment;
                
            Markus Diem: Aufbereitung und Erkennung von Tabellen mit Transkribus und nomacs.
         
         Programm/Ablauf des Workshops 
         
            Begrüssung und Einführung in READ und Transkribus :45’
            Kurze Beschreibungen der vermittelten Forschungsaspekte (je 15’): 45’
            Kaffeepause: 30’
            Arbeit in Kleingruppen am jeweiligen Forschungsaspekt: 60’ (nach ca. 40 Minuten besteht die Möglichkeit die Gruppe zu wechseln)
            Diskussion der Resultate, weiterer Ausblick und Evaluation: (15-)30'
         
         Nach Interesse der Teilnehmenden können während der Gruppenarbeit weitere Tools und Ansätze, die im Rahmen von READ entwickelt wurden, kurz diskutiert werden: 1. Matching von Text und Bild (bspw. aus bestehenden Transkriptionen), 2. Transkribus Learn (e-Learningumgebung), 3. Crowdsourcing-Infrastruktur, 4. ScanTent und DocScan (Fotografieren von Dokumenten mit Android App). 
         Während des gesamten Workshops stehen vier wissenschaftliche Mitarbeitende des Projekts für Fragen und Auskünfte zur Verfügung.
         Tobias Hodel nimmt bereits im Vorfeld gerne Dokumente oder Projektideen an, damit sich die Veranstalter bereits vor dem Workshop Gedanken zu möglichen technischen Umsetzungen machen können. 
         Das Projekt READ und somit die Weiterentwicklung von Transkribus werden finanziert durch einen Grant der Europäischen Union im Rahmen des Horizon 2020 Forschungs- und Innovationsprogramms (grant agreement No 674943). 
         Zahl der möglichen Teilnehmerinnen und Teilnehmer: Max. 30 Personen.
                 Benötigte technische Ausstattung: Beamer und Whiteboard. 
            
         Teilnehmende: Eigener Rechner (wenn möglich Installation von Transkribus; Hilfe zur Installation von Transkribus wird 15 Minuten vor der Veranstaltung angeboten).
         Rückfragen bitte an tobias.hodel@ji.zh.ch 
         Kontaktdaten aller Beitragenden (inkl. Forschungsinteressen)
         Sofia Ares Oliveira, École Polytechnique de Lausanne, 
                CDH-DHLAB, 
                INN 116 / Station 14 / CH-1015 Lausanne / Switzerland; sofia.oliveiraares@epfl.ch (
                Electrical engineering, signal processing, computer vision).
            
         Markus Diem, Technische Universität Wien, Institute of Computer Aided Automation Computer Vision Lab, Favoritenstr. 9/183-2, A-1040 Vienna, Österreich; diem@caa.tuwien.ac.at (Computer Vision, Document Analysis, Layout Analysis/Page Segmentation, Cluster Analysis, Automated Flow Cytometry Analysis).
         Tobias Hodel, Staatsarchiv des Kantons Zürich, Winterthurerstrasse 170, CH-8057 Zürich, Schweiz; tobias.hodel@ji.zh.ch (Digital Humanities; Automatic Text Recognition; eArchiving; Information Retrieval).
         Max Weidemann, Institut für Mathematik, Ulmenstraße 69, Universität Rostock, 18051 Rostock, Deutschland; max.weidemann@uni-rostock.de; (Deep Learning, Information Retrieval und Natural Language Processing). 
      
      
         
            
                    Einführend siehe die online Tutorials: 
                    https://read.transkribus.eu/transkribus/. Als 
                    hands-on Anleitung wird der Beitrag von Martin Prell empfohlen: »ps: ich bitt noch mahl umb ver gebung meines confusen und üblen schreibens wegen« – Frühneuzeitliche Briefe als Herausforderung automatisierter Handschriftenerkennung. Online: https://doi.org/10.22032/dbt.34849.
                
         
         
            
               Bibliographie
               
                  Leifert, Gundram / Strauß, Tobias / Grüning, Tobias / Wustlich, Welf / Labahn, Roger (2016): „Cells in Multidimensional Recurrent Neural Networks” in: Journal of Machine Learning Research 17, 1-37.
                    
               
                  Prell, Martin (2018): „»ps: ich bitt noch mahl umb ver gebung meines confusen und üblen schreibens wegen« – Frühneuzeitliche Briefe als Herausforderung automatisierter Handschriftenerkennung“. Online: https://doi.org/10.22032/dbt.34849.
                    
               
                  READ (2018): „Tutorials and How To Guides“. Online: https://read.transkribus.eu/transkribus/.
                    
            
         
      
   



      
         Das Projekt ZuMult – „Zugänge zu multimodalen Korpora gesprochener Sprache – Vernetzung und zielgruppenspezifische Ausdifferenzierung“ (zumult.org) – hat sich zum Ziel gesetzt, eine Architektur zu entwickeln, die einen einheitlichen Zugriff auf verschiedene Korpora gesprochener Sprache (Audio- und Videoaufzeichnungen mündlicher Interaktion mit zugehörigen Metadaten, Transkripten, Annotationen) an verschiedenen Standorten ermöglicht, und auf deren Basis Zugangswege gestaltet werden können, die für die Bedarfe spezifischer Nutzergruppen (z.B. Sprachlehrforschung, Variationslinguistik) optimiert sind. Mit unserem Poster stellen wir das technische Konzept und eine prototypische Implementierung einer solchen Basisarchitektur vor.
         Ausgehend von einer vergleichenden Analyse vorhandener Plattformen (u.a. Datenbank für Gesprochenes Deutsch, Schmidt 2016; GeWiss-Korpus-Interface, Fandrych, Meißner & Wallner 2017; Repositorium des Hamburger Zentrums für Sprachkorpora, Hedeland et al. 2014; sowie mehrere Lösungen, die außerhalb des deutschsprachigen Raums entwickelt wurden, z.B. Eshkol-Taravella et al. 2012, Komrsková et al. 2018) und einer Bestandsaufnahme existierender Standards im Bereich multimedialer Daten (vgl. dazu auch Schmidt 2014 und Schmidt et al. 2010) haben wir eine Dreiebenen-Lösung entwickelt, die so weit wie möglich auf etablierte (De Facto-)Standards aufbaut und anschlussfähig an existierende Lösungen ist. Damit wird eine transferfähige Basis für einen flexiblen Zugriff auf multimodale Korpora geschaffen. 
         Kern der Architektur ist zum einen eine objektorientierte Modellierung der Korpus-Bestandteile (Aufnahmen, Metadaten zu Sprechereignissen und Sprechern, Transkripte, Annotationen und Zusatzmaterialien) und ihrer Beziehungen zueinander. Für deren digitale Repräsentation (Serialisierung) werden Standards verwendet, soweit sie existieren. Für Medienobjekte können wir auf industrielle Standards insbesondere aus dem Kontext der Moving 133_final-* Expert Group (MPEG) zurückgreifen. Die Repräsentation von Transkripten und Annotation folgt dem in ISO (2016) definierten und auf den Richtlinien der Text Encoding Initiative (TEI) basierenden Format für „Transcriptions of Spoken Language“. Metadaten werden grundsätzlich in XML repräsentiert; in Ermangelung eines echten Standards, der in der Lage wäre, der Bandbreite und Komplexität von Metadaten im Bereich multimodaler Korpora vollständig gerecht zu werden, orientieren wir uns in diesem Bereich an CMDI-Profilen, die im CLARIN-Kontext für solche Korpora entwickelt wurden (z.B. Hedeland & Wörner 2012).
         Zum anderen beinhaltet die Architektur ein vereinheitlichtes Konzept zur Query auf Transkriptions- und Annotationsdaten. Dieses baut auf Überlegungen zu einer „Corpus Query Lingua Franca“ (Banski et al. 2016, ISO 2018) auf und berücksichtigt somit in der Korpuslinguistik verbreitete Suchsprachen wie CQP, ANNIS-QL, Poliqarp und weitere, die allerdings für die Besonderheiten angepasst werden müssen, die spontansprachliche Daten gegenüber schriftsprachlichen Korpora aufweisen. 
         Die Basisarchitektur besteht somit aus zwei gleichberechtigten Komponenten: Aus der Modellierung der Korpus-Bestandteile ergeben sich Zugriffs- und Navigationsmöglichkeiten für ganze Objekte bzw. Objekthierarchien, die auf Nutzerseite vor allem für ein exploratives Browsing auf den Daten eingesetzt werden. Die Query-Komponente ermöglicht hingegen eine gezielte Auswahl von (Teilen) von Objekten und damit systematische Recherchen im Sinne einer korpuslinguistischen Methodik. Beide Komponenten werden technisch als „Locators“ bzw. „Filters“ in einer REST API umgesetzt. Diese wird in der weiteren Projektarbeit die Basis darstellen, um zielgruppenspezifisch optimierte Zugänge zu den Daten zu entwickeln.
         Neben einem Überblick über diese Basis-Architektur wird unser Poster auch auf die konkrete Implementierung eingehen, die am Institut für Deutsche Sprache für den Zugriff auf die Daten aus dem Archiv für Gesprochenes Deutsch entwickelt wurde. Diese setzt auf ein vorhandenes Backend auf, das die Grundlage für die Datenbank für Gesprochenes Deutsch bildet und XML-basierte Daten in einer objektrelationalen Oracle-Datenbank hält. Für die Arbeiten in ZuMult wird dieses Backend für die im Projekt definierten Bedarfe angepasst und erweitert. Prototypische Applikationen, die den Einsatz der REST API illustrieren, werden als Software-Demonstrationen die Posterpräsentation ergänzen.
      
      
         
            
               Bibliographie
               
                  Banski, Piotr / Frick, Elena / Witt, Andreas (2016): "Corpus Query Lingua Franca (CQLF)". Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016), Portorož, Slovenia 2804-2809.  https://nbn-resolving.org/urn:nbn:de:bsz:mh39-50405
               
               
                  Eshkol-Taravella, I. / Baude, O. / Maurel, D. / Hriba, L. / Dugua, C. / Tellier, I., (2012): "Un grand corpus oral ,disponible’ : le corpus d’Orléans 1968-2012. " In: Ressources linguistiques libres, TAL. 52,3/2011, 17-46.
                    
               
                  Fandrych, Christian / Meißner, Cordula / Wallner, Franziska (eds.) (2017): "Gesprochene Wissenschaftssprache – digital Verfahren zur Annotation und Analyse müdlicher Korpora." Deutsch als Fremd- und Zweitsprache. Tübingen: Stauffenburg.
                    
               
                  Hedeland, Hanna / Wörner, Kai (2012): "Experiences and Problems creating a CMDI profile from an existing Metadata Schema". Proceedings of LREC-Workshop Describing LRs with Metadata: Towards Flexibility and Interoperability in the Documentation of LR, Istanbul, European Language Resources Association (ELRA) 37-40.http://www.lrec-conf.org/proceedings/lrec2012/workshops/11.LREC2012%20Metadata%20Proceedings.pdf
               
               
                  Hedeland, Hanna / Lehmberg, Timm / Schmidt, Thomas / Wörner, Kai (2014): "Multilingual Corpora at the Hamburg Centre for Language Corpora". In: Ruhi, Şükriye/Haugh, Michael/Schmidt, Thomas/Wörner, Kai (Hrsg.): Best Practices for Spoken Corpora in Linguistic Research. Newcastle: Cambridge Scholars Publishing, 2014. S. 208-224.
                    
               
                  ISO (ed.) (2016): ISO 24624:2016 Language resource management – Transcription of spoken language.
                  https://www.iso.org/standard/37338.html
               
               
                  ISO (ed.) (2018): ISO 24623-1:2018 Language resource management – Corpus query lingua franca (CQLF) -- Part 1: Metamodel.
                  https://www.iso.org/standard/37337.html
               
               
                  Komrsková, Zuzana / Kopřivová, Marie / Lukeš, David / Poukarová, Petra / Goláňová, Hana (2018): “New Spoken Corpora of Czech: ORTOFON and DIALEKT.” Journal of Linguistics 68:2, 219-228. 
                    
               
                  Schmidt, Thomas (2014): "(More) Common Ground for Processing Spoken Language Corpora?" In: Ruhi, Şükriye/Haugh, Michael/Schmidt, Thomas/Wörner, Kai (eds.): Best Practices for Spoken Corpora in Linguistic Research. Newcastle: Cambridge Scholars Publishing, 2014 249-265. http://pub.ids-mannheim.de/autoren/divers/3119.html
               
               
                  Schmidt, Thomas (2017): "DGD – Die Datenbank für Gesprochenes Deutsch. Mündliche Korpora am Institut für Deutsche Sprache (IDS) in Mannheim." In: Zeitschrift für Germanistische Linguistik 45(3), S. 451-463. 
                    
               
                  Schmidt, Thomas / Elenius, Kjell / Trilsbeek, Paul (2010): "Multimedia encoding and annotation". In: Hinrichs, Erhard (ed.): Interoperability and standards. Utrecht: Utrecht University, 2010 121-124. http://www.exmaralda.org/files/CLARIN_Standards.pdf
               
            
         
      
   



      
         
            Einführung
            In diesem 
                Beitrag wird das Redewiedergabe-Korpus (RW-Korpus) vorgestellt, ein historisches Korpus fiktionaler und nicht-fiktionaler Texte, das eine detaillierte manuelle Annotation mit Redewiedergabeformen enthält. Das Korpus entsteht im Rahmen eines laufenden DFG-Projekts und ist noch nicht endgültig abgeschlossen, jedoch ist für Frühjahr 2019 ein Beta-Release geplant, welches der Forschungsgemeinschaft zur Verfügung gestellt wird. Das endgültige Release soll im Frühjahr 2020 erfolgen. Das RW-Korpus stellt eine neuartige Ressource für die Redewiedergabe-Forschung dar, die in dieser Detailliertheit für das Deutsche bisher nicht verfügbar ist, und kann sowohl für quantitative linguistische und literaturwissenschaftliche Untersuchungen als auch als Trainingsmaterial für maschinelles Lernen dienen.
                
         
         
            Motivation und verwandte Forschung
            Redewiedergabe ist sowohl für die Linguistik als auch die Literaturwissenschaft ein interessanter Untersuchungsgegenstand. Die Repräsentation der Figurenstimme in Erzähltexten hat in der Narratologie viel Aufmerksamkeit erfahren und wurde in zahlreichen Kategoriesystemen abgebildet (vgl. z.B. Genette 2010; Martínez / Scheffel 2016). In der Linguistik besteht ein Interesse an sprachlichen Formen der Redewiedergabe, sowie an Redeeinleitungsverben (vgl. z.B. Hauser 2008, Engelberg 2015).
            Detaillierte, manuell annotierte Korpora mit diesem Themenschwerpunkt sind bislang vor allem für das Deutsche sehr rar. Ein Vorbild mit detaillierter, literaturwissenschaftlich motivierter Annotation mehrere Redewiedergabetypen für das Englische ist das Korpus von Semino/Short 2004. Das ebenfalls manuell annotierte DROC-Korpus hat seinen Schwerpunkt auf Figurenreferenzen in Romanen, enthält in diesem Kontext allerdings auch Annotationen direkter Wiedergabe mit Sprecherzuordnung (Krug et al. 2018b). Unser Korpus ist eine direkte Weiterentwicklung des aus 13 Erzähltexten bestehenden Korpus aus Brunner 2015, unterscheidet sich jedoch von diesem vor allem in folgenden Aspekten: Es enthält neben fiktionalen auch nicht-fiktionale Texte, die Annotationen sind durch Mehrfachannotation wesentlich verlässlicher und es ist deutlich umfangreicher (für das Beta-Release ca. 350.000 Tokens vs. 57.000 Tokens in Brunner 2015).
         
         
            Korpuszusammensetzung
            Das RW-Korpus umfasst Textmaterial aus dem Zeitraum 1840-1920. Es beruht auf den folgenden drei Textquellen, aus denen jeweils nur die Texte ausgewählt wurden, die in den Untersuchungszeitraum passen: 
            
               Erzähltexte aus der Digitalen Bibliothek, in TEI-Format konvertiert vom Projekt TextGrid (https://textgrid.de/digitale-bibliothek)
               Texte der Zeitschrift „Die Grenzboten”, digitalisiert durch die Staats- und Universitätsbibliothek Bremen und in TEI-Format konvertiert durch das Deutsche Textarchiv (http://www.deutschestextarchiv.de/)
               Das Mannheimer Korpus Historischer Zeitungen und Zeitschriften (MKHZ, https://repos.ids-mannheim.de/mkhz-beschreibung.html), bereitgestellt vom Institut für Deutsche Sprache und in TEI-Format konvertiert durch das Deutsche Textarchiv
            
            Bei der Korpuszusammenstellung sollte eine möglichst große Diversität der enthaltenen Texte erzielt werden. Um dies zu erreichen, setzt sich das Korpus aus Textausschnitten (Samples) zusammen. Diese haben mindestens 500 Wörter für fiktionale Texte bzw. 200 Wörter für nicht-fiktionale Texte – mit dieser großzügigeren Grenze war es möglich, auch kurze, abgeschlossene Artikel aufzunehmen, die für Zeitungen/Zeitschriften typisch sind. Die Samples wurden mit folgenden Besonderheiten randomisiert aus dem vorhandenen Textmaterial gezogen: Bei den Texten der Digitalen Bibliothek wurde erzwungen, dass jeder vertretene Autor innerhalb einer Dekade gleichermaßen berücksichtigt wird. Entsprechend wurde beim MKHZ erzwungen, dass alle in einer Dekade vertretenen unterschiedlichen Zeitungen/Zeitschriften gleichermaßen berücksichtigt werden. Damit wurde verhindert, dass Autoren bzw. Zeitungen/Zeitschriften mit wenig Material beim Sampling-Prozess vollkommen herausfallen. Das Beta-Release enthält Texte von etwa 140 unterscheidbaren Autoren und aus 20 unterschiedlichen Zeitungen/Zeitschriften.
            Die Quelltexte wurden größtenteils in ihrem Ursprungszustand belassen, mit zwei Ausnahmen: Da für die Zeitschrift „Die Grenzboten” nur automatische OCR-Erkennung durchgeführt wurde, wurden die Samples aus dieser Textquelle manuell nachkorrigiert. In den Texten aus den beiden anderen Quellen wurden häufige Sonderzeichen, wie das Schaft-S, durch moderne Äquivalente ersetzt, jedoch weisen die Texte dennoch in unterschiedlichem Maße altertümliche Schreibungen und z.T. auch Sonderzeichen auf. Insgesamt ist festzuhalten, dass die Textformen im RW-Korpus sehr divers sind, so sind z.B. Texte im Dialekt enthalten, sowie Zeitungsausschnitte, die reine Listen sind. Wir haben uns bewusst dagegen entschieden, solch ungewöhnliches Material herauszufiltern, um eine realistische Repräsentation des Textmaterials aus den untersuchten Dekaden zu erhalten. 
            Beim RW-Korpus wurde eine Ausgewogenheit in der zeitlichen Dimension (Textmaterial pro Dekade) sowie zwischen fiktionalen und nicht-fiktionalen Texten angestrebt.
            Entgegen ursprünglicher Annahmen stellte es sich als nicht sinnvoll heraus, die Trennung fiktional - nicht-fiktional rein aufgrund der Textquelle zu treffen: Es liegt in der Natur der Textsorte Zeitung/Zeitschrift, dass dort auch fiktionale Texte abgedruckt werden (im Feuilleton, als Fortsetzungsromane u.Ä.). Somit wurde das Kriterium ‚fiktional’ für jedes Sample individuell festgelegt. Unsere Definition für ‚Fiktion’ ist dabei angelehnt an Gabriel 2007: „Ein erfundener (‚fingierter’) einzelner Sachverhalt oder eine Zusammenfügung solcher Sachverhalte zu einer erfundenen Geschichte” (Gabriel 2007: 594). Bei der Identifizierung wurde besonderer Wert auf paratextuelle Merkmale (z.B. Untertitel, Rubriken u.Ä.) gelegt. Von den Samples aus dem MKHZ und den „Grenzboten” wurden auf diese Weise ca. 12% als fiktional eingestuft. 
            Die folgende Tabelle zeigt die wichtigsten Metadaten des RW-Korpus, welche nach dem Sampling und der Textkorrektur vergeben werden.  
            
               
                  Metadatum
                  Werte
                  Beschreibung
               
               
                  year
                  Zahl zwischen 1840 und 1919
                  Erscheinungsjahr des Textes (bei digBib-Texten: Ersterscheinungsjahr, falls verfügbar)
               
               
                  decade
                  Zahl in 10er Schritten
                  Erscheinungsdekade des Textes
               
               
                  source
                  digBib, grenz, mkhz
                  Textquelle; bei mkhz wird noch ein Kürzel für die jeweilige Zeitung/Zeitschrift beigefügt
               
               
                  title
                  String, undefined
                  Titel des Textes, falls bekannt
               
               
                  author
                  String, undefined
                  Autor des Textes, falls bekannt
               
               
                  fictional
                  yes, no 
                  Ist der Textausschnitt fiktional?
               
               
                  text_type
                  Erzähltext, Kommentar, Anzeige, Reportage, Nachrichten, Biographie, Rezension, Reisebericht/Brief, unsure 
                  Texttyp; wenn ein Ausschnitt mehrere Texttypen enthält (z.B. Kommentar und Anzeigen), wird nach dem dominanten Typ klassifiziert oder ansonsten ‚unsure’ vergeben
               
            
            Aufgrund der Diversität der in Zeitungen/Zeitschriften vertretenen Texte wurde für jedes Sample eine nähere Klassifikation des Texttyps vorgenommen, so dass auch dessen Einfluss auf die Verteilung der Redewiedergabetypen untersucht werden kann. Die folgende Abbildung gibt einen ersten Eindruck, welch deutliche Abweichungen hier erkennbar sind. Gezeigt werden nur die Texttypen, für beim Korpusstand vom 25.09.2018 mehr als 10 Samples vorlagen. Die Y-Achse zeigt Prozent der Tokens im Text.
            
               
            
         
         
            Annotationssystem
            Wir unterscheiden die vier Typen direkte, indirekte, frei indirekte (‚erlebte’) und erzählte Wiedergabe, sowie die drei Medien Rede 
                (speech), Gedanke 
                (thought) und Schrift 
                (writing), so dass sich insgesamt zwölf Annotationsmöglichkeiten ergeben.
                
            Außerdem annotieren wir die Rahmenformel, die eine direkte oder eine indirekte Wiedergabe einleiten kann. In den Rahmenformeln sowie den Instanzen von erzählter Wiedergabe wird das zentrale Wort markiert, das auf die Sprech-/Gedanken-/Schreibhandlung verweist 
                (z.B. sagte, Gedanke). Zudem wird für alle Wiedergabetypen der Sprecher markiert, falls vorhanden.
                
            Während die Unterscheidung der drei Medien nur in Ausnahmefällen problematisch ist, bedürfen die vier Typen genauerer Definitionen.
            Die direkte Wiedergabe 
                (direct) ist eine wörtlich zitierte Äußerung einer Figur. Sie kann von einer Rahmenformel eingeleitet werden und als einziger Wiedergabetyp Anführungszeichen verwenden.
                    
               Er fragte: 
               „Wo ist das Mittagessen?”
            
            Die indirekte Wiedergabe 
                (indirect) ist eine nicht-wörtliche Wiedergabe einer Äußerung. Sie ist in unserem Annotationssystem formal definiert und besteht aus einer Rahmenformel und einem abhängigen Nebensatz, der häufig im Konjunktiv steht. Dies kann ein Nebensatz mit Verbzweitstellung sein, mit 
                    dass, 
                    ob oder 
                    w-Fragewort oder ein (erweiterter) Infinitivsatz. 
                    
               Er fragte, 
               wo das Mittagessen sei.
            
            Die freie indirekte Wiedergabe 
                (free indirect) – in der Literatur oft ‚erlebte Rede‘ genannt – definiert sich über die Überlagerung von Figuren- und Erzählerstimme und ist daher eine typische Form fiktionaler Texte. Sie weist keine Rahmenformel und keine sonstigen formalen Markierungen wie Anführungszeichen auf. Finden sich Elemente der Erzählerstimme wie das Tempus Präteritum oder Personalpronomen der dritten Person und gleichzeitig Elemente der Figurenstimme wie Deiktika, Subjektivität, Ausrufe oder figurentypischer Wortschatz, sind dies Indikatoren für freie indirekte Wiedergabe.
                    
               Woher sollte er denn jetzt bloß ein Mittagessen bekommen?
            
            Die erzählte Wiedergabe 
                (reported) ist die Erwähnung eines Sprech-, Denk oder Schreibakts, aus der man nicht auf den eigentlichen Inhalt schließen kann. Hinweise auf erzählte Wiedergabe geben Wiedergabewörter, die Thematisierung einer Wiedergabehandlung sowie der Inhalt derselben.
                    
               Er sprach über das Mittagessen.
            
            Ein Sonderfall sind uneingeleitete Konjunktivsätze, die zur Wiedergabe verwendet werden. Diese werden als Mischform zwischen indirekter und frei indirekter Wiedergabe markiert. 
                    
               Sie stellte viele Fragen. 
               Wo sei das Mittagessen?
            
            Darüber hinaus gibt es zusätzliche Attribute, die Besonderheiten bei der Wiedergabe markieren und in der folgenden Tabelle dargestellt werden:
            
               
                  level
                  Verschachtelungstiefe der Wiedergabe
               
               
                  non-fact
                  nicht-faktische Wiedergaben (z.B. Negationen, zukünftigen Aussagen oder Absichten)
               
               
                  border
                  Fälle, die an der Grenze von Rede-, Gedanken- oder Schriftwiedergabe liegen, also nicht alle prototypischen Kriterien der jeweiligen Wiedergabeart erfüllen
               
               
                  prag
                  sprachliche Wendung, die das Muster einer Wiedergabe aufweist, pragmatisch aber einen anderen Zweck erfüllt (z.B. Höflichkeitsfloskeln)
               
               
                  metaph
                  Metaphern in Form von Wiedergaben 
                        (z.B. Die Klugheit riet mir davon ab.)
                        
               
            
            Die detaillierten Annotationsrichtlinien können unter http://redewiedergabe.de/richtlinien/richtlinien.html eingesehen werden.
         
         
            Annotationsprozess
            Die genaue Identifizierung und Klassifizierung der Redewiedergaben auf der Grundlage des detaillierten Annotationssystems ist eine schwierige Aufgabe.
            Jedes Sample des RW-Korpus durchläuft darum einen mehrschrittigen Prozess. Zunächst wird es von zwei Annotatoren unabhängig voneinander annotiert. Danach vergleicht ein weiterer Experte die Annotationen und erstellt, falls notwendig, eine Konsens-Annotation, die dann ins finale Korpus aufgenommen wird. Jedes Sample wird also von drei Personen bearbeitet, um größtmögliche Konsistenz zu gewährleisten.
            Die Annotatoren arbeiten mit dem eclipse-basierten Annotationswerkzeug ATHEN (entwickelt von Markus Krug im Projekt Kallimachos, www.kallimachos.de), für das im Projekt eine spezielle Oberfläche für die Redewiedergabe-Annotation implementiert wurde (für eine detaillierte Beschreibung vgl. auch Krug et al. 2018a). Das Werkzeug ist frei verfügbar unter der Adresse 
                    http://ki.informatik.uni-wuerzburg.de/nappi/release/.
                
         
         
            Verfügbarmachung und Ausblick
            Das Beta-Release wird in einem standardisierten und dokumentierten Textformat im Langzeitarchiv des Instituts für Deutsche Sprache zur freien Nutzung zur Verfügung gestellt 
                (https://repos.ids-mannheim.de/). Spätestens für das finale Release im Frühjahr 2020 garantieren wir ein TEI-kompatibles XML-Format. Zudem wird weiteres im Kontext des Redewiedergabe-Projekts entstandenes Material zur Verfügung gestellt, wie nur einfach annotiertes Textmaterial und annotierte Volltexte. Im Jahr 2020 werden auch Werkzeuge fertiggestellt sein, die es erlauben, in Texten verschiedene Formen der Redewiedergabe automatisch zu erkennen.
                
            Nutzungsszenarien für das Korpus sind vielfältig: Aus NLP-Perspektive kann es als Test- und Trainingsmaterial für automatische Redewiedergabeerkenner verwendet werden. Aus linguistischer Perspektive bieten sich Korpusstudien zu sprachlichen Eigenheiten der Redewiedergabe an, wie z.B. die laufenden Studien zu Redewiedergabeeinleitern von Tu. Aus literaturwissenschaftlicher Perspektive erlaubt das Korpus z.B. Untersuchungen zu der Häufigkeit und Form von Wiedergaben in Erzähltexten in ihrer Relation zur Figurencharakterisierung.
         
      
      
         
            
                    Die ersten beiden Autoren haben zu gleichen Teilen an der Erstellung dieses Beitrags mitgewirkt.
                
         
         
            
               Bibliographie
               
                  Brunner, Annelen (2015): Automatische Erkennung von Redewiedergabe. Ein Beitrag zur quantitativen Narratologie (=Narratologia 47). Berlin: De Gruyter.
                    
               
                  Engelberg, Stefan (2015): „Quantitative Verteilungen im Wortschatz. Zu lexikologischen und lexikografischen Aspekten eines dynamischen Lexikons“, in: Eichinger, Ludwig M. (eds.): Sprachwissenschaft im Fokus. Positionsbestimmungen und Perspektiven. Jahrbuch 2014 des IDS. Tübingen: Narr 205-230.
                    
               
                  Gabriel, Gottfried (2007): „Fiktion“, in: Fricke, Harald et al. (eds.): Reallexikon der deutschen Literaturwissenschaft. Bd. 1: A-G. Berlin / New York: De Gruyter 594-598.
                    
               
                  Genette, Gérard (2010): Die Erzählung. 3., durchges. und korrigierte Aufl. Paderborn: Fink.
                    
               
                  Hauser, Stefan (2008): „Beobachtungen zur Redewiedergabe in der Tagespresse. Eine kontrastive Analyse“, in: Lüger, Heinz-Helmut / Lenk, Hartmut E.H. (eds.): Kontrastive Medienlinguistik. Landau: Verlag Empirische Pädagogik 271-286.
                    
               
                  Krug, Markus / Tu, Ngoc Duyen Tanja / Weimer, Lukas / Reger, Isabella / Konle, Leonard / Jannidis, Fotis / Puppe, Frank(2018a): „Annotation and beyond – Using ATHEN Annotation and Text Highlighting Environment“, in: Digital Humanities im deutschsprachigen Raum – Konferenzabstracts 19-21.
                    
               
                  Krug, Markus / Weimer, Lukas / Reger, Isabella / Macharowsky, Luisa / Feldhaus, Stephan / Puppe, Frank / Jannidis, Fotis (2018b): Description of a Corpus of Character References in German Novels - DROC [Deutsches ROman Corpus]. DARIAH-DE Working Papers Nr. 27, Göttingen: DARIAH-DE, 2018, URN: urn:nbn:de:gbv:7-dariah-2018-2-9.
                    
               
                  Martínez, Matías / Scheffel, Michael (2016): Einführung in die Erzähltheorie. 10. Auflage. München: C.H.Beck.
                    
               
                  Semino, Elena / Short, Mick (2004): Corpus stylistics. Speech, writing and thought presentation in a corpus of English writing. London / New York: Routledge.
                    
            
         
      
   



      
         
            Einleitung
            Die Arbeit zur digitalen historischen Semantik in Frankfurt am Main erreicht mit der neuen webbasierten Plattform und Datenbank „Latin Text Archive“ 
                (LTA) im Rahmen der Dateninfrastrukturen der Berlin-Brandenburgischen Akademie der Wissenschaften (BBAW) ein neues Level. Nach über zehn Jahren Entwicklungsarbeit an den Datenbanken „Historical Semantics Corpus Management 
                (HSCM)“, „Frankfurt Latin Lexicon (FLL)“ sowie an der Webseite 
                „www.comphistsem.org“ ergaben sich drei grundlegende Herausforderungen: (1) Nachhaltigkeit und Verfügbarkeit der geleisteten Arbeit mussten gesichert, (2) Benutzerfreundlichkeit und Funktionalität verbessert sowie (3) Akzeptanz und Verankerung innerhalb der Fachwissenschaft gesteigert werden. Das LTA antwortet auf diese Herausforderungen und führt mit historischen Referenzkorpora neue Arbeitsinstrumente ein. Das LTA ist auch das Ergebnis einer neuen strategischen Partnerschaft, um die bisherige Arbeit aus der stets zeitlich begrenzten Projektentwicklung an der Universität in einen dauerhaften Betrieb an einer Akademie zu überführen. 
                
            Zur Standortbestimmung der Frankfurter Arbeit lässt sich Michael Piotrowskis Definition von 
                    digital humanities (Piotrowski 2018: 2) anwenden: Danach ist sie den 
                    applied digital humanities zuzuorten, da ein konkretes geschichtswissenschaftliches Forschungsziel mit digitalen Techniken verfolgt wird. Der Historiker Bernhard Jussen hat zur Umsetzung seines Postulates einer „kulturellen Semantik“ nach Wegen gesucht, wie computerlinguistische Methoden helfen können, Bedingungen, Mittel und Formen multimedialer Sinnproduktion in vergangenen Gesellschaften zu erforschen (Jussen 2000: 24ff.). Es geht um die kontrollierte Analyse von semantischem Wandel innerhalb der lateinischen Textproduktion in poströmischer Zeit. Ein Anwendungsgebiet ist die politische Geschichte. So lässt sich mittels der computergestützten Semantik danach fragen, welche impliziten politischen Ordnungsmodelle in Texten sichtbar werden, bevor mit der Wiederentdeckung der Politik des Aristoteles der Begriff des Politischen aufkommt? Außerdem kann die Begriffs- und Ideengeschichte Verwendungszusammenhänge von zentralen Vokabeln untersuchen (Geelhaar 2015; Schwandt 2018). Am Ende sollen aber nicht nur Forschungsergebnisse und -methoden, sondern auch Arbeitsinstrumente und Forschungsdaten einem breiten fachwissenschaftlichen Publikum ohne Programmierkenntnissen zur Weiterverwendung bereitstehen. Hierzu hat Bernhard Jussen den Computerlinguisten Alexander Mehler und dessen Text Technology 
                    Laboratory für eine Kooperation gewonnen, in der die texttechnologische und die geschichtswissenschaftliche Seite ihre jeweiligen Agenden verfolgen und vom interdisziplinären Austausch profitieren. Aus dieser Konstellation ergibt sich, dass die Vorstellung des LTA aus geschichtswissenschaftlicher Perspektive den Fokus nicht auf technische bzw. technologische, sondern auf programmatische und anwenderbezogene Aspekte legt.
                
         
         
            Das Latin Text Archive: Teil des DTA-Markenstamms
            Das LTA ist eine frei zugängliche, webbasierte Plattform zu Korpusaufbau und Korpusanalyse sowie auch eine Datenbank. Technisch baut es auf den am Deutschen Textarchiv (DTA, DFG-gefördert zwischen 2007 und 2016, siehe Geyken et al. 2018) entwickelten Komponenten zur Textpräsentation auf und erweitert somit den Markenstamm des DTA um eine lateinische Textkomponente. Diese umfasst die Textproduktion im lateinischsprachigen Europa von (zunächst) 400 bis 1500. Die versammelten Texte basieren auf kritischen und somit für die Geschichtswissenschaft validen Editionen, soweit sie in Open Access verfügbar sind. Sie werden zum Zweck des Text Mining um den kritischen Apparat 
                gekürzt, im TEI-P5-Format nach dem für HSCM entwickelten Datenmodell aufbereitet, vollständig lemmatisiert und mit einer aufwändigen Metadaten-Annotierung angereicht, die zugleich die Vernetzung zu anderen digitalen Ressourcen herstellt – nicht zuletzt zu den textgebenden Institutionen 
                selbst. Hierbei handelt es sich u. a. um die 
                    Monumenta Germaniae Historica (MGH). Dieses wichtige deutsche Editionsunternehmen für mittelalterliche Texte stellt seine Editionen im „openMGH“-Projekt unter Creative-Commons-Lizenz in TEI-konformen Versionen zur 
                    Verfügung. Doch erst durch die Datenintegration ins LTA können auch reine Anwender vom openMGH-Projekt profitieren, da die Editionen nun erst wortstatistisch vergleichend analysiert werden können. Des Weiteren ist das LTA auf kontrollierte Datenerweiterung durch die gezielte Aufbereitung und Übernahme aus projektexternen Quellen ausgelegt, um in Zukunft ein repräsentatives Korpus historischer, lateinischer Textproduktion analysierbar zu machen. Hierzu können die Daten entweder als Gesamtkorpus und nach freier Auswahl durch den Anwender an Analysemodule weitergereicht werden, von denen die Voyant Tools bereits verfügbar 
                    sind. Die in den Vorgängerprojekten entwickelten Analysetools werden als weiteres, externes Modul zugänglich gemacht. Dabei handelt es sich um Konkordanz- und Kookkurrenzanalysen sowie die Berechnung semantischer Netzwerke. 
                
         
         
            Zusammenhang zwischen LTA und HSCM
            Das LTA unterscheidet sich in mehrfacher Hinsicht von seinen Vorgängeranwendungen. Die Überführung des Datenbestandes aus HSCM in das LTA als Teil der von der BBAW betreuten Dateninfrastruktur dient dem Zweck der nachhaltigen Verfügbarkeit. Zudem wird das LTA als explizites Parallelangebot zum DTA vom Renommee der BBAW profitieren, die durch eigene Datenprojekte nicht nur eine ausgezeichnete Expertise in den DH vorweisen kann, sondern auch bereits hohe Anerkennung in den Geisteswissenschaften genießt. Zudem gewinnt das LTA durch die Anlehnung an das DTA, da es einen Wiedererkennungseffekt in der Benutzerführung gibt, der das Arbeiten mit dem LTA erleichtert. Die wesentlichen Neuerungen gegenüber HSCM bestehen aber nicht nur in der verbesserten Benutzerführung; wichtiger noch ist die Trennung der Primärdatenaufbereitung von der Datenverwaltung, indem die Daten in HSCM kuratiert und im LTA zur Verfügung gestellt werden. Das Preprocessing neuer Texte wird weiterhin über HSCM als Teil des eHumanities-
                Desktops laufen und über den eigens vom Text Technology Lab entwickelten TT Lab Tagger (vor der Brück/Mehler 2016; Eger/Gleim/Mehler 2016) für die automatische Lemmatisierung lateinischer Texte, über das dafür nötige morphologische Lexikon („Frankfurt Latin Lexicon“) sowie über Editoren zur kontrollierenden, manuellen Nachlemmatisierung und zur nachträglichen Korrektur des TEI-Codes. Die vollständig bearbeiteten Texte werden anschließend in das LTA überführt, wo es nicht mehr möglich sein wird, in den jeweiligen Source-Code des Textes einzugreifen. Dies erlaubt eine feste Indexierung (auch der Lemmatisierungsinformationen), wodurch die Schnelligkeit bei der Verarbeitung von Suchanfragen bedeutend gesteigert wird. Darüber hinaus ist durch den Datentransfer die Analyse des Materials nicht mehr auf die in HSCM vorhandenen Tools beschränkt, sondern können im Grunde von allen denkbaren Toolkits wie eben den Voyant Tools oder 
                Diacollo weiterverwendet werden. 
                
         
         
            Geschichtswissenschaftliche Referenzkorpora
            Die dritte wesentliche Neuerung sind die unter geschichtswissenschaftlichen Aspekten kontrollierten 
                Referenzkorpora, um Veränderungen im Sprachgebrauch zeitlich wie genrespezifisch berechnen und visualisieren zu können. Wie die DTA-Referenzkorpora beinhalten diese Referenzkorpora ganze Werke und nicht nur Samples wie linguistische Korpora (z. B. das British National Corpus). Das Clustering von Texten wird nach Vierteljahrhunderten und nicht nach Zehn-Jahres-Schritten wie im DTA 
                geschehen, weil eine präzisere zeitliche Zuordnung aufgrund fehlender Datierungen und mitunter komplizierten Überlieferungsgeschichten nicht möglich ist. Dieser Arbeitsschritt erforderte zudem eine erneute klassische Quellenkritik zur Chronologie einzelner Texte. Die Textmengen pro Zeiteinheit sollen quantitativ nicht zu sehr voneinander abweichen, was angesichts der teilweise eklatanten Disparität historischer Schriftproduktion eine große Herausforderung darstellt. Außerdem wird, soweit möglich, der Verbreitungsgrad handschriftlicher Überlieferung berücksichtigt, wenngleich das LTA ansonsten an der Idee des Textes als abstrakter Größe aus konzeptionellen Gründen festhalten 
                muss. Das erste Referenzkorpus besteht aus narrativen Texten, die aus den Scriptores-Reihen der MGH stammen und historiographische wie hagiographische Texte beinhalten. Künftige Korpora werden Briefe bzw. Urkunden und vor allem auch theologische bzw. juristische Traktate umfassen, um somit Sprachgebrauch in verschiedenen Genres vergleichen zu können.
                
         
      
      
         
            
               http://lta.bbaw.de
            
            
                    Jussen/Mehler/Ernst 2007; Cimino/Geelhaar/Schwandt 2015. HSCM wurde zwischen 2008 und 2014 aus den Mitteln des Gottfried Wilhelm Leibniz-Preises der DFG sowie aus den Mitteln des LOEWE-Schwerpunktes „Digitale Humanities“ finanziert, um im BMBF-Projekt „Computational Historical Semantics“ (2013-2016) weiterentwickelt zu werden.
                
            
                    Eine Präsentation der Frankfurter Projekte für das CCeH 2017 findet sich unter: 
                    http://www.geschichte.uni-frankfurt.de/43013259/geelhaart (alle folgenden Links wurden eingesehen am 6.10.2018)
                
            
               https://www.texttechnologylab.org/
            
            
                    Kritisch hierzu Fischer 2017: S266.
                
            
                    Es gibt Verlinkungen, soweit möglich, zum Repertorium Fontium Mediae Aevi 
                    (www.geschichtsquellen.de), zu VIAF (Personen und Werke) und zum Katalog der Staatsbibliothek zu Berlin für bibliographische Angaben.
                
            
               http://www.mgh.de/dmgh/openmgh/
            
            
               https://voyant-tools.org/ Zu dessen Anwendung in der Geschichtswissenschaft siehe Schwandt 2018: 125-133.
                
            
                    HSCM ist ein Modul der VRE „eHumanities Desktop 2.2“ 
                    (www.hudesktop.hucompute.org) des Text Technology Laboratory.
                
            
               https://clarin-d.de/de/kollokationsanalyse-in-diachroner-perspektive
            
            
                    Zu den Schwierigkeiten mit historischen Korpora und von Historikern organisierten Korpora siehe Geelhaar 2015: 11f.
                
            
                    Unser Kooperationspartner IRHT/CNRS verfolgt im Corpus-Building-Project VELUM 
                    (http://www.agence-nationale-recherche.fr/Project-ANR-17-CE27-0015) eine sehr viel gröbere Stratifikation.
                
            
                    Eine Korpusanreicherung mittels digital edierter Handschriften ist technisch in HSCM/LTA realisierbar, würde aber zu Inkonsistenzen im Materialbestand führen. Hierzu auch Fischer 2017: S280.
                
         
         
            
               Bibliographie
               
                  Vor der Brück, Tim / Mehler, Alexander (2016): „TLT-CRF: A Lexicon-supported Morphological Tagger for Latin Based on Conditional Random Fields”, in: “Proceedings of the 10th International Conference on Language Resources and Evaluation”.
                    
               
                  Eger, Steffen/ Gleim, Rüdiger / Mehler, A. (2016): „Lemmatization and Morphological Tagging in German and Latin: A comparison and a survey of the state-of-the-art”, in: “Proceedings of the 10th International Conference on Language Resources and Evaluation”.
                    
               
                  Cimino, Roberta / Geelhaar, Tim / Schwandt, Silke (2015): “Digital Approaches to Historical Semantics: new research directions at Frankfurt University”. In: Storicamente 11. http://storicamente.org/historical_semantics [letzter Zugriff 12.10.2018] 7. DOI: 10.12977/stor594 
               
               
                  Fischer, Franz (2017): „Digital Corpora and Scholarly Editions of Latin Texts: Features and Requirements of Textual Critism”, in: Speculum 92/S1: S266-S287. https://doi.org/10.1086/693823
                    
               
                  Geelhaar, Tim (2015): “Talking About christianitas at the Time of Innocent III (1198–1216): What Does Word Use Contribute to the History of Concepts?” in: Contributions to the history of concepts 10/2: 7–28. https://doi.org/10.3167/choc.2015.100202
                    
               
                  Geyken, Alexander / Boenig, Matthias / Haaf, Susanne / Jurish, Bryan / Thomas, Christian / Wiegand, Frank (2018): „Das Deutsche Textarchiv als Forschungsplattform für historische Daten in CLARIN“, in: Lobin, Henning/ Schneider, Roman / Witt, Andreas (Hgg.): Digitale Infrastrukturen für die germanistische Forschung (= Germanistische Sprachwissenschaft um 2020, Bd. 6). Berlin/Boston: De Gruyter, 219–248. https://doi.org/10.1515/9783110538663-011
                    
               
                  Jussen, Bernhard, Mehler, Alexander / Ernst, Alexandra (2007): „A Corpus Management System for Historical Semantics. Sprache und Datenverarbeitung“, in: International Journal for Language Data Processing 31/2: 81-87. 
                    
               
                  Jussen, Bernhard (2000): Der Name der Witwe. Erkundungen zur Semantik der mittelalterlichen Bußkultur. (VMPIG, Bd. 158). Göttingen.
                    
               
                  Piotrowski, Michael (2018): „Digital Humanities – An explication”, in: Burghardt, Manuel, Müller-Birn, Christian (eds.): INF-DH 2018 – Workshopband, 25. Sept. 2018, Berlin https://doi.org/10.18420/infdh2018-07
               
               
                  Schwandt, Silke (2018): „Digitale Methoden für die Historische Semantik. Auf den Spuren von Begriffen in digitalen Korpora“, in: Geschichte und Gesellschaft 44: 107-134. https://doi.org/10.13109/gege.2018.44.1.107
               
               
                  Mehler, Alexander / vor der Brück, Tim  / Gleim, Rüdiger / Geelhaar, Tim (2015): „Towards a Network Model of the Coreness of Texts: An Experiment in Classifying Latin Texts using the TTLab Latin Tagger”, in Text Mining: From Ontology Learning to Automated text Processing Applications, C. Biemann and A. Mehler, Eds., Berlin/New York: Springer, 2015, pp. 87-112. 
					
               
                  Mehler, Alexander / Schwandt, Silke / Gleim, Rüdiger / Jussen, Bernhard: „Der eHumanities Desktop als Werkzeug in der historischen Semantik: Funktionsspektrum und Einsatzszenarien”", Journal for Language Technology and Computational Linguistics (JLCL), vol. 26, iss. 1, pp. 97-117, 2011.
					
            
         
      
   



      
         
            Einleitung
            Obwohl sich infrastrukturell einiges getan hat, sieht ein typischer Operationsmodus der digitalen Literaturwissenschaft immer noch so aus, dass eine bestimmte Forschungsmethode auf ein oft nur ephemeres Korpus angewandt wird. Im besten Fall ist das Ergebnis 
                    irgendwie reproduzierbar, im schlechtesten Fall gar nicht. Im besten Fall gibt es ein offen zugängliches Korpus in einem Standardformat wie TEI, einer anderen Markup-Sprache oder zumindest als txt-Datei. Im schlechtesten Fall ist das Korpus gar nicht zugänglich, d. h., die Forschungsergebnisse müssen einfach hingenommen werden.
                
            Doch seit kurzem gibt es Anzeichen, dass sich dies ändert. Einige Digital-Humanities-Projekte stellen Schnittstellen zu stabilen Korpora zur Verfügung, über die man mannigfaltige Zugriffsmöglichkeiten bekommt und reproduzierbar arbeiten kann. Eines dieser Projekte ist DraCor, eine offene Plattform zur Dramenforschung, die in diesem Vortrag vorgestellt werden soll (zugänglich unter 
                    bzw. über die Repos und verschiedene Schnittstellen). DraCor transformiert vorliegende Textsammlungen zu ›Programmable Corpora‹ – ein neuer Begriff, den wir mit diesem Vortrag ins Spiel bringen möchten.
                
         
         
            Die Bausteine
            
               Vanillekorpora
               Ähnlich wie die COST Action zu europäischen Romanen (Schöch et al. 2018), versucht das DraCor-Projekt als Basis für eine digitale Komparatistik einen Stamm an multilingualen Dramenkorpora aufzubauen, die in basalem TEI kodiert sind. Ein selbst betriebenes russischsprachiges 
                    () und ein deutschsprachiges Korpus 
                    () dienen dabei als Einstieg. Diese Korpora sind, ähnlich wie die Sammlung »Théâtre classique« von Paul Fièvre, im weitesten Sinne als Vanillekorpora angelegt, die über das notwendige Markup hinaus zunächst kaum weitere spezielle Auszeichnungen enthalten, allerdings frei zur Verfügung stehen und damit fork- und erweiterbar sind. Zur Demonstration, dass auch andere, reicher kodierte Korpora dazugebunden und sofort alle bereits bestehenden Extraktions- und Visualisierungsmethoden der Plattform angeboten werden können, wurden das Shakespeare Folger Corpus sowie das schwedische Dramawebben-Korpus geforkt und angedockt 
                    (bzw. 
                        ). Dramenkorpora in weiteren Sprachen sollen folgen; einzige Voraussetzung dabei ist jeweils, dass diese in TEI vorliegen.
                    
               Die Vorteile von frei auf GitHub gehosteten Korpora liegen auf der Hand. Unabhängig von den letztlich durch die Plattform zur Verfügung gestellten Schnittstellen können die Korpora alternativ durch Klonen oder andere Downloadmethoden, etwa über den SVN-Wrapper von GitHub, direkt bezogen und individuell weiterverarbeitet werden. Ein offen zugängliches GitHub-Repositorium heißt auch, dass Pull Requests zur Fehlerkorrektur und Forks für Erweiterungen möglich und erwünscht sind.
            
            
               XML-Datenbank (eXist-db) und Frontend
               DraCor als Plattform setzt auf die eXist-Datenbank, um die TEI-Dateien zu verarbeiten und Funktionen zur Beforschung der Korpora zur Verfügung zu stellen. Das Frontend wurde mit ReactJS gebaut, ist responsiv und einfach erweiterbar. Der Schwerpunkt liegt aber nicht auf der GUI, sondern auf der API (vgl. generell zur Unterscheidung zwischen beiden Schnittstellenansätzen Bleier/Klug 2018).
            
            
               API und Entwicklungsumgebung
               Um dem Ideal und der Möglichkeit nahe zu kommen, auf einfache Weise »alle Methoden auf alle Texte« anwenden zu können (Frank/Ivanovic 2018), braucht es mehr als offene Korpora. Der zitierte Text von Frank/Ivanovic macht sich hinsichtlich dessen für SPARQL-Endpunkte stark; auch DraCor bietet einen solchen an, besitzt darüber hinaus aber eine reiche API, die über Swagger dokumentiert und erläutert wird 
                    (). In einem Teilbereich der Korpusphilologie, den Digital Scholarly Editions, hat die Diskussion um eine proaktivere Nutzung von APIs bereits begonnen (zur Vorgeschichte vgl. wiederum Bleier/Klug 2018), als Beispiel hierfür diene die Folger Digital Texts API 
                    (), über die man sich spezifische Querys zusammenbauen kann. Der Vorteil einer moderneren Lösung wie Swagger besteht darin, dass API-Querys live und direkt ausgeführt und die Outputs genauer kontrolliert und gesteuert werden können.
                    
               Ein einfaches Use-Case-Szenario sieht dann so aus, dass man etwa im RStudio mit zwei, drei Zeilen Code einen Blick in ein Korpus werfen kann, etwa über die zeitliche Entwicklung der Anzahl der Charaktere im russischen Drama zwischen 1740 und 1940, die in der Metadatentabelle festgehalten sind 
                    (). Diese Datei, beziehbar im JSON- oder CSV-Format, wird in eine Data.Table eingelesen, woraufhin die Werte zweier Spalten (Erscheinungsjahre und Number of Speakers) einfach über ggplot visualisiert werden können (Abb. 1).
                    
               
                  
                     
                     Abbildung 1: Anzahl der Charaktere pro Drama in chronologischer Ordnung (Quelle: RusDraCor).
                  Anhand dieses sehr simplen Beispiels zeigt sich dann recht deutlich, dass sich mit Puschkins an Shakespeare angelehntem historischen Drama »Boris Godunow« (1825), in dem Sprechakte von 79 Charakteren vorkommen, eine strukturelle Diversifizierung der russischen Dramenlandschaft Bahn bricht.
                    
               Die Möglichkeiten beschränken sich aber nicht darauf, vorgefertigte API-Funktionen zu benutzen. Neue Forschungsideen zeitigen immer auch neue Bedarfe an einfach bezieh- und reproduzierbaren Daten und Metriken; die API kann dementsprechend erweitert werden. Dies wird dadurch erleichtert, dass über Apache Ant die gesamte Entwicklungsumgebung auf dem eigenen System nachgebaut werden kann.
               Durch bereits implementierte Funktionen können neben Struktur- und Metadaten etwa auch Volltexte ohne Markup bezogen werden (auch Untermengen von Volltexten wie Regieanweisungen), etwa wenn Methoden wie die Stilometrie oder das Topic Modeling der Endzweck sind, also Methoden, die nach dem »bag of words«-Prinzip arbeiten, für das kein Markup vonnöten ist.
               Insgesamt wird durch den Aufbau und die Dokumentation offener APIs die bisher oft aufwendige Reproduzierbarkeit von Forschungsergebnissen erheblich erleichtert.
            
            
               Shiny App
               Ein Beispiel für die vielseitigen Nutzungsmöglichkeiten der DraCor-API ist die Shiny App, die Ivan Pozdniakov aufgesetzt hat 
                    (). Shiny ist ein auf R basierendes Framework, das es ermöglicht, interaktive Visualisierungen im Browser darzustellen. Die DraCor-Shiny-App tut genau dies und setzt dabei vollkommen auf die DraCor-API für den Datenbezug. So kann zu Lehr- und Forschungszwecken, aber auch zur einfacheren Datenkorrektur, auf Visualisierungen des aktuellen Datenbestandes zugegriffen werden.
                    
            
            
               Didaxe
               Das Markup oder andere Formalisierungen literarischer Texte sind nicht selbsterklärend. Zwar gibt es einige Standards, aber die jeweilige Operationalisierungslösung hängt von der Forschungsfrage ab. Allein das Extrahieren von Figurennetzwerkdaten ist auf viele Arten und Weisen möglich, was dazu führt, dass etwa alle von verschiedenen Forschungsgruppen extrahierten Netzwerke aus Shakespeares »Hamlet« zu leicht verschiedenen Ergebnissen kommen. Selbst für Dramen ist dies also schon ein nicht-trivialer Akt, von Romanen dann ganz zu schweigen (beispielhaft seien Grayson et al. 2016 genannt, die verschiedene Extraktionsmethoden für Romane durchtesten und die Ergebnisse vergleichen). Um diese Erkenntnis schon in der Lehre zu fördern, wurde das Tool »Easy Linavis« 
                    () entwickelt und in die DraCor-Toolchain integriert. Per Hand können Netzwerkdaten aus Texten extrahiert und dabei das Bewusstsein für die Kontingenz dieses Vorgangs geschärft werden, eine wichtige Vorstufe zur Operationalisierung.
                    
               Neben einem Ansatz zur Gamifizierung des TEI-Korrekturvorgangs (Göbel/Meiners 2016) haben wir für Lehrzwecke auch ein Dramenquartett entwickelt, um spielerisch das Verständnis von Netzwerkwerten zu trainieren (Fischer at al. 2018).
               Die aufgezählten, um die Plattform herumgruppierten didaktischen Mittel sind integraler Bestandteil des ganzen Projekts, da sie auf dessen Daten und Operationalisierungen aufsetzen. Wichtig dabei war die Erkenntnis, dass Daten mehrere Gestalten annehmen und für Forschung und Lehre gleichermaßen von Bedeutung sein können.
            
            
               Linked Open Data (LOD)
               Im TEI-Code sind PND- bzw. Wikidata-Identifier sowohl für Autor*innen als auch für die Werke hinterlegt. Auf diese Weise lassen sich verschiedene Realien, die außerhalb der eigenen Korpusarbeit liegen, hinzufügen. Eine automatisch erstellte Autor*innengalerie hat dabei noch eher illustrativen Charakter (de la Iglesia/Fischer 2016).
               Darüber hinaus kann man aber zum Beispiel feststellen, ob es nicht einen unbewussten regionalen Bias im Korpus gibt. Dafür lässt man sich über die Wikidata-Identifier die Verteilung der Geburts- und Sterbeorte der Autor*innen auf einer Karte anzeigen. So konnte dann für das deutschsprachige Korpus GerDraCor ausgeschlossen werden, dass es einen solchen Bias gibt, da sich die Orte relativ gleichmäßig über die (historisch) deutschsprachigen Gebiete verteilen (Göbel/Fischer 2015).
               Ebenso lässt sich über die Wikidata-ID der Stücke herausfinden, wo diese uraufgeführt worden sind (Beispiel-Query: 
                        ), d. h., Aspekte der Aufführungsgeschichte lassen sich zuschalten, obwohl diese gar nicht im Fokus des Kernprojekts liegen. Programmable Corpora verbinden sich also auch mit der Welt um sie herum, was sie u. a. von den nach innen gerichteten Workbenches der Korpuslinguistik unterscheidet.
                    
            
            
               Infrastruktur statt Rapid Prototyping
               Projekte wie DraCor versuchen nichts anderes als den digitalen Literaturwissenschaften eine verlässliche und ausbaufähige Infrastruktur zu geben, damit sie sich stärker auf eigentliche Forschungsfragen konzentrieren und reproduzierbare Ergebnisse hervorbringen können.
               Eine wichtige Folgerung für uns war, dass wir die Weiterentwicklung unserer seit vier Jahren entwickelten all-in-one Python-Skriptsammlung 
                        dramavis aufgeben und uns lieber der Arbeit an der API widmen. 
                        Dramavis (Kittel/Fischer 2014–2018 sowie Fischer et al. 2017) folgte dem in den Digital Humanities nicht untypischen Rapid Prototyping mit direkter Verarbeitung literarischer XML-Daten (Trilcke/Fischer 2018) und einer mittlerweile stark gewachsenen Codebasis, die alles auf einmal kann, deren Maintenance aber immer schwieriger geworden ist und oft genug von den eigentlichen Forschungsfragen weggeführt hat.
                    
            
         
         
            
               Fazit
                
            In Anlehnung an das Projekt »ProgrammableWeb« – das eine Datenbank von offenen APIs unterhält und dessen Slogan lautet: »APIs, Mashups and the Web as Platform« (zugänglich unter 
                    ) – schlagen wir für infrastrukturell-forschungsorientierte, offene, erweiterbare und LOD-freundliche Korpora den Begriff ›Programmable Corpora‹ vor.
                
            Programmable Corpora erleichtern es, Forschungsfragen auf viele Arten und Weisen um Korpora herum programmieren zu können. Es steht zu erwarten, dass sich infrastrukturelle Anstrengungen dieser Art für die gesamte Community auszahlen mit Effekten, wie sie John Womersley in seiner Präsentation auf der ICRI2018 in Wien aufgezählt hat: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction.
            Der Anschlussmöglichkeiten sind viele, egal ob man gar nicht programmieren möchte, sondern nur eine GEXF-Datei für Gephi benötigt, ob ein Korpus über seine Verbindungen zur Linked Open Data Cloud beforscht oder einfach aus R oder Python heraus bestimmte Daten bezogen werden sollen, ohne dass man sich mit dem Korpus und dessen Maintenance und Reproduzierbarkeit selbst kümmern muss (all dies bleibt natürlich aber eine Option). Programmable Corpora erleichtern die Entscheidung, auf welcher Ebene der eigene Forschungsprozess einsetzt.
         
      
      
         
            
               Bibliographie
               
                  Bleier, Roman / Klug, Helmut W. (2018): Discussing Interfaces in Digital Scholarly Editing. In: Digital Scholarly Editions as Interfaces. BoD, Norderstedt, S. V–XV. URL: 
               
               
                  de la Iglesia, Martin / Fischer, Frank (2016): The Facebook of German Playwrights. URL: 
               
               
                  Fischer, Frank / Dazord, Gilles / Göbel, Mathias / Kittel, Christopher / Trilcke, Peer (2017): Le drame comme réseau de relations. Une application de l‘analyse automatisée pour l’histoire littéraire du théâtre. In: Revue d'historiographie du théâtre. № 4. URL: 
               
               
                  Fischer, Frank / Kittel, Christopher / Milling, Carsten / Schultz, Anika / Trilcke, Peer / Wolf, Jana (2018): Dramenquartett – Eine didaktische Intervention. In: Konferenzabstracts zur DHd2018, Universität zu Köln. S. 397 f. DOI: 
               
               
                  Göbel, Mathias / Fischer, Frank (2015): The Birth and Death of German Playwrights. URL: 
               
               
                  Göbel, Mathias / Meiners, Hanna-Lena (2016): Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus. In: Konferenzabstracts zur DHd2016, Bern/CH. S. 140–143. URL: http://www.dhd2016.de/abstracts/vortr%C3%A4ge-007.html
               
               
                  Grayson, Siobhán / Wade, Karen / Meaney, Gerardine / Greene, Derek (2016): The Sense and Sensibility of Different Sliding Windows in Constructing Co-Occurrence Networks from Literature. In: 2nd IFIP International Workshop on Computational History and Data-Driven Humanities. Trinity College Dublin 2016. PDF: 
               
               
                  Kittel, Christopher / Fischer, Frank (2014–2018): dramavis. Python-Skriptsammlung. Repo: 
               
               
                  Schöch, Christoph et al. (2018): Distant Reading for European Literary History. A COST Action [Poster]. In: DH2018: Book of Abstracts / Libro de resúmenes. Mexico: Red de Humanidades Digitales A. C. URL: https://dh2018.adho.org/en/?p=11345
               
               
                  Trilcke, Peer / Fischer, Frank (2018): Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen. In: Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden. Hrsg. von Martin Huber und Sybille Krämer (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3). DOI: 
               
            
         
      
   



      
         
            Einleitung
            In unserem Vortrag fassen wir Ergebnisse einer laufenden, interdisziplinären Kooperation im Bereich digitaler Epigraphie zwischen dem Akademieprojekt "Textdatenbank und Wörterbuch des Klassischen Maya" (TWKM, Universität Bonn, Prager) und einem auf Schriftforschung spezialisierten Team am Interdisciplinary Center for Scientific Computing (IWR Heidelberg, Mara, Bogacz und Feldmann) zusammen (vgl. Bogacz, Feldmann, Prager, Mara 2018). Im Mittelpunkt der Kooperation zwischen Schriftforschung und angewandter Informatik steht die vollautomatische Erkennung von Zeichen der Hieroglyphenschrift der Klassischen Mayakultur in 3D (Zeitraum: 250 - 900 n.Chr., Region: südliches Mexiko, Guatemala, Belize und Honduras). Am IWR in Heidelberg wird dieses Verfahren seit mehreren Jahren erfolgreich bei der automatischen Erkennung und Transliteration von Keilschriftzeichen auf Tontafeln angewendet, die mittels eines hochauflösenden 3D-Scanners dokumentiert wurden (Bogacz, Gertz und Mara 2015; Bogacz, Klingmann, Mara 2017) (Abbildung 1) .
            
               
               
                  Abbildung 1. Ergebnisauswahl einer automatisierten Abfrage des akkadischen Keilschriftzeichens /ta/ in einem Textkorpus. Korrekte Erkennungen sind grün markiert (Bogacz, Gertz und Mara 2015, Abb. 4).
                    
            
            Im Zuge der Arbeit für die Textdatenbank und das digitale Wörterbuch des Klassischen Maya hat das Projekt Mayainschriften in 3D dokumentiert und zusammen mit den Projektpartnern am IWR Möglichkeiten getestet, ob die aus rund 1000 Elementen bestehende Hieroglyphenschrift der Maya ebenfalls dazu geeignet ist, automatisiert erkannt zu werden. Der wesentlichste Unterschied zu aktuellen Vorgehensweisen in der digitalen Epigraphie, denen typischerweise die manuelle Kodierungen und Verlinkung der Texte (z.B. mit Hilfe von XML TEI, RDF), sowie darauf aufbauende korpuslinguistische Analysen zu Grunde liegen (vgl. Diehr et al. 2018 zum Klassischen Maya; Chiarcos et al. 2018 am Beispiel des Sumerischen Textkorpus), ist der weitgehend vollautomatische Ansatz auf Bilddaten (Abbildung 2). In der konkreten Anwendung handelt es sich um Krümmungsvisualisierungen, die aus 3D-Messdaten berechnet werden. In diesem Sinne entsteht eine Optical Character Recognition (OCR) für Schrift in 3D. Ein Ziel der Kooperation ist es mit Hilfe dieses automatisierten Verfahrens einen auf der Zeichenmorphologie basierenden Katalog der Mayaschriftzeichen einschließlich ihrer Varianten automatisiert zu erstellen. 
            
               
            
            
               
               
                  Abbildung 2. Beispiel für automatische Zeichenerkennung der Hieroglyphe /chi/ aus der Dresdner Mayahandschrift mit Hilfe eines 
                        Histogram of Oriented Gradients (HOG) (Feldmann, Bogacz, Prager, Mara 2017, Abb. 3).
                    
            
            Wir präsentieren Methoden, Herausforderungen und Ergebnisse und zeigen am Material auf, wo wir Fortschritte und Durchbrüche, aber auch (aktuell) Grenzen bei der (voll)automatisierten Erkennung von Maya-Schriftzeichen festgestellt haben. Fokussierend auf Differenzen zwischen manueller und automatischer Zeichenbestimmung ziehen wir Schlussfolgerungen für die Erforschung der Maya-Schrift und die angewandte Informatik.
         
         
            Geburtsstunden und -wehen der digitalen Epigraphie des Klassischen Maya
            Die digitale Epigraphie des Klassischen Maya erlebte ihre Geburtsstunde zu Beginn des Kalten Krieges, als sowjetische Forscher elektronische Rechenmaschinen einsetzten um das Rätsel der Mayaschrift zu lösen. Wenige Jahre zuvor veröffentlichte ein damals führender deutscher Experte für die Maya-Hieroglyphen, die Entschlüsselung der Mayaschrift sei nach über fünfzig Jahren vergeblicher Arbeit ein unlösbares Problem (Schellhas 1945). Nur fünfzehn Jahre später verkündeten sowjetische Mathematiker und Archäologen, dass ihnen gemeinsam mit Hilfe eines Elektronenmaschinenrechners innerhalb von vierzig Stunden die vollständige Entzifferung und Übersetzung der Maya-Handschriften gelungen sei (Sobolev 1961; O'Kane 1962). Für die maschinelle Verarbeitung übertrug man die Texte von zwei erhaltenen Maya-Handschriften mit Hilfe von Nummernschlüsseln in ein maschinenlesbares Format und speicherte sie über Lochkarten in der Rechenmaschine. In Minutenschnelle prozessierte der Rechner das Datenmaterial und erzeugte Häufigkeits-, Vorkommens- und Kookkurrenzanalysen der Schriftzeichen, Zeichenkombinationen, Wörter und ganzen Wortfolgen. Die Ergebnisse der lexikometrischen Erhebung wurde mit Häufigkeiten von Silben, Silbenkombinationen, Wörtern und Sätzen aus yukatekisch-sprachigen und in lateinischer Schrift aufgezeichneten Texten und Wörterbüchern aus dem 16. und 17. Jahrhundert verglichen und korreliert. Die Grundannahmen des Forscherteams waren jedoch falsch, der Versuch das Problem der Mayaschrift maschinell zu lösen, galt schon kurz nach seiner Veröffentlichung als gescheitert (Schlenther 1964). 
         
         
            Digitaler Dornröschenschlaf oder wie das Rätsel doch gelöst wurde
            Weitere Versuche die Mayaschrift digital zu erforschen folgten Mitte 1960 (Rendón 1965), dann erst wieder in den 1980er Jahren (Ringle und Smith-Stark 1996). Beide Projekte beschränkten sich darauf Konkordanzen einzelner Hieroglyphen zu kompilieren. Keines der Projekte hatte jedoch einen nennenswerten Impakt für die Forschung: die Anfang der 1960er Jahre begonnene digitale Epigraphik des Klassischen Maya fiel in einen Dornröschenschlaf, aus der sie erst wieder durch das Forschungsprojekt Textdatenbank und Wörterbuch des Klassischen Maya erweckt wurde (Prager et al. 2016). Die Entzifferung der Mayaschrift gelang in den vergangenen 50 Jahren gänzlich ohne komputationelle Hilfsmittel. Wegweisend dazu waren Arbeiten des Ägyptologen Juri Knorozov (1956). Aufgrund der Zeichenzahl schloss er daraus, dass es sich bei der Mayaschrift um ein dem Altäyptischen vergleichbares Schriftsystem handelte und hatte dadurch den logo-syllabischen Charakter des Mayaschriftsystems erkannt. Die Entzifferung der Mayaschrift ist bis heute im Prozess, wir kennen etwa von 60% der rund 1000 verschiedenen Schriftzeichen den Lautwert. Obschon die Ergebnisse des sowjetischen Teams Anfang der 1960er Jahre die Forschung nicht nachhaltig beeinflusste, war es aus heutiger Sicht das erste Projekt, das nicht nur interdisziplinär zusammenarbeitete, sondern EDV zur Lösung eines epigraphisch-linguistischen Problems herangezogen hatte.
         
         
            Herausforderungen beim Verständnis des Maya-Schriftsystems: Graphematische und graphetische Herausforderungen für die Erkennung von Schriftzeichen
            Bedeutend für den Durchbruch bei der Entzifferung war die Entdeckung des Prinzips der Zeichensubstitution, die auch den großen Variantenreichtum in der Mayaschrift erklärt. Abbildung 3 zeigt drei morphologisch unterschiedliche Graphe des Zeichens /pa/, die lediglich das gemeinsame, diagnostische Merkmal einer schraffierten Fläche aufweisen.
            
               
               
                  Abbildung 3. Sogenannte Standard-, Kopf- und Körpervariante des Zeichens für die Silbe /pa/ in der Mayaschrift (Marc Zender, 1999).
                    
            
            Heute kennen wir eine Bandbreite an Schreib- und Gestaltungsprinzipien, womit nicht nur das einzelne Graphem, sondern auch Wörter des Klassischen Maya variantenreich realisiert wurden. Die Schreiber strebten ein Höchstmaß an visueller Prachtentfaltung und formaler Variation an. Eintönigkeit, Konformität und Wiederholung sollten vermieden werden, kalligraphische Spielarten bestimmten das Werk des Schreibers und stellen heute eine immense Herausforderung für die automatische Erkennung dar.  
         
         
            Automatisiertes Suchen von Mayaschriftzeichen: erste Ergebnisse
            
               
               
                  Abbildung 4. Ähnlichkeitsnetzwerk von Zeichen der Mayaschrift, die der Algorithmus vollautomatisch erkannt und verlinkt hat (Bartosz Bogacz et al. 2018).
                    
            
            Mit dem heutigen Stand der Technik in Form von Rechenleistung und Methoden der Bildverarbeitung, Mustererkennung und Maschinellem Lernen konnten wir zeigen, dass die manuellen Entzifferungen der 1980er Jahre in Algorithmen abgebildet werden können. Dabei ist anzumerken, dass z.B. die berechneten Ähnlichkeitsmaße mit Arbeitsplatzrechnern innerhalb weniger Minuten bestimmt werden können. Dies ermöglicht das Testen verschiedener Hypothesen über Zeichenähnlichkeiten in Form von Änderungen an Parametern und Kombinationen von unterschiedlichen Algorithmen. Hierbei wird der bereits in der digitalen Erforschung von Keilschrifttafeln angewendete 
                    Multiscale Integral Invariant Filter (MSII) eingesetzt um Schriftzeichen in 3D-Objekten zu isolieren. Die 3D-Objekte werden in diesem Verfahren in 2D umgesetzt und mit Hilfe von Projektionsprofilen segmentiert, um ein Gitter aus Spalten und Zeilen zu erzeugen. Anschließend werden die Hieroglyphenblocks selbst nach dem Zufallsprinzip segmentiert, wobei Hintergrund und Vordergrund aufgrund der Oberflächenkrümmung der ursprünglichen 3D-Oberfläche getrennt werden. Die abgerufenen Zeichen werden zunächst nach ihrer Größe zu einem Satz gängiger Größen zusammengefasst. Für jede Glyphe wird ein auf dem Histogramm der Gradienten (HOG) basierender Merkmalsvektor berechnet und für ein hierarchisches Clustering verwendet (Abb. 3). Ein bemerkenswertes Ergebnis ist das Erkennen von Linienelementen komplexer Zeichen, die auch in gestauchter Form vorkommen können. Damit verhält sich die vollautomatische 
                    Machine Learning Pipeline sehr ähnlich zur Diagnostik, wie sie von Experten angewendet wird. Mit den jetzigen Parametern werden gedoppelte und gestauchte Elemente der Mayaschrift korrekt identifiziert. Dies wird in einer Visualisierung der Zeichen in einem Graphen bzw. Ähnlichkeitsnetzwerk besonders deutlich. Das System errechnet die Grenzen der Hieroglyphenblöcke (Abbildung 4) und kann zwischen Bild- und Textinformationen unterscheiden. 
                
            Das Datenmaterial für unsere Experimente stammt mit einer Tafel aus Cancuen und einer anderen Tafel aus dem Fundort La Corona zudem aus unterschiedlichen Epochen und Regionen. Durch die Verwendung von hoch-aufgelösten 3D-Messdaten handelt es sich hierbei um digitale Primärquellen im Unterschied zu (retro-)digitalisierten Handzeichnungen, die eine Interpretation beinhalten, und somit eher als interpretierte Sekundärquellen zu verstehen sind.
            Im erzeugten Ähnlichkeitsnetzwerk der Primärquellen ist klar zu erkennen, dass Übereinstimmungen bei konstanten Elementen wie Zahlzeichen verbunden werden, während komplexere Zeichen in geschlossenen Clustern gezeigt werden, die dem Inschriftenträger entsprechen. Bei erodierten Zeichen im gleichen Cluster wurden korrekte Vorschläge gemacht (Abbildung 4, links). Deutlich werden Verbindungen aufgezeigt zu denen bereits Vermutungen über Übereinstimmungen vorliegen (Abbildung 4, rechts). Bei sauber gearbeiteten Schreibungen ohne Abweichungen ist die Identifizierung immer perfekt und basiert auf den diagnostischen Elementen eines Zeichens. Dies entspricht der intuitiven Analyse durch einen Experten.
            
               
               
                  Abbildung 5. Korrekte Rekonstruktion des Silbenzeichens /na/, das im oberen Fall fast gänzlich verloren ist.
                    
            
            
               
               
                  Abbildung 6. korrekte Übereinstimmung identischer Zeichen. Grafik: Bartosz Bogacz, 2018.
                    
            
         
         
            Schlussfolgerung
            Aus der Sicht der angewandten Informatik ist die Erkennung von Mayaschriftzeichen eine interessante Herausforderung, da die Grundwahrheit nicht bekannt ist. Weil die digitale Beschreibung und Verarbeitung der Zeichen kontinuierlich verbessert wird, kann das Wissen über die Zeichen immer weiter an die Grundwahrheit angenähert werden. Damit entsteht ein Spannungsfeld in dem sich die Entwicklung von Algorithmen und die Entwicklung von geisteswissenschaftlichen Fragestellungen gegenseitig beeinflussen. Dabei kommt es immer wieder zu analytischen hermeneutischen Fragen über die Anwendbarkeit der neuesten Entwicklungen in der Informatik. Insbesondere die – in anderen Anwendungsgebieten – sehr erfolgreichen 
                    Convolutional Neural Networks bzw. 
                    Deep Learning scheinen auf Grund der relativ geringen Datenmengen zur Zeit nicht direkt anwendbar. Basierend auf den aktuellsten positiven Ergebnissen sind wir zuversichtlich, dass hier neue Methoden für die digitale Epigraphie in Arbeit sind und Synergien zwischen Gedächtnisleistung, klassischem 
                    Machine Learning und 
                    Deep Learning zu weiteren Verbesserungen führen werden.
                
         
      
      
         
            
               Bibliographie
               
                  Bogacz, Bartosz / Gertz, Michael / Mara, Hubert (2015): "Character Retrieval of Vectorized Cuneiform Script", in: 13th IAPR International Conference on Document Analysis and Recognition (ICDAR2015) https://www.researchgate.net/publication/281781820_Character_Retrieval_of_Vectorized_ Cuneiform_Script [letzter Zugriff 28. September 2018].
                    
               
                  Bogacz, Bartosz / Feldmann, Felix / Prager, Christian / Mara, Hubert (2018): "Visualizing Networks of Maya Glyphs by Clustering Subglyphs", in: Sablatnig, Robert et al. (eds): Eurographics Workshop on Graphics and Cultural Heritage. Geneva: The Eurographics Association 105-111 http://doi.org/10.2312/gch.20181346 [letzter Zugriff 12. Januar 2019].
                    
               
                  Bogacz, Bartosz / Klingmann, Maximilian / Mara, Hubert (2017): "Automatic Transliteration of Cuneiform from Parallel Lines with Sparse Data", in: 14th IAPR International Conference on Document Analysis and Recognition (ICDAR2017) https://www.researchgate.net/publication/321491564_Automating_Transliteration_of_ Cuneiform_from_Parallel_Lines_with_Sparse_Data [letzter Zugriff 28. September 2018].
                    
               
                  Chiarcos, Christian / Pagé-Perron, Émile / Khait, Ilya / Schenk Niko / Reckling, Lucas (2018): "Towards a Linked Open Data Edition of Sumerian Corpora", in: Calzolari, Nicoletta et al.: Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018. Paris: European Language Resources Association 2437-2444. http://www.lrec-conf.org/proceedings/lrec2018/pdf/862.pdf [letzter Zugriff 28. September 2018].
                    
               
                  Diehr, Franziska / Brodhun, Maximilian / Gronemeyer, Sven / Diederichs, Katja / Prager, Christian / Wagner, Elisabeth / Grube, Nikolai (2018): "Ein digitaler Zeichenkatalog als Organisationssystem für die noch nicht entzifferte Schrift der Klassischen Maya", in: Wartena, Christian et al. (eds): Knowledge Organization for Digital Humanities: Proceedings of the 15th Conference on Knowledge Organization WissOrg’17 of the German Chapter of the International Society for Knowledge Organization (ISKO). Berlin: Freie Universität Berlin 37–43 doi: https://doi.org/10.17169/FUDOCS_document_000000028863 [letzter Zugriff 28. September 2018].
                    
               
                  Feldmann, Felix / Bogacz, Bartosz / Prager, Christian / Mara, Hubert (2017): "Histogram of Oriented Gradients for Maya Glyph Retrieval", in: Schreck, Tobias et al. (eds): Eurographics Workshop on Graphics and Cultural Heritage. Geneva: The Eurographics Association 115–118 http://dx.doi.org/10.2312/gch.20171301 [letzter Zugriff 28. September 2018].
                    
               
                  Knorozov, Yuri (1956): "New Data on the Maya Written Language", in: Journal de la Société des Américanistes 45: 209–217 https://www.persee.fr/doc/jsa_0037-9174_1956_num_45_1_961 [letzter Zugriff 28. September 2018].
                    
               
                  O'Kane, Lawrence (1962): "Computers Solve Mayan Writings; Soviet Mathematicians Use Devices for Translation Original Writing System Glossaries Developed Samples of Translations Expert Reserves Judgment", in: The New York Times, 15 April http://query.nytimes.com/gst/abstract.html?res=9800EEDA143DE532A25756C1A9629C946391D6CF [letzter Zugriff 28. September 2018].
                    
               
                  Rendón, Juan / Spescha, Amalia (1965): "Nueva clasificación plástica de los glifos mayas", in: Estudios de Cultura Maya 5: 189-252 http://dx.doi.org/10.19130/iifl.ecm.1965.5.668 [letzter Zugriff 28. September 2018].
                    
               
                  Ringle, William M. / Thomas C. Smith-Stark (1996): A Concordance to the Inscriptions of Palenque, Chiapas, Mexico (= Middle American Research Institute Publication 62) New Orleans, LA: Middle American Research Institute, Tulane University.
                    
               
                  Schellhas, Paul (1945): "Die Entzifferung der Mayahieroglyphen: ein unlösbares Problem?", in: Ethnos 10(1): 44–53 https://doi.org/10.1080/00141844.1945.9980637 [letzter Zugriff 28. September 2018].
                    
               
                  Schlenther, Ursula (1964): "Kritische Bemerkungen zur kybernetischen Entzifferung der Maya-Hieroglyphen (mit 10 Abbildungen und 3 Tabellen", in: Ethnographisch-archäologische Zeitschrift 5(5): 111-139.
                    
               
                  Sobolev, Sergei L'vovich (1961): "Die vollständige Entzifferung der Maya-Handschriften durch mathematische Methoden", in: Wissenschaftliche Zeitschrift der Humboldt Universität 10(4–5): XVII–XXI.
                    
               
                  Zender, Marc (1999): Diacritical Marks and Underspelling in the Classic Maya Script: Implications for Decipherment. M.A. Thesis. Department of Archaeology, University of Calgary http://dx.doi.org/10.5072/PRISM/19313 [letzter Zugriff 28. September 2018].
                    
            
         
      
   



      
         
            Projektübersicht
            Ziel des an der Universität Jena durchgeführten Projekts ist die Entwicklung eines uneingeschränkt zugänglichen Onlineportals zur Publikation digitaler historisch-kritischer Editionen von vorrangig handschriftlichen Quellen der Neuzeit. Es wird an der Thüringer Universitäts- und Landesbibliothek (ThULB) gehostet und in Zusammenarbeit mit zahlreichen Partnern (darunter READ/Transkribus, Deutsches Textarchiv, Herzog August Bibliothek Wolfenbüttel, Landesarchiv Thüringen, Sammlungs- und Forschungsverbund Gotha und viele weitere) realisiert. Mit TEI-basierten Meta- und Volltextdaten, Digitalisaten, Registern, Paratexten, Visualisierungen, gezielten Such-, Sortierungs- und Filteroptionen wird das Editionsportal Thüringen eine wissenschaftlich hochwertige digitale Publikations- und Rechercheumgebung anbieten. Das Portal spricht in erster Linie Projekte an, die ihre Edition dauerhaft verfügbar und intuitiv zugänglich machen wollen, aber kein eigenes Onlineportal entwickeln und nachhaltig betreiben können oder möchten. Damit diese Projekte den Heraus- und Anforderungen exzellenter digitaler Editionen [s. bspw. Sahle 2014] entsprechen, legt das Portal Schwerpunkte auf die nachfolgend erläuterten Aspekte.
         
         
            Fokus Nachnutzung und Langzeitverfügbarkeit
            Um die im Portal befindlichen Editionen nachnutzen und in andere digitale Wissensbasen einbinden zu können, liegt ein Fokus auf der Bereitstellung von Datenschnittstellen, der Verwendung offener und standardisierter Datenformate und der Nachnutzung bewährter Open-Source-Software. So basiert nicht allein das technische Backend-System MyCoRe auf entsprechenden Technologien (Abb. 1). Auch das eigens für das Editionsportal entwickelte TEI-Basisformat (ThULBBf) ist eng an das TEI-Subset des Deutschen Textarchivs (DTABf) angelehnt und erweitert dieses handschriftenspezifisch (bspw. in den Bereichen Normdatenanreicherung, Materialität, Textzeugen-Wiedergabe, Transkriptions- und Auszeichnungsrichtlinien, auto-generierte Register etc., Abb. 2.), um die Ausspielung der Portaldaten in das DTA zu ermöglichen. Damit geht sowohl die Speicherung und Verwendung der Forschungsdaten in der CLARIN-D-Infrastruktur als auch die Nachnutzung etablierter linguistischer DTA-Tools einher, ohne dass (ggf. redundante) Neuentwicklungen erforderlich werden.
            Damit ist bereits ein weiteres zentrales Portalmerkmal, die Langzeitverfügbarkeit der darin dargebotenen Informationen, benannt. Wenngleich die Frage der Langzeitspeicherung noch ungelöst ist [Carusi / Reimer 2010: 42], so folgt das Portal doch derzeitigen 
                Nachhaltigkeitsempfehlungen. Dazu gehört unter anderem, dass die nachhaltige Speicherung der Portaldaten im Rahmen der von EFRE geförderten Langzeitarchivierungsplattform des Landes Thüringen auf Grundlage des Digitalen Archives NRW (Kooperation mit dem Landschaftsverband Rheinland) erfolgt [Mutschler 2017: 316].
                
            
               
               MyCoRe-Architektur (
                        http://www.mycore.de/features/index.html)
                    
            
            
               
               Das TEI-Basisformat des Editionsportals (ThULBBf)
            
         
         
            Fokus multimodale editorische und fachwissenschaftliche Benutzung
            Das Portal verfolgt einen generischen Ansatz mit fest definiertem Arbeitsworkflow (Abb. 3), um Editionen projektübergreifend multimodal recherchier-, visualisier- und erforschbar zu machen. Zugleich wird jede Edition des Portals als eigenständige Forschungsleistung in Form einer projektspezifischen Publikation sichtbar und zitierbar. Editoren können zudem zwischen verschiedenen Werkzeugen zur Editionserstellung wählen. Je nach Ziel und Kenntnisstand können Microsoft Word, die Trierer Editionsforschungsumgebung FuD oder ein beliebiger XML-Editor eingesetzt werden. Für NutzerInnen bietet das Portal multimodale Recherche- und Visualisierungsinstrumente für disziplinübergreifende Fragestellungen (Netzwerke, Zeitleisten, Diagramme, Karten etc.). Durch die Einbettung des Editionsportals in ein umfassenderes 
                Cultural-Heritage-Internetportal können die Quellenbestände dutzender Thüringer Institutionen und Partner außerhalb Thüringens in die Recherche und Analyse unmittelbar einbezogen werden.
                
            
               
               Der verbindliche Workflow für Editionsprojekte des Portals
            
         
         
            Verortung innerhalb der editorischen Portallandschaft
            Innerhalb der editorischen Portallandschaft verortet sich das Editionsportal Thüringen zwischen Einzeleditionsprojekte weitestgehend nivellierenden Textsammlungen wie bspw. dem Deutschen Textarchiv oder dem TextGrid-Repository einerseits und Einzeleditionsportalen mit zu projektspezifisch ausgerichteten Eigenschaften und Funktionalitäten ohne editionsübergreifende Such- und Analysemöglichkeiten andererseits. Zu letzteren sind bspw. die Editionen der Wolfenbütteler Digitalen Bibliothek oder des Geisteswissenschaftlichen Asset Management Systems (GAMS) zu rechnen. Auch die Repositorien (trans)nationaler Infrastrukturen wie bspw. DARIAH oder CLARIN bleiben aufgrund ihrer wiederum zu unspezifischen Forschungsdatenausrichtung hinter den anwendungsorientierten Datenmodellen und Funktionalitäten des Editionsportals Thüringen zurück. Zum Novum des Editionsportals gehört ferner seine betont breite Ausrichtung auf möglichst zahlreiche Handschriftengattungen, unterschiedliche Zielgruppen, disparate Editionswerkzeuge und eine priorisierte Softwarenachnutzung.
            Das Portal wird in einem ersten Schritt die bereits existierenden und in Arbeit befindlichen Editionen der digitalen ThULB-Bibliothek UrMEL aufnehmen und an diesen evaluiert. Es ist aber als Plattform konzipiert, die zur digitalen Aufbereitung von Quellenbeständen verschiedenster, auch kleinerer Einrichtungen (Archive, Bibliotheken, Museen, Vereine) anregen soll und keinesfalls an Thüringer Quellen, Institutionen o.ä. gebunden ist. 
         
         
            Zielgruppenorientierung
            Das Portal versteht sich in erster Linie als wissenschaftliches Angebot, das schwer zugängliche Handschriften aus fünf Jahrhunderten verfügbar macht und hochwertige Volltexte für die computergestützte Weiterverarbeitung liefert. In editionswissenschaftlicher Hinsicht leistet es einen Forschungsbeitrag zu der Frage, wie ein generisches Editionsportal sowohl projektspezifische, aus der Heterogenität der Quellen und den Erwartungen der Edierenden, Rezipienten und Förderer resultierende, als auch gesamtportalische, editionsübergreifende Interessen und Ansprüche gleichermaßen verbinden kann [s. bspw. Dogunke 2017]. Das Editionsportal richtet sich aber nicht ausschließlich an die Wissenschaft, sondern auch an Schulen und Bildungseinrichtungen und die interessierte Öffentlichkeit. Diesen wird mit der Integration des e-Learning-Tools „TranskribusLearn“ ein Werkzeug zum Erlernen altdeutscher Schrift angeboten sowie die Möglichkeit gegeben, das Portal mit eigenen Transkriptionen anzureichern. Eine hohe Qualität der Editionsinhalte und -daten wird durch Empfehlungen und Anleitungen, Reviewing, verbindliche Workflows (Abb. 3) und Dateneingaben sowie verschiedene technische Validierungsinstanzen gewährleistet.
         
         
            Projektgenese und Arbeitsstand
            Das Projekt ist aus einem Editionsprojekt [Prell / Schmidt-Funke 2017] hervorgegangen, für das nur sehr geringe Ressourcen zur Verfügung standen. Da dies eine häufig zu beobachtende Rahmenbedingung von Editionen darstellt, werden die im Editionsprojekt entwickelten Lösungen ausgebaut, institutionalisiert und anderen ForscherInnen kostenfrei zugänglich gemacht. Aktuell befindet sich das Portal in der zweiten Förderphase, nachdem in den Jahren 2017 und 2018 konzeptuelle und anpassende Maßnahmen des Backend-Systems vorgenommen worden. Derzeit findet die Entwicklung des TYPO3-Frontends statt.
         
      
      
         
            
                Vgl. beispielsweise die durch Sustainability-Zertifikate wie das Data Seal of Approval bzw. CoreTrustSeal formulierten Kriterien für nachhaltige Datenrepositorien (
               
                  https://www.coretrustseal.org/
               
               , 
               
                  https://www.datasealofapproval.org
               
                (letzter Zugriff 27.09.2018) sowie Buddenbohm et al. 2014.
            
            
                Das „Digitale Kultur- und Wissensportal Thüringen“ wird im 1. Quartal 2019 online geschaltet.
            
         
         
            
               Bibliographie
               
                  Buddenbohm, Stefan / Enke, Harry / Hofmann, Matthias / Klar, Jochen / Neuroth, Heike / Schwiegelshohn, Uwe (2014): „Erfolgskriterien für den Aufbau und nachhaltigen Betrieb Virtueller Forschungsumgebungen“. Göttingen. http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2014-7.pdf [letzter Zugriff 27.09.2018].
                    
               
                  Carusi, Annamaria / Reimer, Torsten (2010): “Virtual Research Environment Collaborative Landscape Study. A JISC funded project.” Oxford/London: JISC http://www.jisc.ac.uk/publications/reports/2010/vrelandscapestudy.aspx [letzter Zugriff 27.09.2018].
                    
               
                  Dogunke, Swantje (2017): „Tagungsbericht: Editionsportale, 03.08.2017 – 04.08.2017 Jena“, in: H-Soz-Kult, 10.10.2017,  [letzter Zugriff 28.09.2018].
                    
               
                  Mutschler, Thomas (2017): „Neue Wege der Kulturgutdigitalisierung in Thüringen“, in: Bibliotheksdienst 51, 310-321.
                    
               
                  Prell, Martin / Schmidt-Funke, Julia (Hg.) (2017): „Digitale Edition der Briefe Erdmuthe Benignas von Reuß-Ebersdorf (1670-1732)“. Jena http://erdmuthe.thulb.uni-jena.de [letzter Zugriff 28.09.2018].
                    
               
                  Sahle, Patrick (2014): „Kriterienkatalog für die Besprechung digitaler Editionen, Version 1.1“ (unter Mitarbeit von Georg Vogeler und den Mitgliedern des IDE) https://www.i-d-e.de/publikationen/weitereschriften/kriterien-version-1-1/ [letzter Zugriff 27.09.2018].
                    
            
         
      
   



      
         
            Einleitung
            Geisteswissenschaftliche Forschung ist schon seit langem eine auch digitale Praxis. Dies spiegelt sich u. a. in den Methoden der Ergebnissicherung wider: Präsentationssysteme, interaktive Visualisierungen, Recherche-Datenbanken und nicht zuletzt Digitale Editionen haben sich neben der klassischen Publikation längst als digitale Instrumente der Ergebnissicherung etabliert. Während für die Persistenz statischer Daten wie z. B. digitaler Dokumente bereits gut etablierte Strategien und Standards wie TEI-XML (TEI 2018) oder OASIS-DITA (OASIS 2018) existieren, stellt Forschungssoftware im Hinblick auf die langfristige Ergebnissicherung noch immer eine besondere Herausforderung dar. 
            Dies äußert sich in erster Linie in einem Mangel an konkreten Nachhaltigkeitsstrategien und erwächst u. a. aus einem als „Software-Aging“ (Parnas 1994) bekannten Problem, dem Forschungsanwendungen als „Lebende Systeme“ (Sahle/Kronenwett 2013) grundsätzlich unterworfen sind, da sie, wie jede Software, nicht unabhängig von ihrer Laufzeitumgebung bzw. ihrem digitalen Ökosystem gedacht werden können. Die kontinuierliche Evolution dieser Umgebungen sorgt dafür, dass Softwaresysteme, die nicht stetig an diese veränderten Umweltbedingungen angepasst werden, mit der Zeit veralten und letztendlich unbenutzbar werden.
            In dem von der DFG geförderten Kooperationsprojekt „SustainLife 
                    – Erhalt lebender, digitaler Systeme für die Geisteswissenschaften“, das in einer Zusammenarbeit zwischen dem Data Center for the Humanities (DCH, siehe 
                    
                  http://dch.phil-fak.uni-koeln.de)
                     der Universität zu Köln und dem Institut für Architektur von Anwendungssystemen (IAAS, siehe 
                    
                  http://www.iaas.uni-stuttgart.de)
                     der Universität Stuttgart durchgeführt wird, arbeiten wir an einem Lösungsvorschlag für dieses Problem (vgl. dazu Barzen et al. 2018 sowie Neuefeind et al. 2018). Gegenstand des Projekts ist die Adaption und Weiterentwicklung von Verfahren und Technologien aus dem Cloud-Deployment für die Digital Humanities (DH) mit dem Ziel, Management und Provisionierung von DH-Anwendungen zu optimieren und deren Sicherung und nachhaltigen Betrieb zu realisieren. 
                
            Das Projekt setzt hierbei auf den OASIS-Standard TOSCA 
                (Topology and Orchestration Specification for Cloud Applications, siehe OASIS 2013, OASIS 2016) sowie dessen Open-Source-Implementierung OpenTOSCA (Binz et al. 2013, Breitenbücher et al. 2017). Mithilfe von TOSCA können (Forschungs-)Anwendungen mitsamt ihrer jeweiligen Laufzeitumgebung als Topologien von zusammenhängenden Software-Artefakten und Schnittstellen in standardisierter Weise modelliert werden. Daraufhin können diese Topologie-Modelle und damit alle benötigten Komponenten und Dateien der modellierten Applikation in sogenannten CSARs 
                (Cloud Service Archives) paketiert werden, so dass diese portablen Archive von jeder TOSCA-Runtime interpretiert und automatisiert bereitgestellt werden können.
                
         
         
            Methodisches Vorgehen
            Eine wesentliche Voraussetzung für eine Lösung, die eine standardisierte, TOSCA-konforme Beschreibung von Softwaresystemen vorsieht, ist ein möglichst genaues Bild der technologischen Landschaft innerhalb der Digital Humanities. Grundlage für den im Projekt verfolgten Ansatz ist daher eine ausführliche Bedarfs- und Anforderungsanalyse, anhand derer die Spezifikation häufig eingesetzter Systemkomponenten sowie die Identifikation von Schlüsselkomponenten zur Erstellung von Anwendungsvorlagen erfolgen kann. 
            Im Projekt setzen wir hierfür im Wesentlichen auf zwei Instrumente: Zum einen quantitativ angelegte Umfragen in der Community in Form von Fragebögen, zum anderen die qualitativ orientierte Untersuchung von ausgewählten Beispielanwendungen auf Basis von gezielten Codeanalysen. Letzteres zielt auf eine genauere Analyse der quantitativ erhobenen Daten bezüglich der eingesetzten Technologien, um konkrete Systemkomponenten identifizieren zu können, die im weiteren Projektverlauf u. a. für die Modellierung von Usecases auf Basis des TOSCA-Standards eingesetzt werden sollen.
            Eine erste Kurzumfrage wurde im Rahmen der DHd2018 in Köln unter den Teilnehmern der Konferenz durchgeführt (Barzen et al. 2018). Primäres Ziel des eingesetzten Fragebogens war es, eine explorative Typisierung von DH-Anwendungen vorzunehmen und das Spektrum der dabei eingesetzten Technologien zu konturieren. Die Umfrage ergab im Wesentlichen, dass sich Entscheidungen bezüglich der Technologienutzung vor allem am jeweiligen Anwendungstyp orientieren. Um eine möglichst zielgenaue Analyse typischer Technologien zu erreichen bietet es sich deshalb an, den Skopus der Befragung entsprechend auf einzelne Anwendungstypen hin einzugrenzen.
         
         
            Technologienutzung im Kontext Digitaler Editionen
            Nachdem sich der Zusammenhang zwischen der Art der Anwendung und den konkreten technologischen Entscheidungen besonders deutlich bei Digitalen Editionen zeigte, und da diese zugleich als besonders repräsentative Form digitaler Ergebnissicherung im Bereich der Digitalen Geisteswissenschaften angesehen werden können, haben wir uns dazu entschieden, hier den ersten Ausgangspunkt für eine gezielte Erhebung von Daten über die Technologienutzung zu setzen. Hierfür richteten wir gemeinsam mit dem Cologne Center for eHumanities (CCeH, siehe 
                    
                  http://cceh.uni-koeln.de)
                     sowie in Kooperation mit der Landesinitiative NFDI der Digitalen Hochschule NRW (siehe 
                    
                  https://fdm-nrw.de)
                     und der Nordrhein-Westfälischen Akademie der Wissenschaften und der Künste (AWK, siehe 
                    
                  http://www.awk.nrw.de
                einen Workshop zum Thema „Nachhaltigkeit Digitaler Editionen“ aus, der am 17.9.2018 in den Räumen der AWK stattfand (siehe 
                    ).
                
            Der Workshop hatte das Ziel, vorhandene Lösungsansätze und Aktivitäten, die der nachhaltigen Bereitstellung von Digitalen Editionen gewidmet sind, mit der Fachcommunity zu diskutieren. So wird bereits seit einigen Jahren an verschiedenen Stellen an Konzepten gearbeitet, die dieses Problem adressieren. Bspw. wird in der Schweiz derzeit eine „Nationale Infrastruktur für Editionen“ (NIE-INE, siehe 
                    
                  https://www.nie-ine.ch)
                     aufgebaut, die auf die Homogenisierung von Editionsprojekten zielt, und in Österreich wurde mit dem „Kompetenznetzwerk Digitale Editionen“ (KONDE, siehe 
                    
                  http://www.digitale-edition.at)
                     ein umfangreiches Verbundprojekt eingesetzt, das ebenfalls den Aufbau und die Weiterentwicklung einer nationalen Forschungsinfrastruktur für Digitale Editionen zum Ziel hat. Auch in Deutschland gibt es vergleichbare, wenn auch lokaler ausgerichtete Initiativen, die auf die Schaffung einer gemeinsamen Infrastruktur für Editionsprojekte und damit längerfristig auf eine Standardisierung von Digitalen Editionen zielen. Im Rahmen des Workshops hatten wir die Gelegenheit viele der maßgeblichen Akteure als Sprecher zu gewinnen. Unter den Teilnehmern fanden sich ebenfalls viele Personen, die selbst an Digitalen Editionen arbeiten oder an der Nachhaltigkeitsproblematik anderweitig interessiert waren, z.B. aus Perspektive der Drittmittelgeber.
                
         
         
            Begleitende Umfrage
            Im Vorfeld des Workshops führten wir unter den insgesamt 80 Teilnehmern (70 Gäste und 10 eingeladene Sprecher) eine Umfrage durch, mit dem Ziel, Informationen über die Technologielandschaft speziell in diesem Bereich der DH zu gewinnen. Die Umfrage enthielt Fragen zu persönlichen Vorerfahrungen in der Arbeit mit Digitalen Editionen, zu den formalen Charakteristika von Editionsprojekten sowie zu den dabei eingesetzten Technologien. Schon anhand einer ersten Auswertung der Umfrage, die unmittelbar vor dem Workshop vorgenommen wurde und bei der die Angaben zu 31 verschiedenen Editionsprojekten berücksichtigt wurden, ließen sich deutliche Tendenzen in Bezug auf die eingesetzten Nachnutzungs- und Nachhaltigkeitsstrategien ablesen. So zeigen die in Abbildung 1 wiedergegebenen Angaben bezüglich der Datenmodellierung sehr deutlich, dass mit TEI-XML mittlerweile ein weit verbreiteter Standard zur nachhaltigen Sicherung von Daten vorliegt, der für speziellere Bedarfe durch weitere standardisierte Formate ergänzt wird.
            
               
                  
                  
                     Abbildung 1: Nutzung von Standards für Metadaten und für die Modellierung von Primärdaten.
                        
               
            
            Aufseiten der Implementierungen dagegen fehlen solche Standards, was entsprechend zu einer deutlich höheren Heterogenität der eingesetzten Technologien führt (siehe Abbildung 2). Aus der Zusammenschau der verschiedenen Fragekategorien lässt sich ersehen, dass es zwar durchaus auch in Bezug auf die genutzten Technologien und Systemkomponenten Favoriten gibt, jedoch wird unsere Annahme einer technologischen und methodischen Heterogenität des Feldes dennoch klar bestätigt.
            
               
                  
                  
                     Abbildung 2: Technologienutzung im Kontext Digitaler Editionsprojekte.
                        
               
            
            Während sich also im Bereich der Primärdaten in Gestalt verschiedener TEI-Dialekte eine zunehmende Standardisierung abzeichnet, so gilt dies nicht für das Layout und die Präsentation von Editionen. Hinsichtlich Aussehen, Funktionalitäten und technischer Architekturen besteht hier weiterhin eine sehr große Heterogenität. Zudem deuten sich hier auch bereits verschiedene Technologie-Stacks an, die sich aus typischen Kombinationen von Systemkomponenten ergeben. Allerdings scheinen sich solche Stacks v. a. innerhalb verschiedener Trägerinstitutionen herauszubilden, was sich in Teilen auf personelle Überschneidungen zwischen Projekten oder auf institutionelle Lösungsansätze zur Bewältigung von Nachhaltigkeitsanforderungen zurückführen lässt. Diese Ergebnisse spiegelten sich auch in den Diskussionen auf dem Workshop wider. Während die Nachhaltigkeit der Daten weitestgehend gelöst zu sein scheint, brachte die Frage nach der Nachhaltigkeit der Software eine Reihe von Lösungsansätzen hervor. Die Vor- und Nachteile von verschiedenen Technologie-Stacks wurden kontrovers diskutiert. Weitestgehende Einigkeit bestand darin, dass eine Digitale Edition aus Daten plus Software besteht, dass also eine alleinige Konservierung der Daten für die Konservierung der Digitalen Edition an sich nicht ausreicht.
         
         
            Fazit und Ausblick
            In unserem Beitrag möchten wir die bisherigen Ergebnisse unserer Anforderungsanalyse an lebende Systeme in den Digital Humanities zur Diskussion stellen. Neben den hier gezeigten Kennzahlen fokussieren wir dabei verschiedene Technologie-Stacks, die typische Kombinationen von Systemkomponenten widerspiegeln. Die hier vorgestellten Ergebnisse der Anforderungsanalyse dienen uns als Basis für die weitere Projektarbeit. Unser Ziel ist es, häufig genutzte Schlüsselkomponenten und typische Anwendungsstrukturen digitaler Erkenntnisträger im Bereich der Digital Humanities zu identifizieren, sie anschließend in TOSCA zu modellieren und zu einer Komponentenbibliothek zusammenzufassen sowie in Form von Anwendungsvorlagen für weitere Modellierungen bereitzustellen, um dadurch konsistente und zukunftssichere Standards und Nachhaltigkeitsstrategien für Forschungssoftware zu etablieren. Über die konkrete Projektarbeit hinaus sind die hier vorgestellten Ergebnisse auch von generellem Interesse für die Community. So sind die erhobenen Daten u. a. auch für Datenzentren wie das DCH Köln von hohem Nutzen, um beispielsweise Strategien und Betreuungskonzepte für Forschungssoftware zu entwerfen.
         
      
      
         
            
               Bibliographie
               
                  Barzen, J. / Blumtritt, J. / Breitenbücher, U. / Kronenwett, S. / Leymann, F. / Mathiak, B. / Neuefeind, C. (2018): "SustainLife - Erhalt lebender, digitaler Systeme für die Geisteswissenschaften." In: Book of Abstracts der 5. Jahrestagung der Digital Humanities im deutschsprachigen Raum (DHd 2018), Köln 26.2.–2.3.2018, S. 471–474. [letzter Zugriff 8.1.2019].
                    
               
                  Binz, T. / Breitenbücher, U. / Haupt, F. / Kopp, O. / Leymann, F. / Nowak, A. / Wagner, S. (2013): “OpenTOSCA - A Runtime for TOSCA-based Cloud Applications“. In: ICSOC, 2013, S. 692–695.
                    
               
                  Breitenbücher, U. / Barzen, J. / Falkenthal, M. / Leymann, F. (2017): "Digitale Nachhaltigkeit in den Geisteswissenschaften durch TOSCA: Nutzung eines standardbasierten Open-Source Ökosystems", in: DHd 2017: Digitale Nachhaltigkeit, S. 235–238. [letzter Zugriff 8.1.2019].
                    
               
                  Neuefeind, C. / Harzenetter, L. / Schildkamp, P. / Breitenbücher, U. / Mathiak, B. / Barzen, J. / Leymann, F. (2018): "The SustainLife Project – Living Systems in Digital Humanities". In: Proceedings of the 12th Advanced Summer School on Service-Oriented Computing, 2018 (IBM Research Report RC25681), S.101-112.
                    
               
                  OASIS (2013): „Topology and Orchestration Specification for Cloud Applications Version 1.0“. 25 November 2013. OASIS Standard. . [letzter Zugriff 8.1.2019]
                    
               
                  OASIS (2016): „TOSCA Simple Profile in YAML, Version 1.0“. Edited by Derek Palma, Matt Rutkowski, and Thomas Spatzier. 21 December 2016. OASIS Standard. . [letzter Zugriff 8.1.2019]
                    
               
                  OASIS (2018): „Darwin Information Typing Architecture (DITA) Version 1.3 Errata 02“. Edited by Robert D. Anderson and Kristen James Eberlein. 19 June 2018. OASIS Approved Errata.  [letzter Zugriff 8.1.2019].
                    
               
                  Parnas, D. L. (1994): ”Software Aging“. In: Proceedings of the 16th International Conference on Software Engineering (ICSE 1994). IEEE, Mai 1994, S. 279–287.
                    
               
                  Sahle, P. und Kronenwett, S. (2013): ”Jenseits der Daten: Überlegungen zu Datenzentren für die Geisteswissenschaften am Beispiel des Kölner ’Data Center for the Humanities’“. In: LIBREAS. Library Ideas 23, S. 76–96.
                    
               
                  TEI Consortium, eds. (2018): “TEI P5: Guidelines for Electronic Text Encoding and Interchange.” Version 3.4.0 vom 23.7.2018. [letzter Zugriff 8.1.2019].
                    
            
         
      
   



      
         
            Einleitung
            
               ”Auf der Konferenz [DHd 2018; d. Verf.] war zu jeder Zeit an jedem Ort genug Kompetenz versammelt, um in einer Kaffeepause eine digitale Publikation der Abstracts zu bauen, mit Inhaltsverzeichnis, Volltext- und Schlagwortsuche und anderen netten Features, die sonst zum Standard jeder Webpräsentation in den DH gehören.” So postuliert Fabian Cremer in seinem Blog-Post “Nun sag, wie hältst Du es mit dem Digitalen Publizieren, Digital Humanities?” (Cremer 2018). Nur, wie Cremer weiter ausführt, niemand habe es gemacht oder irrt der Autor in dieser Aussage, da die Aufgabe nicht so trivial ist, wie auf dem ersten Blick erscheint? Der hier vorgeschlagene Workshop “DHd 2019 Book of Abstracts Hackathon” soll der DHd-Community den Raum bieten, dieser Frage nachzugehen, eine gemeinsame digitale Publikation der Konferenz-Abstracts zu realisieren und so einen Diskussionsimpuls zur Zukunft des Digitalen Publizierens in den Digital Humanities zu geben.  
            
         
         
            Digitales Publizieren und Digital Humanities
            
               Das digitale Publizieren hat sich im Rahmen des Kanonisierungsprozesses zu einem etablierten Bestandteil der Digital Humanities entwickelt. Dies umfasst sowohl methodische Überlegungen (Kohle 2017) wie auch die praktische Umsetzungen (DHd-AG 2016). Längst existieren etablierte digitale Publikationsorgane der Digital Humanities, welche die Vielfalt und Potentiale der digitalen Publikationsformate demonstrieren und als Vorbild der Publikationspraxis und Wissenschaftskommunikation 
                    gelten.
               
                Diesen Entwicklungen zum Trotz werden in der Breite und Spitze der Digital Humanities Forschung diese Potentiale nicht ausgeschöpft und traditionelle Publikationspraktiken weiterhin gepflegt (Stäcker 2012). Dies gilt für die Ebene der Technologie, so liegt das Book of Abstracts der DHd 2018 als PDF ohne Strukturdaten vor (Vogeler 2018). Aber gleichfalls auch für die offene Zugänglichkeit, so wurde das deutschsprachige Standardwerk zu den DH nicht als Open Access publiziert (Jannidis et al. 2017 vgl. dazu Stäcker 2017). Die Anforderungen an digitale Publikationen sind schon seit längerem formuliert. “Digital publishing is not simply repackaging a book or article as a computer file, although even a searchable pdf has advantages over paper”, bemerkt Borgman in ihrem “Call to Action for the Humanities” und adressiert hier auch dezidiert die Digital Humanities (Borgman 2010: #p16).
            
            
               Neben den wissenschaftsökonomischen und wissenschaftspolitischen Vorteilen digitalen Publizierens eröffnen digitale Formate neue wissenschaftliche Methoden zur Weiterverarbeitung. Diese Potentiale sind sich insbesondere die mit digitalen Quellen und Daten arbeitenden  Geisteswissenschaftler*innen bewusst und Formulieren entsprechende Ansprüche an die Digitalisierung und Bereitstellung der Untersuchungsgegenstände, wie etwa Volltexte mit standardisierter Strukturierung und Interoperabilität sowie mit Entitäten und komplexen Strukturmerkmalen angereichert 
                    (Klaffki et al. 2018: 19-20).
               
                Diese Ansprüche müssten in den Geisteswissenschaften, in der die eigenen Texte in Form einer kritischen Rezeption und Iteration Teil der Informationsquellen sind, auch an die eigene Textproduktion gestellt werden. Folgerichtig lautet die Empfehlung der DHd-AG “Digitales Publizieren”, die semantischen Strukturen zu kodieren, die Dokumente maschinenlesbar und prozessierbar zu machen und PDF nicht als primäres Publikationsformat zu verwenden (DHd-AG 2016). Die TEI-basierten Veröffentlichungen der Digital Humanities Community demonstrieren das Potential der Publikationen als Untersuchungsgegenstand des Faches (Sahle/Henny-Krahmer 2018 und Hannesschläger/Andorfer 2018). Allein die Zusammenführung der strukturierten Datenbasis der Texte mit den vorhandenen Technologien und Methoden neuer Publikationsformate steht häufig noch aus. Diese Lücke adressiert das vorliegende Konzept.
            
         
         
            Konzeption des Workshops
            
               Ziele
               
                  Der Hackathon liefert einerseits ein Proof-of-Concept für die Implementierung digitaler Technologien in einen Publikationsprozess (der digitalen Geisteswissenschaften) und möchte andererseits ein partizipatives Format zur Unterstützung der DHd-Tagung durch die DHd-Community in kollaborativer Arbeitsform (Hackathon) darstellen. Der Einsatz vorhandener Frameworks aus der Community demonstriert die Leistungsfähigkeit und das vorhandene Potential. Die erarbeiteten Transformationsskripte und Workflows können die Basis für eine Weiterentwicklung und Nachnutzung in institutionellen Publikationsprozessen darstellen. Eine Analyse des Workshops, der Ergebnisse und der damit verbundenen Rezeption liefert die Grundlage für die Formulierung von Empfehlungen zum möglichen zukünftigen Umgang mit den DHd-Abstracts und deren stetig steigende Relevanz als Publikationsformat. Der Hackathon kann dabei sowohl technologische wie methodische Impulse für das Digitale Publizieren in der DHd-Community liefern.
               
            
            
               Vorbereitungen
               
                  Die Datengrundlage für den Workshop bilden die zur DHd 2019 eingereichten Abstracts im dhc-Format und idealerweise in einer harmonisierten TEI-Version. Die Workshopleiter vertrauen hier auf die wertvolle Redaktionsarbeit der DHd-Konferenzorganisation, wie es die DHd 2018 vorbildlich umgesetzt 
                        hat.
                  
                   Um die hier angestrebten Verarbeitungsprozesse und Methoden des Digitalen Publizierens umsetzen zu können, müssen die Daten weiter vorbereitet und angereichert werden. Die vorbereitende Prozessierung soll umfassen: 
               
               
                  Harmonisierung (Sichtung; Vereinheitlichung von Ambiguitäten, Setzen der Mindeststandards)
                  
                     Strukturdatenauszeichnung (Überschriften, Absatznummerierung, Metadaten)
                  
                  Verknüpfung mit Normdaten (Erkennung und Auszeichnung von zentralen Entitäten wie Personen, Orte, Institutionen, Werke)
                  Rahmenwerke (Generierung von Listen und Registern für Schlagworte, Titel, Namen)
               
               
                  Die Aufbereitung und Anreicherung der Datenbasis wird von den Organisatoren in Zusammenarbeit mit dem Austrian Centre for Digital Humanities (ACDH) im Vorfeld der DHd 2019 vorgenommen. Die Workshopleitung wird dahingehend mit der Konferenzorganisation und dem Programmkomitee eine enge Kooperation anstreben.
               
            
            
               Themen und Arbeitsgruppen
               
                  Die Entwicklungsphasen des Hackathons finden als Gruppenarbeit statt. Die parallel arbeitenden Gruppen widmen sich verschiedenen Repräsentationen und Verarbeitungsprozessen der gemeinsamen Datenbasis. Die hier skizzierten Themen und Arbeitsgruppen sind als Vorschläge von Seiten der Organisatoren zu verstehen und werden abhängig von den Kompetenzen und Interessen der Teilnehmenden zu Beginn des Workshops adaptiert. Die beiden Arbeitsbereiche “Transformation” und “Präsentation” werden für die Ziele des Workshops als zwingend notwendig erachtet und sind daher gesetzt. Weitere Themen und Arbeitsgruppen können von den Teilnehmenden ausgestaltet werden. Die Organisatoren moderieren die Bildung der Arbeitsgruppen und begleiten beratend die Entwicklungsphasen.
               
               
                  AG “Style und Sheet”: Transformation
                  
                     Die Transformation der Ausgangsdaten (XML/TEI) in verschiedene Zielformate bildet die Grundlage für die verschiedenen Nutzungs- und Rezeptionsformen. Als Zielformate der Stylesheets bieten sich u.a. an: HTML, LaTex, PDF, MS-Word, Markdown, JATS. Idealerweise können die Teilnehmenden eigene, bereits genutzte Transformationen in der Gruppe diskutieren, erweitern und optimieren.
                  
               
               
                  AG “Web und App”: Präsentation
                  
                     Der Zeitrahmen des Workshops erlaubt keine Neuentwicklung eines Frontends. Für die Realisierung verschiedener Präsentationschichten müssen die Basisdaten in vorhandene Publikationsframeworks integriert werden. Dabei sind generische Ansätze (z.B. eXist Webapp, GitHub pages, eLife Lens Viewer, Jupyter) ebenso möglich wie spezifische oder instiutionelle Lösungen der DH-Community.
                  
               
               
                  AG “Maschine und Modell”: Schnittstellen
                  Die Bereitstellung der Daten über standardisierte maschinenlesbare Schnittstellen (API) ist eine grundlegende Repräsentationsform zukünftiger Publikationspraktiken. Idealerweise wird hier prototypisch ein Protokoll oder eine Spezifikation implementiert (z.B. OAI-PMH, FCS, DTS).
               
               
                  AG “Wolke und Vektor”: Textanalyse
                  
                     Durch Tokenisieren, Lemmatisieren, Vektorisieren werden Wortlisten, Wortfrequenzen, Wortwolken, Topicmodelling realisiert. Die Strukturierungs- und Visualisierungsformen sind die Grundlage für alternative Rezeptionsformen und textinterne Analysen bis zu (korpus)linguistischen Methoden.
                  
               
               
                  AG “Beziehung und Geflecht”: Netzwerke
                  
                     Die mit Normdaten angereicherte Datenbasis bietet die Möglichkeit, die Beziehungen zwischen den Entitäten (Personen, Orte, Institutionen, Werke) zu extrahieren, zu visualisieren und zu analysieren. Zu den Anwendungsfällen gehören Netzwerkanalysen und explorative Navigationsformen.
                  
               
               
                  AG “Medial und Modular”: Multimedia
                  Die Verwaltung, Adressierung und Einbettung multimedialer Inhalte die sowie Erzeugung und Integration interaktiver Elemente, wie dynamische Visualisierungen, gehören zu den großen Herausforderungen des Digitalen Publizierens. Prototypische Umsetzungen können hierfür Impulse liefern oder Lücken in bestehenden Frameworks aufzeigen.
               
            
            
               Agenda:
               
                  15’
                   
                  Begrüßung, Organisatorisches, Vorstellungsrunde,
               
               
                  15’
                   
                  Vorstellung der Daten und Aufteilung auf Arbeitsgruppen
               
               
                  60’
                   
                  Entwicklungsphase I
               
               
                  30’
                   
                  Pause
               
               
                  45’
                   
                  Entwicklungsphase II
               
               
                  30’
                   
                  Vorstellung der Ergebnisse
               
               
                  15’
                   
                  Abschlussdiskussion
               
            
         
         
            Organisation
            
               
                  Ergebnissicherung und Outreach
               
               
                  Die ausgearbeiteten Ergebnisse der Arbeitsgruppen (Daten, Skripte, Anwendungen) werden offen lizensiert und frei zur Verfügung gestellt. Ein Workshopbericht fasst die Ergebnisse übersichtlich als Blogpost zusammen. Gemeinsam mit interessierten Teilnehmenden plant die Workshopleitung eine Ausarbeitung der Ergebnisse als Empfehlung für mögliche Umsetzungen der DHd-Abstracts. Die konkreten Implementierungen sollen während der Konferenz online verfügbar gehalten werden und werden von der Workshopleitung (und freiwillig Teilnehmenden) über soziale Medien bekannt gegeben und zur Diskussion gestellt. Weitere erwünschte Kommunikationskanäle (Website, Email) werden mit der Konferenzorganisation besprochen.
               
            
            
               Datenmanagement und Infrastruktur
               
                  Die Einrichtung einer dezidierten Organisation auf GitHub.com für den Workshop erlaubt das Management der verschiedenen Entwicklungen und kollaborative Arbeitsformen. Die Dokumentation und das Projektmanagement werden ebenfalls über git umgesetzt (GitHubWiki/  GitHub-Projects/Issues). Die Publikation der Ergebnisse erfolgt über GitHub-Repositories via Zenodo. Grundsätzlich stehen auch nichtkommerzielle Gitlab-Instanzen und das DARIAH-DE-Repository zur Sicherung zur Verfügung. Das ACDH kann für das kurz- und mittelfristige Hosting der im Rahmen des Workshops entwickelten Applikationen und Services die erforderliche Server-Infrastruktur zur Verfügung stellen. Weiter ist die Bereitstellung von mehreren Instanzen der Applikationen Voyant und eXistDB zur Nutzung im Workshop über DARIAH-DE vorgesehen.
               
            
            
               Teilnehmer*innen
               
                  Die praktischen Arbeitsphasen der Gruppenarbeit erlauben nur kleine Gruppengrößen. Die Zahl der möglichen Teilnehmer*innen beträgt daher maximal 25 Personen. Für die Workshopteilnahme werden zwar keine spezifischen technischen oder methodischen Grundlagen (jenseits des Umgangs mit den TEI/XML-Basisdaten) vorausgesetzt, jedoch erfordert ein Hackathon von den Teilnehmenden selbständiges Arbeiten und die Anwendung vorhandener Kompetenzen auf den Gegenstandsbereich. Der Workshop bietet den Rahmen und Raum für die gemeinsames Arbeiten und offenen Austausch.
               
            
         
      
      
         
            
                    Dies zeigt sich am Beispiel der Zeitschrift für digitale Geisteswissenschaften (ZfdG), die als Vorbild für die Zeitschrift Medieval and Early Modern Material Culture Online (MEMO) fungierte.
                
            
                    Siehe die Digitalisierungsklassen IV und V für Texte.
                
            
                    Die vielgestaltigen TEI-Kodierungen aus den Einreichungen und Transformationen wurden einheitlich angeglichen, dokumentiert auf: 
                    
                  https://github.com/GVogeler/DHd2018
               .
                
         
         
            
               Bibliographie
               
                  Borgman, Christine L. (2010): “The Digital Future is Now. A Call to Action for the Humanities”, in: Digital Humanities Quarterly 3 (4): #p16, http://www.digitalhumanities.org/dhq/vol/3/4/000077/000077.html [letzter Zugriff 26. September 2018].
                    
               
                  Cremer, Fabian (2018): “Nun sag, wie hältst Du es mit dem Digitalen Publizieren, Digital Humanities?”, in: Digitale Redaktion (Blog), 21.03.2018, https://editorial.hypotheses.org/113 [letzter Zugriff 26. September 2018].
                    
               
                  DHd-AG "Digitales Publizieren" (2016): “Digitales Publizieren”, Working Paper, 01.03.2016, http://diglib.hab.de/ejournals/ed000008/startx.htm [letzter Zugriff 26. September 2018].
                    
               
                  Hannesschläger, Vanessa / Andorfer, Peter (2018): Menschen gendern? Datenmodellierung zur Erhebung von Geschlechterverteilung am Beispiel der TEI2016 Abstracts App, DHd 2018, Köln,https://doi.org/10.5281/zenodo.1182576
               
               
                  Jannidis, Fotis /  Kohle, Hubertus / Rehbein, Malte (2017)(eds.): Digital Humanities. Eine Einführung,  Stuttgart: J. B. Metzler.
                    
               
                  Kohle, Hubertus (2017): “Digitales Publizieren”, in: Jannidis, Fotis et al. (eds.): Digital Humanities. Eine Einführung,  Stuttgart: J. B. Metzler 199-205.
                    
               
                  Klaffki, Lisa / Schmunk, Stefan / Stäcker Thomas (2018): “Stand der Kulturgutdigitalisierung in Deutschland. Eine Analyse und Handlungsvorschläge des DARIAH-DE Stakeholdergremiums 'Wissenschaftliche Sammlungen'”, DARIAH-DE Working Papers 26, Göttingen, URN: urn:nbn:de:gbv:7-dariah-2018-1-3.
                    
               
                  Stäcker, Thomas (2012): “Wie schreibt man digital humanities?”, in: DHd-Blog, 19.08.2012, https://dhd-blog.org/?p=673 [letzter Zugriff 26. September 2018].
                    
               
                  Stäcker, Thomas (2017): “Digital Humanities : eine Einführung / Fotis Jannidis, Hubertus Kohle, Malte Rehbein (Hg.)”, in:  O-Bib. Das Offene Bibliotheksjournal 4(3): 142-148, https://doi.org/10.5282/o-bib/2017H3S142-148
               
               
                  Vogeler, Georg (2018)(ed.): DHd 2018. Kritik der digitalen Vernunft. Konferenzabstracts, Köln: Universität zu Köln 2018.
                    
            
         
      
   



      
         
            Einführung
            Gegenstand des umliegenden Projekts ist die Umstellung eines Langzeitvorhabens der Österreichischen Akademie der Wissenschaften (ÖAW) auf eine XML-basierte digitale Edition nach den Maßgaben der Vorschläge der Text Encoding Initiative 
                (TEI). Der Ministerrat war das zentrale Organ der Regierungstätigkeit in der Habsburgermonarchie beziehungsweise (nach 1867) in 
                Österreich-Ungarn. Seine Sitzungsprotokolle präsentieren alle Facetten staatlichen Lebens, von Fragen der Struktur und der Organisation des Staates bis zu gesellschaftlichen, wirtschaftlichen und technischen Entwicklungen sowie kulturellen und sozialen Problemen. Die Edition der Ministerratsprotokolle, die seit den 1970er Jahren unter wechselnder editorischer Verantwortung erscheint und in der ersten Serie 28 gedruckte Bände mit rund 18000 Druckseiten vorzuweisen hat, wird aktuell am Institut für Neuzeit- und Zeitgeschichtsforschung der ÖAW auf ein TEI-basiertes Editionsverfahren umgestellt, das die zu edierenden Bände in einer Print- und einer Online-Komponente aus einer Quelle 
                speist.
            
            Die wissenschaftliche Edition der Ministerratsprotokolle zeichnet sich aus durch: 
            
               den Umfang der zu edierenden Quellen
                        
                     die große abzudeckende Zeitspanne von 1848 bis 1867 für die retrodigitalisierte erste Serie; von 1867 bis 1918 für die digitale Edition; 
                     als Folge des umfassenden Zeitraums die Varianz etwa in den Schreibungen (mit Konsequenzen für die Textkritik, aber auch für die Arbeit mit Named Entities)
                  
               
               die unterschiedliche Provenienz der Quellen
                        
                     teilweise durch den Justizpalastbrand 1927 beschädigte Protokolle („Brandakten“)
                     Ergänzungsmaterial aus verschiedenen Archivbeständen
                  
               
               vielfältige interne und externe Bezüge
                        
                     zwischen Tagesordnungspunkten und Sitzungen, aber auch z.B. bandübergreifend
                     zwischen ediertem Text und wissenschaftlichem Kommentar
                     zu den Registern und Verzeichnissen, ergänzt durch Normdaten und Linked (Open) Data
                  
               
            
         
         
            Scope der Posterdokumentation
            Das Poster präsentiert vor dem Hintergrund des Gesamtworkflows für die TEI-Edition eine Schlüsselstelle auf dem Weg zu einer digitalen Edition, die den mit dem Begriff verbundenen Anforderungen gerecht werden soll – die „Registerdaten“: 
            
               Einerseits werden die Register der bereits edierten Bände rückerschlossen und ergänzt um neue Editionsdaten als Named Entities in einer relationalen Datenbank abgelegt, die das 
                    APIS-Projekt als biographische, prosopographische und mit Start- und Enddaten versehene Entitäten modelliert;
                    
               andererseits werden die Sachregister in Form eines Thementhesaurus modelliert, der Politikbereiche sowie die administrative Gliederung der Habsburgermonarchie berücksichtigt.
            
            
               Herausforderungen
               Prüfsteine bestehen betreffend die Modellierung und Erstellung von notwendigen Auxiliardaten, wie etwa Named Entities mit diachroner Varianz (Beispiel: Namensänderung durch Personenstands- oder Standesänderungen; Umgang mit Mehrsprachigkeit bei Ortsnamen, wenn sie Zuordnungsveränderungen unterliegen), vor allem aber die Komplexität von Sachregister-Vokabularien und ‑Thesauri.
            
            
               Methoden und Werkzeuge
               Bei der Bearbeitung werden gängige Standards aus den Bereichen Semantic Web und Linked Open Data befolgt. Alle kontrollierten Vokabularien folgen dem Datenmodell von Simple Knowledge Organization System 
                    (SKOS), um Wiederverwendbarkeit und Interoperabilität mit Daten in anderen Projekten zu gewährleisten. Die generierten Daten werden mit Dublin Core-Metadaten versehen und in ARCHE 
                    (A Resource Centre for the HumanitiEs), dem geisteswissenschaftlichen Repositorium der Österreichischen Akademie der Wissenschaften, archiviert.
                    
            
         
         
            Linked-Data-Anbindung
            Folgende Sets von Linked Open Data-Datensätzen werden, so vorhanden, zur Anreicherung der Ministerratsprotokolltexte verwendet:
            
               Orte: GeoNames-IDs
               
               Normdaten für Personen, Körperschaften, Ministerkonferenzen und Veranstaltungen, Geografische Entitäten, aus der Gemeinsamen Normdatei 
                    (GND)
               
               Sachregister: kontrollierte Vokabularien, nach Möglichkeit Anbindung an den GEneral Multilingual Environmental Thesaurus 
                    (GEMET)
               
               Prosopographische und biographische Daten: APIS-Datensatz zum Österreichischen Biographischen Lexikon (ab 2019 unter CC-Lizenz).
            
            Dokumentiert und im Poster zur Diskussion gestellt wird der zum Zeitpunkt der DHd 2019 aktuelle Stand der Arbeiten am Editionsprojekt, inklusive der bisherigen Recherchen zu parallel gelagerten Editionsprojekten 
                (u.a.: Die Protokolle des Bayerischen Ministerrats 1945–1962 Online) und einer Einladung zur Nachnutzung der entstehenden Daten.
                
         
      
      
         
            
               . [XML: eXtended Markup Language]
                
            
                    Vgl. den Einleitungsband zur Serie 1, insbes. darin Rumpler, Helmut: Ministerrat und Ministerratsprotokolle 1848 bis 1867. Wien: ÖBV 1970, S. 11-108.
                
            
                    Zum aktuellen Stand bei der Edition vgl. 
                    .
                
            
                    Vgl. 
                    ; APIS steht für Austrian Prosopographical Information System; das 2019 auslaufende Projekt wurde in Kooperation zwischen dem Austrian Centre for Digital Humanities (ACDH), dem Institut für Stadt- und Regionalforschung (ISR) und dem Institut für Neuzeit- und Zeitgeschichtsforschung (INZ) – alle an der Österreichischen Akademie der Wissenschaften – entwickelt und bildete in einem ersten Schritt prosopographische Daten aus dem Österreichischen Biographischen Lexikon (ÖBL) als Set von Relationen zwischen persons, places, institutions, events und works ab. Modelliert als relationale Datenbank, ermöglicht APIS die Darstellung der Entitäten und Relationen in mehreren Exportformaten, die Linked-Open-Data-fähig sind, darunter neben diversen Serialisierungen auch RDF und [TEI]-XML.
                
            
                    Vgl. Miles/Bechhofer 2009.
                
            
               ; das Repositorium wird betrieben von ÖAW-ACDH und dem Rechenzentrum der Österreichischen Akademie der Wissenschaften (ARZ) und basiert auf Fedora Commons.
                
            
                    Vgl. 
                    . Vorgesehen ist auch eine Verknüpfung mit einem weiteren am ÖAW-ACDH angesiedelten Projekt, das geografische Informationsdaten mit einer historischen Tiefendimension erweitert: HistoGIS, siehe 
                    . 
                
            
                    Die von vielen überregionalen Bibliotheksverbünden des deutschsprachigen Raums betriebene Datei (verwaltet von der Deutschen Nationalbibliothek, 
                    ) bietet einerseits Ressourcen, andererseits ±eindeutige Identifikatoren zur Disambiguierung. Für den deutschsprachigen Raum des 19. Jahrhunderts sind in der Mehrzahl der Fälle auch für die zweite Reihe der administrativen Rangordnung bereits zumindest Rumpfdaten vorhanden, und auch das Virtual International Authority File (VIAF) der Library of Congress (ein Superset von GND) hätte diesen gegenüber keine Vorteile zu bieten. Eine Online-Abfrage bietet auch das Bibliotheksservicezentrum Baden-Württemberg, siehe 
                    . 
                
            
                    Online-Zugang siehe 
                    , Vorteile von GEMET sind die Verbreitung und die Verfügbarkeit (auch als SKOS/RDF).
                
            
                    Siehe 
                    .
                
         
         
            
               Bibliographie
               
                  Viele Herausgeber und Bearbeiter (1970–2015): Die Protokolle des österreichischen Ministerrates 1848–1867. Wien: mehrere Verlage. [https://hw.oeaw.ac.at/ministerrat/].
                    
               
                  Viele HerausgeberInnen und BearbeiterInnen (1973–2018): Die Habsburgermonarchie 1848–1918, 12 Bände, Wien: Verlag der Österreichischen Akademie der Wissenschaften.
                    
               
                  Bernád, Ágoston Zénó / Gruber, Christine / Kaiser, Maximilian (2017): Europa baut auf Biographien. Wien: new academic press.
                    
               
                  Fokkens, Antske / ter Braake, Serge / Ockeloen, Niels / Vossen, Piek / Legêne, Susan / Schreiber, Guus / de Boer, Victor (2017): „BiographyNet: Extracting Relations Between People and Events“, in: Europa baut auf Biographien, 193–223.
                    
               
                  Heath, Tom / Bizer, Christian (2011): Linked Data: Evolving the Web into a Global Data Space (1st edition). Synthesis Lectures on the Semantic Web: Theory and Technology, 1:1, 1-136. San Rafael: Morgan & Claypool
                    
               
                  Miles, Alistair / Bechhofer, Sean (eds.) (2009): SKOS Simple Knowledge Organization System, W3C Recommendation, 18 August 2009, .
                    
               Eine ausführliche historiographische Literaturliste kann bei Bedarf zur Verfügung gestellt werden. 
            
         
      
   



      
         
            Einleitung    
                        
            
            Digitale Editionen gehören zu den „prominentesten Themen“ (Sahle 2017: 237) in den Digital Humanities. Sie leisten Grundlagenarbeit für die geisteswissenschaftliche Forschung und bilden ein eigenes Forschungsfeld in den Digital Humanities. Hier verortet sich auch das digitale Editions- und Forschungsprojekt „DER STURM. Digitale Quellenedition zur Geschichte der internationalen Avantgarde“ (https://sturm-edition.de). 
            Gegenstand ist das 1910 gegründete Berliner Kunstunternehmen „Der Sturm“ um den Publizisten, Komponisten und Kritiker Herwarth Walden, der zahlreichen Künstlerinnen und Künstlern unterschiedlicher Kunstgattungen eine Plattform bot. Das Unternehmen umfasste die Zeitschrift „Der Sturm“, die „Sturm“-Galerie, die „Sturm“-Bühne sowie den „Sturm“-Verlag. Neben den Veröffentlichungen des „Sturm“ selbst bezeugen die überlieferten Briefe an das Ehepaar Walden den internationalen Einfluss des Unternehmens.
            Unser Editionsprojekt, in dem bereits digital verfügbares Material aus dem „Sturm“-Kontext transkribiert, standardkonform nach XML/TEI P5 aufbereitet und mit Normdaten versehen wird, führt diese Quellen erstmals zentral zusammen und setzt sie mittels digitaler Methoden in Relation zueinander.
         
         
            Drei Forschungsansätze im STURM-Projekt
            Integraler Teil des Editionsprojektes ist die Forschung mit den STURM-Quellen. Auf dem Poster stellen wir neben der Quellenedition die folgenden drei Forschungsansätze vor, die sich explizit mit den im Projekt edierten Materialien beschäftigen.
            
               Modellierung der STURM-Domäne
               Ein Ansatz arbeitet mit den in der digitalen Quellenedition DER STURM zusammengeführten Quellen und modelliert deren Verknüpfungen und Beziehungen untereinander. Die Grundlage bildet hierbei das CIDOC Conceptual Reference Model als Domain-Ontologie im Bereich Cultural Heritage (Doerr 2009), die im Bereich der musealen Sammlungen und der bildenden Kunst weit verbreitet ist. Ergänzt wird diese Ontologie durch fachspezifische Vokabulare wie den Getty Art & Architecture Thesaurus (AAT). Die semantischen Verknüpfungen zwischen den einzelnen Quellen und Quellen-Typen beziehen sich auf die in den bereits edierten Quellen annotierten Entitäten (Personen, Orte und Werke) sowie auf weitere Entitäten, die noch in den Editionsprozess aufgenommen werden: auf Körperschaften, Ereignisse und Themen. In der digitalen Quellenedition des STURM sind für diese Entitäten bereits URIs vorhanden bzw. vorgesehen. Durch Anreicherung der Entitäten mit Normdaten werden Verknüpfungen mit weiteren Ressourcen ermöglicht.
               Die semantische Modellierung der STURM-Domäne – bestehend aus den Quellen der Edition und dem, was sie bezeugen – dient der Weiterentwicklung der digitalen Quellenedition DER STURM sowie perspektivisch der Weiternutzung der dabei gewonnenen Daten, denn sie bildet die Grundlage für eine Verfügbarmachung der Daten und ihrer Zusammenhänge in Form von Linked Open Data. In dieser Form können die Daten beim Ermitteln und Zeigen von Zusammenhängen helfen und somit die Basis bilden für weitere Forschungen in einzelnen Fachwissenschaften, insbesondere in der Kunst- und in der Literaturwissenschaft.
            
            
               Historische Netzwerkforschung zum „Sturm“
               Ein weiterer Ansatz im Projekt beschäftigt sich mit der historischen Netzwerkforschung (Düring et al. 2016). In der bisherigen „Sturm“-Forschung steht aufgrund der Komplexität und Dezentralität der Quellen eine dezidiert „kunsthistorische Beschäftigung mit dem 
                        Sturm“ aus (van Rijn 2013: 11). Insbesondere an einer ‚Gesamtschau‘ zum „Sturm“ fehlt es – ein Desiderat, an das hier mit der Methode der Historischen Netzwerkforschung angeknüpft werden soll. Dafür wird für das zu untersuchende Phänomen des historischen „Sturm“ ein Gesamtnetzwerk modelliert und anhand algorithmischer sowie hermeneutischer Methoden analysiert (Brandes et al. 2013: 4). Datengrundlage der Netzwerkerhebung bilden archivalische Metadaten und die im STURM-Projekt maschinenlesbar modellierten Quellen.
                    
               Das Netzwerk ist multimodal, bestehen die einzelnen zueinander in Relation stehenden Entitäten doch aus im Kontext des „Sturm“ aufkommenden Personen, multimedialen Werken, Briefdaten und einigem mehr. Diese Daten gilt es in der Visualisierung des erhobenen Netzwerkes anschaulich zu machen (Baillot 2018: 357). Darüber hinaus offenbart die computergestützte Historische Netzwerkforschung auch in der Analyse komplexer Netzwerke ihre Stärken. Durch eine rein klassische Untersuchung des facettenreichen „Sturm“-Netzwerkes, so die These, würden Darstellung und Analyse unübersichtlicher und damit fehleranfälliger werden.
               Die kritische Untersuchung der Quellen selbst – und damit einhergehend die Interpretation des erhobenen Gesamtnetzwerkes „Sturm“ – bildet einen weiteren wichtigen Schritt in der Gesamtanalyse des Kunstunternehmens hinsichtlich eines möglichen ‚gesamtgesellschaftlichen Spiegels‘. Ziele der historischen Netzwerkstudie sind das Ausmachen und die Analyse von 
                        broken ties (Jannidis 2017: 158) im komplexen „Sturm“-Netzwerk und die Einordnung des „Sturm“ in den zeitgenössischen kultur- und soziopolitischen Kontext.
                    
               
                  
                  
                     Abbildung 1. Gerichtetes Korrespondenznetzwerk um Herwarth Walden (rot), Else Lasker-Schüler (gelb) und Nell Walden (grün); Layout: ForceAtlas 2; Knotengröße: 
                            outdegree; Knotenfarbe: 
                            indegree; Kantendicke: 
                            weight (Menge der überlieferten Briefe).
                        
               
            
            
               Diskursanalyse des Simultaneitätsbegriffs im „Sturm“         
               Ein Beispiel für die fachspezifische Nutzung digitaler Editionen gibt die Studie zu avantgardistischen Simultaneitätskonzepten im „Sturm“. Als ein die Avantgarde bestimmendes Strukturprinzip kommt Simultaneität in verschiedenen Kunstrichtungen vor, die innerhalb ihrer Programmatik mitunter konkurrierende Modelle entwickeln. F. M. Marinettis „Technisches Manifest der futuristischen Literatur“, die kubistischen „Fenster“-Bilder Robert Delaunays oder die Simultangedichte der Dadaisten ähneln sich im grundlegenden Bestreben, nach dem avantgardistischen Primat von „Transgression und Diffundierung“ (Asholt / Fähnders 2000: 17) beim Rezipienten gleichzeitig unterschiedliche Wirklichkeitsansichten und -ebenen zu erzeugen. Im medialen Komplex des „Sturm“, mit seinen vielfältigen Publikations- und Distributionswegen sowie seinen dezidiert antimimetischen Gestaltungsprinzipien, finden diese bildkünstlerischen wie literarischen Werke trotz Gattungs- und Stilunterschieden gleichermaßen eine adäquate Präsentationsform. Eingebettet in den spezifischen historischen Kontext des „Sturm“-Netzwerks werden sie von Para- und Metatexten begleitet, die ihre Rezeption lenken und kommentieren – und die nicht zuletzt in den privaten Korrespondenzen an Walden vorbereitet und verhandelt werden.
               Die webbasierte Zusammenführung der verschiedenen Quellen des „Sturm“ erlaubt es nun, wechselseitige Bezüge zwischen den unterschiedlichen Textsorten offenzulegen und so diskursiv etablierte sprachlich-rhetorische Muster zu rekonstruieren. Mit Hilfe korpuslinguistisch informierter diskursanalytischer Verfahren wird der Frage nachgegangen, inwiefern solche multimodalen Diskurspraktiken die Bedeutung von Gleichzeitigkeit im historischen „Sturm“-Netzwerk unterschiedlich konstituieren und auf diese Zeitkonstruktion als typisch avantgardistischen Diskurs verweisen. Um die entsprechenden „Diskursfragmente“ (Jäger 2012: 98ff.) im Netzwerkkontext situieren und den einzelnen Künstlern und Kunstrichtungen zuordnen zu können, sollen zusätzlich netzwerkanalytische Zugänge berücksichtigt werden. Schließlich zielt die Auswertung der im Projekt erarbeiteten Daten darauf ab, den Begriff der Simultaneität im Problemfeld von Scheitern (Habermas) und Überleben (Luhmann) der Avantgarde zu konturieren.
            
         
      
      
         
            
               Bibliographie
               
                  Asholt, Wolfgang / Fähnders, Walter (2000): „Einleitung“, in: Asholt, Wolfgang / Fähnders, Walter (eds.): Der Blick vom Wolkenkratzer. Avantgarde – Avantgardekritik – Avantgardeforschung. Amsterdam / Atlanta: Editions Rodopoi 9–27.
                    
               
                  Baillot, Anne (2018): „Die Krux mit dem Netz. Verknüpfung und Visualisierung bei digitalen Briefeditionen“, in: Bernhart, Toni / Willand, Marcus / Richter, Sandra / Albrecht, Andrea (eds.): Quantitative Ansätze in den Literatur- und Geisteswissenschaften. Systematische und historische Perspektiven. Open Access: De Gruyter 355–370.
                    
               
                  Brandes, Ulrik / Robins, Garry / McCranie, Ann / Wasserman, Stanley (2013): Editorial. What is network science? Network Science 1: 1–15.
                    
               
                  Chytraeus-Auerbach, Irene / Uhl, Elke (2013): „Vorwort“, in: Chytraeus-Auerbach, Irene / Uhl, Elke (eds.): Der Aufbruch in die Moderne. Herwarth Walden und die europäische Avantgarde. Berlin: LIT Verlag 7–11.
                    
               
                  Doerr, Martin (2009): „Ontologies for Cultural Heritage“, in: Staab, Steffen / Studer, Rudi (eds.): Handbook on Ontologies. Berlin / Heidelberg: Springer Verlag 463–486.
                    
               
                  Düring, Marten / Eumann, Ulrich / Stark, Martin / Keyserlingk, Linda v. (2016) (eds.): Handbuch Historische Netzwerkforschung. Grundlagen und Anwendungen. Berlin: LIT Verlag.
                    
               
                  Jäger, Siegfried (2012): Kritische Diskursanalyse. Eine Einführung. 6. vollständig überarbeitete Aufl. Münster: Unrast.
                    
               
                  Jannidis, Fotis (2017): „Netzwerke“, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): Digital Humanities. Eine Einführung. Stuttgart: J. B. Metzler 147–161.
                    
               
                  Pirsich, Volker (1985): Der Sturm. Eine Monographie. Herzberg: Traugott Bautz 1985.
                    
               
                  Rijn, Maaike van (2013): Bildende Künstlerinnen im Berliner „Sturm“ der 1910er Jahre. Tübingen: TOBIAS-lib, http://tobias-lib.uni-tuebingen.de/volltexte/2013/6987.
                    
               
                  Sahle, Patrick (2017): „Digitale Edition“, in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): Digital Humanities. Eine Einführung. Stuttgart: J. B. Metzler 234–249.
                    
            
         
      
   



      
         
            Einführung
            Für die Erstellung und Bearbeitung digitaler Editionen im Bereich der Digital Humanities werden vorzugsweise XML-basierte Lösungen verwendet. Eine der dabei am häufigsten benutzten Datenbanktechnologien ist eXist-db1. Dabei ist eXist-db nicht nur eine Lösung zum Bearbeiten und Speichern von XML-Daten, sondern bietet auch Möglichkeiten zur Programmierung von Webanwendungen auf Grundlage von XQuery. Im weiteren Programmierer*innen-Umfeld sind X-Technologie-Lösungen sowohl für die Datenhaltung als auch für das Erstellen von Webanwendungen eher eine Nischenlösung2.
            In den letzten Jahren werden verstärkt Frameworks wie Vue.js3 oder React4 für die Erstellung von Frontendmodulen in Webanwendungen verwendet.5 Ein großer Vorteil solcher Frameworks gegenüber herkömmlichen Lösungen ohne Framework oder mit jQuery6 ist die Möglichkeit, in sich geschlossene, reaktive User-Interface-Komponenten zu erstellen. Reaktiv heißt in diesem Fall, dass wenn sich das Datenmodell innerhalb einer Komponente ändert, ohne weiteren Aufwand sofort auch die Benutzeroberfläche entsprechend angepasst wird. Gleiches gilt umgekehrt, z.B. wenn eine Nutzerin ein Textfeld ausfüllt, auch das Datenmodell der Komponente sofort den entsprechenden Wert übernimmt, und so weitere Operationen im Hintergrund ausführen kann.
            Da diese JavaScript-Frameworks jedoch mit JSON statt mit XML arbeiten, soll der hier vorgeschlagene Workshop eine Einführung in die Verwendung von Vue.js in Verbindung mit eXist-db geben. Im Workshop soll konkret gezeigt werden, wie man mit eXist-db abgefragte Daten nach JSON serialisieren kann, um diese dann für die Erstellung einer Vue.js basierten, reaktiven Benutzeroberfläche einzusetzen. Grundsätzlich soll den Teilnehmenden hierdurch das Zusammenspiel zwischen einem beliebigen Backend - JSON-API - JavaScript-Frontend-Framework vermittelt werden.
         
         
            Workshop
            Als praktisches Beispiel für den Workshop werden die Daten und der Quellcode von quoteSalute7 zur Veranschaulichung benutzt. quoteSalute aggregiert Grußformeln aus verschiedenen digitalen Briefeditionen, bereitet diese auf, sodass NutzerInnen Briefabschlüsse von z.B. Alexander von Humboldt, Friedrich Schleiermacher oder Anna Gräfin von Lehndorff über einen Klick in ihre eigene E-Mail-Korrespondenz einfügen können. Die Daten dafür werden in einer eXist-db-Instanz als TEI-XML vorgehalten und die Benutzeroberfläche über Vue.js realisiert. Dabei kommuniziert die Benutzeroberfläche im Browser mit dem eXist-db-Backend, sodass auf Knopfdruck via XQuery in der Datenbank eine zufällige Grußformel ausgewählt und nach JSON serialisiert wird, sodass letztendlich die Nutzeroberfläche mit der neuen Grußformel angepasst werden kann. Um die passende Grußformel für die eigene E-Mail zu finden, können Nutzer*innenden Korpus nach Höflichkeitsstufe (formell, informell), Sprache (derzeit deutsch, englisch, französisch, italienisch, spanisch, griechisch, latein) oder nach Geschlecht des Absenders und Empfängers filtern.
            Sowohl der Datenkorpus, als auch der Quellcode sowie die Dokumentation sind als freie Software verfügbar. 8
            
            Der Workshop ist für zwei halbe Tage und für ca. 15-30 Teilnehmende ausgelegt. Dabei ist das Workshopprogramm in insgesamt vier Zwei-Stunden-Blöcke aufgeteilt, die inhaltlich aufeinander aufbauen. Im Folgenden werden die im Vorfeld des Workshops zu verteilenden Materialien, die zu erwartenden Voraussetzungen an die Teilnehmenden, sowie die Inhalte der einzelnen Blöcke und ihre Lernziele beschrieben.
            
               Voraussetzungen
               Von den Teilnehmenden wird erwartet, dass sie grundlegende Kenntnisse in der JavaScript- und Web-Programmierung (HTML, CSS) haben. Weiterhin wird darum gebeten, einen Computer mitzubringen und vorab eXist-db (kostenfrei) sowie einen Texteditor zum Programmieren wie Visual Studio Code oder Atom zu installieren.
            
            
               Materialien
               Vor dem Workshop stellen wir eine eXist-db-Applikation zusammen, die bereits alle benötigten Daten, Skripte und Datenbankabfragen enthält, um im Workshop sofort in die Arbeit einsteigen zu können, ohne selbst noch alles einrichten zu müssen. Die Teilnehmenden können dann diese App in ihrer eXist-db per Drag & Drop installieren. Eine entsprechende Anleitung wird bereitgestellt. 
               Sollte die Einrichtung der eXist-Instanz oder die Installation der App wider Erwarten nicht funktionieren, stellen wir auch eine eigene eXist-Instanz für den Workshop mit personalisierten Logins für die Teilnehmenden sowie eine JSON-REST-API bereit, sodass die weiteren Workshopinhalte in jedem Fall praktisch nachvollzogen werden können.
               Des Weiteren bereiten wir ein HTML- und VueJS-Code-Skelett vor, sodass auf JavaScript-Seite wenig Zeit auf die Einrichtung der Programmierungsumgebung verwendet werden muss. 
               Die hier aufgelisteten Daten werden auf Github bereitgestellt und dokumentiert.
            
            
               Inhalte
               
                  Block 1
                  Zu Anfang sollen die im Workshop verwendeten Tools und Konzepte, insbesondere das Zusammenspiel zwischen exist-db, JSON und Vue.js, vorgestellt werden. Es wird den Teilnehmenden noch etwas Zeit eingeräumt, die nötige Software herunterzuladen, zu installieren und ihre Entwicklungsumgebung einzurichten. Anschließend wird der Aufbau der quoteSalute-eXist-db-App inklusive des mitgelieferten Datenkorpus erklärt und gezeigt, wie innerhalb von eXist-db Daten abgefragt und ins JSON-Format serialisiert werden. Diese Schritte sind bereits voreingerichtet, sodass auch Teilnehmende ohne XQuery oder eXist-db-Kenntnisse den Ablauf nachvollziehen können. Aus praktischer Sicht werden die Teilnehmenden den quoteSalute-Grußformel-Korpus abfragen, eine zufällige Grußformel extrahieren und nach JSON serialisieren.
               
               
                  Block 2
                  Im zweiten Block sollen die Teilnehmenden anfangs versuchen, die vorgegebenen Beispiele für Datenbankabfragen und JSON-Serialisierungen anzupassen bzw. zu erweitern, um beispielsweise nur Grußformeln, die an Frauen adressiert waren, aus dem Datenbestand herauszufiltern. So soll die Arbeit mit eXist-db und JSON-Serialisierung geübt und gefestigt werden.
                  Um die aus der eXist-db gelieferten JSON-Daten client-seitig verwenden zu können, muss das JavaScript-Frontend-Framework Vue.js mit der JSON-REST-API kommunizieren. Dafür soll zuerst in die Grundprinzipien von Vue.js (data, templates, model-view-binding, methods, computed) anhand kleiner, aufeinander aufbauenden Code-Beispiele eingeführt werden. 
               
               
                  Block 3
                  In Block 3 sollen die in den ersten beiden Blöcken erarbeiteten Methoden und Abläufe miteinander in Verbindung gebracht werden. Über eine asynchrone Abfrage zum lokalen exist-db-Server (JavaScript-Skelett wird gestellt), soll die Vue.js-App eine neue Grußformel vom Server zu erfragen und diese dann im eigenen Datenmodell und so auch in der Benutzeroberfläche abbilden. Um asynchrone Operationen mit JavaScript nachvollziehen zu können, wird an dieser Stelle zusätzlich das Konzept von Promises vorgestellt und erläutert.
                  Die Teilnehmenden können die verbleibende Zeit in diesem Block dazu nutzen, ihre Vue.js-App zu programmieren und gegebenenfalls visuell anzupassen.
               
               
                  Block 4
                  Sollte die Zeit für die Programmierung der Vue.js-App nicht genügen, dient der vierte Block als zusätzlicher Puffer. Für bereits fertige Teilnehmer und Interessierte bieten wir noch ein optionales Programm mit zusätzlichen Aufgaben an. So soll das Zusammenspiel zwischen eXist-db und Vue.js noch an einem weiteren, konkreten Fallbeispiel für die Digital Humanities geübt werden. Die Teilnehmenden erhalten einen Auszug aus einer digitalen Edition und sollen mit den vorgestellten Technologien ein Personenregister erzeugen. Durch diese Transferleistungsübung sollen die erlernten Vue.js-Methoden und Prinzipien gefestigt werden.
               
               
                  Übersicht
                  Das Grundkonzept des Workshops lässt sich in folgender Tabelle zusammenfassen:
                  
                     
                        
                        Technologien & Konzepte
                        Lernziel
                     
                     
                        Block 1
                        Überblick eXist-db, VueJS & JSON
                        JSON-Serialisierung von XML-Daten mit eXist-db verstehen
                     
                     
                        Block 2
                        eXist-db, JSON, XPath, XQuery
                        eXist-db Datenbankanfragen modifizieren und Ergebnisse nach JSON serialisieren
                     
                     
                        VueJS
                        Grundkonzepte von VueJS verstehen und anwenden
                     
                     
                        Block 3-4
                        VueJS, Asynchrone Anfragen, Promises
                        VueJS-App programmieren und an JSON-REST-API anbinden
                     
                  
                  Nach dem Workshop sollen die Teilnehmenden in der Lage sein
                  
                     nachzuvollziehen, wie man mit eXist-db XML-Daten nach JSON serialisiert,
                     die Bedeutung von der Vue.js-Begriffe data, methods, model-view-binding und templates zu verstehen, anwenden und selbst erklären zu können,
                     eine einfache, reaktive Vue.js-Applikation zu programmieren,
                     diese Vue.js-Applikation an eine JSON-REST-Schnittstelle anzubinden, und
                     das grundlegende Prinzip im Zusammenspiel von Backend - JSON-API - JavaScript-Frontend auch auf andere Technologien und Frameworks übertragen zu können.
                  
               
            
            
               Technische Ausstattung
               Für die Durchführung des Workshops wird ein Beamer (vorzugsweise mit VGA-Anschluss) sowie Internetzugang benötigt. Aufgrund der Länge des Workshops wären mehrere Mehrfachsteckdosen zum Aufladen der Geräte der Teilnehmenden wünschenswert.
            
         
      
      
         
            
               
               http://exist-db.org
            
            
                In ProgrammiererInnen-Umfragen von JetBrains und StackOverflow oder Programmier- und Datenbank-Beliebheits-Indizes sind eXist-db, XQuery und verwandte Technologien nicht einmal genannt. 
               
               
                  
                  
               
               
                  
                  
               
               
                  
               
            
            
               
               
            
            
               
               
            
            
               
               
               
                  
                   
               
            
            
               
               
               
                  
                   
               
            
            
               
               
            
            
               
               
                  http://quotesalute.net
               
            
            
               
               
            
         
      
   



      
         Die computergestützte Erforschung einer nur teilweise erschlossenen Schrift und Sprache wie im Falle der Hieroglyphenschrift der klassischen Mayakultur steht vor zahlreichen Herausforderungen, insbesondere bei der Erfassung der Komplexität von Schrift- und Bildzeugnissen. Gerade historisierende Wissenschaftsdisziplinen sind auf diverse Informationsquellen angewiesen, um ihre Untersuchungsgegenstände nicht bloß in der historischen Vergangenheit sondern auch in der modernen Gegenwartskultur zu vergesellschaften: Informationen zu ursprünglichen Aufstellungsorten von mit Hieroglyphen reliefierten Stelen, Angaben zum aktuellen Aufbewahrungsort dekorierter und beschriebener Keramiken oder jahrzehntealte Zeichnungen monumentaler Tempelinschriften - das Wissen nicht bloß über die historischen Kontexte sondern auch über die wissenschaftliche Arbeit mit und an den antiken Schriftzeugnissen durch Forscher und Sammler bildet den essentiellen Rahmen, um wissenschaftliche Aussagen und Hypothesen zu formulieren, zu überprüfen und zu plausibilisieren.
         Die Grundlage dieses Rahmens bilden Modelle, die zum einen eine formalisierte Beschreibung der benötigten Informationen erlauben und zum anderen eben jene Informationen miteinander in Beziehung setzen. Hier stellen Ontologien und domänenspezifische (Daten-)Modelle unerlässliche Hilfsmittel und notwendige Werkzeuge dar, um Wissen über Objekte, die im Fokus des wissenschaftlichen Interesses stehen, einheitlich und vor allem aussagekräftig zu dokumentieren. Vor diesem Hintergrund verfolgt das Projekt ‘Textdatenbank und Wörterbuch des Klassischen Maya’ (TWKM)1 das Konzept einer ontologisch-vernetzten Datenbeschreibung: Der antike Text als kulturgeschichtliches Artefakt und somit umfassendes Wissens- und Informationsobjekt wird in einzelne unterschiedliche Informationsbereiche unterteilt, die jeder für sich nach besonderen Anforderungen und eigenen Datenmodellen beschrieben aber aufeinander bezogen werden.
         Um den Informationsgehalt der antiken Textressourcen differenziert in maschinenlesbarer Form zu beschreiben, werden verschiedene Informationsbereiche auf unterschiedlichen Ebenen voneinander abgegrenzt: Zunächst werden die Schriftträger anhand eines ontologisch-basierten Metadatenschemas in RDF erfasst: Hier werden Kerninformationen zum Schriftträger (Bezeichnung, Zustand, Material und Technik, Maße etc.), seines archäologischen Kontexts sowie auch darüber hinausgehend historische Ereignisse und Persönlichkeiten der Maya-Kultur dokumentiert (Textdatenbank und Wörterbuch des Klassischen Maya 2017). Die Auszeichnung der textlichen Informationen wird separat in TEI-P5 Dokumenten vorgenommen, die mit den entsprechenden in RDF dokumentierten Ressourcen über persistente URIs (Uniform Resource Identifier) verknüpft werden.
         Für die Auszeichnung der etwa 10.000 Maya-Texte dient ein projektspezifisches TEI-P5 Anwendungsprofil. Das TWKM unterscheidet hier zwischen den drei Informationsbereichen (1) Form, (2) Inhalt und (3) linguistische Analyse, die nach jeweils spezifischen Anforderungen separat erschlossen werden. So wird die Erschließung der Form bzw. der Texttopographie, d.h. der strukturellen Anordnung von Schrift- und Bildbereichen auf dem Schriftträger, unabhängig von der linguistischen Analyse und den in diesem Rahmen verwendeten Beschreibungskategorien durchgeführt. 
         Das TWKM bedient sich hier am Konzept des sog. Stand-off-Markups2: die individuierte Auszeichnung von Informationen, die durch Verweise auf andere ausgezeichnete Informationen virtuell3 in einen gemeinsamen Zusammenhang gebracht werden. Durch diesen Ansatz kann nicht nur die Komplexität der Auszeichnung einzelner Informationsbereiche individuell angehoben bzw. abgesenkt werden. Auch der direkte Einfluss struktureller Anforderungen des XML-Formats auf die Auszeichnung, die in der Praxis häufig zu Problemen und Herausforderungen führen (z.B. die Wohlgeformtheitsregel und die hiermit einhergehende Maßgabe, dass XML-Elemente sich nicht überschneiden dürfen), wird hierdurch minimiert.4
         
         Der Inhalt eines Textes lässt sich somit kohärent d.h. in einem logisch-thematischen Zusammenhang beschreiben, obwohl seine topographische formal-strukturelle Anordnung einer gänzlich anderen Logik folgt: z.B. kann ein Text topographisch in geflochtener Form arrangiert und dementsprechend maschinenlesbar beschrieben werden. Die inhaltlich-logische Struktur des Texts wird separat gemäß ihren eigenen Ordnungsprinzipien beschrieben aber mit den topographischen Strukturen in Beziehung gesetzt:
         
            
            
               Abbildung 1. Virtueller Zusammenhang zwischen Bildbereich (oben links), Texttopographie (oben rechts) und Inhalt (unten)
				
         
         Zunächst werden die texttopopgraphischen Eigenschaften des Schriftträgers 
            (tei:sourceDoc) beschreiben. In diesem Kontext werden unterschiedliche Oberflächen 
            (tei:surface) erfasst (z.B. die Flächen einer rechteckigen Stele, die Innen- und Außenseite einer Vase oder Vorder- und Rückseite eines Kodexblattes) und mit digitalen Faksimiles verbunden, die zuvor aus analogen Repräsentationen oder direkt vom Objekt, etwa über 3D-Scanning, erstellt wurden.5 Diese Oberflächen enthalten einen bis mehrere Textbereiche 
            (tei:zone), die mit Hilfe des Text-Bild-Link-Editors (TBLE) des TextGrid-Laboratorys (Neuroth / Rapp / Söring 2015) mit einzelnen Bereichen des Faksimiles assoziiert wurden. In diesem Rahmen können weitere Informationen, wie bspw. Orientierung oder Ausmaße erhoben werden. Die inhaltliche Erschließung der einzelnen Hieroglyphenblöcke und Schriftzeichen werden wiederum separat in 
                tei:body  erfasst. 
            
         Die Beschreibung des logo-sylabischen Schriftsystems des klassischen Maya ist aufgrund ihrer Eigenarten und Komplexität eine herausfordernde Aufgabe. Nicht nur aufgrund ihrer vergleichsweise jungen Entzifferungsgeschichte seit den 1950er Jahren gibt es noch eine Reihe von Desiderata bei Lesbarkeit und Verständnis des Schriftsystems. Trotz verschiedener Zeichenkataloge ist die genaue Anzahl von Zeichen noch immer nicht gesichert, damit auch nicht, wie viele Zeichen nicht oder nur unzureichend entziffert sind. Die Inventarisierung war auch bisher eine Herausforderung wegen multipler Klassifizierungsansätze der Schriftzeichen, etwa über die Form (Thompson 1962) oder die Ikonizität (Macri / Looper 2003). Das Projekt hat erstmals eine Systematik anhand einer taxonomischen Beschreibung der Bildung von Graphemvarianten entwickelt und trennt auf diesem Wege auch das bedeutungstragende Zeichen von seiner graphischen Repräsentation. Die wissenschaftliche Deutung des Zeichens, seine linguistische Information, wird dabei über ein System von Kriterien, die durch Aussagenlogik miteinander verbunden sind, qualitativ bewertet. Darauf basierend wird die Plausibilität der jeweiligen Entzifferungshypothese automatisiert eingestuft. Aufgrund der Dynamik der Entzifferungsarbeit ist es demnach nicht möglich, Transliterationen als Basis für die Textauszeichnung zu benutzen (Diehr et al. 2017: 1191-1192).
         Deshalb und wegen des Umstands, dass es innerhalb der Mayaforschung keinen Konsens zum Umgang mit den derzeitigen Unicode-Vorschlägen bzw. -Implementierungen zur Mayaschrift gibt (Pallan Gayol / Anderson 2018), verfolgt das TWKM einen eigenen Ansatz zur Schriftbeschreibung gemäß der TEI-P5 Richtlinien.6 Die Graphe 
            (tei:g) sind in nahezu rechteckigen Blöcken angeordnet 
            (tei:ab[@type=’glyph-block’]), die ungefähr einem Wort entsprechen. Jedes Zeichen wird mit einer URI-Referenz 
            (@ref) versehen, die auf die konkrete Graphrepräsentation im Zeichenkatalog verweist. Aufgrund der komplexen Graphematik können einzelne Graphe wiederum zu Gruppen im Block zusammengefasst werden 
            (tei:seg[@type=’glyph-group’]), z.B. bei einer Infigierung. Über weitere Attribute 
            (@corresp und 
                @rend) wird so die Position jedes einzelnen Graphs im Block und in Relation zu den anderen Graphen eindeutig beschrieben.
            
         So kann der hieroglyphische Inhalt eines Texts maschinenlesbar dokumentiert werden, wobei die Entzifferungsgeschichte und hiermit einhergehende Veränderungen und Reinterpretationen einzelner Schriftzeichen auf der Ebene des Zeichenkatalogs abgebildet werden: Sollte sich die Interpretation eines Zeichens beispielsweise hinsichtlich seiner Semantik im Verlauf der fortschreitenden Forschung ändern, so bleibt die hieroglyphische Auszeichnung aller im TWKM erschlossenen Mayatexte unangetastet. Die Veränderung muss lediglich im Zeichenkatalog dokumentiert werden und steht danach als Information für alle Texte zur Verfügung. Gleiches gilt für die Entzifferungsgeschichte eines spezifischen Texts: Sollte die Deutung eines Graphems in einem konkreten Maya-Text revidiert werden, so erfolgt diese Änderung im Zeichenkatalog. Neu klassifizierte Zeichen werden im Zeichenkatalog durch die Relation owl:sameAs7 mit den betreffenden falsch klassifizierten Zeichen verbunden. Damit ist garantiert, dass vormals falsch klassifizierte Zeichen weiterhin über deren URI auffindbar sind.8 Die im Korpus referenzierten URIs vormals falsch klassifizierter Zeichen müssen dadurch nicht geändert werden und das kodierte Korpus bedarf keiner nachträglichen Überarbeitung.
         
            
            
               Abbildung 2. Übergang der maschinenlesbaren Textauszeichnung zur linguistischen Analyse
				
         
         Während die Kodierung des Korpus und die Klassifikation der Zeichen  in TextGrid vorgenommen werden, findet die linguistische Analyse der Mayatexte  in einem separaten Analysetool statt.9 Über eine eigens entwickelte Schnittstelle liest das Programm  sowohl die TEI/XML-Dokumente als auch die in RDF gespeicherten Transliterationswerte aus TextGrid aus und bereitet sie für den folgenden mehrstufigen Annotationsprozess von Transliteration, Transkription und morphosyntaktischer Glossierung auf. Wo bisher nur vereinzelt Studien vorliegen (vgl. Wichmann 2006), besteht nun das Ziel, eine umfassende Grammatik für das Klassische Maya zu entwickeln. Durch ein Verfahren der mehrstufigen Annotation bei gleichzeitigem Anlegen paralleler Analysepfade, die sich jederzeit auf die entsprechende Belegstelle zurückverfolgen lassen, sind ideale Voraussetzungen zur Durchführung grammatikalischer Bestimmungen und Untersuchungen geschaffen.
         Im Fokus des TWKM steht die multiperspektivische Erforschung der Sprache und Schrift des Klassischen Maya. Die über die ganze Welt verstreuten und im Original größtenteils nicht zugänglichen Schriftzeugnisse der antiken Mayakultur werden mittels miteinander verknüpfter Ansätze digital erschlossen. In diesem Rahmen finden zahlreiche Technologien Anwendung, um das derzeitige Wissen über die Maya-Texte nach wissenschaftlichen Standards zu dokumentieren und sukzessive auszuweiten. Durch die kombinierte Verwendung von Ontologien zur Beschreibung und Verknüpfung einzelner Ressourcen auf Metadatenebene einerseits und TEI-P5 XML zur maschinenlesbaren Beschreibung der der Textressourcen andererseits ergibt sich ein engmaschiges Netz aus Informationen zu einem längst vergangenen Kapitel der Menschheitsgeschichte. Ein Netz, das unterschiedliche Zugänge für die wissenschaftliche Forschung sowie für die interessierte Öffentlichkeit bereithält, um tiefe Einblicke in einen vor dem Vergessen bewahrten Teil des kulturellen Erbes zu gewähren - frei, transparent und nachnutzbar.10
         
      
      
         
            
               http://mayawoerterbuch.de
            
             Siehe hierzu die Definition in http://uahost.uantwerpen.be/lse/index.php/lexicon/markup-standoff/. Die Methode wurde erstmalig beschrieben von Thompson / McKelvie (1997).
             “Virtuell” meint, dass der Zusammenhang nicht durch die hierarchische Struktur der Daten vorgegeben, sondern durch den Verweis strukturell entkoppelter Daten aufeinander hergestellt wird. Es ist dementsprechend ein Zusammenhang, der erst durch die Verarbeitung der verknüpften Daten in einem Informationssystem kultiviert wird, d.h. erst durch die Anwendung von Informationsprozessen wird aus den verknüpften Daten eine zusammenhängende Information erzeugt.
             Die TEI-Community führt eine anhaltende Diskussion über Vor- und Nachteile des Stand-Off Markups und dessen fortlaufender Entwicklung (vgl. Bański 2010 und Spadini / Turska / Broughton 2015). Dabei sind Flexibilität, Interoperabilität und Nachhaltigkeit der erzeugten Dokumente jene zentralen Faktoren, die in den Diskussionen immer wieder miteinander abzuwägen sind: Durch die Anwendung von Prozessierungsmechanismen, die benötigt werden, um die Daten der unterschiedlichen Dokumente zusammenzuführen, ergeben sich Probleme für die Nachhaltigkeit und Nachnutzung (vgl. Rehm et al. 2010). Dem gegenüber stehen die vielseitigen Möglichkeiten und Potenziale der Datenanreicherung und -verarbeitung: Separate Ressourcen können unabhängig voneinander bearbeitet und gleichzeitig flexibel ineinander verschränkt werden. Dadurch entstehen semantisch-reichhaltige Dokumente mit hoher Informationsdichte. Diese Vorteile zeigen sich insbesondere im Umgang mit (überlappenden) Hierarchien und Annotationen (z.B. Ide / Romary 2007 und Dipper 2005).
             Das Projekt bemüht sich um die Nachnutzung von digitalen Faksimiles, die aber für viele Bereiche noch nicht oder in nicht ausreichender Qualität vorliegen. Über Kooperationen werden daher Archive digitalisiert, so etwa die etwa 40.000 Objekte umfassende Fotothek von Prof. Karl Herbert Mayer, Graz, von denen bereits über 5.700 Digitalisate publiziert werden konnten ( https://classicmayan.kor.de.dariah.eu/). Für die Arbeiten zum 3D-Scanning siehe https://blog.sketchfab.com/from-the-rainforest-to-virtual-light-scanning-maya-hieroglyphs/.
             Diese Herausforderungen sind auch bei anderen antiken, nicht-alphabetischen Schriftsystemen gegeben (vgl. Rossi / De Santis 2019). Zu diesem Zweck wurde zur Vereinheitlichung von Auszeichnungen 2015 die interdisziplinäre Arbeitsgruppe EnCoWS (Encoding Complex Writing Systems) ins Leben gerufen.
             Zur Definition von owl:sameAs siehe: https://www.w3.org/TR/owl-ref/#sameAs-def.
             Durch diese Methode werden unter anderem auch Untersuchungen zur Klassifikationsgeschichte der Schriftzeichen ermöglicht.
             Eine erste Beschreibung des sich in der Entwicklung befindenden Tools ‘ALMAH’ findet sich im Jahresbericht 2017 des Projekts (Grube et al. 2018: 5-7).
             Die erzeugten Daten werden sukzessive auf dem zukünftigen Projektportal https://www.classicmayan.org/ zugänglich gemacht werden. Des Weiteren werden die Korpusdaten auch frei zugänglich im TextGrid Repository veröffentlicht werden. Die im Projekt entstandenen Schemata sind im öffentlichen Bereich unseres Git-Repositoriums einsehbar und können unter einer CC BY-4.0 Lizenz genutzt werden: https://projects.gwdg.de/projects/documentations/repository.
         
         
            
               Bibliographie
               
                  Bańksi, Piotr (2010):
                  Why TEI stand-off annotation doesn't quite work and why you might want to use it nevertheless, 
                        in: Proceedings of Balisage 2010. Series on Markup Technologies  5 
                        
                     https://doi.org/10.4242/BalisageVol5.Banski01
                   [letzter Zugriff: 05.01.2019].
                    
               
                  Diehr, Franziska  / Gronemeyer, Sven  / Prager, Christian / Brodhun, Maximilian  / Wagner, Elisabeth / Diederichs, Katja / Grube, Nikolai (2017):
                  Modellierung eines digitalen Zeichenkatalogs für die Hieroglyphen des Klassischen Maya, 
                        in: 
                        Eibl, Maximilian / Gaedke, Martin  (eds.):
                  Proceedings der INFORMATIK 2017,
                         Bonn: Gesellschaft für Informatik, 1185–1196 
                        
                     https://doi.org/10.18420/in2017_120
                   [letzer Zugriff 1.10.2018].
                    
               
                  Dipper, Stefanie (2005):
                  XML-based Stand-off Representation and Exploitation of Multi-Level Linguistic Annotation, 
                        in: Proceedings of Berliner XML Tage 2005 39–50.
                    
               
                  Grube, Nikolai / Prager, Christian / Diederichs, Katja / Gronemeyer, Sven / Grothe, Antje / Tamignaux, Céline / Wagner, Elisabeth / Brodhun, Maximilian / Diehr, Franziska (2018):
                  Arbeitsbericht 2017, 
                        in: Textdatenbank und Wörterbuch des Klassischen Maya, Arbeitsstelle der Nordrhein-Westfälischen Akademie der Wissenschaften und der Künste an der Rheinischen Friedrich-Wilhelms-Universität Bonn 
                        
                     http://dx.doi.org/10.20376/IDIOM-23665556.18.pr005.de
                   [letzter Zugriff: 05.01.2019].
                    
               
                  Ide, Nancy / Romary, Laurent (2007):
                  Towards International Standards for Language Resources, 
                        in: 
                        Dybkjaer, Laila / Hemsen, Holmer / Minker, Wolfgang (eds.):
                  Evaluation of Text and Speech Systems, 
                        Springer 263–284 
                        
                     https://doi.org/10.1007/978-1-4020-5817-2_9
                   [letzter Zugriff: 05.01.2019].
                    
               
                  Macri, Martha J. / Looper, Matthew G. (2003):
                  The New Catalog of Maya Hieroglyphs: The Classic Period Inscriptions, 
                        in: The Civilization of the American Indian Series 247.  Norman, OK: University of Oklahoma Press.
                    
               
                  Neuroth Heike / Rapp,  Andrea / Söring, Sibylle (2015):
                  TextGrid: Von der Community – für die Community. Eine Virtuelle Forschungsumgebung für die Geisteswissenschaften,
                        Göttingen: Universitätsverlag Göttingen 
                        
                     https://doi.org/10.3249/webdoc-3947
                    [letzer Zugriff 1.10.2018].
                    
               
                  Pallan Gayol, Carlos / Anderson, Deborah (2018):
                  Achieving Machine-Readable Mayan Text via Unicode: Blending “Old World” Script-encoding with Novel Digital Approaches, 
                        in: 
                        Ortega, Élika / Worthey, Glen / Galina, Isabel / Priani, Ernesto (eds.):
                  Book of Abstracts Digital Humanities 2018, 
                        Puentes-Bridges 256–261.
                    
               
                  Rehm, Georg / Schonefeld, Oliver / Trippel, Thorsten / Witt, Andreas (2010):
                  Sustainability of linguistic resources revisited, 
                        in: Proceedings of the International Symposium on XML for the Long Haul: Issues in the Long-term Preservation of XML. Balisage Series on Markup Technologies 6 
                        
                     https://doi.org/10.4242/BalisageVol6.Witt01
                   [letzter Zugriff: 05.01.2019].
                    
               
                  Rossi, Irene / De Santis, Annamaria (2019):
                  Crossing Experiences in Digital Epigraphy: From Practice to Discipline,
                        Berlin: De Gruyter.
                    
               
                  Spadini, Elena / Turska, Magdalena / Broughton, Misha (2015):
                  TEI Standoff Markup - A work in progress, 
                        in: TEI Members Meeting 2015 
                        
                     urn:nbn:nl:ui:17-f4d0afe1-5c62-4999-8271-7e8cadcd4805
                   [letzter Zugriff: 05.01.2019].
                    
               
                  Textdatenbank und Wörterbuch des Klassischen Maya (2017): Ontology of the Sign Catalogue for Classic Mayan
                  
                     https://classicmayan.org/documentations/idiomschema.html
                   [letzter Zugriff 1.10.2018].  
                    
               
                  Thompson, J. Eric S. (1962):    
                        A Catalog of Maya Hieroglyphs, 
                        in: The Civilization of the American Indian Series 62. Norman, OK.: University of Oklahoma Press.
                    
               
                  Thompson, Henry S. / McKelvie, David (1997):
                  Hyperlink semantics for standoff markup of read-only documents, 
                        in: Proceedings of SGML Europe.
                    
               
                  Wichmann, Søren (2006):
                  Mayan Historical Linguistics and Epigraphy: A New Synthesis, 
                        in: Annual Review of Anthropology 35: 279-294.
                    
            
         
      
   



      
         Seit der „Geburt des Zeitzeugen“ (Sabrow/Frei 2012) sind in Deutschland und Europa Hunderte von Oral History-Sammlungen entstanden. Zu den thematischen Schwerpunkten dieser „Era of the Witness“ (Wieviorka 2006) zählten neben der Zeit des Nationalsozialismus auch die Erfahrungen von DDR-Bürger/innen sowie die Geschlechter-, Migrations- und Minderheitengeschichte (Klingenböck 2009, Leh 2015). Vor allem seit den 1980er Jahren entstanden neben groß angelegten Dokumentationsprojekten mit jeweils Hunderten von Interviews auch zahlreiche kleine Sammlungen im Bereich von Geschichtswerkstätten, Museen und Gedenkstätten.
         Audio-visuell aufgezeichnete lebensgeschichtliche Interviews sind zu einer wichtigen Quelle der Geschichtswissenschaft und ihrer Nachbardisziplinen geworden (Andresen/Apel/Heinsohn 2015). Allerdings ist der Status Quo der Sicherung, Erschließung und Bereitstellung von Oral History-Sammlungen an den verschiedenen Einrichtungen noch unzureichend. 
         Andererseits sind mit der raschen Entwicklung der Video- und Webtechnologie seit der Jahrtausendwende große digitale Oral History-Archive entstanden, die neue Analysemöglichkeiten bieten (Apostolopoulos/Barricelli/Koch 2016). Einige der am besten erschlossenen Sammlungen stehen am Center für Digitale Systeme der Freien Universität Berlin bereit: Das „Visual History Archive“ der USC Shoah Foundation umfasst über 53.000 Interviews, von denen CeDiS 950 Interviews transkribiert hat (
                www.vha.fu-berlin.de, 
                www.zeugendershoah.de). Die 590 Interviews von „Zwangsarbeit 1939-1945“ wurden in einem spezialisierten Backend mit Workflow-Management wissenschaftlich erschlossen und in einem mehrsprachigen Online-Archiv mit timecodierten Transkripten, facettierter Suche, interaktiver Karte und Notizfunktion bereitgestellt (
                www.zwangsarbeit-archiv.de, Apostolopoulos/Pagenstecher 2013, Pagenstecher 2018b). Das über 90 Interviews umfassende Projekt „Erinnerungen an die Okkupation in Griechenland“ setzt den gesamten Prozess von der Interviewführung bis zur Online-Bereitstellung um (
                www.occupation-memories.org, Droumpouki 2016). Auch die 150 Video-Interviews der britischen Sammlung „Refugee Voices“ und die 4.500 Interviews des „Fortunoff Archives“ der Yale University stehen bereit. 
            
         In Vorbereitung sind Sammlungen zur deutsch-chilenischen sowie zur DDR-Geschichte sowie ein sammlungsübergreifender Katalog von Zeitzeugeninterviews. In Zusammenarbeit mit Sammlungsinhabern wie dem Archiv „Deutsches Gedächtnis“ an der FernUniversität Hagen und linguistischen Experten wie dem Bayerischen Archiv für Sprachsignale an der LMU München werden darüber hinaus zukunftsträchtige Wege der digitalen Archivierung, Aufbereitung und Analyse von Oral History-Interviews gesucht.
         
            Herausforderungen der digitalen Erschließung
            Digitale Interview-Archive wie „Zwangsarbeit 1939-1945“ stellen einen ersten Schritt der Oral History in Richtung Digital Humanities dar, aber noch nicht mehr: Die Archive konnten nur mit hohem manuellen Aufwand erstellt werden. Sie sind Einzelprojekte mit unterschiedlichen Erschließungssystemen, was eine sammlungsübergreifende Recherche erschwert. Für digitale Analysen sind die Daten noch unzureichend aufbereitet. Zum Schutz von Urheber- und Persönlichkeitsrechten unterliegen die Bestände unterschiedlichen Zugangsregelungen. 
            Damit sind einige der Herausforderungen angesprochen, die mit der digitalen Kuratierung von Oral History-Interviews verbunden sind: Spracherkennung, Alignment, Strukturierung, Interoperabilität, Forschungsethik. Die Digital Humanities können hier Lösungswege oder Anregungen anbieten. Gleichzeitig werfen die audiovisuellen, biografisch-narrativen Daten hier besondere technologische, methodische und ethische Fragen auf.
            Die audiovisuellen Medien bilden den Kern der Oral History; für Recherche, Analyse und Publikation sind aber textgebundene, timecodierte Transkriptionen von zentraler Bedeutung. Die automatische Spracherkennung hat in den letzten Jahren erhebliche Fortschritte gemacht, liefert aber für die oft dialektal und in mäßiger Aufnahmequalität vorliegenden Zeitzeugen-Interviews heute noch keine Transkripte in lesefähiger Qualität. Immerhin kann sie aber die Volltextsuche in nicht transkribierten Interviews unterstützen (Stanislav/Švec/Ircing 2016). 
            Um die Transkripte mit den mehrstündigen Audio- oder Videoaufnahmen zu koppeln, müssen Timecodes in die Texte eingefügt werden. Erst diese Segmentierung erlaubt eine Volltextsuche in der Audiodatei und eine synchrone Untertiteldarstellung. Verschiedene Programme unterstützen eine manuelle Transkription und Segmentierung, die aber zeitaufwändig ist. Die in der Linguistik genutzten Werkzeuge wie ELAN sind den meisten Oral Historians zu komplex, so dass in der Transkriptionspraxis meistens unstrukturierte Textdokumente erstellt werden. Erst jüngst sind automatische Alignment-Werkzeuge wie WebMAUS (
                    https://clarin.phonetik.uni-muenchen.de/BASWebServices
               ) so leistungsfähig geworden, dass auch mehrstündige Oral History-Interviews damit bearbeitet werden können. Ein nutzerfreundliches Transkriptionsportal für die Oral History ist in Vorbereitung (
                    https://www.phonetik.uni-muenchen.de/apps/oh-portal/). 
                
            Bisher werden Interviews nach unterschiedlichen Richtlinien und Methoden transkribiert und indexiert. Anzustreben ist dagegen eine sammlungsübergreifend standardisierte, maschinenlesbare Auszeichnung der Interviews und ihrer timecodierten Transkripte, die oft auch weitere Auszeichnungselemente, z. B. Sprecherwechsel oder Ortsnamen, enthalten. Damit über die reine Textsuche hinaus auch strukturnutzende Suchverfahren möglich sind, müssen die Transkriptionen mit diesen verschiedenen Annotations-Ebenen strukturiert abgebildet werden. Dafür empfiehlt sich ein auf den TEI-Guidelines der Text Encoding Initiative (
                    http://www.tei-c.org/release/doc/tei-p5-doc/de/html/TS.html) basierendes Schema, das derzeit für die an der FU Berlin erschlossenen Interviewsammlungen erarbeitet wird. Aus linguistischer Sicht wurde dafür der ISO-Standard 24624:2016 entwickelt (Schmidt 2011). 
                
            Eine interoperable Erschließung wird erschwert durch die unterschiedlichen Communities, in denen Oral History-Bestände beheimatet sind. Je nach Anbindung der Interview-Sammlungen an ein Archiv, eine Bibliothek, ein Museum oder ein sprachwissenschaftliches Zentrum kommen unterschiedliche Metadatenstandards zur Verwendung. In der Archivwelt ist EAD verbreitet, die angelsächsischen Bibliotheken nutzen MARC21, CLARIN setzt auf das CMDI-Framework. Übergreifende Crosswalks und Discovery-Systeme fehlen, was eine sammlungsübergreifende Recherche erschwert.
            Schließlich sind die Persönlichkeitsrechte der Interviewten besonders zu beachten. Angesichts der kollaborativen Produktion sensibler Daten im Interviewprozess hat die Oral History-Community schon früh über forschungsethische Verantwortung diskutiert (Leh 2000). Für die digitale Bereitstellung und Analyse von Interviews ist daher große Sensibilität und ein abgestuftes Rechtemanagement erforderlich, oft auch eine Anonymisierung der Interviews. Dies stellt – zusammen mit Fragen der Langzeitarchivierung und persistenten Auffindbarkeit von Audio- und Videodateien – eine weitere Herausforderung dar. 
         
         
            Forschungsfragen und Analysemöglichkeiten
            Während sich die Linguistik verstärkt für die umfangreichen Datenkorpora der Oral History interessiert (Kasten/Roller/Wilbur 2017, Armaselu/Danescu/Klein 2018), bleiben viele Historiker/innen skeptisch gegenüber quantifizierenden, womöglich dekontextualisierenden Analysemethoden. Hier dominieren hermeneutische und textbasierte Zugänge in der Auswertung weniger, oft selbst geführter Interviews anhand der Transkriptionen. Nun aber erleichtern die digitalen Interview-Archive die Sekundäranalyse vorhandener Interviews unmittelbar anhand der Ton- und Videoaufnahmen. Damit entstehen neue Forschungsfragen, einerseits in Bezug auf Multimodalität und Interaktion in der Gesprächssituation, andererseits auf komparative und korpuslinguistische Auswertungsmöglichkeiten.
            Eine Fallstudie untersuchte Erzählmuster und Ausdrucksweisen, Intonation und Mimik in zwei Interviews aus dem „Visual History Archive“ und dem Online-Archiv „Zwangsarbeit 1939-1945“. In den beiden Aufnahmen von 1998 und 2006 berichtet die gleiche Zeitzeugin über ihr Leben: Anita Lasker-Wallfisch, britische Cellistin, Holocaust-Überlebende und Breslauer Jüdin. Im Vergleich zeigt das spätere Interview einen klareren Anspruch auf epistemische Autorität sowie eine gewachsene narrative Erfahrung und performative Leistung (Pagenstecher 2018a). Dabei fällt besonders die nonverbale Interaktion auf: Gerade in kritischen Momenten schmiedet die Erzählerin eine visuell-argumentative Allianz mit dem Interviewer. Diese narrativen Muster blieben unbemerkt bei einer konventionellen Interview-Analyse anhand des Transkripts. Digitale Methoden helfen auch, die im Zentrum jedes Interviews stehende Arbeitsallianz zwischen Erzähler/in und Interviewer/in besser zu verstehen. Dieser mehr oder weniger versteckte Dialog ist zwar aus Sicht der praktischen Interviewführung vielfach empfehlend beschrieben, gelegentlich auch selbstkritisch reflektiert, aber noch kaum vergleichend analysiert worden (Pagenstecher/Pfänder 2017).
            Quantitative Ansätze können helfen, spezifische Erinnerungs- und Erzählmuster in großen Interviewsammlungen zu erkennen. Als Pilotstudie wird hier die Nutzung von Begriffen wie „Sklaverei“, „Sklavenarbeit“ oder „versklavt“ in den Interviews untersucht. Da die Daten für korpuslinguistische Tools noch nicht ausreichend exportierbar sind, werden hier nur die Analysemöglichkeiten des Online-Archivs „Zwangsarbeit 1939-1945“ genutzt. Seit den Nürnberger Prozessen wurde die nationalsozialistische Zwangsarbeit immer wieder in den historischen Kontext der Sklaverei gestellt. Im Laufe der Zeit bekam der Begriff „Sklavenarbeit“ in den verschiedenen Öffentlichkeiten der betroffenen Länder sehr unterschiedliche Konnotationen (vgl. Pagenstecher 2010). 
            Wie aber sprechen ehemaligen Zwangsarbeiter/innen 2005/2006, also kurz nach der Entschädigungsdebatte, geführten Interviews über ihre Erfahrung der „Sklavenarbeit“? Eine Volltextsuche nach dem Wortteil „*sklav*“ liefert Treffer in 140 von 477 Interviews. Der insgesamt in 29% aller Interviews auftauchende Sklaven-Begriff wird besonders häufig in italienischen (78%) und englischen (66%) Interviews verwendet. Dem entspricht die Verteilung auf Erfahrungsgruppen: Italienische Militärinternierte (60%) und jüdische Überlebende (42%) nutzen den Terminus häufiger, Religiös Verfolgte (14%) sowie Sinti und Roma (16%) seltener als der Durchschnitt. 
            Entscheidend ist dabei offensichtlich weniger die gruppenspezifische Erfahrung als der landesspezifische Erinnerungsdiskurs. So sprechen jüdische Überlebende in Israel oder Osteuropa seltener über Sklaventum als die im englischen Sprachraum Interviewten. Dass in angelsächsischen Ländern mehr über Sklaven gesprochen wird, liegt vermutlich an der dem Interviewprojekt vorausgehenden, längeren öffentlichen Debatte über die Zwangsarbeiter-Entschädigung, die vor allem dort stark vom Terminus Sklavenarbeit geprägt war. Typisch dafür war etwa die Schlagzeile „Nazi slaves take case to US“ (BBC 1999). Offenbar reagieren die Interviewten auf diese Diskurse, teilweise auch direkt auf sprachliche Vorgaben der Interviewer/innen. 
            Die Art der Zwangsarbeit spielt dagegen eine geringere Rolle; allerdings wird Sklaventum bei Erfahrungen im gemeinhin besonders schweren Einsatzbereich Bau/Steine/Erden (40%) häufiger erwähnt. Hier ist also eine genauere Untersuchung der Verwendungskontexte erforderlich. Zu prüfen ist beispielsweise, ob mit der „Sklaven“-Referenz eher eine damalige (Arbeits-, Gewalt- oder Diskriminierungs-)Erfahrung wiedergegeben oder eher aus heutiger (biografischer oder politischer) Sicht über eigene Erfahrungen reflektiert wird. Für solche Fragestellungen reicht der von der Korpus-Software angebotene Kontext von einigen Wörtern links und rechts der Fundstelle nicht aus. Hier ist ein Close Reading oder Viewing des Interviews erforderlich. 
         
         
            Resümee
            Anhand der digitalen Interview-Sammlungen an der Freien Universität Berlin skizzierte dieser Beitrag die Potentiale und Herausforderungen des Zusammenwirkens von Digital Humanities und Oral History in der Kuratierung und Analyse. 
            Digitale Technologien ermöglichen die softwaregestützte Sicherung, Erschließung und Bereitstellung von Zeitzeugen-Interviews sowie ihre sammlungsübergreifende Recherche und quellennahe Analyse. In Zukunft können auch quantitative Analysen genutzt werden, um individuelle und kollektive Muster des Erfahren, Erinnerns und Erzählens zu entdecken.
            Gewiss verliert das digital aufbereitete Zeugnis im Zeitalter der technischen Reproduzierbarkeit ein Stück seiner Aura. Seiner fundierten Analyse und sorgsamen, kontextualisierenden Interpretation sollte dies freilich keinen Abbruch tun. Die Digital Humanities eröffnen der Oral History jedenfalls faszinierende neue Forschungsperspektiven.
         
      
      
         
            
               Bibliographie
               
                  Andresen, Knud / Apel, Linde / Heinsohn, Kirsten (eds.) (2015): 
                  Es gilt das gesprochene Wort. Oral History und Zeitgeschichte heute, 
                        Göttingen.
                    
               
                  Apostolopoulos, Nicolas / Pagenstecher, Cord (eds.) (2013): 
                  Erinnern an Zwangsarbeit. Zeitzeugen-Interviews in der digitalen Welt, 
                        Berlin.
                    
               
                  Apostolopoulos, Nicolas / Barricelli, Michele / Koch, Gertrud (eds.) (2016): 
                  Preserving Survivors' Memories. Digital Testimony Collections about Nazi Persecution: History, Education and Media, 
                        Berlin: Stiftung „Erinnerung, Verantwortung und Zukunft“ (EVZ), URL:
                        http://www.stiftung-evz.de/fileadmin/user_upload/EVZ_Uploads/Handlungsfelder/Auseinandersetzung_mit_der_Geschichte_01/Bildungsarbeit-mit-Zeugnissen/Testimonies_Band3_Web.pdf [zuletzt abgerufen: 10. Januar 2019]
               
               
                  Armaselu, Florentina / Danescu, Elena / Klein, Francois (2018):
                  Oral History and Linguistic Analysis. A Study in Digital and Contemporary European History,
                        in: CLARIN Annual Conference 2018 Proceedings: 11-15, URL:
                        https://office.clarin.eu/v/CE-2018-1292-CLARIN2018_ConferenceProceedings.pdf [zuletzt abgerufen: 10. Januar 2019]
                    
               
                  BBC (1999):
                  Nazi slaves take case to US, 
                        12.10.1999, URL:
                        http://news.bbc.co.uk/2/hi/europe/472104.stm [zuletzt abgerufen: 10. Januar 2019]
               
               
                  Droumpouki, Anna Maria (2016):
                  Erinnerungen an die Okkupation in Griechenland. Entstehung, Entwicklung und gesellschaftliche Bedeutung eines deutsch-griechischen Dokumentationsprojekts,
                        in: BIOS. Zeitschrift für Biographieforschung und Oral History, 29/1: 141-151. 
                        https://doi.org/10.3224/bios.v29i1.09 [zuletzt abgerufen: 10. Januar 2019]
               
               
                  Kasten, Erich / Roller, Katja / Wilbur, Joshua (eds.) (2017):
                  Oral History Meets Linguistics, 
                        Fürstenberg: SEC, 185-207, URL: 
                        http://www.siberian-studies.org/publications/orhili_E.html
                        [zuletzt abgerufen: 10. Januar 2019]
                    
               
                  Klingenböck, Gerda (2009): 
                  Stimmen aus der Vergangenheit. Interviews von Überlebenden des Nationalsozialismus in systematischen Sammlungen von 1945 bis heute, 
                        in: 
                        Daniel Baranowski (eds.): 
                  „Ich bin die Stimme der sechs Millionen“. Das Videoarchiv im Ort der Information,
                        Berlin: Stiftung Denkmal für die ermordeten Juden Europas, 27-40
                    
               
                  
                  
                  Leh, Almut (2000): 
                  Forschungsethische Probleme in der Zeitzeugenforschung, 
                        in: BIOS. Zeitschrift für Biographieforschung und Oral History, 13: 64-76
                    
               
                  Leh, Almut (2015):
                  Vierzig Jahre Oral History in Deutschland. Betrag zu einer Gegenwartsdiagnose von Zeitzeugenarchiven am Beispiel des Archivs ‚Deutsches Gedächtnis‘, 
                        in: Westfälische Forschungen. Zeitschrift des LWL-Instituts für westfälische Regionalgeschichte, 65: 255-268
                    
               
                  Pagenstecher, Cord (2010):
                  ‘We were treated like slaves.‘ Remembering forced labor for Nazi Germany, 
                        in: 
                        Gesa Mackenthun, Raphael Hörmann (eds.), 
                        Human Bondage in the Cultural Contact Zone. Transdisciplinary Perspectives on Slavery and Its Discourses, 
                        Münster: Waxmann 275-291.
                    
               
                  Pagenstecher, Cord / Pfänder, Stefan (2017):
                  Hidden dialogues. Towards an interactional understanding of Oral History interviews, 
                        in: 
                        Kasten, Erich / Roller, Katja / Wilbur, Joshua (eds.): 
                  Oral History Meets Linguistics, 
                        Fürstenberg: SEC: 185-207
                    
               
                  Pagenstecher, Cord (2018a): 
                  Testimonies in digital environments: comparing and (de-)contextualising interviews with Holocaust survivor Anita Lasker-Wallfisch, 
                        in: Oral History Journal, 46 (2): 109-118
                    
               
                  Pagenstecher, Cord (2018b): 
                  Curating and Analyzing Oral History Collections, 
                        in: CLARIN Annual Conference 2018 Proceedings , ed. by Inguna Skadin and Maria Eskevich: 34-38, URL: 
                        https://office.clarin.eu/v/CE-2018-1292-CLARIN2018_ConferenceProceedings.pdf
                  [zuletzt abgerufen: 10. Januar 2019]
               
               
                  Sabrow, Martin / Frei, Norbert (eds.) (2012): 
                  Die Geburt des Zeitzeugen nach 1945,
                        Göttingen: Wallstein
                    
               
                  Schmidt, Thomas (2011):
                  A TEI-based Approach to Standardising Spoken Language Transcription, 
                        in: Journal of the Text Encoding Initiative, 1, DOI:10.4000/jtei.142.
                    
               
                  Stanislav, Petr / Švec, Jan / Ircing, Pavel (2016):
                  An Engine for Online Video Search in Large Archives of the Holocaust Testimonies, 
                        in: Interspeech 2016: Show & Tell Contribution, 
                        https://www.isca-speech.org/archive/Interspeech_2016/pdfs/2016.PDF
                  [zuletzt abgerufen: 10. Januar 2019]
               
               
                  Wieviorka, Annette (2006): 
                  The Era of the Witness, 
                        New York.
                    
            
         
      
   



      
         
            Der Webservice correspSearch
            1
             wird an der Berlin-Brandenburgischen Akademie der Wissenschaften entwickelt, um digitale und gedruckte Briefeditionen editionsübergreifend zu vernetzen und durchsuchbar zu machen. Darüber hinaus sollen die Briefmetadaten mit correspSearch für neue digitale Forschungsmethoden (wie z.B. aus der Historischen Netzwerkforschung) bereitgestellt werden. Mittlerweile sind über 44.000 edierte Briefe in correspSearch nachgewiesen.
         
         
            Ein wesentlicher Bestandteil von correspSearch ist die Möglichkeit, den Datenbestand mit Metadaten zu eigenen - digitalen oder gedruckten - Briefeditionen zu erweitern. Durch die damit einhergehende Standardisierung der Metadaten auf der Grundlage  der Richtlinien der Text Encoding Initiative (TEI) werden Editionen für die digitale Vernetzung erschlossen. Je mehr Briefmetadaten bereitgestellt und  aggregiert werden, desto größer ist der Nutzen dieser Daten für die Recherche und Forschung. Mit dem CMIF Creator erfasste Daten werden im „Correspondence Metadata Interchange Format” (CMIF) kodiert, das von der TEI Correspondence Special Interest Group entwickelt und gepflegt wird. Anschließend werden die TEI XML-Daten auf Anbieterseite online verfügbar gemacht und vom Webservice per URL abgerufen. Die Metadaten der Briefeditionen werden somit dezentral aggregiert und erfordern nicht den Betrieb einer zentralen Speicherlösung. Der Webservice correspSearch, sowie das zugrundeliegende CMIF und das TEI-Elementset correspDesc wurden 2018 mit dem 
            Rahtz Prize for TEI Ingenuity
            2
             ausgezeichnet.
         
         
            
            
               Abbildung 1. Oberfläche des CMIF Creator, Schritt 1: Metadaten.
				
         
         
            
            
               Abbildung 2. Listenansicht der Briefdatensätze.
				
         
         
            War es bis vor kurzem nötig, das CMIF manuell zu kodieren, bietet der 
            CMIF Creator
            3
             nun eine benutzerfreundliche grafische Oberfläche, um eigene Brief-Metadaten in das nötige Format zu übertragen.
            4
             Der mittlerweile in seiner zweiten Version verfügbare CMIF Creator bietet eine einfach zugängliche Möglichkeit, online und browserbasiert Briefeditionen in TEI-konforme CMIF-Dateien zu übertragen. Diese können dann in das Datenbanksystem von correspSearch überführt werden. Durch die browserbasierte Anwendung, die nicht auf serverseitige Speicherroutinen angewiesen ist, können Nutzer*innen die gesamte Verarbeitung lokal ausführen. Das bedeutet, dass die Nutzer*innen jederzeit vollständige Kontrolle über ihre Daten haben, da diese ausschließlich lokal gespeichert werden. Das Zwischenspeichern und Laden begonnener CMIF-Dateien ermöglicht weiterhin eine Bearbeitung über einen längeren Zeitraum. Der CMIF Creator unterstützt die Abfrage von Normdaten der 
            Gemeinsamen Normdatei
             (GND) der Deutschen Nationalbibliothek
            5
             über die von 
            lobid
             bereitgestellte API
            6
            , sowie das Abrufen von ortsbezogenen Normdaten aus der Datenbank des Ortsnamendienstes 
            GeoNames
            7
            , und ermöglicht eine in das User Interface integrierte Auswahl und Zuweisung von Normdaten zu Korrespondent*innen und Orten. Die Unterstützung von Normdaten ist essentiell, da correspSearch  für die Auflösung der Ambiguitäten von Personen- und Ortsnamen wesentlich auf die Nutzung von eindeutigen Normdaten-IDs zurückgreift. Die Oberfläche des Editors ist bewusst einfach und gleichzeitig so flexibel wie möglich gestaltet, um das CMIF Format im Frontend möglichst gut abzubilden und auch für weniger technisch versierte Nutzer*innen verfügbar zu machen. 
         
         
            
            
               Abbildung 3. Listenauswahl von Normdaten.
				
         
         
            In den letzten Jahren hat sich JavaScript durch die breite Verwendung von hochentwickelten JavaScript-Frameworks zu einer der zentralen Skriptsprachen im Bereich des Frontend-Development entwickelt. Das zugrundeliegende Datenformat JSON bietet seinerseits eine bewährte und einfache Möglichkeit der Datenspeicherung im Frontend. Da XML-Frameworks oder -Bibliotheken für einen browserbasierten Editor dieses Umfangs als weitestgehend veraltet
            8
             und unzureichend performant evaluiert wurden, erfolgte die Entscheidung schließlich zugunsten einer JavaScript- und JSON-basierten Anwendung. Durch die Crossplatform-Tauglichkeit von JavaScript bietet sich in der Perspektive darüber hinaus großer Spielraum für die Weiterentwicklung des CMIF Creators analog zum Format und den Bedürfnissen der Nutzer*innen.
         
         
            
            
               Abbildung 4. Ansicht eines Briefdatensatzes.
				
         
         
            Während der Arbeit im Editor verbleiben die Daten im JSON-Format und ermöglichen so eine dynamische Nutzung im Kontext des dem Editor zugrundeliegenden reaktiven JavaScript-Frameworks 
            vue.js
            9
            . Durch die Verwendung von 
            vue.js 
            in Verbindung mit dem CSS-Framework 
            Bootstrap
            10
             wird jede Eingabe und Änderung der Nutzer*innen unmittelbar im JSON-Datenmodell abgebildet. Beim Speicherungsvorgang werden die JSON-Daten über die correspSearch-eigene API in valides XML serialisiert, das dann später von correspSearch abgerufen werden kann. Im Falle von invaliden Eingaben enthält der CMIF Creator interne Validierungsfunktionen, die sowohl nach der Eingabe in einzelnen Feldern, als auch vor dem abschließenden Speichervorgang, die Eingaben auf technische Korrektheit überprüft. Finden sich Fehler, so werden diese nicht nur lokalisiert, sondern auch verlinkt, sodass Nutzer*innen direkt zum entsprechenden Eintrag springen können. Beim Laden von validen XML-Dateien in den Editor findet die Umwandlung in JSON ebenfalls über die correspSearch-API statt. Der CMIF Creator wird als OpenSource-Software entwickelt und unter der GNU LGPL lizensiert. Die Quellen sind auf der GitHub-Seite von correspSearch
            11
             frei zugänglich.
         
         
            
            
               Abbildung 5. Bei fehlerhaften Einträgen kann die CMIF-Datei nur als JSON-Entwurf gespeichert werden.
				
         
         
            
            
               Abbildung 6. Gibt es keine Fehler, können die Daten als valide .xml-Datei gespeichert werden.
				
         
      
      
         
            
               https://correspsearch.net/
            
            
               http://www.bbaw.de/presse/pressemitteilungen/pressemitteilungen-2018/Rahtz-Prize-for-TEI-Ingenuity-2018
            
            
               https://correspsearch.net/creator/index.xql
            
             Der CMIF Creator basiert auf der schon in Andert et al. 2015 geäußerten Annahme, dass die Eingabe von Briefmetadaten möglichst effizient gestaltet sein sollte. Im Gegensatz zu dem von Andert et al. skizzierten Tool arbeitet der CMIF Creator allerdings standardbasiert. Darüber hinaus überlässt der CMIF Creator die Daten in der Obhut ihrer Ersteller*innen, die jederzeit vollständige Einsicht in die und volle Kontrolle über die (lokal) gespeicherten XML-Daten haben.
            
               http://www.dnb.de/
            
            
               http://lobid.org/gnd/api
            
            
               http://www.geonames.org/
            
             So lief z.B. die Unterstützung für betterForm in existdb Anfang 2018 aus, siehe hierzu auch den Eintrag in der Mailingliste https://sourceforge.net/p/exist/mailman/message/36239166/ bzw. den entsprechenden Issue https://github.com/eXist-db/exist/pull/1736
            
            
               https://vuejs.org/
            
            
               https://bootstrap-vue.js.org/
            
            
               https://github.com/correspSearch
            
         
         
            
               Bibliographie
               
                  Andert, Martin / Frank Berger / Paul Molitor / Jörg Ritter (2015):
                  An Optimized Platform for Capturing Metadata of Historical Correspondence,
                        Literary and Linguistic Computing Advance Access 30 (4): 471–480.
                        
                     https://doi.org/10.1093/llc/fqu027.
                  
               
               
                  Dumont, Stefan (2016):
                  CorrespSearch – Connecting Scholarly Editions of Letters,
                        Journal of the Text Encoding Initiative, Nr. Issue 10 (Dezember).
                        
                     https://doi.org/10.4000/jtei.1742.
                  
               
               
                  Stadler, Peter (2012):
                  Normdateien in der Edition,
                        editio 26: 174–83.
                    
               
                  TEI Consortium, Hrsg. o. J. 
                  TEI P5: Guidelines for Electronic Text Encoding and Interchange. Version 3.4.0 vom 23.07.2018.
                        TEI Consortium.
                        
                     http://www.tei-c.org/Guidelines/P5/.
                  [zuletzt abgerufen: 12. Oktober 2018]
                    
               
                  TEI Correspondence SIG, Hrsg. (2015):
                  Correspondence Metadata Interchange Format (CMIF),
                        
                     https://github.com/TEI-Correspondence-SIG/CMIF.
                  
               
            
         
      
   



      
         In Kooperation der Universität Bern und des 
                Cologne Center for eHumanities CCeH entsteht seit August 2016 die Editions- und Forschungsplattform 
                hallerNet, die im April 2019 veröffentlicht werden soll. Eine Spezialität von 
                hallerNet sind die systematischen Verknüpfungen zwischen grossen Quellenkorpora und umfangreichen Metadaten, woraus sich ein grosses Analysepotenzial ergibt, das sich z.B. in explorativen Visualisierungen nutzen lässt.
            
         
            Metadaten aus drei Jahrzehnten
            In 
                    hallerNet integriert werden die umfangreichen prosopografischen und bibliografischen Strukturdaten, die in Form einer relationalen Verbunddatenbank seit anfangs der 1990er-Jahre im Rahmen der beiden SNF-Projekte zum Universalgelehrten Albrecht von Haller (1708-1777)1 und zur aufgeklärten Reformsozietät 
                    Oekonomische Gesellschaft Bern (gegr. 1759)2 erhoben worden waren. Ein mehrjähriges Transformationsprojekt (Kreditvolumen 1.18 Mio CHF) überführt diese untereinander vielfältig verknüpften Daten (vgl. Abb. 1) zu rund 48'000 Publikationen, 25'000 Personen, 20'000 Briefen, 3'000 Orten, 2'900 Pflanzenarten, 1'000 Versammlungen und 850 Institutionen in eine TEI-konforme XML-Datenstruktur (Recker-Hamm / Stuber 2015). Bemerkenswert ist dabei die grosse Tiefe der Metadatenerschliessung, die beispielsweise nicht nur biografische Eckdaten wie Geburts-/Sterbedatum und -ort oder die Hauptbeschäftigung umfasst, sondern die Ausbildungsstationen, Reiseziele, Ämter und Mitgliedschaften usw. einbezieht und auch das Beziehungsnetz (Verwandte, Briefpartner) soweit möglich erfasst.3 Während die prosopografische Erhebung weitgehend auf biografischen Werken sowie auf edierten Universitätsmatrikeln und Mitgliederlisten von Akademien und Sozietäten basiert, stützen sich die bibliografischen Daten in erster Linie auf zwei autoptisch erarbeitete Grundlagenwerke (Monti 1983-1994, Steinke / Profos 2004).
                
            
               
               
                  Abb. 1: Die rund 100'000 Metadatenobjekte sind vielfältig verknüpft, sowohl innerhalb des gleichen Entitätstyps als auch typenübergreifend.4
               
            
            Auf der Grundlage der entstehenden Plattform startete anfangs 2018 ein vom 
                    Schweizerischen Nationalfonds SNF unterstütztes Editions- und Forschungsprojekt, das im geplanten Projektzeitraum von sechs Jahren zum einen die Gesamtedition der rund 9'000 Buchbesprechungen Albrecht von Hallers erarbeitet. Zum anderen soll eine begründete Auswahl von inhaltlich damit zusammenhängenden rund 8'000 Briefen ediert werden, dies als Zwischenetappe zur längerfristig angestrebten Gesamtedition von Hallers Korrespondenz, die insgesamt rund 17'000 Briefe umfasst.5 In der systematischen Verknüpfung zweier komplementärer Quellenkorpora der privaten (und tw. im kleinen Kreis öffentlichen) Kommunikation (Briefe) und des öffentlichen Diskurses (Rezensionen) liegt eine erste Innovation der Plattform.6
            
         
         
            “Datenzentrierte” versus “textzentrierte” digitale Editionen
            Auf 
                    hallerNet werden die v.a. quantitativen und formalen Bezüge der älteren Forschungsdatenbank mit edierten Textinhalten in Verbindung gesetzt. Die über Jahrzehnte systematisch erhobenen und homogenisierten Metadaten erleichtern die Editionsarbeit erheblich, namentlich die Referenzierung von Entitäten wie Akteure, Orte, Publikationen und Institutionen. In dieser Verbindung tief erschlossener Strukturdaten mit edierten Textinhalten liegt die zweite Innovation der entstehenden Plattform. Bisher sind “datenzentrierte” digitale Editionen die Ausnahme gegenüber den vielen existierenden und sich in Entwicklung befindlichen “textzentrierten” digitalen Editionen. Wo es sie gibt, verfolgen sie zumeist ganz spezifische Forschungsfragen und legen dazu ein entsprechend verengtes Datenmodell zugrunde.7 Daneben existieren biografische, prosopografische oder bibliografische Datenbanken und Forschungsportale, die sich einzig auf Metadatenerhebung und -analyse konzentrieren.8 Und schließlich gibt es digitale Editionsportale wie das Nowa Panorama Literatury Polskiej,9 die relativ dichte biografische Daten erheben, diese aber nicht in strukturierter Form bereitstellen.
                
            Selbstredend lagen auch der Datenerfassung in den verschiedenen Projekten um Albrecht von Haller seit den 1990er-Jahren immer spezifische Forschungsinteressen zugrunde. So folgte auf eine erste Phase, welche die Gesamterschliessung von Hallers Korrespondenz (Boschung et al. 2002) mit einer umfassenden prosopografischen Strukturanalyse (Stuber et al. 2005) kombinierte, eine zweite Phase, in der Netzwerkanalysen im Vordergrund standen und die in den Blick genommenen Akteure über Hallers Korrespondenznetz hinaus erweitert wurden (Dauser et al. 2008, Stuber / Krempel 2013). Die dieser Forschung zugrundeliegenden Daten wurden von Anfang an in der gleichen Datenbankanwendung (FAUST) und mit möglichst viel struktureller Homogenität gepflegt, die Datenpublikation erfolgte allerdings fast ausschliesslich in aggregierter Form und auf traditionelle Weise (Druck). Im Zuge der Datentransformation wird durch weitere Homogenisierung und die semantische Auszeichnung nach TEI-Richtlinien eine weitergehende Öffnung der Daten angestrebt, so dass diese auch in anderen Kontexten nutzbar werden.10 Die Metadaten werden zu diesem Zweck über spezifische Formate wie das CMI-Format bzw. 
                    CorrespSearch zugänglich gemacht, daneben aber auch in vollem Umfang parallel im FAIR-Repositorium 
                    Zenodo veröffentlicht werden.11
            
         
         
            Referenzannotation im Zentrum
            Die Tatsache, dass die Briefe je nach Qualität der vorliegenden Grundlagen auf drei unterschiedliche Arten ediert werden – den Standards genügende Druckeditionen werden re-ediert, bei schlechteren Druckeditionen wird der Rohtext zur Arbeitserleichterung herangezogen, aber intensiv nachbearbeitet, und zuvor nicht edierte Dokumente werden ab dem Original neu ediert –, bedingte auch eine Reflektion über das Wesen der Fussnoten bzw. der Stellenkommentare beim Transfer vom Druck ins Digitale. Aus diesem Prozess resultierte ein Referenzsystem, das drei Annotationstypen vorsieht:
            
               Referenzannotationen: Referenzen auf Datenbankobjekte (``), deren lokales Auftreten in einer `` näher erläutert werden kann
               freie Annotationen zum Inhalt (``), d.h. inhaltliche Anmerkungen, die nicht in direktem Bezug zu einem bestimmten Datenbankobjekt stehen, sondern z.B. zu einem ganzen Absatz
               Annotationen zur Textkonstitution (``)
            
            Während für die letzten beiden Typen die digitale Umsetzung relativ nahe beim Druck bleibt (Anmerkungsziffer mit Tooltip bzw. Verweis zur Anmerkung), bieten die Referenzannotationen mehr Interaktivität und Zugangswege zu den Datenbankdaten. Wir erwarten, dass dieser Anmerkungstyp für künftige Editionen der 
                    hallerNet-Plattform noch an Bedeutung gewinnen wird. Es sind dann auch diese Referenzannotationen, die das Rückgrat der Entitätsauszeichnung bilden.
                
         
         
            Doppelter Zweck der systematischen Entitätsauszeichnung
            Innerhalb der Plattform verfolgt die Anbindung der digitalen Edition an die systematisierten Objekte der Datenbank einen doppelten Zweck: Zum einen macht sie die Quellentexte über plattformübergreifende Normdaten anschlussfähig zu anderen Ressourcen. Zum anderen lassen sich bei entsprechender Datenmodellierung aus solchen Referenzannotationen grosse Datenmengen für Netzwerkanalysen und räumliche Visualisierungen gewinnen. Diese Daten weisen erstens präzise Beziehungsqualitäten auf, so beispielsweise zum Verhältnis zwischen den Rezensenten und den Verfassern der rezensierten Publikationen. Zweitens sind die Nennungen der referenzierten Entitäten leicht den i.d.R. tagesdatierten Quellen Brief und Rezension zuzuordnen. Die generierten Forschungsdaten liefern damit Präzision sowohl in der raumzeitlichen Entwicklung als auch in der inhaltlichen Qualität der Netzbeziehungen und erlauben daher Antworten auf wichtige Postulate der aktuellen historischen Netzwerkforschung.12 Dies möchten wir an einigen konkreten Beispiel andeuten. Ausgangspunkt ist die auf 
                    hallerNet als Prototyp erstellte digitale Edition der Münchhausen-Haller-Korrespondenz von Otto Sonntag, die ein erweitertes Briefsample von rund 850 Briefen (inkl. Beilagen) umfasst (Sonntag 2019). Dabei werden die vorhandenen prosopografischen Forschungsdaten angereichert mit standardisierten Angaben über soziale Position und Aufenthaltsort sämtlicher in diesem Briefsample erwähnten rund 340 Personen (ca. 1'500 Nennungen) zum Zeitpunkt des entsprechenden Briefdatums. Damit wird diese Korrespondenz als zeitlich und sozial differenzierter Kommunikationsraum darstellbar. Diese Informationen lassen sich einerseits für einzelne edierte Dokumente nutzen, etwa um eine Brieftranskription um eine Akteursliste zu ergänzen, die auch die zum Briefdatum jeweils aktuelle Position der genannten Individuen aufführt. Sie sind aber andererseits vor allem auf aggregierter Ebene interessant, da sich mit verschiedenen Visualisierungsformen bestimmte Interaktionsmuster eruieren lassen.
                
            Die DHd-Konferenz fällt zeitlich direkt in die letzte Phase vor dem Launch der Plattform. Nachdem die Encodings der Münchhausen-Briefe in den letzten Monaten erstellt und mit den bestehenden und neuen Datenbankobjekten referenziert wurden, bietet sich bis dahin die Gelegenheit für erste analytische Auswertungen auf der neuen Datengrundlage.
            Den beschriebenen Visualisierungsformen kommen in vereinfachter Form auf 
                    hallerNet auch Navigations- und Filterfunktionen zu. Der Weg von der einzelnen Textstelle in der edierten Quelle zur Gesamtsicht in der Visualisierung ist also durchaus in beide Richtungen möglich.
                
         
      
      
         
            
               Albrecht von Haller und die Gelehrtenrepublik des 18. Jahrhunderts (1991–2003); Leitung: Urs Boschung (Institut für Medizingeschichte der Universität Bern); siehe allg. zu Albrecht von Haller: http://www.albrecht-von-haller.ch.
            
               Nützliche Wissenschaft, Naturaneignung und Politik. Die Oekonomische Gesellschaft Bern im europäischen Kontext (1750–1850); Leitung: André Holenstein, Christian Pfister (Historisches Institut der Universität Bern).
             Siehe zum alten Haller-/OeG-Datenbankverbund: Flückiger / Stuber 2009, Steinke 2003.
             Die Visualisierung zeigt die Verlinkung und die Anzahl der Referenzen zwischen sechs Entitätstypen. Die Daten wurden im Rahmen von Forney et al. (2018) berechnet und die Grafik mit d3-chord erstellt: https://github.com/d3/d3-chord.
             Online-Edition der Rezensionen und Briefe Albrecht von Hallers. Expertise und Kommunikation in der entstehenden Scientific community. Leitung: André Holenstein (Historisches Institut), Hubert Steinke (Institut für Medizingeschichte), Oliver Lubrich (Germanistisches Institut).
             Siehe zur Grundidee: Stuber 2004.
             Beispiele hierfür sind etwa die Projekte Proceedings of the Old Bailey (https://www.oldbaileyonline.org/), buckhardtsource.org (http://burckhardtsource.org/), Intoxicants and Early Modernity (https://www.intoxicantsproject.org/), Jahrrechnungen der Stadt Basel 1535 bis 1610 (https://gams.uni-graz.at/context:srbas), Urfehdebücher der Stadt Basel (https://gams.uni-graz.at/context:ufbas), Sound Toll Registers (http://soundtoll.nl) oder das leider dem digitalen Zerfall anheimgefallene Alcalá account book.
             Als Beispiele zur Gelehrtenwelt lassen sich hier Amore Scientiae Facti sunt Exules (http://asfe.unibo.it) oder das Repertorium Academicum Germanicum (http://rag-online.org) nennen. Bemerkenswert sind auch die der Symogih-Plattform (Système modulaire de gestion de l'information historique, http://symogih.org) angegeliederten Projekte, die teilweise auch edierte Texte umfassen.
             Nowa Panorama Literatury Polskiej / New Panorama of Polish Literature; http://nplp.pl und spezifischer http://tei.nplp.pl.
             Zur Transformation ins TEI-Format vgl. Forney et al. 2018.
             Diese Strategie verfolgt zwei Ziele: die langfristige Zugänglichkeit der Forschungsdaten, aber auch die Auffindbarkeit über spezialisierte Suchmaschinen wie DataCite oder die Google Dataset Search.
             Siehe eine kritische Diskussion der Möglichkeiten und Grenzen der Netzwerkanalyse aus literaturwissenschaftlicher Perspektive (Baillot 2018).
         
         
            
               Bibliographie
               
                  Baillot, Anne (2018):
                  Die Krux mit dem Netz. Verknüpfung und Visualisierung bei digitalen Briefeditionen, 
                        in: 
                        Bernhart, Toni et al. (eds.):
                  Quantitative Ansätze in den Literatur- und Geisteswissenschaften. Systematische und historische Perspektiven, 
                        Berlin/Boston: De Gruyter 355-370 .
                    
               
                  Boschung, Urs et al. (2002):
                  Repertorium zu Albrecht von Hallers Korrespondenz 
                        1724-1777. 2 Bde. Basel.
                    
               
                  Dauser, Regina et al. (2008):
                  Wissen im Netz. Botanik und Pflanzentransfer in europäischen Korrespondenznetzen des 18. Jahrhundert, 
                        Berlin.
                    
               
                  Flückiger, Daniel / Stuber, Martin (2009):
                  Vom System zum Akteur. Personenorientierte Datenbanken für Archiv und Forschung, 
                        in: 
                        Kirchhofer, André et al. (Hrsg.):
                  Nachhaltige Geschichte. Festschrift für Christian Pfister, 
                        Zürich, 253-269.
                    
               
                  Forney Christian et al. (2018):
                  Vom geschützt zugänglichen Datenbankverbund zur offenen Editions- und Forschungsplattform: kritischer Rückblick auf halber Strecke, 
                        DHd 2018 (Poster).
                    
               
                  Monti, Maria Teresa (1983-1994):
                  Catalogo del Fondo Haller della Biblioteca Nazionale Braidense di Milano, a cura di Maria Teresa Monti, 
                        13 Bde. Milano.
                    
               
                  Recker-Hamm, Ute / Stuber, Martin (2015):
                  Haller Online – Konzept für den Umbau, Ausbau und die langfristige Sicherung der Haller-/ OeG-Datenbank, 
                        25 Seiten (8.6.2015).
                    
               
                  Sonntag, Otto (2019):
                  The Albrecht von Haller-Gerlach Adolph von Münchhausen Correspondence, 
                        hallerNet (in Vorbereitung).
                    
               
                  Steinke, Hubert (2003):
                  Archive databases as advanced research tools: the Haller Project, 
                        in: 
                        Antonio Vallisneri :
                  L`edizione del testo scientifico d`età moderna, a cura di Maria Teresa Monti, 
                        Firenze 2003, 191-204.
                    
               
                  Steinke, Hubert / Profos, Claudia (2004):
                  Bibliographia Halleriana. Verzeichnis der Schriften von und über Albrecht von Haller, 
                        Basel.
                    
               
                  Stuber, Martin (2004):
                  Journal and letter. The interaction between two communication media in the correspondence of Albrecht von Haller, 
                        in: 
                        Lüsebrink, Hans-Jürgen / Popkin, Jeremy (eds.):
                  Enlightenment, Revolution and the periodical press (Studies on Voltaire and the Eighteenth Century), 
                        114-141.
                    
               
                  Stuber, Martin et al. (2005):
                  Hallers Netz. Ein europäischer Gelehrtenbriefwechsel zur Zeit der Aufklärung, 
                        Basel.
                    
               
                  Stuber, Martin / Krempel, Lothar (2013):
                  Las redes académicas de Albrecht von Haller y la Sociedad Económica: un análisis de redes a varios niveles, 
                        in: REDES. Revista hispana para el análisis. De redes sociales, 24/1 (2013), 1-26: https://doi.org/10.5565/rev/redes.450 / REDES Online English: The Scholarly Networks of Albrecht von Haller and the Economic Society of Bern – a Multi-Level Network Analysis: http://revista-redes.rediris.es/webredes/novedades/Stuber_Krempel_scholarly_networks.pdf.
                    
            
         
      
   



      
         
            Beschreibung
            
               
               Briefeditionen sind ein Typus der digitalen Edition, in dem die Vorteile des digitalen Mediums bereits mit am intensivsten fruchtbar gemacht werden.
               1
                
               In der alltäglichen Arbeit des Edierens sowie der Software-Entwicklung richtet sich der Blick zum großen Teil meist auf den einzelnen Brief und seine Tiefenerschließung, weniger auf die Menge an Briefen eines Korrespondenzkorpus. Weiterführende quantitative Analysen auf Basis der Tiefenerschließung (vollständige Transkription, Modellierung in XML/TEI, Normdaten etc.) und mit digitalen Methoden, die gerade auch für korpusübergreifende Untersuchungen den Weg ebnen würden, sind traditionellerweise in Editionsprojekten (noch) nicht vorgesehen.
               2
                Mit dem 
               eintägigen 
               Workshop „Distant Letters“ möchten wir ein Panorama an quantitativ orientierten Analysemethoden und -praktiken für Daten digitaler Briefeditionen vorstellen, 
               vermitteln und diskutieren, um so neue Perspektiven 
               auf
                Briefkorpora 
               zu
                erproben.
               3
                
               Der Workshop gliedert sich in vier Abschnitte:
            
            
               Auswertung von Metadaten und Entitäten
               
                  
                  Auf der Grundlage von standardisiert kodierten Briefmetadaten in XML/TEI sollen mit der Abfragesprache XQuery zunächst Fragen formuliert werden wie: „Wie viel hat Sender A an Empfänger B insgesamt geschrieben? Wie viel in einem bestimmten Jahr?“ Im Anschluss sollen vergleichende Untersuchungen angestellt werden, denen Fragen wie „Wie viel hat Sender A an Empfänger B und Empfänger C geschrieben?“ oder „Wie gestaltet sich das Verhältnis von gesendeten und empfangenen Briefen in der Korrespondenz von A und B?“ zugrunde liegen. Auch Entitäten aus dem Brieftext können in die Untersuchung mit einbezogen werden („Wie häufig wird Person X im Verlauf der Korrespondenz erwähnt?“). Das Ergebnis von derlei Fragen sind statistische Werte, die, um sie interpretatorisch zugänglicher zu machen, weiter aufbereitet werden müssen, z.B. als Visualisierungen in Diagrammen, Kreisen und Kurven.
               
            
            
               Analyse linguistischer Merkmale
               
                  
                  Im zweiten Teil wendet sich die Untersuchung den Volltextdaten zu. In den Blick genommen werden dabei linguistische Merkmale auf Token-Ebene (z.B. Lemma und Wortart), die einfachen oder komplexen Abfragen (z.B. nach typischen Adjektiv-Anbindungen bestimmter Substantive, Häufungen einer bestimmten Wortart, festen Wendungen, Kollokationen) an den Text zugrunde gelegt werden können und so u.a. Aufschluss über inhaltliche und stilistische Gegebenheiten ermöglichen. Im Workshop werden Werkzeuge gezeigt und benutzt, die zum einen die automatische linguistische Analyse von Texten, z.B. deren Lemmatisierung und POS-Tagging, erlauben und zum anderen Möglichkeiten der Auswertung annotierter linguistischer Merkmale bieten, z.B. mittels leistungsstarker Suchanfragesprachen oder Möglichkeiten der Visualisierung. Genauer in den Blick genommen und z.T. benutzt werden TXM, Corpus Workbench, DTA und WebLicht.
                  4
               
            
            
               Topic Modeling
               
                  
                  Im dritten Teil des Workshops rücken die Inhalte der Briefkommunikation stärker in das Zentrum des Interesses, wenn Fragen aufgegriffen werden wie „Welche Themen werden 
                  behandelt und wie sind diese zeitlich verteilt?“ oder „Gibt es bestimmte Themen, die in einer bestimmten Personengruppe stärker verhandelt werden als in einer anderen?“. Analysiert wird dabei der Volltext der Briefe, zusätzlich können jedoch auch die Briefmetadaten in die Interpretation der Analyseergebnisse einfließen. Für die Modellierung 
                  der Topics
                   wird das Tool „Mallet“
                   
                  verwendet,
                  5
                   und es wird im Workshop gemeinsam ein Topic Model für ein Briefkorpus erstellt. Für die Auswertung in Kombination mit Metadaten und Visualisierungen wird der „Topic Modeling Workflow“ 
                  (TMW) verwendet.
                  6
                  
                  Diskutiert werden soll 
                  außerdem, wie sich die Konzepte ‚Topic‘ und ‚Thema‘ zueinander verhalten.
                  7
               
            
            
               Stilometrie
               
                  Im letzten Teil des Workshops soll mit Methoden und Tools der Stilometrie der Sprach- bzw. Schreibstil eines Briefkorpus genauer untersucht werden. Analysiert wird dabei erneut der Volltext, diesmal in orthografisch normalisierter Form. Mögliche Fragestellungen der Analyse sind: „Welche Rückschlüsse erlauben stilometrische Analysen hinsichtlich Sender und Empfänger der Briefe?“, „Korrelieren die stilistische Nähe bzw. Distanz mit Faktoren wie Zeit, Raum oder Empfänger?“. Auch Stilvergleiche werden beispielhaft auf Grundlage der Fragen „Ändert sich der Stil von Sender A in seinen Briefen an die Empfänger B und C?“ und „Variiert der Stil zwischen Geschäfts- und Privatkorrespondenz?“ unternommen. Für die stilometrischen Analysen nutzen wir das „Stylo“-Paket für R.8
                  
                  Auch für die stilistischen Analysen ist zu diskutieren, welches Konzept von Stil hinter den gewählten Methoden steht und wie es sich zu anderen Definitionen von Stil verhält.
                  9
               
            
         
         
            Ziele
            
               Ziel des Workshops ist es, ein Panorama quantitativer Analysemöglichkeiten für Briefkorpora vorzustellen, das eine Ergänzung zu den traditionellen ‚close reading‘-Verfahren in wissenschaftlichen Editionen darstellt und die Digitalität der Editionsdaten mit Methoden der Digital Humanities noch stärker für quellenimmanente Forschungsfragen fruchtbar macht. Die Teilnehmerinnen und Teilnehmer sollen den Workshop am Ende des Tages mit einem Set an Skripten und Tools verlassen und in der Lage sein, diese auf andere (ggf. eigene) Datensätze anzuwenden. Neben der Vermittlung von technischen Fertigkeiten ist die Diskussion der Methoden und Ergebnisse mit den Teilnehmerinnen und Teilnehmern fester Bestandteil des Workshops. 
                    
               Es soll dabei gemeinsam eruiert werden, auf welchen theoretischen Annahmen die Methoden jeweils basieren, wo ihre Stärken und Schwächen liegen und auch inwieweit die vermittelten Praktiken eine Chance haben könnten, zukünftig ein Bestandteil bei der Erstellung und Nutzung wissenschaftlicher digitaler Briefeditionen zu werden. 
            
         
         
            Daten
            Die Organisatorinnen und Organisatoren stellen XML/TEI und Plain Text Datensätze aus zwei verschiedenen Briefeditionen für den Workshop bereit: ca. 5500 Brieftexte und ebenso viele Metadatensätze aus „Jean Paul - Sämtliche Briefe digital“ (Bernauer / Miller / Neuber 2018) sowie ca. 400 Brieftexte und 3000 Metadatensätze der „edition humboldt digital“ (Ette 2017-). Darüber hinaus steht es den Teilnehmerinnen und Teilnehmer frei, ihre eigenen Datensets (XML/TEI-kodiert und Plain Text) zu verwenden.
         
         
            Teilnehmerzahl und Vorkenntnisse
            Die Anzahl der Teilnehmerinnen und Teilnehmer ist auf 25 begrenzt. Gewisse Grundkenntnisse in der Programmierung (z.B. XSLT/XQuery, Python, R) sind von Vorteil, die im Workshop verwendeten Skripte werden jedoch so vorbereitet, dass sich die Arbeit daran auf Modifikationen und Erweiterungen unter Anleitung der Lehrenden beschränkt. Im Vorfeld des Workshops werden Installationshinweise für die verwendeten Werkzeuge gegeben und die Übungsdaten zum Download bereitgestellt.
         
         
            Lehrende
            
               
               Stefan Dumont: Wissenschaftlicher Mitarbeiter bei der TELOTA-Initiative der Berlin-Brandenburgischen Akademie der Wissenschaften, dort u.a. zuständig für die „edition humboldt digital”. Wissenschaftlicher Koordinator des DFG-Projekts „correspSearch - Briefeditionen vernetzen“. Co-Convener der TEI Special Interest Group „Correspondence“. Expertise u.a. mit Standardisierung von Briefkodierung und -metadaten und X-Technologien.
                
            
               Susanne Haaf: Wissenschaftliche Mitarbeiterin im Projekt CLARIN-D an der Berlin-Brandenburgischen Akademie der Wissenschaften, u.a. beteiligt am Auf- und Ausbau des Deutschen Textarchivs. Doktorandin im Bereich korpusbasierter Untersuchung von Textsortenspezifika. Spezialisierung in Korpusaufbau, Korpuslinguistik, Standards für Text- und Metadaten (insbes. TEI) sowie Textedition.
                
            
               Ulrike Henny-Krahmer: Wissenschaftliche Mitarbeiterin im Projekt „Computergestützte Literarische Gattungsstilistik” (CLiGS) an der Universität Würzburg. Studium der Regionalwissenschaften Lateinamerika in Köln und Lissabon, Doktorandin in Digital Humanities mit dem Thema „Topic and Style in Subgenres of the Spanish American Novel (1830-1910)“.
                
            
               Benjamin Krautter: Wissenschaftlicher Mitarbeiter im Projekt “Quantitative Drama Analytics” (QuaDramA) an der Universität Stuttgart. Studium der Literaturwissenschaft (Germanistik) und Politikwissenschaft in Stuttgart und Seoul (Südkorea), Doktorand im Bereich Digital Literary Studies mit dem Thema “Quantitative Dramenanalyse - Operationalisierung aristotelischer Kategorien” (Arbeitstitel).
                
            
               Frederike Neuber: Wissenschaftliche Mitarbeiterin bei der TELOTA-Initiative der Berlin-Brandenburgischen Akademie der Wissenschaften, dort u.a. zuständig für die Briefedition “Jean Paul - Sämtliche Briefe digital”. Studium der Italianistik und Editionswissenschaft in Berlin und Rom, Doktorandin in Digital Humanities. Spezialisierung in Editionsphilologie, Datenmodellierung und Programmierung mit X-Technologien.
                
         
      
      
         
            
                Der webservice „correspSearch“ etwa illustriert die Bedeutung von standardisierter Metadatenerfassung mit Normdaten zur Vernetzbarkeit von Korrespondenzen, . 
            
                Vereinzelt werden quantitative Analysemethoden bereits auf Editionsdaten angewandt: Etwa wird im Kontext des Projekts “Mapping the Republic of Letters” (Stanford University 2013) zur Erschließung der Briefkommunikation und -verbreitung mit verschiedenen statistisch- und/oder netzwerkanalytisch-basierten Visualisierungen experimentiert; Andorfer (2017) erprobt Topic Modelling mit dem Korrespondenzkorpus Leo von Thun-Hohensteins.
            
                Nicht Teil dieses Panoramas ist die Netzwerkanalyse, auch wenn diese Form der Auswertung bzw. Visualisierung für Briefdatensätze oft die am naheliegendste scheint. Grundkompetenzen zur Netzwerkanalyse bzw. -visualisierung werden mittlerweile regelmäßig in Workshops vermittelt, z.B. im Rahmen der „Historical Network Research-Community“ (http://historicalnetworkresearch.org/). Der Fokus des Workshops richtet sich daher auf bisher weniger berücksichtigte Formen der Analyse von Briefkorpora.
            
               
                http://textometrie.ens-lyon.fr, , , .
            
               
               
            
            
               
               
            
            
                Zwar ist das Verfahren für die Ermittlung von Schlüsselwörtern und Themen entwickelt worden, je nach verwendetem Korpus ergeben sich aber auch andere Arten von Topics, z.B. sprachspezifische oder motivische. Vgl. dazu u.a. Rhody (2012) und Schöch (2017).
            
               
               
            
            
                Für einen Überblick zu verschiedenen Stilbegriffen in der Literatur- und Sprachwissenschaft siehe Herrmann et al. (2015).
         
         
            
               Bibliographie
               
                  
                  Andorfer, Peter (2017):
                  Turing Test für das Topic Modeling. Von Menschen und Maschinen erstellte inhaltliche Analysen der Korrespondenz von Leo von Thun-Hohenstein im Vergleich, 
                        in: Zeitschrift für digitale Geisteswissenschaften; doi: 
                        
                     10.17175/2017_002
                  [zuletzt abgerufen 7. Januar 2019].
                    
               
                  Bernauer, Markus / Miller, Norbert / Neuber, Frederike (eds.) (2018):
                  Jean Paul – Sämtliche Briefe digital. 
                        In der Fassung der von Eduard Berend herausgegebenen 3. Abteilung der Historisch-kritischen Ausgabe (1952-1964), im Auftrag der Berlin-Brandenburgischen Akademie der Wissenschaften überarbeitet und herausgegeben von Markus Bernauer, Norbert Miller und Frederike Neuber; 
                        
                     http://jeanpaul-edition.de
                   [letzter Zugriff 7. Januar 2019].
                    
               
                  
                  Burrows, John (2002):
                  Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship, 
                        in: Literary and Linguistic Computing 17/3, S. 267–287.
                    
               
                  Dumont, Stefan (2016):
                  correspSearch – Connecting Scholarly Editions of Letters, 
                        in: Journal of the Text Encoding Initiative [Online], Issue 10; 
                         [letzter Zugriff 7.Januar 2019].
                    
               
                  Eder, Maciej / Rybicki, Jan / Kestemont, Mike (2016):
                  Stylometry with R: A Package for Computational Text Analysis, 
                        in: The R Journal 8/1 (2016), S. 107–121.
                    
               
                  Ette Ottmar (eds.) (seit 2016):
                  edition humboldt digital. 
                        Berlin-Brandenburgische Akademie der Wissenschaften, Berlin. Version 3 vom 14.09.2018; 
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  Graham, Shawn / Weingart, Scott / Milligan, Ian (2012):
                  Getting Started with Topic Modeling and MALLET, 
                        in: The Programming Historian 1;
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  
                  Heiden, Serge (2010):
                  The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme, 
                        24th Pacific Asia Conference on Language, Information and Computation, Nov 2010, Sendai, Japan. Institute for Digital Enhancement of Cognitive Development, Waseda University, S.389–398. 
                    
               
                  
                  Herrmann, Berenike J. / van Dalen-Oskam, Karina / Schöch, Schöch (2015):
                  Revisiting Style, a Key Concept in Literary Studies, 
                        in: Journal of Literary Theory 9/1, S. 25–52.
                    
               
                  
                  Rhody, Lisa M. (2012):
                  Topic Modeling and Figurative Language, 
                        in:  Journal of Digital Humanities 2/1; 
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  
                  Schöch, Christof (2017):
                  Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama, 
                        in: Digital Humanities Quarterly 11/2; 
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  Walmsley, Priscilla (2009):
                  XQuery: Search Across a Variety of XML Data.
                        O’Reilly Media.
                    
            
         
      
   



      
         
            Einleitung
            In diesem Beitrag wird ein Verfahren vorgestellt, das Netzwerkvisualisierungen dramatischer Texte für eine spezifische Form der kommunikativen Interaktion zwischen Figuren fokussiert.
            Es wird gezeigt, inwiefern gewichtete, gerichtete und dynamische Figurennetzwerke narrative Informationsvermittlung in der Figurenrede visualisieren können und auf diesem Weg dramennarratologische Analysen bzw. Annotationen ausgewertet werden.
            Im Gegensatz zu literaturwissenschaftlichen Netzwerkanalysen, die um die automatisierte Analyse 
                    des „kompositorische[n] Grundgerüst[s]“ 
                    
                    (Trilcke 2013: 224) von großen Dramenkorpora 
                    
                    (Piper et al 2017; Trilcke et al. 2015) bemüht sind, steht in diesem Beitrag also die Visualisierung von manuellen Annotationen im Vordergrund.
                
            Darüber hinaus werden mit Rückgriff auf die ermittelten Netzwerkdaten Deutungspotenziale exemplarisch an Kleists 
                    Die Familie Schroffenstein (DFS) diskutiert.1 Das Erkenntnisinteresse zielt also auf zwei Aspekte: (1) Inwiefern lassen sich narrative Redebeiträge, die ein zentrales Element der inneren und äußeren Informationsvermittlung im Drama (Pfister 2001: 20-22) sind, durch Annotationen netzwerkgraphisch visualisieren? (2) Inwiefern stellt die literaturwissenschaftliche Netzwerkanalyse in diesem Kontext einen Mehrwert dar?
                
         
         
            Annotation narrativer Figurenrede
            Ausgangspunkt der vorgestellten Netzwerke ist eine Typologie narrativer Figurenrede bzw. von Binnenerzählungen, die zur Annotation der Dramen Heinrich von Kleists genutzt wurden.2 Dabei wurden über 800 Vorkommnisse narrativer Figurenrede in den Dramen manuell annotiert. In einem ersten Schritt unterscheidet die Typologie zwischen narrativen Äußerungen, mit denen Figuren über ihre eigene Wirklichkeit erzählen, und narrativer Figurenrede, bei der das nicht der Fall ist. Der erste Phänomentyp, die horizontalen Binnenerzählungen, können mit dem narratologischen Kategorieninventar zur Beschreibung anachronen Erzählens (Lahn & Meister 2008: 138-141) genauer beschrieben und annotiert werden:3
            
            
               
                  Binnenerzählungen
               
               
                  Horizontal
                  Analepsen
                  488
               
               
                  
                  Simullepsen
                  123
               
               
                  
                  Prolepsen
                  33
               
               
                  Vertikal
                  Pseudoanalepsen
                  33
               
               
                  
                  Pseudosimullepsen
                  6
               
               
                  
                  Pseudoprolepse
                  29
               
            
            Tabelle 1: Vorkommen narrativer Figurenrede in Kleists Dramen
            Diese manuellen Annotationen sind die Grundlage dafür, dass unterschiedliche Formen der Informationsvermittlung netzwerkgraphisch visualisiert werden können.
         
         
            Netzwerkerstellung
            Unter Rückgriff auf die Annotationen der narrativen Figurenrede und die TEI-Annotationen von Sprecherfiguren und Szenen- sowie Aktwechseln im TextGrid-Korpus wurden chronologisierte Sender-Adressaten-Kanten erstellt. Dazu wurden die vier Sprecherfiguren, die auf eine narrative Äußerung folgen oder ihr vorangehen, als Adressaten berücksichtigt, sofern keine Akt- oder Szenenwechsel zwischen narrativer Figurenrede und potenziellem Adressaten liegt und es sich um unterschiedliche Figuren handelt. Die Anzahl der erzeugten Kanten ist somit deutlich höher als die Anzahl der narrativen Äußerungen (siehe exemplarisch in Tabelle 2 die Kanten 4 und 5 sowie 8-10). 
            
               
                  Id
                  
                     Source (Sprecher)
                  
                  
                     Target (Adressat)
                  
                  
                     Label (Akt)
                  
                  
                     Timeset (Beginn & Ende)
                  
                  
                     Weight
                  
               
               
                  1
                  Rupert
                  Eustache
                  1
                  
                     ""
                  
                  1
               
               
                  2
                  Rupert
                  Jeronimus
                  1
                  
                     ""
                  
                  1
               
               
                  3
                  Rupert
                  Ottokar
                  1
                  
                     ""
                  
                  1
               
               
                  4
                  Jeronimus
                  Ottokar
                  1
                  
                     ""
                  
                  1
               
               
                  5
                  Jeronimus
                  Ottokar
                  1
                  
                     ""
                  
                  2
               
               
                  6
                  Ottokar
                  Jeronimus
                  1
                  
                     ""
                  
                  1
               
               
                  7
                  Jeronimus
                  Kirchenvogt
                  1
                  
                     ""
                  
                  1
               
               
                  8
                  Kirchenvogt
                  Jeronimus
                  1
                  
                     ""
                  
                  1
               
               
                  9
                  Kirchenvogt
                  Jeronimus
                  1
                  
                     ""
                  
                  2
               
               
                  10
                  Kirchenvogt
                  Jeronimus
                  1
                  
                     ""
                  
                  3
               
            
            Tabelle 2: Auszug aus der Kantenliste zu DFS
            Das Kantengewicht 
                (Weight) zwischen einer Sender- und einer Adressatenfigur steigt mit jeder narrativen Äußerung, die eine Senderfigur im ‚Beisein‘ derselben Adressatenfigur äußert (siehe z.B. Kante 4 und 5). 
                
            Die Figurennetzwerke, die auf dieser Grundlage erstellt werden, illustrieren, welche Figuren sich zu welchem Zeitpunkt des Dramenverlaufs narrativ äußern, welche Figuren narrative Informationen bekommen und wie häufig Figuren an narrativem Informationsaustausch beteiligt sind.
         
         
            Visualisierungs- und Analysebeispiele
            Abbildung 1 zeigt dieses Potential der Netzwerkvisualisierung exemplarisch für 
                    Die Familie Schroffenstein.4
            
            
               
               
                  Abbildung 1.
                        Narrative Informationsvermittlung in DFS
               
            
            Die Größe der Knotenbeschriftung repräsentiert hier die betweenness centrality5 der Figuren und damit den Stellenwert bzw. Einfluss der Figur auf die narrative Informationsvermittlung innerhalb des Dramas.
                
            
               
                  Label
                  
                     betweenness centrality
                  
                  
                     weighted indegree
                  
                  
                     weighted outdegree
                  
                  indegree
                  outdegree
                  degree
               
               
                  Jeronimus
                  125,45
                  99
                  82
                  25
                  28
                  53
               
               
                  Gertrude
                  98,74
                  46
                  70
                  15
                  15
                  30
               
               
                  Sylvester
                  85,37
                  107
                  31
                  29
                  14
                  43
               
               
                  Rupert
                  74,47
                  145
                  17
                  36
                  10
                  46
               
               
                  Agnes
                  45,90
                  59
                  85
                  18
                  23
                  41
               
               
                  Eustache
                  17,25
                  37
                  73
                  14
                  19
                  33
               
               
                  Ottokar
                  5,11
                  180
                  51
                  33
                  14
                  47
               
               
                  Ursula
                  3,31
                  1
                  4
                  1
                  4
                  5
               
               
                  Santing
                  2,94
                  10
                  49
                  6
                  12
                  18
               
               
                  Johann
                  2,35
                  4
                  106
                  3
                  15
                  18
               
               
                  Kirchenvogt
                  0,00
                  1
                  80
                  1
                  14
                  15
               
               
                  Ein Diener
                  0,00
                  2
                  2
                  2
                  2
                  4
               
               
                  Sylvius
                  0,00
                  7
                  3
                  4
                  2
                  6
               
               
                  Gärtner
                  0,00
                  2
                  0
                  2
                  0
                  2
               
               
                  Aldöbern
                  0,00
                  1
                  0
                  1
                  0
                  1
               
               
                  Theistiner
                  0,00
                  5
                  15
                  4
                  5
                  9
               
               
                  Der Wanderer
                  0,00
                  0
                  1
                  0
                  1
                  1
               
               
                  Zweiter Wanderer
                  0,00
                  0
                  4
                  0
                  3
                  3
               
               
                  Die Kammerzofe
                  0,00
                  2
                  13
                  2
                  6
                  8
               
               
                  Barnabe
                  0,00
                  1
                  21
                  1
                  8
                  9
               
               
                  Ein Ritter
                  0,00
                  0
                  2
                  0
                  2
                  2
               
            
            Tabelle 3: Betweenness centrality, (weighted) in- und outdegree in DFS
            Schon anhand dieses Beispiels und der netzwerkmetrischen Daten in Tabelle 3 lassen sich einige Vorzüge einer netzwerkgraphischen Annotationsauswertung zeigen:
            
               Die Reichweite einer Figurenerzählung wird im Hinblick auf den Adressatenkreis ermittelt.
               Die narrativen Funktionen der Figuren werden durch ihre Netzwerkeigenschaften erkennbar:
                        
                     
                        Brückenfiguren: hohe betweenness centrality; Verbindung getrennter Netzwerkbereiche 
                                (Trilcke 2013: 217): z.B. Jeronimus (siehe hier und nachfolgend Tabelle 2).
                            
                     
                        Botenfiguren: Geringe betweenness centrality; mind. eine narrative Äußerung: 
                                
                           
                              Botenfiguren i.e.S., die 
                                        nach dem ersten Akt erzählend in Erscheinung treten: z.B. die Wanderer, der Ritter und Barnabe.
                                    
                           
                              Expositionsfiguren, die 
                                        im ersten Akt/Dramenteil erzählend in Erscheinung treten: z.B. der Kirchenvogt.6
                           
                        
                     
                     
                        Zielfiguren: hohe betweenness centrality; hoher gewichteter Eingangsgrad; geringer gewichteter Ausgangsgrad: z.B. Sylvester und Rupert, die häufig die Adressaten, aber selten die Sprecher narrativer Figurenrede sind. (Die Handlung des hier gewählten Beispieltexts legt die These nahe, dass diese Figuren entscheidungsmächtige Figuren sind und daher zahlreiche Informationen bekommen.)
                            
                     
                        Figuren der Informationskontrolle: hohe betweenness centrality; sehr hoher Ausgangsgrad; Netzwerke mit geringer Kantendichte: z.B. Hermann in Kleists 
                                Hermannsschlacht.
                        7
                     
                  
               
               Es lassen sich Figurenpaare und Netzwerkbereiche identifizieren, zwischen denen es keinen oder nur vermittelten Informationsaustausch gibt. Hier sind natürlich Dyaden besonders interessant, bei denen die beiden Figuren eine hohe betweenness centrality aufweisen: z.B. Rupert und Sylvester.
               Die Informationsstrukturen geben Aufschluss über den allgemeinen Grad der Informiertheit der Figuren: z.B. die Kantendichte.8
               
            
            Zudem können unterschiedliche Formen der narrativen Figurenrede netzwerkgraphisch miteinander verglichen werden. Die Abbildungen 3 und 4 zeigen dies exemplarisch. In Abbildung 3 werden Figurenerzählungen visualisiert, in denen sich Figuren in Übereinstimmung mit der fiktionalen Wirklichkeit äußern. Abbildung 4 zeigt narrative Äußerungen, bei denen das Gegenteil der Fall ist. Es handelt sich also um narrative Falschaussagen. Der Vergleich ist in diesem Fall aufgrund der vorhandenen Parallelen 
                    und Unterschiede aufschlussreich. Bei beiden Netzwerken behält Jeronimus die zentrale Position im Netzwerk. Hier schlägt sich nieder, dass er für die Verbreitung von wirklichkeitsgemäßen Informationen ebenso verantwortlich ist, wie für die Verbreitung von falschen Verdächtigungen, Lügen und Vorurteilen. Agnes‘ Netzwerkposition verändert sich hingegen stark (Abb. 4). Sie ist innerhalb ihrer Familie und in der kommunikativen Interaktion mit Ottokar die zentrale Figur bei der Weitergabe falscher Informationen.
                
         
         
            Informationsvermittlung im Dramenverlauf: Dynamische Netzwerke
            Wie 
                    
                    Agarwal et al. (2012: 94) gezeigt haben, hat die Erstellung von dynamischen Netzwerken den Vorteil, die Veränderlichkeit der netzwerkmetrischen Eigenschaften einer Figur, einer Figurengruppe oder eines gesamten Netzwerks im Verlauf eines Roman- oder Dramengeschehens berücksichtigen zu können. Das bestätigen die netzwerkmetrischen Auswertungen der 
                    Familie Schroffenstein in Abbildung 2. Hier wird der gewichtete Ausgangsgrad für vier ausgewählte Figuren aktweise dokumentiert. So tritt der Kirchenvogt narrativ nur im ersten Akt in Erscheinung, was seine Funktion als Expositionsfigur unterstreicht. Auch Barnabes Funktion als Vermittlerin von Anagnorisis-Informationen zum Dramenende spiegelt sich wider. Jeronimus Bedeutung relativiert sich, weil ersichtlich wird, dass er – aufgrund seiner Ermordung am Ende des dritten Akts – nur in den ersten drei Akten als informationsvermittelnde Figur auftritt. Seine hohen Werte in Akt zwei und drei zeigen jedoch, dass er für den Handlungsverlauf entscheidende Informationen äußert. Bei Rupert bestätigt sich seine geringe narrative Aktivität (Tabelle 3) als ein relativ konstantes Verhalten. Der niedrige gewichtete Ausgangsgrad für das gesamte Drama ist also nicht darauf zurückzuführen, dass er nur in wenigen Szenen (erzählerisch) in Erscheinung tritt. 
                
            
               
               
                  Abbildung 2.
                        Gewichteter Ausgangsgrad im Dramenverlauf in DFS
               
            
         
         
            Schluss
            Solange die automatische Annotation narrativer Figurenrede nicht möglich ist, setzt das vorgestellte Verfahren einen relativ großen Annotationsaufwand voraus. Es ermöglicht somit keinen umfassenden Vergleich von Dramen, was unter anderem zur Einordnung der vorgestellten quantitativen Netzwerkanalysen wünschenswert wäre.
            In diesem Beitrag wurde jedoch exemplarisch gezeigt, inwiefern netzwerkgraphische Visualisierungen für die Auswertung narratologischer Annotationen einen analytischen Mehrwert haben können. Die formalen Annotationen können und sollen durch inhaltsbezogene Annotationen angereichert werden. Auf dieser Grundlage könnte netzwerkgraphisch der Informationsaustausch über bestimmte Themen oder Figuren visualisiert werden.
         
         
            Anhang
            
               
               
                  Abbildung 3. Narrative Informationsvermittlung (Horizontale Binnenerzählungen/Wirklichkeitserzählungen) in DFS 
            
            
               
               
                   Abbildung 4.
                        Narrative Informationsvermittlung (Pseudoanalepsen/Falschaussagen) in DFS
               
            
            
               
                  Label
                  
                     betweenness centrality
                  
                  
                     weighted indegree
                  
                  
                     weighted outdegree
                  
                  indegree
                  outdegree
               
               
                  Hermann
                  482,7
                  66
                  47
                  36
                  27
               
               
                  Thuiskomar
                  107,25
                  5
                  10
                  5
                  6
               
               
                  Thusnelda
                  69,62
                  28
                  25
                  13
                  9
               
               
                  Ventidius
                  61,95
                  7
                  27
                  5
                  14
               
               
                  Varus
                  57,8
                  10
                  9
                  6
                  8
               
               
                  Dagobert
                  27
                  3
                  3
                  2
                  3
               
               
                  Zweite Hauptmann
                  21
                  2
                  1
                  2
                  1
               
               
                  Gertrud
                  20
                  7
                  6
                  4
                  3
               
               
                  Marbod
                  16,2
                  11
                  5
                  7
                  4
               
               
                  Wolf
                  11,96
                  5
                  4
                  5
                  4
               
               
                  Aristan
                  8,5
                  1
                  5
                  1
                  4
               
               
                  Rinold
                  6
                  1
                  5
                  1
                  4
               
               
                  Erste Cherusker
                  6
                  1
                  5
                  1
                  4
               
               
                  Der zweite Cherusker
                  5,2
                  1
                  2
                  1
                  2
               
               
                  Gueltar
                  5
                  1
                  2
                  1
                  2
               
               
                  Der Mann
                  4
                  1
                  3
                  1
                  3
               
               
                  Das Volk
                  2
                  2
                  1
                  2
                  1
               
               
                  Erste Älteste
                  0,5
                  1
                  1
                  1
                  1
               
               
                  Scäpio
                  0,33
                  1
                  5
                  1
                  4
               
            
            Tabelle 4: Figuren mit höchster betweenness centrality in Kleist Hermannsschlacht
            
         
      
      
         
             Textgrundlage der Annotationen war Kleists Werkausgabe von Siegfried Streller, die durch das TextGrid Repositorium digital zur Verfügung steht: https://textgrid.de/en/digitale-bibliothek.
             Dazu wurde das Annotationstool CATMA (Meister et al. 2016) verwendet. CATMA bietet die Möglichkeit, mit selbstdefinierten literaturwissenschaftlichen Analysetaxonomien zu annotieren und ist damit für narratologische Forschungsprozesse besonders geeignet.
             Grundlegend für die Annotation ist ein Narrativitätskonzept, das berücksichtigt, dass Texte aller Gattungen narrative Elemente enthalten können, wie es u.a. Wolf (2002) beschreibt. Zu der narratologischen Terminologie vgl. Lahn/Meister 2016: 147-149.
             Alle Visualisierungen und Netzwerkanalysen wurden mit dem Tool GEPHI (Bastian et al. 2008) erstellt.
             Mit der betweenness centrality wird gemessen, für wieviele Knotenpaare ein Knoten den kürzesten Netzwerkpfad darstellt. Eine hohe betweenness centrality in Netzwerken, die Informationsflüsse abbilden, indiziert also einen großen Einfluss der Figur auf die Informationsvermittlung im Netzwerk, da sie als Brückenfigur fungiert.
             Vgl. zum Unterschied Pfister 2001: 280f.
             Kantendichte der Hermannsschlacht: 0,051; Kantendichte Die Familie Schroffenstein: 0,388. Siehe zur Hermannsschlacht Tabelle 4 im Anhang, in der sich Hermanns propagandistische „Überzeugungsarbeit“ (Müller-Salget 2009: 78) widerspiegelt.
             Hier ist zu berücksichtigen, dass die Kantendichte natürlich auch durch andere Faktoren beeinflusst wird (Trilcke 2013: 225).
         
         
            
               Bibliographie
               
                  
                  Agarwal, A. / A. Corvalan / J. Jensen / O. Rambow (2012):
                  Social Network Analysis of Alice in Wonderland, 
                        Proceedings of the Workshop on Computational Linguistics for Literature: 88–96.
                    
               
                  
                  Bastian, M. / S. Heymann / M. Jacomy (2008):
                  Gephi: An open source sofware for exploring and manipulating networks. 
                        AVI 2008 – Proceedings of the Working Conference on Advanced Visual Interfaces.
                    
               
                  
                  Lahn, Silke/ J. C. Meister (2008):
                  Einführung in die Erzähltextanalyse. Stuttgart: Verlag J.B. Metzler.
                    
               
                  Meister, J. C. / M. Petris / E. Gius / J. Jacke (2016): 
                  CATMA 5.0. software for text annotation and analysis.
                    
               
                  
                  Moretti, F. (2011):
                  Network Theory, Plot Analysis. 
                        Stanford Literary Lab Pamphlets 2.
                    
               
                  
                  Müller-Salget, Klaus (2009):
                  Die Herrmannsschlacht, 
                        in: 
                        Ingo Breuer (Hg.):
                  Kleist-Handbuch. Leben – Werk – Wirkung. 
                        Stuttgart: Verlag J.B. Metzler. S. 76-79.
                    
               
                  Piper, Andrew / Mark Algee-Hewitt / Koustuv Sinha / Derek Ruths / Hardik Vala (2017):
                  Studying Literary Characters and Character Networks.
                        Digital Humanities 2017, Conference Abstracts.
                    
               
                  
                  Pfister, Manfred (2001):
                  Das Drama. 
                        München. Wilhelm Fink Verlag.
                    
               
                  Trilcke, P. / F. Fischer / D. Kampkaspar (2015):
                  Digital Network Analysis of Dramatic Texts. 
                        Digital Humanities 2015: Book of Abstracts.
                    
               
                  
                  Trilcke, Peer (2013):
                  Social Network Analysis (SNA) als Methode einer textempirischen Literaturwissenschaft. 
                        Empirie in der Literaturwissenschaft. Hrsg. von Christoph Rauen, Katja Mellmann und Philip Ajouri. Münster: 201–247.
                    
               
                  
                  Wolf, Werner (2002):
                  Das Problem der Narrativität in Literatur, bildender Kunst und Musik: Ein Beitrag zu einer intermedialen Erzähltheorie. 
                        Erzähltheorie transgenerisch, intermedial, interdisziplinär. Hrsg. von Vera Nünning und Ansgar Nünning. Trier: 23–104.
                    
            
         
      
   



      
         
            Situation
            Brief-Editionen sind in vielerlei Hinsicht ein unerlässliches Hilfsmittel für die (historische) Forschung. Gerade weil Briefe ein höchst subjektives Ausdrucksmedium sind, enthalten sie oftmals wertvolle Informationen, die für das Verständnis größerer Zusammenhänge essentiell sein können. Doch um ihr volles Potential entfalten zu können, müssten sie leicht zugänglich sein. Dabei liegen die allermeisten Editionen ausschließlich in gedruckter Form vor, nur neuere Vorhaben publizieren wenigstens teilweise digital. 
            Der Forscherin oder dem Forscher entgehen im Zweifelsfall wichtige Schriftzeugnisse, weil sie sie schlichtweg nicht finden. Oder um es mit Goethe zu sagen: „Man erblickt nur, was man schon weiß und versteht.“ Es stellt sich also die Frage, wie man diese zwischen Buchdeckel gepressten Schätze heben und der Forschung zugänglich machen kann. 
            OCR-gestützte Retrodigitalisierung von Volltexten kritischer Editionen ist hier qualitativ zumeist noch nicht ausreichend. Erschwerend kommt hinzu, dass diese nicht-semantisch ist – und bei urheberrechtlich geschützten Werken in aller Regel völlig unmöglich. 
         
         
            Lösungsansatz
            Als möglicher Lösungsansatz wäre eine sogenannte semantische Minimal-Retrodigitalisierung vorzuschlagen, um gedruckte Editionen zu erschließen und online auffindbar zu machen. Hierbei werden ausschließlich die Metadaten der Briefe digitalisiert, das heißt die Namen des Schreibers und des Empfängers sowie das Datum des Briefes. Weitere Informationen, beispielsweise zu den Orten, sind wünschenswert, jedoch nicht notwendig. Versieht man diese Metadaten mit Normdaten und stellt sie im TEI-Austauschformat 
                    CMIF 
                    (Correspondence Metadata Interchange-Format) bereit, könnten sie auch im Kontext von vielen tausend weiteren Briefen im Webservice 
                    
                  correspSearch.net
                sichtbar werden, der von der Berlin-Brandenburgischen Akademie der Wissenschaften bereitgestellt wird.
                
            Der große Vorteil hierbei ist, dass auch jüngste Publikationen erschlossen werden können, da auf den notwendigen Metadaten kein Urheberrechtsschutz liegt. Nicht selten enthalten solche Editionen sogar ein tabellarische Korrespondenzregister, das relativ einfach gescannt und aufbereitet werden kann. Generell scheint es allerdings nicht ratsam die Brief-Metadaten direkt in XML zu kodieren, da ein solches Vorgehen relativ fehleranfällig und zeitaufwändig ist. Aufgrund des hohen Grades an Strukturiertheit ist es sinnvoller sie gleichsam in Tabellen zu erfassen; dies erhöht die Übersichtlichkeit und reduziert die Fehlerquote. Wünschenswert ist natürlich eine Anreicherung mit Normdaten. Diese kann manuell oder auch halbautomatisch mit entsprechenden Tools erfolgen. Die kompilierten Daten können dann mit dem Programm 
                    
                  CSV2CMI
                automatisch in das entsprechende TEI-XML umgewandelt werden. Die resultierende Datei muss dann nur noch im World Wide Web bereitgestellt werden.
                
            Ein weiterer, charmanter Vorteil hierbei ist, dass in der Tabelle beliebig viele zusätzliche Informationen untergebracht werden können, die im CMIF nicht oder noch nicht kodiert werden können. So besteht folglich sogar die Möglichkeit zwei Dateien bereitzustellen, eine für den für den automatisierten und standardisierten Austausch 
                (machine-readable) und eine für Endbenutzer leicht lesbare Übersicht 
                (human-readable).
                
         
         
            Ausblick
            Gleichwohl das hier vorgestellte Verfahren vergleichsweise unkompliziert ist, macht sich die Arbeit dennoch natürlich nicht von allein. Für wen erscheint es also sinnvoll diese Daten aufzubereiten und bereitzustellen? 
            Zunächst kommen natürlich Bibliotheken in den Sinn, die – als Informations-Anbieter des 21. Jahrhunderts – ein höchst eigenes Interesse daran haben (sollten), Wissen zu und aus Ihren Beständen anzubieten. Des Weiteren kommen Forscher in den Sinn, die sowieso intensiv mit einem Korrespondenzkorpus arbeitet. Das Eigeninteresse an verbesserter Auffindbarkeit und Zugänglichkeit dort sollte ausreichen, die notwendigen Daten zu digitalisieren und vorrätig zu halten. Hinzu kommt in letzterem Falle natürlich die Möglichkeit, die so aufbereiteten Daten als Forschungsdaten zu publizieren. 
         
         
            Zusammenfassung
            Das Poster soll die hier skizzierte Idee der Semantischen Minimal-Retrodigitalisierung von Brief-Editionen vorstellen und den vorgeschlagenen Workflow sowie die bereit stehenden Software-Tools präsentieren. 
         
      
      
         
            
               Bibliographie
               
                  Woesler, Winfried (1988):
                  Vorschläge für eine Normierung von Briefeditionen, in: 
                        
                        Editio 2, S. 8–18. doi: 
                        10.1515/9783110241938.8
               
               
                  Ball, Rafael (2014):
                  Bibliotheken im 21. Jahrhundert. Vom Leser zum Kunden, in: 
                        Ceynowa, Klaus / Hermann, Martin:
                  Bibliotheken: Innovation aus Tradition, 
                        Berlin: De Gruyter Saur 226–231, doi: 
                        10.1515/9783110310511.226
               
               
                  Stadler, Peter (2014):
                  Interoperabilität von digitalen Briefeditionen, in: 
                        Wolzogen, Hanna Delf von / Falk, Rainer:
                  Fontanes Briefe ediert, Würzburg: Königshausen & Neumann 278–287.
                    
               
                  Dumont, Stefan (2016):
                  correspSearch – Connecting Scholarly Editions of Letters, in: 
                        Journal of the Text Encoding Initiative 10, 
                         [letzter Zugriff 10. Oktober 2018].
                    
               
                  Rettinghaus, Klaus (2018):
                  saw-leipzig/csv2cmi (Version v1.6.2), Zenodo, doi: 
                        10.5281/zenodo.1461642 [letzter Zugriff 13. Oktober 2018].
                    
            
         
      
   



      
         
         
            Stand der Forschung und Problemaufriss 
            Im Zuge des anhaltenden Aufschwungs der Schreibprozessforschung geraten seit einiger Zeit Quellentypen in den Blick der literatur- und kulturwissenschaftlichen Forschung, die Prozesse der auktorialen Selbstorganisation, der Notation oder der Lektüre dokumentieren. Dazu gehören etwa Notizbücher (Hoffmann 2008, Efimova 2018), Zettelkästen (Gfrereis, Strittmatter 2013; Krajewski 2011; Schmidt 2016) oder auch Autor*innenbibliotheken. Während dabei für Notizbücher und Zettelkästen avancierte, auf Transkriptionen und TEI-Editionen basierende Techniken der digitalen Präsentation entwickelt werden (vgl. etwa Radecke 2018 für Notizbücher, Schmidt 2016 für Luhmanns Zettelkasten), beschränkt sich die digitale Präsentation von Autor*innenbibliotheken derzeit auf die Bereitstellung entweder von elektronischen Findmitteln, die bibliothekarische Metadaten zur Verfügung stellen (vgl. etwa die Bibliothek Paul Celans: 
                    https://www.dla-marbach.de/bibliothek/spezialsammlungen/bestandsliste/bibliothek-paul-celan/) oder von einfachen Digitalisaten, die in einem Viewer und/oder als PDF-Download angeboten werden (vgl. etwa die Grimm-Bibliothek 
                    https://www.digi-hub.de/viewer/browse/gelehrtenbibliotheken.grimmbibliothek/-/1/-/-/). Wenngleich diese Präsentationsformen dem bibliothekarischen Charakter der Bücher einer Autor*innenbibliothek durchaus gerecht werden, gelingt es ihnen nicht, den autographischen Charakter dieser Bücher zu erfassen, deren Besonderheit in den Lese- und Gebrauchsspuren liegt, die die Autorin oder der Autor (bzw. später die Erb*innen, Nachlassverwalter*innen, besitzenden Institutionen etc.) in ihnen hinterlassen haben. Zugleich erweist sich die Handhabung der Bücher einer Handbibliothek nach den Standards der Edition von Autographen als unverhältnismäßig, ist das Ziel ihrer Präsentation doch eben nicht ein edierter Text; vielmehr soll durch sie der Nachvollzug kreativer Lektüre- und Gebrauchspraktiken ermöglicht werden. 
                
            Dabei lassen sich für eine solche Präsentation, in Bezugnahme auf die Bedürfnisse der Forschung, drei Ziele formulieren: 1) die Bereitstellung möglichst der Gesamtheit der Bände einer Autor*innenbibliothek; 2) die Implementierung von Suchanfragen-basierten Findmitteln; 3) die Bereitstellung von Anwendungen, mit denen sich das Profil der Sammlung und die in ihr sich abzeichnenden Kreativitäts- und Lektüremuster entdecken, erkennen und erforschen lassen. 
            Für Ziel 1) stehen gängige Techniken zur Präsentation von digitalisierten Büchern (zumeist auf METS/MODS basierend) bereits zur Verfügung (DFG-Viewer, Visual Library o.Ä.); auch für Ziel 2) werden heute mit bibliothekarischen oder archivarischen Discovery-Tools oder mit einer auf den X-Technologien basierenden Umgebung bereits Lösungen angeboten. Für Ziel 3), das im Fokus unseres Projektes steht, liegen für den Spezialfall Autor*innenbibliotheken hingegen bisher weder Lösungsansätze noch Konzeptualisierungen vor, obwohl in den letzten Jahren Forderungen nach der Ergänzung von such-basierten Interfaces durch ›glückliche‹ Zufälle ermöglichende Konzepte laut wurden (Dörk et al. 2011, Thudt et al. 2012, Whitelaw 2015). Auch wenn die Etablierung sogenannter Explore-Modi zur Erfüllung dieser Ziele schon teilweise beigetragen hat, fehlt es in der digitalen Verfügbarmachung noch an flexibler Navigation entlang der Relationen zwischen den Objekten (Kreiseler et al. 2017).
         
         
            Projekt, Vorgehen und Korpus 
            Im Folgenden stellen wir einen im Verfahren des forschungsbasierten ›Rapid Prototyping‹ entwickelten Entwurf einer explorier- und skalierbaren Gesamtrepräsentation einer Autorenbibliothek vor. Dabei verbindet unsere Prototypenstudie die in der Regel als eine philologische und archiv- bzw. bibliothekswissenschaftliche Problematik adressierte Präsentation von Autor*innenbibliotheken mit der gestaltungsorientierten Forschung zur Visualisierung kultureller Sammlungen (Glinka et al. 2017; Dörk et al. 2017; Windhager et al. 2018). Im Fokus des Forschungsprojekts – das im Mai 2018 gestartet ist und bis März 2019 durchgeführt wird – steht ein in enger Koordination zwischen digitaler Archivwissenschaft, Literaturwissenschaft und informatischer Visualisierungsforschung entwickelter, webbasierter Software-Prototyp, der eine Idee realisiert, wie sich Autor*innenbibliotheken digital repräsentieren ließen. Das Projekt nimmt dabei zwei Impulse auf: Zum einen ist es dem ›Distant Reading‹ (Moretti 2013) verpflichtet, das nach Möglichkeiten einer Mustererkennung auf der Gesamtheit oder auf Teilen einer Autorenbibliothek sucht; zum anderen sollen die Repräsentationen skalierbar sein (Weitin 2017), was Fragen zu Übergängen zwischen verschiedenen Granularitäten aufwirft.
            Grundlage für die modellhafte Erschließung ist die ca. 155 Bände umfassende Handbibliothek Theodor Fontanes, die im Theodor-Fontane-Archiv der Universität Potsdam bewahrt und ergänzt wird. Die Bedeutung dieser überlieferten Autorenbibliothek ergibt sich in erster Linie aus ihrer Provenienz. Sie ist Teil der ›Schriftstellerwerkstatt‹, unersetzbar wegen der zahlreichen von Fontane verfassten Marginalien und wertvoll durch die verschiedenen Widmungsexemplare (Rasch 2005). Die Bände, die Fontane zur Abfassung von Essays und Rezensionen herangezogen hat, weisen beispielsweise besonders viele Bewertungen aus Fontanes Feder auf.
            Neben der vollständigen Verscannung des Bestandes nach archivarischen Standards und der Verzeichnung der Einzelbände nach bibliothekarischen Kriterien erfolgt eine Datenerfassung sowohl auf Seiten- wie auch auf Korpusebene. Die Daten zu Fontanes Handbibliothek sind gemäß relationalem Datenmodell in Tabellen verzeichnet, in denen die Verknüpfungen der Gegenstände abgebildet werden. Der Zugang zu den Daten erfolgt multiperspektivisch: auf Gesamtkorpus-, auf Objekt-, auf Seiten- und Einzelphänomenebene. Die Offenlegung der Zugänge im Rahmen einer Visualisierung zeigt beispielhaft die thematische und personelle Clusterbildung innerhalb der Sammlung oder die Verbindungen verschiedener Benutzungsspuren. 
         
         
            Visualisierungskonzept 
            Das im Projekt entwickelte Visualisierungskonzept legt einen besonderen Fokus auf die kontinuierliche, auf mehreren Granularitätsebenen zoom- und filterbare Navigation, welche die Erkundung einzelner Objekte ebenso zulässt wie deren Vergleich. Der interaktive Prototyp, der im März 2019 veröffentlicht wird, bietet drei grundlegende Ebenen, auf denen man sich durch den Bestand bewegt: Autor*innen, Bücher und Seiten.
            
               
               
                  Abbildung 1. Startseite/Buch-Ebene (Screenshot des Prototypen)
					
            
            Ausgangspunkt für die Exploration der Visualisierung ist die Buch-Ebene, die eine Übersicht aller Bücher der Handbibliothek, geordnet nach Autor*innen, darstellt (Abb. 1). Dies folgt einem bei Visualisierungsinterfaces typischen Prinzip, den ersten Zugang über einen Überblick zu schaffen (Shneiderman 1996, Whitelaw 2015).
            Jedes Buch wird durch einen vertikalen Balken dargestellt, in dem eine Seite wiederum durch ein abgegrenztes Segment repräsentiert wird, sodass sich eine Leseordnung der einzelnen Bücher von oben nach unten ergibt. Die Seitensegmente sind farblich entsprechend ihrer Lese- und Gebrauchsspuren kodiert. Während Seiten ohne Spuren weiß dargestellt werden, unterteilen sich die farbigen Lesespuren in die Kategorien: 1) Provenienzangaben (Grautöne), 2) Markierungen (Rottöne), 3) Marginalien (Blautöne) und 4) zusätzliches Material (Gelb). Mouseover über ein Segment zeigt eine Vorschau des jeweiligen Seiten-Scans und den Buchtitel an (Abb. 2).
            
               
               
                  Abbildung 2. Seiten-Ebene mit hervorgehobenen Marginalien und Mouseover über ein Element
					
            
            Ablesbar sind auf der Buch-Ebene, die einem individuellen Strichcode der Bücher ähnelt, zum einen der Umfang eines Buches (Gesamtlänge des Balkens), aber auch die Verteilung der Lesespuren in diesem (Farbkodierung). Die Filterleiste über der Visualisierung dient hierbei als Legende für die Farbkodierung und bietet die Möglichkeit zur Fokussierung auf bestimmte Lesespur-Typen. Die Auswahl eines Lesespur-Typs löst die Entfaltung der entsprechenden Unterkategorien in der Filterleiste aus. 
            Mit Hilfe eines Suchfeldes können spezifische Textstellen, die einer Suchanfrage entsprechen, hervorgehoben werden. Durch die Selektion eines Buches werden die anderen Bücher zusammengestaucht und eine Detailansicht des ausgewählten Buches wird entfaltet, die zusätzliche Meta-Informationen zum Werk bietet.
            Über die Scrollfunktion des Browsers können die Granularitätsebenen der Visualisierung erreicht werden. Dem Funktionsprinzip des Semantic Zoom folgend (Perlin & Fox 1993) führt Scrollen nach oben zu einer höheren Abstraktion und nach unten zu einem höheren Detailgrad – es erlaubt also einen Wechsel zwischen den drei Ebenen. Das Scrollen ermöglicht dabei kontinuierliche, sinnhafte Übergänge zwischen den Ansichten und bietet die Möglichkeit, in eigener Geschwindigkeit vor- und zurückzugehen, mit dem Ziel, die Ansichtswechsel nachvollziehbarer zu gestalten.
            
               
               
                  Abbildung 3.  Autor*innen-Ebene. Alle Filter-Kategorien sind entfaltet. Hier deutlich zu sehen ist z.B. die bei allen Büchern gleich stark ausgeprägten Provenienzangaben in grau zu Beginn der Bücher.
					
            
            Im Gegensatz zur mittleren Buch-Ebene sind auf der höheren Ebene (Abb. 3) alle Bücher eines Autors zusammengefasst, indem die Gesamtverteilung der Lesespuren in Form eines Flächen-Diagramms dargestellt wird. Hierdurch ist ein Vergleich von Fontanes Lesespuren, verteilt über die Werke unterschiedlicher Autor*innen, möglich, aber es lassen sich durch die höhere Abstraktion auch umfassende Muster nachvollziehen. Navigiert man von der mittleren Buch-Ebene in die andere Richtung auf die untere Ebene, wird die Visualisierung ins Detail entfaltet (Abb. 2), sodass für eine*n ausgewählte*n Autor*in einzelne Seiten gezielt ausgewählt werden können und Marginalien detailliert auch in der Transkription sichtbar werden. 
            Alle Selektionen, Filterungen und die ausgewählte Granularitätsebene werden in der URL kodiert, wodurch sowohl die Nutzung der Verlaufsfunktionen des Browsers als auch das Speichern unter Favoriten oder das Teilen und Referenzieren von Ansichten per Link möglich wird.
         
         
            Reflexion der Ergebnisse
            Die im Rahmen des Prototypen entwickelte, neuartige visuelle Annäherung an Autor*innenbibliotheken und die in ihnen bezeugten Lektürespuren verbindet gestaltungsorientierte Ansätze zur Visualisierung kultureller Sammlungen mit philologisch-, archiv- und bibliothekswissenschaftlichen Forschungsfragen.
            Anhand visueller Filter können Untermengen identifiziert sowie Kategorien gebildet werden, die Mustererkennungen in der Sammlung ermöglichen. Nutzer*innen sollen in die Lage versetzt werden, Begriffs- und Themenräume innerhalb dezidierter Kategorien und über deren Grenzen hinweg zu erfassen. Gefragt wurde nach der Integration von Suchfunktionen, Skalierung und Sichtbarmachung in Interfaces und wie durch attraktive Einstiege in einen Bestand weitergehende Explorationsmöglichkeiten eröffnet werden können.
            Die Entdeckung neuer Forschungsfragen während des Prototypingprozesses und die damit einhergehende Nachjustierung in der Erschließung beleuchtet die Wechselwirkungen zwischen visueller Forschung, Metadatenmanagement und Philologie. Deutlich wurde, dass die Sammlung als Konstrukt zu verstehen ist, das erst die (multimodale) Rückschau generiert. Dabei entsteht ›die‹ Sammlung aus der Verschränkung von Perspektive und Objekt. Die sich daraus ergebende Konsequenz, multiple Sichten auf ›das‹ und auf ›die‹ Objekt(e) anzubieten, schlägt sich in den verschiedenen Granularitätsebenen des Protoypen nieder und hat eine Ablösung der statischen Verzeichnung eines OPAC-Katalogs durch eine beobachtungsabhängige Visualisierung der verfügbaren Quelldaten zur Folge. Diese Sichtbarmachung fußt auf einem dynamischen Modell der Sammlungserfassung geleitet durch eine digitale Repräsentationsmodellierung.
         
      
      
         
            
               Bibliographie
               
                  Dörk, Marian / Carpendale, Sheelagh / Williamson, Carey (2011):
                  The Information Flaneur: A Fresh Look at Information Seeking, in: 
                        Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 1215-1224.
                    
               
                  Dörk, Marian / Pietsch, Christopher / Credico, Gabriel (2017): 
                  
                     One view is not enough: High-level visualizations of a large cultural collection
                  , in: 
                        Information Design Journal, 23:1, 39-47. 
                        
                     http://mariandoerk.de/papers/idj2017.pdf
                  
               
               
                  Efimova, Svetlana (2018): 
                  Das Schriftsteller-Notizbuch als Denkmedium in der russischen und deutschen Literatur. Paderborn: Wilhelm Fink.
                    
               
                  Gfrereis, Heike / Strittmatter, Ellen (eds.) (2013): 
                  Zettelkästen. Maschinen der Phantasie. Ausstellungskatalog. Deutsche Schillergesellschaft. Marbach a.N.
                    
               
                  Glinka, Katrin / Pietsch, Christopher / Dörk, Marian (2017): 
                  
                     Past Visions and Reconciling Views: Visualizing Time, Texture and Themes in Cultural Collections
                  , in: 
                        Digital Humanities Quarterly 11.2. 
                        
                     http://www.digitalhumanities.org/dhq/vol/11/2/000290/000290.html
                  
               
               
                  Haber, Peter (2010):
                  Autorenbibliotheken im digitalen Zeitalter, in: 
                        Quatro. Zeitschrift des Schweizerischen Literaturarchivs 30/31, 39-43.
                    
               
                  Höppner, Stefan / Jessen, Caroline / Münkner, Jörn (eds.) (2018):
                  Autorschaft und Bibliothek. Sammlungsstrategien und Schreibverfahren. Mit einem Vorwort von Reinhard Laube. Göttingen: Wallstein.
                    
               
                  Hoffmann, Christoph (2008):
                  Wie lesen? Das Notizbuch als Bühne der Forschung, 
                        in:
                        Birgit Griesecke (ed.): 
                  Werkstätten des Möglichen 1930-1936: L. Fleck, E. Husserl, R. Musil, L. Wittgenstein, Würzburg: 45-57.
                    
               
                  Knoche, Michael (ed.) (2015):
                  Autorenbibliotheken. Erschließung, Rekonstruktion, Wissensordnung. Wiesbaden: Harrassowitz.
                    
               
                  Krajewski, Markus (2011): 
                  Paper Machines. About cards & catalogs, 1548-1929. Cambridge: MIT Press.
                    
               
                  Kreiseler, Sarah / Brüggemann, Viktoria / Dörk, Marian (2017):
                  Tracing exploratory modes in digital collections of museum web sites using reverse information architecture, in: 
                        First Monday 22.4.
                    
               
                  Moretti, Franco (2013): 
                  Distant Reading. London: Verso.
                    
               
                  Perlin, Ken / Fox, David (1993):
                  Pad: an alternative approach to the computer interface, in: 
                        SIGGRAPH '93: Proceedings of the 20th annual conference on Computer graphics and interactive techniques, 57-64.
                    
               
                  Rasch, Wolfgang (2005):
                  Zeitungstiger, Bücherfresser. Die Bibliothek Theodor Fontanes als Fragment und Aufgabe betrachtet, 
                        in: 
                        Ute Schneider (ed.): 
                  Imprimatur. Ein Jahrbuch für Bücherfreunde. N.F. [Bd.] XIX. Wiesbaden: Harrassowitz 103-144.
                    
               
                  Rohmann, Ivonne (2015):
                  Aspekte der Erschließung und Rekonstruktion nachgelassener Privatbibliotheken am Beispiel der Büchersammlungen Herders, Wielands, Schillers und Goethes, 
                        in: 
                        Michael Knoche (ed.):
                  Autorenbibliotheken. Erschließung, Rekonstruktion, Wissensordnung. Wiesbaden: Harrassowitz 17-59.
                    
               
                  Schmidt, Johannes (2016):
                  Niklas Luhmann´s Card Index: Thinking Tool, Communication Partner, Publication Machine, 
                        in: 
                        Alberto Cevolini (ed.): 
                  Forgetting Machines. Knowledge Management Evolution in Early Modern Europe. Leiden: Brill 289-311.
                    
               
                  Shneiderman, Ben (1996):
                  The eyes have it: A task by data type taxonomy for information visualizations, in: 
                        Visual Languages, 1996. Proceedings., IEEE Symposium. IEEE, 336-343.
                    
               
                  Thud, Alice / Hinrichs, Uta / Carpendale, Sheelagh (2012):
                  The bohemian bookshelf, in: 
                        Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, 1461-1470.
                    
               
                  Weitin, Thomas (2017):
                  Scalable Reading, in: 
                        
                            Zeitschrift für Literaturwissenschaft und Linguistik
                         47.1: 1-6. 
                        
                     https://doi.org/10.1007/s41244-017-0048-4
                  
               
               
                  Wieland, Magnus (2015):
                  Materialität des Lesens. Zur Topographie von Annotationsspuren in Autorenbibliotheken, 
                        in: 
                        Knoche, Michael (ed.):  
                        Autorenbibliotheken. Erschließung, Rekonstruktion, Wissensordnung. Wiesbaden: Harrassowitz 147-173.
                    
               
                  Windhager, Florian / Federico, Paolo / Schreder, Günther / Glinka, Katrin / Dörk, Marian / Miksch, Silvia/ Mayr, Eva (2018):
                  
                     Visualization of Cultural Heritage Collection Data: State of the Art and Future Challenges.
                   in: 
                        TVCG: IEEE Transactions on Visualization and Computer Graphics.
                        
                     http://mariandoerk.de/papers/tvcg2018.pdf
                  
               
               
                  Whitelaw, Mitchell (2015):
                  
                     Generous Interfaces for Digital Cultural Collections,
                   in: 
                        Digital Humanities Quarterly, 9.1. 
                        
                     http://www.digitalhumanities.org/dhq/vol/9/1/000205/000205.html
                  
               
            
         
      
   



      
         Anhand der Aufgabe einer sprachenübergreifenden Kollationierung berichtet dieser Beitrag von “multimodalen” Analysen digitaler Texte: von einer statistischen über lexikalische bis zu wissensmodellierenden Perspektiven auf den Datensatz. Wir greifen auf diese Ansätze zurück, um verschiedene Überarbeitungsstufen und Übersetzungen eines Textes zu alignieren, und wir diskutieren, warum die Aufgabe noch immer keine in der Praxis zufriedenstellende Lösung gefunden hat. So hilft der Beitrag, eine offene Forschungsfrage der Digital Humanities genauer zu bestimmen.
         Das Projekt “Das Beichthandbuch des Martín de Azpilcueta und das Phänomen der Epitomierung” untersucht anhand der Entwicklung eines besonderen Texts und seiner Entwicklung den Wandel normativen Wissens in der Rückkopplung mit diversen Praxiszusammenhängen: Der spanische Kirchenrechtler Martín de Azpilcueta (1492-1586) publizierte 1549 sein “Manual de Confesores y Penitentes” mit Regeln für Verfahren und Beurteilung von Beichten. Die ursprüngliche Publikation erschien auf Portugiesisch, allein zu Azpilcuetas Lebzeiten folgten noch über 60 weitere Editionen, in denen der Autor selbst Übersetzungen und Anpassungen vornahm, etwa um auf Beschlüsse des Konzils von Trient einzugehen, oder um das Werk anderen Adressatenkreisen zu erschließen (vgl. Bragagnolo 2018).
         Unser Korpus umfasst zunächst 5 zwischen 1549 und 1573 gedruckte Editionen. Zwei auf portugiesisch: (A) Coimbra 1549, 8°, 720 Seiten umfassend, (B) Coimbra 1552, 8°, 1.000 S.; zwei auf spanisch: (C) Coimbra 1553, 4°, 588 S. und (D) Salamanca 1556, 4°, 813 S.; und auf Latein (E) Rom 1573, 4°, 1.136 S. Wir gehen von drei verschiedenen Transformationsmodi aus: Änderungen des Inhalts innerhalb einer Sprache (A → B, C → D); Übersetzung in eine andere Sprache ohne größere Änderungen des Inhalts (B → C); Übersetzung unter gleichzeitiger Änderung des Inhalts (D → E).
         
            
               
            
         
         Ein erster Beitrag digitaler Methoden zur Analyse dieser Entwicklungen besteht in der systematischen Alignierung von Texten der verschiedenen Versionen über Modifikationen und Übersetzungen hinweg. Wir diskutieren im Folgenden verschiedene Ansätze der automatischen Alignierung von sogenannten Bitexten und wie diese Ansätze sich in der Konfrontation mit den Besonderheiten des Projekts (historisches Vokabular, Orthographie und Grammatik, publizistische oder typographische Eigenheiten in den Texten, inhaltliche Überarbeitungen in den Übersetzungen usw.) bewähren. Ein wichtiger Gesichtspunkt sind dabei immer auch die Art, der Umfang und die Auswirkungen der nötigen manuellen/intellektuellen Vor- und Nachbereitungen.
         Für die Evaluation der verschiedenen Ansätze alignieren wir einen Teil der im Projekt als TEI XML transkribierten Texte in der LERA Umgebung1 manuell. Da die Texte zum Teil umfangreiche Überarbeitungen enthalten, wird zu sehen sein, ob automatische Methoden der Evaluation (wie Papineni et al. 2002 oder Lin/Och 2004) Verwendung finden können, oder ob doch auf eine manuelle Evaluation zurückgegriffen werden muss (ähnlich Darriba Bilbao et al. 2005).
            
         
            I. Statistische Modi
            Im ersten Teil diskutieren wir Algorithmen, die ausblenden, dass es sich bei unseren Daten um symbolische bzw. sprachliche Ausdrücke handelt. Sie werden gleichsam jeweils als “rohe” Datenmengen verstanden, die auf statistische Weisen vermessen werden können und es werden Übereinstimmungen in den Mustern oder in den Intervall-Längen zwischen spezifischen Datenpunkten gesucht.2 Obwohl in allen Fällen bestimmte Besonderheiten des historischen Forschungsgegenstands zu Komplikationen führen, ist die Leistungsfähigkeit dieser Ansätze nicht zu unterschätzen. Denn ihre Unzulänglichkeiten sind weitgehend mit jenen besonderen Zusammenhängen historischer Texte verschränkt, in denen ohnehin manuell nach- oder vorgearbeitet werden muss, und es ist nicht von vornherein auszuschließen, dass es sich lohnen könnte, mit manuellem Aufwand die Texte besser vorzubereiten, um dann mit diesen Ansätzen sehr gute Ergebnisse erzielen zu können.
                
            
               Für die meisten Methoden der computergestützten Übersetzung 
                    (Machine Translation) stellt der 
                        Satz die grundlegende Einheit der Übersetzung dar und es haben sich eine Reihe von Ansätzen etabliert, die zur Erkennung von Satzkorrelationen in Bitexten allein auf die bloße Satzlänge als eines der besten Maße für die Wahrscheinlichkeit abstellen, mit der ein Satz im zu untersuchenden Dokument die Übersetzung eines Referenz-Satzes aus dem Quell-Dokument ist.3 Die Unterschiede in den typischen Satzlängen zwischen zwei Sprachen schlagen sich offenbar in allen Sätzen eines Dokuments in ähnlicher Weise nieder, so dass sich in zwei Dokumenten die Verhältnisse der Satzlängen zueinander stark ähneln. Fälle, in denen allzu kurze Sätze beim Übersetzen verbunden, oder sehr lange Sätze aufgeteilt werden, werden mit geringerer Genauigkeit erkannt; wie häufig dieser Fall aber vorkommt, hängt von den involvierten Sprachen und Übersetzern ab.
                    
               Ein zweiter Ansatz aus dem 
                        Machine Translation-Umfeld sind geometrische Ansätze (vgl. Melamed 1999). Sie basieren auf der Annahme, dass es ausreichend sein müsste, sehr grob markierte “Kandidaten” für Satzkorrespondenzen in die richtige Reihenfolge zu bringen. Mit anderen Worten liegt der Fokus nicht auf der eigentlichen Übereinstimmung, sondern auf der Position im Text: Die Ausgangsannahme ist, dass die zu vergleichenden Texte synchron fortschreiten und der erste Satz im einen Text den ersten Satz im anderen übersetzt, der zweite den zweiten usw. Diese Annahme kann in einem durch den Fortschritt in beiden Texten aufgespannten Koordinatensystem als ansteigende Diagonale repräsentiert werden. In einer geometrischen Betrachtung wird dann versucht, durch Umsortierung der vorgefundenen Sätze, die Punkte an diese Diagonale anzunähern.
                    
               Da unsere Texte in eng verwandten Sprachen vorliegen – Portugiesisch, Spanisch und Latein –, erscheint es lohnenswert, auch mit Ansätzen, die Übereinstimmungen auf der Ebene von Wortstämmen oder -fragmenten untersuchen, einen Versuch zu unternehmen (vgl. Darriba Bilbao et al. 2005). Wir untersuchen also Ähnlichkeiten in den Vektorräumen für die vorkommenden 3- und 4-Gramme.4
               
            
         
         
            II. Lexikalische Modi
            Eine zweite Menge von Methoden der 
                    Machine Translation verarbeitet die Daten nur in sprachlogisch aufbereiteter Form, hebt insbesondere auf die übereinstimmende Bedeutung der sprachlichen Ausdrücke ab und setzt viele “klassische DH”-Ansätze ein (vgl. Ma 2006). Diese Ansätze setzen Arbeitsschritte wie Tokenisierung und Stemmatisierung oder Lemmatisierung voraus und in unseren Experimenten evaluieren wir verschiedene weitere, optionale Schritte, um zunächst zu einer treffenden Charakterisierung 
                    eines Textes zu gelangen. Dies wird für beide Sprachversionen vorgenommen, bevor dann diese “konzentrierten” oder “gefilterten” Charakterisierungen endlich auf der Basis eines Wörterbuchs 
                    miteinander verglichen werden.5
            
            Die von uns evaluierten optionalen Schritte zur Etablierung einer Charakteristik von Sätzen sind (a) “Filter” wie Stopwörter und TF/IDF-Topwerte und (b) “Booster” wie stärker gewichtete Zahlen, Zahlwörter und Named Entities. Offenkundig hängt allerdings das Ergebnis der Vergleiche in dieser zweiten Perspektive mindestens ebensosehr von der Qualität der Wörterbücher wie von der Leistung und der Auswahl der vorgeschalteten “Charakterisierungs”-Algorithmen ab. Daher legen wir ein besonderes Augenmerk auf das relative Gewicht der Qualität des Wörterbuchs und ihrer manuellen Verbesserung auf der einen, des Aufwands und Gewinns beim Einsatzes von Filtern und Boostern auf der anderen Seite.
         
         
            III. Wissensbasierte Modi
            Abschließend stellen wir mit der Graphanalyse eine Perspektive vor, die in aktuellen Diskussionen zur sprachübergreifenden Plagiatserkennung diskutiert wird und eine Modellierung des im Text beschriebenen Wissens unternimmt (vgl. Franco-Salvador/Rosso/Montes-y-Gómez 2016). Anstelle eines Wörterbuchs zur Überbrückung des Sprachunterschieds wird hier ein semantisches Netz – in unserem Beispiel BabelNet (Navigli/Ponzetto 2012) – verwendet, um die Wörter der Texte mit “sprachunabhängigen” Konzepten zu verbinden, die untereinander in taxonomischen, synonymen, kontradiktorischen u.a. Beziehungen stehen. Dabei wird durch die Texte jeweils ein Ausschnitt eines umfassenderen Begriffsgraphen instanziiert, um anschließend die resultierenden Teilgraphen miteinander zu vergleichen. Dies erlaubt die Disambiguierung der verwendeten Wörter und eine differenziertere Vergleichsbasis durch die Einbeziehung des semantischen Kontexts der verglichenen Textpassagen. Die Konstruktion und die Vergleiche der zahlreichen Teilgraphen sind offenkundig rechenintensivere Aufgaben, und im Übrigen setzt der Ansatz ebenfalls Schritte wie Tokenisierung und Lemmatisierung voraus, so dass der mögliche Gewinn in der Vergleichsgenauigkeit hier durch einen höheren Aufwand erkauft wird, der zu einem kaum verminderten Aufwand der Textaufbereitung (z.B. der Normalisierung) hinzu kommt.
         
         
            Diskussion
            Wir diskutieren im Rahmen des Beitrags insbesondere, welche Komplikationen sich in der Arbeit in der Folge von Besonderheiten unseres Fragezusammenhangs und Materials gezeigt haben, und wie diese sich auf die unterschiedlichen Ansätze jeweils auswirken. Als wichtige Faktoren konnten wir historische Orthographie und Abkürzungen, uneindeutige und inkonsistente Interpunktion sowie die Kodierung von Layout-Besonderheiten wie Fußnoten identifizieren und so versuchen wir zu bestimmen, welchen Gewinn entsprechende manuelle Vorarbeiten wie Satzsegmentierung, absatzweise Alignierung, Verbesserung des Wörterbuchs, Normalisierung von Schreibungen und Typographie erzielen können.
         
      
      
         
             Paul Molitor, Jörg Ritter et al.: LERA - Locate, Explore, Retrace and Apprehend complex text variants, eine im Rahmen des vom Bundesministerium für Bildung und Forschung (BMBF) geförderten Projekts SaDA - Semi-automatische Differenzanalyse von komplexen Textvarianten erstellte Arbeitsumgebung.
             Im linguistischen Kontext entspricht die Mustersuche des ersten Ansatzes etwa einer n-Gramm-Analyse, und wenn die herausgehobenen Datenpunkte des zweiten Ansatzes Repräsentationen von Interpunktionszeichen sind, entspricht dieser einer Analyse der Satzlängen. Obwohl die Auswahl der verwendeten Maße so durchaus durch sprach- und texttheoretische Überlegungen inspiriert und angeleitet ist, sind die Maße selbst von diesen Motiven doch im Grunde vollkommen unabhängig und könnten in gleicher Weise mit ganz anders gearteten Datenreihen angewandt werden. (In Sankoff/Kruskal 1983 etwa werden Anwendungen der Sequenz-Alignierung in ganz anderen Feldern beschrieben.)
             Die frühesten Versuche in dieser Richtung wurden wohl im IBM Machine Translation Lab unternommen; vgl. Brown et al. 1990. Klassisch wurde der Algorithmus und der Aufsatz von Gale/Church 1993; aktueller, mit weiteren Methoden kombiniert und auf sog. “low ressourced languages” zielend etwa bei Varga et al 2005.
             Als zusätzliche Dimension haben wir dem Vektorraum die Position des jeweiligen Satzes im Text sowie die Behandlung der benachbarten Sätze hinzugefügt, so dass von zwei Satzpaaren mit gleichen n-Gramm-Häufigkeiten dasjenige den Vorzug erhalten kann, dessen Sätze näher beieinander liegen oder das eine mit den benachbarten Sätzen vergleichbare Verschiebung darstellt.
             Ansätze wie Kay/Röscheisen 1993, Fung/Church 1994 oder auch Varga et al. 2005 können ein solches Wörterbuch auf der Basis allein der vorliegenden Texte erstellen.
         
         
            
               Bibliographie
               
                  Manuela Bragagnolo:
                  Les voyages du droit du Portugal à Rome. Le ‘Manual de confessores’ de Martín de Azpilcueta (1492-1586) et ses traductions, 
                        (The Travels of Law from Portugal to Rome. Martín de Azpilcueta’s ‘Manual de confessores’ (1492-1586) and its Translations), Max Planck Institute for European Legal History Research Paper Series No. 2018-13 (https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3287684)
                    
               
                  Peter F. Brown / John Cocke / Stephen A. Della Pietra / Vincent J. Della Pietra / Fredrick Jelinek / John D. Lafferty / Robert L. Mercer / Paul S. Roossin:
                  A statistical approach to machine translation, 
                        in: Computational Linguistics 16 (1990): 79-85, https://dl.acm.org/citation.cfm?id=92860.
                    
               
                  V.M. Darriba Bilbao / J.G. Pereira Lopes / T. Ildefonso:
                  Measuring the impact of cognates in parallel text alignment, 
                        in: Proceedings of the Portuguese Conference on Artificial Intelligence (2005): 338-343. DOI: 10.1109/EPIA.2005.341306.
                    
               
                  Ábel Elekes / Adrian Englhardt / Martin Schäler / Klemens Böhm:
                  Toward meaningful notions of similarity in NLP embedded models, 
                        in: International Journal on Digital Libraries (2018), DOI: 10.1007/s00799-018-0237-y.
                    
               
                  Samuel Fernando / Mark Stevenson:
                  A semantic similarity approach to paraphrase detection, 
                        in: Proceedings of the 11th Annual Research Colloquium of the UK Special Interest Group for Computational Linguistics (2008), 45-52, https://pdfs.semanticscholar.org/d020/eb83f03a9f9c97e728355c4a9010fa65d8ef.pdf.
                    
               
                  Marc Franco-Salvador / Paolo Rosso / Manuel Montes-y-Gómez:
                  A systematic study of knowledge graph analysis for cross-language-plagiarism detection, 
                        in: Information Processing and Management 52 (2016), 550-570. DOI: 10.1016/j.ipm.2015.12.004.
                    
               
                  Pascale Fung / Kenneth W. Church:
                  K-vec: A new approach for aligning parallel texts, 
                        in: Proceedings of the 15th Conference on Computational Linguistics, Vol. 2 (1994), 1096-1102, DOI: 10.3115/991250.991328.
                    
               
                  William A. Gale / Kenneth W. Church:
                  A program for aligning sentences in bilingual corpora, 
                        in: Computational linguistics 19/1 (1993): 75-102, https://dl.acm.org/citation.cfm?id=972455.
                    
               
                  Martin Kay / Martin Röscheisen:
                  Text-translation Alignment, 
                        in: Computational Linguistics 19/1 (1993), 121-142.
                    
               
                  Tom Kenter / Maarten de Rijke:
                  Short Text Similarity with Word Embeddings, 
                        in: CKIM ’15 Proceedings (2015) DOI: 10.1145/2806416.2806475.
                    
               
                  Chin-Yew Lin / Franz Josef Och:
                  Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-bigram Statistics, 
                        in: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics (2004). DOI: 10.3115/1218955.1219032.
                    
               
                  Xiaoyi Ma:
                  Champollion: A robust parallel text sentence aligner, 
                        in: 5th International Conference on Language Resources and Evaluation (LREC) 2006, 489-492.
                    
               
                  Helena de Medeiros Caseli / Maria das Graças Volpe Nunes: 
                  Evaluation of sentence alignment methods for brazilian portuguese and english parallel texts, 
                        in Brazilian Symposium on Artificial Intelligence (SBIA) (2004), 184-193, DOI: 10.1007/978-3-540-28645-5_19.
                    
               
                  I. Dan Melamed:
                  A Portable Algorithm for Mapping Bitext Correspondence, 
                        in: ACL ’98 Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics (1997), 305-312, DOI: 10.3115/976909.979656.
                    
               
                  I. Dan Melamed:
                  Bitext Maps and Alignment via Pattern Recognition, 
                        in: Computational Linguistics 25/1 (1999), 107-130, https://dl.acm.org/citation.cfm?id=973218.
                    
               
                  Robert C. Moore:
                  Fast and accurate sentence alignment of bilingual corpora, 
                        in: S.D. Richardson (ed.): AMTA 2002. Machine Translation: From Research to Real Users, LNCS 2499 (2002), pp. 135-144. DOI: 10.1007/3-540-45820-4_14.
                    
               
                  Roberto Navigli / Simone Paolo Ponzetto:
                  BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network, 
                        in: Artificial Intelligence 193 (2012), 217-250, DOI: 10.1016/j.artint.2012.07.001
                    
               
                  Kishore Papineni / Salim Roukos / Todd Ward / Wei-Jing Zhu:
                  BLEU: A Method for Automatic Evaluation of Machine Translation, 
                        in: Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (2002): 311-318. DOI: 10.3115/1073083.1073135.
                    
               
                  Christian Paul / Achim Rettinger / Aditya Mogadala / Craig A. Knoblock / Pedro Szekely:
                  Efficient graph-based document similarity,
                        in: 
                        H. Sacks et al. (eds.):
                  ESWC ’16 European Semantic Web Conference / LNCS 9678 Lecture Notes in Computer Science 
                        (2016), 334-349, DOI: 10.1007/978-3-319-34129-3_21.
                    
               
                  Alexandr Rosen:
                  In search of the best method for sentence alignment in parallel texts, 
                        in: 
                        R. Garabık (ed.):
                  Computer treatment of Slavic and East European languages. 
                        Third international seminar (2005), 174-185, http://utkl.ff.cuni.cz/~rosen/public/slovko05.pdf.
                    
               
                  David Sankoff / Joseph Kruskal:
                  Time Warps, String Edits, and Macromolecules. The Theory and Practice of Sequence Comparison. 
                        Addison-Wesley (1983).
                    
               
                  André Santos / José João Almeida / Nuno Carvalho: 
                  Structural Alignment of plain text books, 
                        in: LREC ’12 Proceedings of the Eigth International Conference on Language Resources and Evaluation (2012), 2069-2074, http://www.lrec-conf.org/proceedings/lrec2012/pdf/967_Paper.pdf.
                    
               
                  Danica Seničić:
                  Automatic alignment of bilingual sentences. 
                        The case of English and Serbian. M.A. thesis, Louvain, 2016, https://dial.uclouvain.be/memoire/ucl/en/object/thesis%3A11186.
                    
               
                  Daniel Stein:
                  Machine translation: Past, present and future, 
                        in: 
                        Georg Rehm / Felix Sasaki / Daniel Stein / Andreas Witt (eds.):
                  Language technologies for a multilingual Europe, 
                        TC3 III. Language Science Press (2018), pp. 5-17. DOI: 10.5281/zenodo.1291924.
                    
               
                  Joseph P. Turian / Luke Shen / I. Dan Melamed:
                  Evaluation of Machine Translation and its Evaluation, 
                        in: Proceedings of Machine Translation Summit Proceedings of Machine Translation Summit IX (2003), 386-393, https://nlp.cs.nyu.edu/pubs/papers/turian-summit03eval.pdf.
                    
               
                  Dániel Varga / Péter Halácsy / András Kornai / Viktor Nagy / László Németh / Viktor Trón:
                  Parallel Corpora for medium density languages [Hunalign], 
                        in: RANLP ’05 Proceedings of Recent Advances in Natural Language Processing (2005), 247-258, http://kornai.com/Papers/ranlp05parallel.pdf.
                    
               
                  Krzysztof Wołk / Krzysztof Marasek:
                  A Sentence Meaning Based Alignment Method for Parallel Text Corpora Preparation, 
                        in: Advances in Intelligent Systems and Computing 275 (2014), 107-114, DOI: 10.1007/978-3-319-05951-8_22.
                    
            
         
      
   



      
         
            Die Digital Humanities haben sich im Verlauf der letzten zehn Jahre aus einem randständigen Thema an den deutschen Universitäten zu einem etablierten Ausbildungsbereich verwandelt. Die seit Jahren anhaltende Diskussion um konvergente Curricula zeugt von dieser Entwicklung (Sahle 2013). Digitale Editionen waren vor zehn Jahren im deutschsprachigen Raum selten anzutreffen. In ihren Ausprägungsformen waren sie noch sehr unterschiedlich und trugen den Charakter vereinzelter Leuchtturmprojekte, die die Grenzen neuer Verfahren in den Geisteswissenschaften ausloteten. Mittlerweile ist die “digitale Editorik” ein eigener Forschungsbereich. Während Fachkenntnisse in diesem Bereich vor zehn Jahren nur in außeruniversitären Sonderveranstaltungen wie Summer Schools und Workshops erworben werden konnten, gibt es heutzutage an einigen deutschsprachigen Universitäten regelmäßige Lehrveranstaltungen zum Thema, die im Kontext der bisher entstandenen Lehrstühle
            1
             der Digital Humanities verortet sind.
         
         
            Obwohl sich die universitäre Ausbildung in den letzten Jahren merklich und kontinuierlich verbessert hat, ist dennoch der Bedarf nach den Schools des Instituts für Dokumentologie und Editorik (IDE) ungebrochen; dies ist ein deutliches Zeichen, dass die Digital Humanities weiterhin stärker an den Universitäten verankert werden müssen. Daneben bieten die IDE-Schools ein gutes Angebot für InteressentInnen sowohl des außeruniversitären, als auch des postdoktoralen Sektors.
         
         
            Deshalb bleiben komprimierte Angebote jenseits von Studiengängen wie die Veranstaltungsreihe ESU Leipzig
            2
             oder eine Vielzahl vereinzelter Workshops oder Summer Schools
            3
             die einzige Möglichkeit, sich grundlegende Kompetenzen für die von individuellen Forschungsfragen angetriebene Arbeit in den Digital Humanities anzueignen. Daneben wurden auch verschiedene Online-Angebote für E-Learning entwickelt. Für den Bereich der digitalen Editorik seien hier beispielsweise Kurse bei #dariahTeach, Schulungsmaterialien von DiXiT und DARIAH-DE oder Dokumentationen auf Webseiten oder GitHub genannt.
            4
         
         
            Das IDE bietet seit 2008 regelmäßig einwöchige Schools an, die sich auf Themen rund um digitale Editionen konzentrieren. Bis 2018 wurden insgesamt 13 Schools in Wien (4), Köln (2), Chemnitz (2), Graz (2), Berlin (1), Weimar (1) und Rostock (1) mit insgesamt fast 300 TeilnehmerInnen durchgeführt.
            5
             Sie vermitteln wesentliche Kenntnisse für AnfängerInnen und Fortgeschrittene auf dem Gebiet der XML-basierten digitalen Editorik. In der Regel organisieren dabei lokale InteressentInnen die finanziellen und örtlichen Rahmenbedingungen und können grobe inhaltliche Vorgaben machen. Das IDE übernimmt die inhaltliche Ausgestaltung und die Auswahl des Lehrpersonals. Dabei wird in die Planung neuer Schools immer die Auswertung von Evaluationsbögen der vorangegangenen Schools einbezogen.
         
         Das IDE legt Wert darauf, dass die TeilnehmerInnen an ihren eigenen Editionsprojekten arbeiten, um die Motivation, die eigene Arbeit konsequent auf den neuen Methoden aufzubauen, zu erhöhen. Es hält jedoch auch eigene Übungsmaterialien bereit, um den Einstieg durch gemeinsames Erarbeiten der jeweils neuen Lernstoffe sowohl an der “Tafel” und zeitgleich am eigenen Arbeitsgerät zu erleichtern.
         
            Der erfolgreiche Besuch einer School wird stets durch ein Zertifikat bescheinigt, das in manchen Fällen als “credit points” in Studiengängen angerechnet werden konnte. Besonderes Augenmerk wird auf eine gute personelle Betreuung der TeilnehmerInnen durch zusätzliche TutorInnen und einen hohen Praxisanteil für Übungen gelegt. Die Kurse behandeln Basistechnologien wie XML, XSL, XQuery, Python, kontrollierte Vokabularien und Normdaten, editionsrelevante Kapitel der TEI, Metadaten, Text Mining, sowie allgemeine Webtechnologien wie HTML und JavaScript oder neuere Ansätze wie Graphentechnologien. Die konsequente Online-Bereitstellung von Vortragsfolien und Übungsaufgaben auf der Website des IDE ermöglicht auch nachträglich, sich Inhalte der Schools anzueignen und fügt das Angebot in die wachsende Zahl von online verfügbaren Tutorials ein (s.o.). Im Zuge der Schools wurden essentielle Technologien in Flyerform
            6
             kurz zusammenzufassen. Das auf den Schools vermittelte Wissen lässt sich so einerseits direkt nachnutzen, andererseits können diese Angebote auch zeitlich versetzt in andere Schools eingebunden werden.
         
         
            Das Poster wird die mit den Schools gewonnenen Erfahrungen der letzten Jahre zusammenfassen. Es vergleicht die IDE-Schools mit thematisch benachbarten Veranstaltungen,
            7
             analysiert Trends und Konstanten der curricularen Struktur, erhebt statistische Angaben über die BesucherInnen, präsentiert die Sicht der TeilnehmerInnen durch die Auswertung einer umfassenden Befragung und verortet das Angebot im Gesamtfeld der digitalen Editorik bzw. der Digital Humanities im Allgemeinen. Es leistet damit einen Beitrag für die Untersuchung der Vermittlungsformen und Lehrinhalte in der Ausbildungslandschaft der Digital Humanities außerhalb ordentlicher Studiengänge und die Auswirkungen dieser Ausbildungsformen auf Forschung und Karriere.
         
      
      
         
            
               https://dhd-blog.org/?p=6174
            
            
               http://www.culingtec.uni-leipzig.de/ESU_C_T/node/97
            
             Siehe z.B. Digital Humanities at Oxford Summer School (https://digital.humanities.ox.ac.uk/dhoxss/), Digital Humanities Summer Institute der University of Victoria (http://www.dhsi.org/courses.php).
             Siehe https://teach.dariah.eu/; zu den Schulungsmaterialien im Rahmen des Marie Sklodowska Curie Doktorandenprogramms DiXiT: http://dixit.uni-koeln.de/programme/materials/; zu den Lehrmaterialien von DARIAH-DE siehe exemplarisch “Digitale Textedition mit TEI” von Christof Schöch: https://de.dariah.eu/tei-tutorial; eine Workshop-Dokumentation unter https://www.lib.ncsu.edu/workshops/introduction-to-xml-and-digital-scholarly-editing-using-the-text-encoding-initiative-tei-1, ein Github-Repository unter https://github.com/slstandish/lrbs-scholarly-editing.
             Zur Dokumentation der Schools siehe https://www.i-d-e.de/aktivitaeten/schools/.
            
               https://www.i-d-e.de/publikationen/weitereschriften/xml-kurzreferenzen/.
             Zu digitalen Editionen siehe z.B. die Reihe „Edirom“ in Paderborn 2013-2018 (https://ess.uni-paderborn.de/) oder als Einzelveranstaltungen Madrid "Edición digital académica" 2015 (https://extension.uned.es/actividad/idactividad/9408) und 2016 (https://formacionpermanente.uned.es/tp_actividad/idactividad/8680, München “Digital Humanities” 2017 (https://dhmuc.hypotheses.org/summerschool-2017), Prag 2017 (https://praguebeast.hypotheses.org/program) und Grenoble 2018 (https://edeen.sciencesconf.org/).
         
         
            
               Bibliographie
               
                    Digital Humanities als Beruf. Fortschritte auf dem Weg zu einem Curriculum. Akten der DHd-Arbeitsgruppe 
                        
                     "
                  Referenzcurriculum Digital Humanities
                        
                     "
                  . Graz 2015.
                    
               Digital Humanities Course Registry. Dariah/Clarin 2014-2018. 
                        https://registries.clarin-dariah.eu/courses/
               
               
                  Fritze, Christiane / Rehbein, Malte (2012):
                  Hands-On Teaching Digital Humanities: A Didactic Analysis of a Summer School Course on Digital Editing, 
                        in: 
                        Hirsch, Brett D. (ed.): 
                  Digital Humanities Pedagogy: Practices, Principles and Politics [Online]. 
                        Cambridge: Open Book Publishers. 
                        
                     http://books.openedition.org/obp/1617
                  
               
               
                  Henny, Ulrike (2012): 
                  Digitale Editionen – Methoden und Technologien für Fortgeschrittene 
                        [Tagungsbericht zur IDE-School, Chemnitz 2012], in: 
                        H-Soz-Kult, 11.12.2012, 
                        www.hsozkult.de/conferencereport/id/tagungsberichte-4540
               
               
                  Locke, Brandon T. (2017): 
                  Digital Humanities Pedagogy as Essential Liberal Education: A Framework for Curriculum Development, 
                        in: DHQ 11.3 (2017). 
                        
                     http://www.digitalhumanities.org/dhq/vol/11/3/000303/000303.html
                  
               
               
                  Neuber, Frederike (2015): 
                  Spring in Graz – Sunshine and X-technologies 
                        [Bericht zur IDE-School Graz 2015], in: DiXiT Blog 26.4.2015. 
                        https://dixit.hypotheses.org/633
               
               
                  Sahle, Patrick (2008):
                  Digitale Editionen – Methodische und technische Grundfertigkeiten 
                        [Tagungsbericht zur IDE-School, Köln 2008], in: H-Soz-Kult, 21.11.2008, 
                        www.hsozkult.de/conferencereport/id/tagungsberichte-2353
               
               
                  Sahle, Patrick (2013): 
                  DH studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities. 
                        (= DARIAH-DE Working Papers Nr. 1). Göttingen: GOEDOC. 
                        http://nbn-resolving.de/urn.nbn.de.gbv:7-dariah-2013-1-5
               
            
         
      
   



      
         
            EINLEITUNG
            In den vergangenen 30 Jahren ist ein beträchtlicher Teil des in Deutschland gedruckten Materials aus der Zeit von 1500 bis ca. 1850 in mehreren, durch die Deutsche Forschungsgemeinschaft (DFG) geförderten Kampagnen in den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke des 16.-18. Jahrhunderts (VD16, VD17, VD18) zunächst nachgewiesen und seit 2006 digitalisiert worden. Zusätzlich vorliegender Volltext wird mittlerweile auf breiter disziplinärer Front als Schlüssel zu einer ganzen Reihe von geistes- und kulturwissenschaftlichen Forschungsfragen gesehen und gilt zunehmend als elementare Voraussetzung für die Weiterentwicklung der transdisziplinär arbeitenden Digital Humanities. Deshalb werden bereits an verschiedenen Stellen OCR-Verfahren angewendet; viele dieser Unternehmungen haben allerdings noch sehr starken Projektcharakter. Die informationswissenschaftliche Auseinandersetzung mit OCR kann an der großen Zahl wissenschaftlicher Studien und Wettbewerbe ermessen werden, die Möglichkeiten zur Verbesserung der Textgenauigkeit sind in den letzten Jahrzehnten enorm gestiegen. Der Transfer der auf diesem Wege gewonnenen, oftmals sehr vielversprechenden Erkenntnisse in produktive Anwendungen ist jedoch häufig nicht gegeben: Es fehlt an leicht nachnutzbaren Anwendungen, die eine qualitativ hochwertige Massenvolltextdigitalisierung aller historischen Drucke aus dem Zeitraum des 16. bis 19. Jahrhundert ermöglichen. 
                    Auf dem DFG-Workshop „Verfahren zur Verbesserung von OCR-Ergebnissen“ (Deutsche Forschungsgemeinschaft 2014) im März 2014 formulierten Expertinnen und Experten daher folgende Desiderate um die Weiterentwicklung von OCR-Verfahren zu ermöglichen. Es bestehe eine dringende Notwendigkeit für freien Zugang zu historischen Textkorpora und lexikalischen Ressourcen zum Training von vorhandener Software zur Texterkennung bestehe. Ebenso müssen Open-Source-OCR-Engines zur Verbesserung der Textgenauigkeit weiterentwickelt werden, wie auch Anwendungen für die Nachkorrektur der automatisch erstellten Texte. Daneben sollten Workflow, Standards und Verfahren der Langzeitarchivierung mit Blick auf zukünftige Anforderungen an den OCR-Prozess optimiert werden. Als zentrales Ergebnis dieses Workshops stand fest, dass eine koordinierte Fördermaßnahme der DFG notwendig ist. Die „Koordinierte Förderinitiative zur Weiterentwicklung von Verfahren der Optical Character Recognition (OCR)“, kurz OCR-D, begann im September 2015 und versucht seitdem einen Lückenschluss zwischen Forschung und Praxiseinsatz, indem für die Entwicklungsbedarfe Lösungen erarbeitet und der aktuelle Forschungsstand zur OCR mit den Anforderungen aus der Praxis zusammengebracht werden. 
                
         
         
            ARBEITEN IM PROJEKT OCR-D
            Das Vorhaben hat zum Ziel, einerseits Verfahren zu beschreiben und Richtlinien zu erarbeiten, um einen optimalen Workflow sowie eine möglichst weitreichende Standardisierung von OCR-bezogenen Prozessen und Metadaten zu erzielen, andererseits die vollständige Transformation des schriftlichen deutschen Kulturerbes in digitale Forschungsdaten in (xml-strukturierter Volltext) konzeptionell vorzubereiten. Am Ende des Gesamtvorhabens (d.h. unter Einschluss der Modulprojektphase) sollte ein in allen Aspekten konsolidiertes Verfahren zur OCR-Verarbeitung von Digitalisaten des schriftlichen deutschen Kulturerbes stehen und eine Dokumentation, die Antworten auf die damit verbundenen technischen, informationswissenschaftlichen und organisatorischen Probleme und Herausforderungen gibt sowie Rahmenbedingungen formuliert.
            Das Projekt ist in zwei Phasen geteilt: In der ersten Phase hat das Koordinierungsgremium von OCR-D Bedarfe für die Weiterentwicklung von OCR-Technologien analysiert und sich intensiv mit den Möglichkeiten und Grenzen der Verfahren zur Text- und Strukturerkennung auseinandergesetzt. Zahlreiche Gespräche mit ExpertInnen aus Forschungseinrichtungen und Bibliotheken sowie Sichtung vorhandener Werkzeuge aber auch Betrachtung vorhandener Textsammlungen sowie aktueller und geplanter Digitalisierungsvorhaben mündeten in der Erkenntnis, dass der Lückenschluss zwischen Wissenschaft und Praxis das primäre Desiderat im Bereich der Textdigitalisierung darstellt. Zudem hat sich im Lauf der ersten Projektphase eine technologische Wende auf dem Gebiet der Zeichenerkennung vollzogen - an die Stelle traditioneller Verfahren der Mustererkennung, die auf einer Segmentierung von Textabschnitten in Zeilen, Wörter und schließlich einzelne Glyphen basieren, die anschließend aufgrund charakteristischer Merkmale (z.B. Steigung an Kanten) erkannt werden, ist eine zeilenorientierte Sequenzklassifizierung auf Basis statistischer Modelle, insbesondere verschiedener Arten neuronaler Netze (sog. 
                    Deep Learning), getreten. Grund für diesen Technologiewechsel ist die vielfach nachgewiesene Überlegenheit segmentierungsfreier Erkennungsverfahren bezüglich der resultierenden Textgenauigkeit. Diese Überlegenheit gilt insbesondere für schwierige, historische Vorlagen. Dieser Technologiewandel hat sich bisher nicht oder nur äußerst begrenzt auf die Digitalisierungspraxis ausgewirkt. Der Grund dafür liegt vor allem in den bisher bestehenden Hürden beim Einsatz verfügbarer OCR-Lösungen auf Basis neuronaler Netze. Ohne weitreichende projektspezifische Anpassungen ist ein produktiver Einsatz derzeit nicht möglich. Das betrifft unter anderem die Erstellung passender Erkennungsmodelle, die durch das Trainieren eines neuronalen Netzes auf Basis ausgewählter Ground-Truth-Daten generiert werden. Dafür sind zum einen hochqualitativer und umfangreicher Ground Truth aber auch Erfahrungen bzgl. freier Parameter wie z.B. Anzahl der Trainingsschritte, Lernrate, Modelltiefe unabdingbar. Aus OCR-D heraus ist daher ein Datenset mit Trainings- und Ground-Truth-Daten entstanden, welches für Trainings und Qualitätsanalysen im Vorhaben selber genutzt wird aber auch durch andere Forschungsprojekte nachgenutzt werden kann. Neben der Qualität der Zeichenerkennung sind es vor allem Umfang und Korrektheit der strukturellen Annotationen, die die Utilität eines Volltexts für wissenschaftliche Kontexte determinieren. Auch im Bereich der automatischen Layouterkennung (OLR) gab es innerhalb des bisherigen Projektzeitraums vielversprechende Forschungsergebnisse durch den Einsatz innovativer statistischer Verfahren. Der Übertrag in die Praxis in Form nachnutzbarer Software ist hier jedoch noch nicht gegeben. Kommerzielle OCR-Lösungen ignorieren diesen Bereich weitestgehend und bieten nur minimale Strukturinformationen auf Seitenebene (Text, Tabelle, Abbildung etc.) an. Tiefergehende strukturelle Auszeichnungen (Kapitelstruktur, Bildunterschriften, Inhaltsverzeichnisse) werden daher manuell erfasst und in METS/MODS repräsentiert. Eine Verknüpfung zwischen Struktur und Volltext findet, obwohl technisch möglich, in vielen Digitalisierungsvorhaben nicht statt. Für die philologische, editorische oder linguistische Wissenschaftspraxis bedeutet das eine massive Einschränkung die bspw. eine sinnvolle Transformation in hochstrukturierte Formate wie TEI verhindert. 
                
            Die Erkenntnisse dieser Bedarfsanalyse mündeten in einem OCR-D-Funktionsmodell, welches den Rahmen für die Modulprojekt-Ausschreibung der DFG im März 2017 bot. Vor diesem Hintergrund wurden acht Modulprojekte bewilligt die seit 2018 an Lösungen zur Bildvorverarbeitung, Layouterkennung, Textoptimierung (inkl. Nachkorrektur), zum Modelltraining und zur Langzeitarchivierung der OCR-Daten arbeitet. Die Entwicklungen schöpfen dabei das Potential innovativer Methoden für den gesamten Bereich der automatischen Texterkennung für die Massenvolltextdigitalisierung von historischen Drucken aus. Sie werden anschließend nahtlos in den OCR-D-Workflow zur optimierten OCR-basierten Texterfassung integriert. Das so entstehende OCR-D-Softwarepaket steht damit Kultureinrichtungen wie Forschenden für die automatische Texterkennung als Open-Source-Software zur Verfügung.
            Die meisten Arbeiten werden im Sommer 2019 abgeschlossen sein, aber bereits Anfang des Jahres wird die Alpha-Version einen Einblick in die zu erwartende Gesamtlösung bieten können.
         
         
            ZIEL DES WORKSHOPS
            Der Workshop soll neben der Vorstellung des Projektes und der Software die Gelegenheit bieten selber die Software zu testen und zugleich über Optimierungen und Anforderungen seitens der Wissenschaft an diese Technologien zu diskutieren. Teilnehmende erhalten somit einen exklusiven Einblick in die Entwicklungsarbeit und haben die Möglichkeit proaktiv auf die Arbeiten Einfluss zu nehmen, die Ihren späteren Forschungsalltag begleiten und verbessern soll.
                
         
         
            PROGRAMM
            Der Workshop gliedert sich in drei Abschnitte:
            
               Vorstellung des Projekts OCR-D, des Ground-Truth-Datensets und der Guidelines (30min)
               Demonstration der Eigenentwicklung und eines Test-Workflows (120min)
               Diskussion zu Anforderungen und Optimierungen aus Sicht der Digital Humanities (30min)
            
            Der erste Abschnitt stellt die Hintergründe zum Vorhaben vor und geht auf Besonderheiten der Volltextdigitalisierung von historischen Beständen ein. Anschließend wird das Trainings- und Ground-Truth-Datenset präsentiert, das im Rahmen von OCR-D auf- und weiter ausgebaut wird. Besonders die dazu entwickelten Guidelines geben Hinweise für eine spätere Nachnutzung und die Erstellung eigener Ground-Truth-Daten in anderen Projekten. Der Fokus des Workshops liegt auf dem zweiten Abschnitt, in welche der derzeitige Entwicklungsstand präsentiert wird. Die benötigten Test-Dateien werden auf 
                GitHub1 veröffentlicht. Abgerundet wird der Workshop durch eine Diskussionsrunde zu Anforderungen aus der Wissenschaft heraus an OCR-Techniken und die dafür eingesetzte Software.
                
         
         
            VORAUSSETZUNG
            Teilnehmende benötigen einen eigenen Laptop mit Internetanbindung und Ubuntu 18.04 als Betriebssystem. Alternativ kann auch Windows/Mac OSX mit der Software VirtualBox verwendet werden. Die VM wird den Teilnehmenden vom OCR-D-Projekt vor Ort zur Verfügung gestellt. Die Anzahl der Teilnehmenden ist auf 20-25 begrenzt. Python- und Linux-Kommandozeilen-Kenntnisse sind wünschenswert
         
      
      
         
             OCR-D Git-Hub: https://github.com/OCR-D/
         
         
            
               Bibliographie
               
                  Deutsche Forschungsgemeinschaft (2014):  
                        Workshop 
                        Verfahren zur Verbesserung von OCR-Ergebnissen. 
                        Protokoll zu den Ergebnissen und Empfehlungen des Workshops. 
                        
                     http://www.dfg.de/download/pdf/foerderung/programme/lis/140522_ergebnisprotokoll_ocr_workshop.pdf
                   [Zuletzt abgerufen 07.01.2019]
                    
            
         
      
   



      
         
            Text und Graph
            Zahlreiche geisteswissenschaftliche Fachdatenrepositorien setzen zur Modellierung ihrer Forschungsdaten auf die Richtlinien der Text Encoding Initiative (TEI) und somit auf XML als primäres Datenformat. XML eignet sich sehr gut zur Lösung editorisch-philologischer Aufgabenstellungen. Durch die standardkonforme Auszeichnung der Forschungsgegenstände in TEI werden diese formal und inhaltlich erschlossen. TEI-kodierte Daten beinhalten mannigfaltige semantische Bezüge – aus der Perspektive einer Graphmodellierung sind diese Bezüge jedoch zunächst nur implizit und nicht explizit in den Daten vorhanden (Schrade 2013).
            Während die formale Erschließung geisteswissenschaftlicher Forschungsgegenstände mittels XML-basierter Annotationsmethoden mittlerweile als weit fortgeschritten gelten kann, kann die semantische Erschließung häufig noch verdichtet werden. Zwar wird in den Daten oft das Auftreten bestimmter Ortsnamen, Personennamen, Werktitel etc. annotiert. Dennoch gehen diese Annotationen meist nicht darüber hinaus, anzuzeigen, dass eine bestimmte Entität an einer spezifischen Stelle erwähnt ist. Damit bleibt die Vernetzung der Fachdaten hinter den Möglichkeiten zurück, die Graphentechnologien bieten (Iglesia u.a. 2015; Grüntgens/Schrade 2016).
            Graphentechnologien sind hervorragend für die Modellierung, Speicherung und Analyse semantisch vernetzter Daten – auch verschiedener Modalitäten – geeignet. Einerseits sind in Graphen modellierte Daten hinreichend genau und berechenbar, andererseits bietet die Schema- und Hierarchiefreiheit dieser Datenstrukturierung eine ausreichend große Flexibilität zur Erfassung auch komplexer geisteswissenschaftlicher Sachverhalte (Kuczera 2017).
            Eine gegenseitig ausschließende oder separate Modellierung von Forschungsgegenständen entweder in klassischen Strukturen – linearem Text (etwa in TEI-XML kodiert), hierarchischer Baumstruktur (Ontologien) – oder als Graph ist nicht mehr zwangsläufig. Möglich ist auch die Verknüpfung beider Technologien, so dass das geeignetste Datenmodell für jeden Aspekt der Daten zur Anwendung kommt. Dies erlaubt die Synthese und zusammenfassende Analyse verschiedener Daten- und Objekttypen.
            Darüber hinaus werden die Grenzen zwischen den Technologien zunehmend überbrückt: Die Extraktion von Graphstrukturen aus (annotierten) Texten ist ebenso möglich wie die Modellierung von annotiertem Text als Graph in Form von standoff property markup.
            Es breitet sich also ein Spektrum an Möglichkeiten aus: Von der Ableitung von (ephemeren) Graphen aus (führenden) XML-Texten, über die verlustlose Migration von XML zu Graph, bis zu text-as-a-graph als führendes Datenformat mit geeigneten Editionsumgebungen.
            Die Gangbarkeit jeder dieser Möglichkeiten und ihre Unterstützung durch flexible Werkzeuge soll in diesem Workshop nachvollziehbar gemacht werden, sowie die Grundlagen zur eigenständigen Anwendung gelegt werden.
         
         
            Werkzeuge
            Zur Abdeckung des oben dargestellten Spektrums stellen wir vier Werkzeuge vor, die in der Digitalen Akademie der Akademie der Wissenschaften und der Literatur entwickelt werden. Teilweise eng aufeinander aufbauend, bieten sie einen aktuellen Werkzeugkasten der Graphentechnologien.
            
               XTriples
               XTriples (http://xtriples.spatialhumanities.de) ist ein Webservice zur Extraktion von RDF-Statements aus XML-Daten zur Vernetzung von Ressourcen im semantic web. Dieses Werkzeug ist insbesondere geeignet zur (einmaligen oder wiederkehrenden) Ableitung von RDF-Graphen aus XML-Daten (RDF-Lifting).
               Grundfunktion des generischen Dienstes ist das Crawling beliebiger XML-Datenbestände und die anschließende Generierung semantischer Aussagen aus den XML-Daten auf Basis definierter Aussagemuster. Wird eine Dateneinheit in einer Ressource als das Subjekt einer semantischen Aussage begriffen, können diesem Subjekt über Prädikate aus kontrollierten Vokabularen weitere Werte aus den XML-Daten bzw. URIs zu weiteren Datenressourcen als Objekte zugeordnet werden. Im Übersetzungsvorgang zwischen XML und RDF geht es also vor allem um die Bestimmung semantischer Aussagemuster, die sich gesamthaft auf alle Ressourcen eines XML-Datenbestandes anwenden lassen.
               Die Aussagemuster werden in Form einer einfachen, XPath-basierten Konfiguration an den Dienst übermittelt. Dabei ist es auch möglich, über die Bestände eines spezifischen XML-Repositoriums hinauszugehen und externe Ressourcen oder Dateneinheiten in die Transformation mit einzubeziehen (bspw. aus der GND, der DBpedia, aus GeoNames u.a). Die technische Realisierung als Webservice hat den Vorteil, dass AnwenderInnen keine weitere Software zur semantischen Übersetzung von Forschungsdaten benötigen.
            
            
               eXGraphs
               eXGraphs ist der Ausbau des Grundprinzips von XTriples zur Extraktion von property graphs, d.h. Graphstrukturen, die über die Subjekt-Prädikat-Objekt-Tripelstruktur von RDF hinausgehen. Der Dienst ist so weit generalisiert, dass grundsätzlich keine Einschränkungen der Komplexität der extrahierten Graphen bestehen.
               eXGraphs basiert auf der Graphdatenbank neo4j, das heißt es importiert entweder die gewonnenen Graphen direkt in eine spezifizierte Datenbank, oder oder gibt sie als Cypher-Abfrage zurück. Das Tool ist somit geeignet, Datenbestände von XML in gerichtete Property-Graphen zu migrieren oder wiederkehrend zur Aktualisierung der Graph-Datenbank aufgerufen zu werden. Die Konfiguration der Extraktion und Transformation wird in einer unkomplizierten XML-Konfiguration spezifiziert, deren hierarchische Struktur direkt mit den notwendigen Extraktionsschritten korrespondiert. Die gesuchten Informationen werden mittels XPath angesteuert.
            
            
               GRACE
               GRACE (
                     graph content editor) ist eine Web-App, die das Erstellen und Pflegen von Graphdaten in neo4j-Datenbanken über eine GUI anwenderfreundlich ermöglicht. Unterstützt ist die Suche nach bestehenden Daten, das Verknüpfen von bestehenden Knoten mittels neuer Kanten, das Bearbeiten von Knoten, und die Neuanlage von Knoten. Die Attribute (properties) der Knoten werden als Tabelle bzw. bei der Bearbeitung als Formular dargestellt; das Nutzererlebnis ist also durchaus einer klassischen Datenbankeingabe oder Registerpflege vergleichbar.
               Gegenüber einer klassischen Sacherschließung innovativ ist, dass Kanten zur Modellierung von Beziehungen, Zusammengehörigkeiten – generell für semantische Relationen verwendet werden können. Die Flexibilität und Aussagekraft ist Querverweisen klar überlegen, da die Kanten grundsätzlich klassifiziert sind und durch Attribute weiter spezifiziert werden können. Die Darstellung und Verwaltung der Kanten ist in die Nutzeroberfläche integriert, so dass die Sacherschließung im Graphen ohne Kenntnisse von Datenbankabfragesprachen aufgebaut werden kann.
            
            
               SPEEDy
               SPEEDy (s
                  tandoff 
                  p
                  roperty 
                  edi
                  tor, https://github.com/argimenes/standoff-properties-editor) ist ein Editor zur Bearbeitung von text-as-a-graph – sowohl zur nativen Erfassung wie auch zur Weiterpflege von Datenbeständen nach Konvertierung.
               Bei standoff properties werden Text und Annotationen voneinander getrennt gespeichert. Im Unterschied zu Standoff XML sind die in SPEEDy verwendeten standoff properties resistent gegen nachträgliche Änderungen, da der Editor die Indizes nach jeder Bearbeitung neu berechnet.
               Mit diesem Konzept sind überlappende und auch konkurrierende Annotationshierarchien möglich. Annotationen lassen sich auch in Layern organisieren und in SPEEDy ein- und ausblenden.
               Gespeichert werden die Texte im json-Format, wobei in der json-Datei als erstes der reine Text und anschließend die verschiedenen Annotationen abgelegt sind.
               Mit dem in SPEEDy realisierten Annotationskonzept mit standoff properties werden multiple Annotationshierarchien möglich, die perspektivisch auch den wissenschaftlichen Diskurs abbilden könnten.
            
         
         
            Material
            Für den Workshop werden Beispieldaten aus den Sozinianischen Briefwechseln1 herangezogen, die derzeit erfasst und annotiert werden. Charakteristikum dieser Korrespondenzen ist die enge Verzahnung verschiedener Themengebiete – beispielsweise werden astronomische Beobachtungen von Person zu Person entlang akademischer und familiärer Verbindungen weiter berichtet und von Ort zu Ort weitergetragen, um politisch und theologisch interpretiert zu werden … Dieses komplexe Ineinandergreifen der Themenfelder in den Korrespondenzen erfordert eine entsprechend verzahnte Registerstruktur, die die diversen Relationen zwischen Entitäten verschiedener Art adäquat abbilden kann.
                
            Die Brieftexte werden im TEI-Subset DTABf kodiert, das eindeutige Kodierungen und verlässliche Extraktion von Informationen ermöglicht.
            Als Gegenstand der Übungen stehen die Korrespondenzmetadaten im correspDesc-Format wie auch das named entity tagging und die Sacherschließung im Brieftext zur Verfügung. Ersteres bildet bereits das Netzwerk von Korrespondenten und Orten ab; zweiteres gewährt Einblick in die inhaltlichen und thematischen Verknüpfungen. Die zugehörigen Register, auf die die Annotationen verweisen, werden sowohl im Ausgangs-XML-Format wie auch in Form des Graphregisters Teil der Übungsdaten sein.
         
         
            Ablauf
            Nach einer kurzen einführenden Standortbestimmung der Graphentechnologien in den DH und einer Vorstellung der Beispieldaten werden in den zwei Workshoptagen die vier oben genannten Werkzeuge vorgestellt. Zu jedem Tool zeigen wir Beispielkonfigurationen bzw. -anwendungen, und bieten Übungen an, die praxisnah an die Forschungsziele des datengebenden Projekts angelehnt sind. Darüber hinaus steht es den TeilnehmerInnen frei, auch mit eigenen Daten zu experimentieren.
            Am ersten Workshoptag gehen wir auf die Werkzeuge XTriples und SPEEDy ein. Damit zeigen wir Optionen auf, welche ohne eine Migration von XML zum Graph zur Verfügung stehen: die Beibehaltung von XML als führendes Format, oder die native Erfassung in text-as-a-graph. In der Übung demonstrieren wir die Erzeugung einer RDF-Datei zur Verknüpfung von Ortserwähnungen mit einer geographischen Normdatenbank.
            Am zweiten Workshoptag liegt unser Fokus auf eXGraphs und GRACE. Am Ende dieses Tages sollen die Grundprinzipien des Zusammenspiels von XML und neo4j klar geworden sein, indem Forschungsdaten zu neo4j migriert und aktualisiert werden, und dort in einer für ein breites Nutzerspektrum zugänglichen GUI bearbeitet werden.
            Beschlossen werden soll der Workshop neben dem ausleitenden Resümee durch eine Feedback-Runde im Plenum insbesondere zu den neu entwickelten Werkzeugen eXGraphs und GRACE, sowie zur weiteren Entwicklung von SPEEDy.
         
         
            Lernziele
            Ziel des Workshops ist, einen Einblick in die Durchlässigkeit zwischen traditionellen (linearen und hierarchischen) Datenstrukturen und der Modellierung im Graphen zu bieten. Die Verwendung von Transformations-, Migrations- und Bearbeitungswerkzeugen wird praktisch vermittelt und ihre Position im DH-Ökosystem umrissen. Die interaktive Demonstration wird anwendungsnah auf reale Forschungsdaten und Auswertungsziele aufgebaut. Neben der technischen Kompetenz wird das Bewusstsein für implizit vorhandene und semantisch auswertbare Graphstrukturen in bestehenden XML-Daten geschärft.
            Die Übungen werden ausgehend von einem Einstiegsniveau konzipiert, mit der Option, zu höheren Komplexitätsstufen weiterzuarbeiten oder die Methoden auf eigene Daten und Fragestellungen zu transferieren.
            Zwei der vorgestellten Werkzeuge sind Neuentwicklungen, so dass dies eine der ersten Gelegenheiten zur Schulung in der Anwendung sein wird.
         
         
            Zahl der TeilnehmerInnen
            Maximal 20.
         
         
            Technische Voraussetzungen
            Die Teilnehmenden benötigen nur Laptops. Es muss im Vorfeld keine Software installiert werden.
         
         
            Beitragende
            
               Julian Jarosch
               Akademie der Wissenschaften und der Literatur | Mainz
                        Geschwister-Scholl-Str. 2
                        55131 Mainz
                    
               2007–2014 Studium der Allgemeinen Sprachwissenschaft an der Johannes Gutenberg-Universität Mainz. 2011 Auslandssemester an der Bangor University, Wales. Magisterarbeit zu »Typography and Legibility: Do Typeface, Serifs and Justification influence Reading Behaviour?«. Seit 2015 wiederum an der JGU Mainz Promotionsprojekt »Empirical Typography« an der Schnittstelle Sprachwissenschaft–Buchwissenschaft, 2015–2017 als Stipendiat der Stipendienstiftung Rheinland-Pfalz. Seit 2018 wissenschaftlicher Mitarbeiter der Digitalen Akademie im DFG-Projekt »Die sozinianischen Briefwechsel«.
            
            
               Andreas Kuczera
               Akademie der Wissenschaften und der Literatur | Mainz
                        Geschwister-Scholl-Str. 2
                        55131 Mainz
                    
               1993-1998 Studium der Physik und Geschichte an der Justus-Liebig-Universität Gießen (Staatsexamen für das Lehramt an Gymnasien), 2001 Promotion »Grangie und Grundherrschaft«. Zur Wirtschaftsverfassung des Klosters Arnsburg als Stipendiat der hessischen Graduiertenförderung. 2001-2006 Mitarbeiter im DFG-Projekt Regesta Imperii Online. Von 2007–2012 leitend in der Projektverwaltung der Akademie Mainz und der Digitalen Akademie tätig. Sachverständiger der IT-Kommission der Akademie Mainz. Seit 2015 Zuständigkeit im Bereich des Projektes Regesta Imperii.
            
            
               Torsten Schrade
               Akademie der Wissenschaften und der Literatur | Mainz
                        Geschwister-Scholl-Str. 2
                        55131 Mainz
                    
               Historiker, Germanist und Anglist, Softwareentwickler und Digitaler Geisteswissenschaftler (seit 2002). Seit 2009 wissenschaftlicher Mitarbeiter der Akademie und Leiter der Digitalen Akademie.
            
            
               Tariq Yousef
               Akademie der Wissenschaften und der Literatur | Mainz
                        Geschwister-Scholl-Str. 2
                        55131 Mainz
                    
               Bachelor in Computer Science (Softwareentwicklung) an der AlBaath Universität (Syrien), Master in Computer Science an der Universität Leipzig. Forschungsinteressen: NLP, Datenextraktion, Datenaufbereitung, data mining, Visualisierung und Webentwicklung.
            
         
      
      
         
            http://www.adwmainz.de/projekte/zwischen-theologie-fruehmoderner-naturwissenschaft-und-politischer-korrespondenz-die-sozinianischen-briefwechsel/informationen.html
         
         
            
               Bibliographie
               
                  Grüntgens, Max / Schrade, Torsten (2016):
                  Data repositories in the Humanities and the Semantic Web: modelling, linking, visualising, 
                        in: 
                        Adamou, A. / Daga, E. / Isaksen, L. (Hrsg.)
                  Hrsg
                  .): Proceedings of the 1st Workshop on Humanities in the Semantic Web (WHiSe), CEUR Workshop Proceedings
                  . Aachen
                  , 
                  S. 53–64.
               
               
                  Iglesia, Martin de la / Moretto, Nicolas / Brodhun, Maximilian
                   (2015): Metadaten, LOD und der Mehrwert standardisierter und vernetzter Daten. In: 
                  Heike Neuroth, Andrea Rapp, Sibylle Söring (Hrsg.): TextGrid: Von der Community – für die Community. Eine Virtuelle Forschungsumgebung für 
                  die Geisteswissenschaften
                  . Göttingen, S. 91–102. DOI: 
                   
                  
                  [Letzter Zugriff 
                  09
                  .
                  01
                  .201
                  9
                  ]
               
               
                  Kuczera, Andreas
                   
                  (
                  2017
                  ):
                   Graphentechnologien in den Digitalen Geisteswissenschaften. abitech 37, 
                  S. 
                  179
                  –196
                  . 
                  
                   
                  [Letzter Zugriff 09.01.2019]
               
               
                  Schrade, Torsten 
                  (2013): Datenstrukturierung. In:
                         
                  Über die Praxis des kulturwissenschaftlichen Arbeitens. Ein Handwörterbuch.
                   Bielefeld: transcript, S. 91–97.
               
            
         
      
   



      
         Gesellschaften fügen sich aus Individuen zusammen. Das gilt auch für die Vergangenheit, aus der die Mehrzahl der Individuen nur schlecht bis gar nicht dokumentiert ist. Es hat sich deshalb ein eigenständiger historischer Forschungsbereich entwickelt, die “Prosopographie”, die sich der Aggregation von Einzelinformationen zu Individuen aus historischen Quellen und ihrer Auswertung widmet (Keats-Rohan 2007). Dieses Forschungsgebiet hat früh digitale Methoden eingesetzt. Der Beitrag widmet sich der Frage, ob die Methoden vergleichbar zu IIIF (International Image Interoperability Framework) in ein „International Proposography Interoperability Framework“ (IPIF) integriert werden können.
         Ein IPIF muss von den Personendatenbanken abweichen, die sich als kontrollierte Vokabularien und Referenzen für Linked Open Data in den Digital Humanities etabliert haben (GND/VIAF, deutsche-biographie), bzw. im Begriff sind, sich zu etablieren (wikidata). Diese berücksichtigen nämlich nicht den Vorgang, mit dem Informationen über eine Person aus historischen Quellen aggregiert werden. Der Ansatz weicht damit auch von der „personography“ der TEI ab, die, wie die Linked-Data-Ressourcen, eine Person mit einer Liste an Eigenschaften beschreiben. Ein IPIF muss dagegen ein Modell realisieren, für das Bradley/Short (2005) die Bezeichnung „Factoid“-Model eingeführt haben. Es geht von drei Informationseinheiten aus: Quelle, Individuum und Aussagen der Quelle über das Individuum. John Bradley hat das Modell mehreren Projekten des King’s College London zu Grunde gelegt (PASE, DPRR, CCEd). Auch das Persondendatenrepositorium (PDR) der Berlin-Brandenburgischen Akademie der Wissenschaften (Neumann et al. 2011) und Projekte, die die Software der BBAW weitergenutzt haben, verwenden das gleiche Modell, auch wenn das PDR nicht explizit auf Bradley referenziert. Ebenso verwendet das Repertorium Academicum Germanicum ein solches dreiteiliges Modell (Andresen 2008). 
         Das dreiteilige Modell impliziert auch, dass (auch widersprechende) Aussagen über dasselbe Individuum aus verschiedenen Quellen an verschiedenen Orten publiziert werden können. Es erscheint also als ein Paradebeispiel für das 
                Web of Data des W3C. Das 
                Web of Data ist die Fortführung der Semantic-Web-Aktivitäten des W3C. Es konzentriert sich auf die Öffnung von Datenbanken und erhebt insbesondere den Anspruch, individuelle kleine Datenmengen als RDF über das Semantic Web abfragbar zu machen. Technisch ist RDF, die Grundlage des 
                Web of Data, eine weit verbreitete und gut unterstützte Technologie. Es ist deshalb auch eine Technologie, mit deren Hilfe immer häufiger Maschinen auf prosopographische Datenbanken zugreifen können. Deshalb haben Bradley/Pasin (2015) eine CIDOC-CRM basierte Version des Factoid-Modells vorgeschlagen und entsprechende Ontologien veröffentlicht (Bradley 2017). Das Basismodell ist aber auch mit anderen Vokabularien realisiert worden: SNAP verwendet z.B. Vokabularien aus dem 
                Linking-Ancient-Wisdom-Projek
               1
            ). Das King’s Digital Lab hat jüngst mit Hilfe von 
                Ontop
            
               2
             die prosopographische Datenbank zur römischen Republik als LOD-Ressource incl. eines SPARQL-Endpoints 
                veröffentlicht.
               3
            
         
         Diese Strategie teilt jedoch das Problem vieler RDF-Ressourcen: Die technische Pflege eines SPARQL-Endpoints ist sehr anspruchsvoll. SPARQL-Endpoints sind hä
               4
             und Core 
            API
               5
             liegen auch Vorschläge vor, derartige API-Definition standardisiert zu beschreiben, so dass die Implementation von einschlägigen API-Anbietern und API-Konsumenten teilweise sogar automatisiert werden 
            kann.
               6
             Aus Sicht des Software-Engineering erscheint es also angemessen, auf eine eigene API-Definition statt auf einen SPARQL-Endpoint zurückzugreifen. Gleichzeitig wird es damit erschwert, Daten aus verschiedenen Datenquellen zu aggregieren, da für jeden Datenanbieter ein eigener API-Konsument programmiert werden müsste. Im Bereich der Bibliotheken hat sich deshalb für die Bereitstellung von Bildern von Büchern mit IIIF eine Kombination aus einem Datenstandard und einer Adressierungs-API durchgesetzt. Es ist an der Zeit, auch für personenbezogene Daten über einen solchen technischen Standard nachzudenken, der die Implementation von Anwendungen erleichtert und die Daten auch praktisch interoperabel macht.
            
         Ein solcher Standard muss von konkreten Anwendungsszenarien ausgehen. Sie können unter den Überschriften „Biographical Lexicon“, „Careers”, „Source Editing“, „Fact Checking“, „New Interpretation“, „Publish a Database”, „Integrate Other Databases“, „Analysis“, „Tool User“, „Tool Builder” zusammengefasst werden. Die Szenarien bilden sowohl Forschung mit prosopographischen Daten wie die Erzeugung solcher Daten ab. Zusätzlich achten die Szenarien darauf, nicht nur explizit prosopographische Workflows zu berücksichtigen, sondern schließen auch wissenschaftliches Edieren als Szenario mit ein, in dem der edierte Text als Beleg für eine Person betrachtet werden kann. In einem Workshop in Wien im Februar 2017 haben Forscher aus dem Themengebiet der Prosopographie religiöser Orden solche Anwendungsszenarien diskutiert und einen Entwurf für eine API entwickelt.
         Ein Ergebnis dieser Arbeit ist eine nach den Standards von OpenAPI beschriebenen Definition einer prosopographischen 
            API.
               7
             Die API baut auf dem dreiteiligen Factoid-Modell auf und erlaubt den Zugriff auf Personen, Aussagen, Quellen und ihr Aggregat, einem „Factoid“. Für alle diese Objekte gibt es eigene Pfade zur Suche und Ausgabe der Daten über die zu ihnen abgelegten IDs. Im Kern der API steht deshalb der Zugriff auf Factoide 
            (/factoid). Sie können individuell über bekannte IDs adressiert werden 
            (/factoid/id). Wichtiger sind aber inhaltliche Filtermöglichkeiten. Sie ergeben sich einfach aus den Eigenschaften des Factoids, als Aussage über eine Person. Die Parameter 
                s
            , 
            st und 
                f lassen also die Suche in den Inhalten der mit dem Factoid verknüpften Quellen 
                (source), Aussagen 
                (statement) und den Metadaten des Factoids selbst 
                (factoid) zu. Dabei ist der Standard eine Volltextsuche. Ebenso lassen sich die Quellen und Personen abfragen. Als Parameter können aber auch Identifikatoren für die einzelnen Informationsgruppen übergeben werden, also z.B. mit 
                /statement/?p_id=Placidus_Seiz alle Aussagen über die Person mit einem Identifikator „Placidus_Seiz“ in einem beliebigen Kontext. Die Anwendung liefert dann ein JSON-Objekt zurück, in dem diese Aussagen formalisiert sind. Zu jeder Aussage gehört eine ID, mit der Entwickler z.B. über die API überprüfen können, woher die jeweilige Aussage stammt.
            
         Als Rückgabewert der API-Definition sind JSON-Serialisierungen vorgesehen. Die Statements können Daten als Text (z.B. der Quelle) ebenso wie strukturiert als Graph enthalten. Die Graphen sollen den Spezifikationen von JSON-LD folgen. Damit können zwei Ziele erreicht werden: Erstens ist damit die Ausgabe der API direkt in Linked-Open-Data-Umgebungen nutzbar, kann prinzipiell auch in einer FROM-Klausel einer SPARQL-Abfrage integriert werden oder in Caching-Mechanismen wie im 2011 als Linked Data Middleware von Virtuoso vorgeschlagenen URI-Burner verwendet werden. Zweitens wird damit ein Standard verwendet, der die Referenzierung der verwendeten Vokabularien und ihre formale Beschreibung mit RDFS und OWL ermöglicht.
         Der Workshop in Wien hat als Kernproblem eines echten Datenaustausches die divergierenden Datenmodelle für die Einzelaussagen über die Individuen identifiziert. Während die Individuen selbst im Factoid-Modell keine beschreibenden Metadaten tragen und damit kaum Probleme beim Datenaustausch erzeugen, sind für die Aussagen über die Individuen je nach Projekt, Verwendungszweck und Forschungsdomäne eine Vielfalt von Vokabularien im Einsatz. Einen Ausweg aus dieser Situation bietet die 2017 gegründete 
            dataforhistory-Initiative.
               8
             Die Initiative arbeitet daran projekt- und domänenspezifische Modellierungen zu erleichtern, die zum CIDOC CRM kompatibel sind. Die derzeitige API-Definition sieht deshalb vor, dass die zurückgegebenen Daten eine Referenz auf ein Schema (in JSON-LD als 
                @context) enthalten müssen, das die verwendeten Klassen und Eigenschaften auf Definitionen im CIDOC CRM abbildet, der es der die API konsumierenden Anwendung erlaubt, die Daten als CIDOC CRM zu interpretieren und darauf aufbauende Operationen durchzuführen. Ergänzend dazu ist ein Parameter 
                format=json/cidoc-crm vorgesehen, bei dem die Transformation serverseitig stattfindet. Die Abbildung auf CIDOC CRM soll insbesondere die grundlegenden Suchoperationen ermöglichen, die Katerina Tzompanaki und Martin Doer 2012 formuliert haben und die im Projekt 
                researchspace
               9
             realisiert werden. Die API definiert die Objekteigenschaft 
                graph für die strukturierte Repräsentation der Daten über Personen.
            
         John Bradley und Michele Pasin haben 2015 eine OWL basierte Ontologie vorgestellt, in der eine „temporal entity documented“ (TED) als Ereignis (E4 und E5 im CIDOC-CRM) oder als eine zeitliche Einheit oder klare zeitliche Grenzen (E3: condition, state) modelliert sind. Das entspricht dem Stand der Diskussion über prosopographische Datenmodelle (Lind 1994, Andresen 2008, Tuominen / Hyvönen / Leskinen 2018).
         Nicht zuletzt der Erfolg von IIIF belegt, dass eine solche API aber auch Referenzimplementationen benötigt. Dabei ist entsprechend der oben beschriebenen Benutzungsszenarien sowohl an Ressourcen zu denken, die Daten bereitstellen, als auch an Anwendungen, die diese Daten konsumieren. Die Nachnutzung des „Archiveditors“, eines zunächst projektinternen Werkzeugs der BBAW, in anderen Projekten zeigt, dass dabei nicht nur an Datenextraktion und –anzeige sondern auch an Datengenerierung zu denken ist. Im Rahmen der Arbeit an der Personendatenbank der Österreichischen Akademie der Wissenschaften ist deutlich geworden, dass gerade automatische Informationsextraktion von „Personenrelationen“ (Schlögl et al. forthcoming, Schlögl et al. 2018) von einer solchen API profitieren kann. Die automatisch generierten Aussagen können als eigenständige Factoide in die Personendatenbanken eingehen. Die Metadaten des Factoids und die Referenz auf die verwendete Quelle stellen sicher, dass sie als automatisch generierte Daten identifizierbar bleiben. Der Vortrag wird Beispiele für Datenangebote aus dem Umfeld mittelalterlicher Urkunden (Register der Urkundenempfänger von Papsturkunden nach den Regesten von August Potthast, Daten aus monasterium.net) und Steuererhebungen (England) vorstellen, und Prototypen für Anwendungen benennen, welche die mit der API bereitgestellten Daten konsumieren können.
      
      
         
            
                http://lawd.info/ontology/
            
             https://github.com/ontop/ontop
            
               
                  http://romanrepublic.ac.uk/rdf
               , Dokumentation von John Bradley: 
                  http://romanrepublic.ac.uk/rdf/doc
               
            
             https://www.openapis.org/ 
             http://www.coreapi.org
             z.B. das Python-Framework Flask in Verbindung mit 
                  https://github.com/zalando/connexion
               , vgl. weitere Tools: 
                  https://swagger.io/tools/open-source/open-source-integrations/
               
            
             https://github.com/GVogeler/prosopogrAPhI
             http://dataforhistory.org
             https://www.researchspace.org/
         
         
            
               Bibliographie
               
                  Andresen, Suse (2008):
                  Das 'Repertorium Academicum Germanicum'. Überlegungen zu einer modellorientierten Datenbankstruktur und zur Aufbereitung prosopographischer Informationen der graduierten Gelehrten des Spätmittelalters, 
                        in: 
                        Sigrid Schmitt u. Michael Matheus (eds.):
                  Städtische Gesellschaft und Kirche im Spätmittelalter (Geschichtliche Landeskunde 62). Stuttgart: Steiner 17-26.
                    
               
                  Bradley, John (2017):
                  Factoids. A site that introduces Factoid Prosopograph, 
                        
                     http://factoid-dighum.kcl.ac.uk/
                   und 
                        
                     https://github.com/johnBradley501/FPO
                  
               
               
                  Bradley, John / Pasin, Michele (2015):
                  Factoid-based Prosopography and Computer Ontologies. Towards an integrated approach, 
                        in: DSH 30,1: 86-97.
                    
               
                  Bradley, John / Short, Harold (2005):
                  Texts into databases. The Evolving field of New-style Prosopography, 
                        in: LLC 20, suppl. 1: 3-24.
                    
               
                  CCEd: 
                  Clergy of the Church of England Database, King’s College London 
                        
                     http://theclergydatabase.org.uk/
                  
               
               
                  DPRR: 
                  Digital Prosopography of the Roman Republic, King’s College London 
                        
                     http://romanrepublic.ac.uk/
                  
               
               
                  Keats-Rohan, Katherine S.B. (ed.) (2007): 
                  Prosopography. Approaches and Applications. A Handbook 
                        (Prosopographica et genealogica 13). Oxford: P&G.
                    
               
                  Lind, Gunner (1994):
                  Data Structures for Computer Prosopography, 
                        in: Yesterday: Proceedings from the 6th International Conference of the Association of History and Computing, Odense 1991. Odense: University Press of Southern Denmark. 77-82.
                    
               
                  Neumann, Gerald / Körner, Fabian / Roeder, Torsten / Walkowski, Niels-Oliver (2011):
                  Personendaten-Repositorium, 
                        in: Berlin-Brandenburgische Akademie der Wissenschaften. Jahrbuch 2010: 320-326.
                    
               
                  PASE:
                  Prosopography of Anglo-Saxon England, King’s College London, URL: 
                        
                     http://www.pase.ac.uk/jsp/index.jsp
                  
               
               
                  Schlögl, Matthias / Katalin Lejtovicz (2018):
                  A Prosopographical Information System (APIS), 
                        in: 
                        Antske Fokkens / ter Braake Serge / Sluijter, Ronald / Arthur, Paul / Wandl-Vogt, Eveline (eds.): 
                  BD-2017. Biographical Data in a Digital World 2017. Proceedings of the Second Conference on Biographical Data in a Digital World 2017. 
                        Linz, Austria, November 6-7, 2017. Budapest: CEUR (CEUR Workshop Proceedings 2119): 53-58.
                    
               
                  Schlögl, Matthias / Lejtovicz, Katalin / Bernád, Ágoston Zénó / Kaiser, Maximilian / Rumpolt, Peter (2018):
                  Using deep learning to explore movement of people in a large corpus of biographies. 
                        Zenodo. 
                        
                     http://doi.org/10.5281/zenodo.1149023
                  
               
               
                  Tuominen, Jouni / Hyvönen, Eero / Leskinen, Petri (2018):
                  Bio CRM. A Data Model for Representing Biographical Data for Prosopographical Research, 
                        in: BD-2017. Biographical Data in a Digital World 2017, hg. v. Antske Fokkens, Serge ter Braake, Ronald Sluijter, Paul Arthur, Eveline Wandl-Vogt, Budapest: CEUR (CEUR Workshop Proceedings 2119): 59-66. 
                    
               
                  Tzompanaki, Katerina / Doerr Martin (2012):
                  Fundamental Categories and Relationships for intuitive querying CIDOC-CRM based repositories, 
                        Technical Report ICS-FORTH/TR-429, April 2012, 
                    
            
         
      
   



      
         
            Einleitung
            Digitale Editionen haben nach einer Phase des Ausprobierens und Entwickelns nunmehr eine Reife erreicht, dass sie in vielen Disziplinen nicht mehr als exotischer Sonder-, sondern als Regelfall angesehen werden, was sich in Publikationen wie  (Apollon et. al 2014; Driscoll/Pierazzo 2016) oder Förderbedingungen (DFG 2015) spiegelt. Trotz dieser greifbaren Fortschritte stehen digitale Editionen nach wie vor in der Kritik. Genannt wird immer wieder die fehlende Stabilität und ungelöste Frage der Langzeitarchivierung und - verfügbarkeit. Doch dieses Gefühl des Mangels, so die These des Beitrags, resultiert nicht aus noch nicht geklärten methodischen oder technischen Fragen, sondern beruht auf einer Fehleinschätzung der Natur digitaler Editionen, die in Analogie zum Druckmedium meist nur von ihrer Oberfläche her beurteilt werden. Mit einem Perspektivwechsel, der die Eigentümlichkeiten digitaler Editionen und die zugrundeliegende strukturellen und algorithmischen Komponenten ernst nimmt, ist indes vergleichbare Stabilität möglich, zumindest wenn man sich über das Dokumentenmodell und über die Form seiner technischen Realisierung verständigt. 
         
         
            Markup und Overlap - auf dem Weg zu einem konsolidierten Dokumentenmodell 
            Die Entwicklung und Nutzung der Markupsprache XML war von Anbeginn an begleitet von Kritik über die Unzulänglichkeit des hierarchischen OHCO Modell (DeRose et al. 1990) für die Repräsentation von Text. Trotz verschiedener Vorschläge konnte bis heute keine abschließende, alle spezifischen Kodierungsprobleme klärende Lösung gefunden werden. Nun hat das der Popularität von XML im Allgemeinen und der in diesem Feld maßgeblichen TEI im Besonderen nicht geschadet. Nach wie vor erfreut sich XML/TEI großer Beliebtheit, auch wenn in jüngerer Zeit der Unterschied von TEI und XML betont wird (Cummings 2017). Das ist umso erstaunlicher, als es an alternativen Ansätzen nicht gemangelt hat (DeRoses 2004; Speerberg-McQueen 2007). Von MECS, GODDAG, TexMECS über LMNL bis zuletzt Text as a Graph (TAGML) entstanden Markup-Konzepte, die für sich in Anspruch nehmen und nahmen, XML und seine Beschränkungen zu überwinden. Gerade mit dem neuesten Konzept des 
                    Text as a Graph (Kuczera 2016; Dekker/Birnbaum 2017) scheint nach dem Selbstverständnis der Autoren nun endlich der Weg aus der Dauerkrise gewiesen. Doch auch dieses Modell, für das ein erster Serialisierungsentwurf vorliegt (Dekker et al. 2018), wirft erneut Fragen auf. Wenn es auch bestechend scheint und mehr Kodierungsflexibilität verheißt, ist doch fraglich, ob die Graphentheorie tatsächlich das Mittel der Wahl ist, um z.B. einem 
                    autopoietischen Textbegriff, wie (McGann 2016) ihn postuliert, Herr zu werden. Auch wenn die Graphentheorie als Modell von großem Nutzen sein kann, da sie uns hilft, mehr „clarity of thought about textuality“ (McGann 2016: 90) zu gewinnen, scheint es klug, in der Frage der „Textdefinition“ Vorsicht walten zu lassen und nicht aus dem Auge zu verlieren, dass es bei der Übersetzung des Textes in maschinenlesbare Form nicht nur darum geht, den “intellektuellen” Textbegriff zu modellieren - als “model of” (McCarty 2005) -,  sondern auch und vor allem darum, das Textverständnis im Sinne eines “model for” zu erweitern. So vermag man einen “Text” zu konzipieren, von dem McCarty sagt, dass er einen “end maker”, keinen “end user” benötigt und der es erlaubt, so etwas wie eine “digitale Pragmatik” oder “digitale Hermeneutik”, vgl. u.a. (Scheuermann 2016), in die digitale Editorik einzuführen - in Anknüpfung an Konzepte von (Robinson 2003), (Shillingsburg 2006)  oder (Gabler 2010). 
                
            Was aber von Anfang an bei der Diskussion um Overlap und die Unzulänglichkeiten von XML vernachlässigt wurde, ist, dass die Limitationen des Textmodells nicht unbedingt mit Limitationen der Serialisierung und der digitalen Technik in eins gesetzt werden können. So ist es zwar richtig, dass SGML und in der Folge XML mit dem OHCO Modell im Kopf entwickelt wurden, doch haben bereits (Renear et al.1993) in ihrer Revision darauf abgehoben, dass im pragmatischen Sinne deskriptives Markup auch unabhängig von der OHCO These verwendet werden kann. Eben weil XML vor allem eine Syntax ist, war es letztlich immer wieder möglich, für „konforme“ Lösungen zu sorgen, wie die Vorschläge der TEI zu nicht-hierarchischen Strukturen (TEI Guidelines: Chap. 20) verdeutlichen, aber auch die Beiträge von Renear zum Konzept des “trojanischen Markup” (Renear 2004) und zur XMLisierung von LMNL in CLIX (Renear 2004) oder xLMNL (Piez 2012). Der Grund lag auch darin, dass XML nicht nur in gut etablierte Strukturen der X-Familie eingebettet ist (XSD, XSLT, XQuery etc.), sondern auch allgemeiner die zentrale Document Object Modell (DOM)-Schnittstelle mit allen wichtigen Webelementen wie HTML bzw. XHTML (HTML 5), CSS oder Javascript teilt. 
         
         
            Multimodalität des Dokumentenmodels - nicht nur Markup
            Trotz aller Experimente ist heute in der Praxis kaum strittig, dass die TEI und das in ihr entwickelte Dokumentenmodel und mit Abstrichen auch ihre Serialisierung in XML das Mittel der Wahl für digitale Editionen ist. Allerdings bleibt das Modell unvollständig, wenn man nicht auch die anderen Komponenten der Edition in die Überlegungen einbezieht. Wie das analoge Buch über die Rolle zum Kodex und über die Handschrift zum Druck gefunden hat, so muss auch das digitale Buch zu einer stabilen Struktur und Form finden, um nicht nur in der Wissenschaft, sondern auch in Gedächtniseinrichtungen wie Bibliotheken langfristig gesichert, reproduziert und als wissenschaftlich referenzierbares Objekt über Schnittstellen zur Verfügung gestellt und genutzt werden zu können. Dabei sind die Besonderheiten digitaler Dokumente bzw. ihre spezifische Dynamik bzw. Potentialität zu beachten (PDF z.B. erfüllt diese Kriterien nicht). So ist es für das Verständnis der digitalen Edition wichtig, in die Debatte um die Zu- oder Unzulänglichkeit bestimmter Markupsprachen auch das eine Edition verwirklichende Ensemble von Dateien und Funktionen einzubeziehen, deren logisches Zusammenspiel zu bestimmen und nicht nur nach dem, wie die TEI es nennt, „abstract model“ und dessen Serialisierungen zu unterscheiden, sondern auch Regelstrukturen, Präsentationsmodelle und differenzierte Metadatenformate als zum Verständnis notwendige Aspekte zu berücksichtigen. Editionen treten uns typischerweise als eine Kombination von Text mit Markup, Schemadatei, Stylesheets, Transformations- und sonstigen Skripten entgegen. Konkret handelt es sich um eine Reihe von Dateien (oder Datenströmen), wie z.B. .xml,.xslt,.xsd,.css oder .html, die zusammen ein funktionales Ganzes bilden, das als solches nicht nur die Stelle des physischen Dokumentes einnimmt, sondern auch die Grundlage der Langzeitarchivierung bildet. Dieser ganzheitlich betrachteten digitalen Edition ist eigentümlich, dass sie erst durch eine konkrete, meist nutzergesteuerte algorithmische Verarbeitungsanweisung nach dem klassischen EVA-Prinzip im Viewport oder Empfängersystem „verwirklicht“ wird, während ihre Persistenz in den in die Edition hineinkodierten und in ihren Darstellungsfunktionen niedergelegten Möglichkeiten, keineswegs aber in der sichtbaren Oberfläche liegt. Letztere reduziert sich zu einem Ausschnitt, der nur bedingt das gesamte Potential der Edition aufzeigen kann. Wenn diese kombinatorisch vollständig beschreibbaren Möglichkeiten der Präsentation, die zutreffend mit dem Begriff der Schnittstelle verbunden werden (Boot/Zundert 2011; Zundert 2018), den Kernbegriff der digitalen Edition konstituieren, resultieren daraus eine Reihe von praktischen und theoretischen Konsequenzen. Ein erster wichtiger Schritt liegt in der Erkenntnis der Superiorität der Kodierungsgrundlage über die erzeugte angezeigte Oberfläche (Turska/Cummings/Rahtz 2016): “Data is the important Long-term Outcome”. Das heißt aber nicht, dass die Oberfläche gleichgültig wäre. Sie darf nur nicht, weil sichtbar, als das einzig wichtige, ja nicht einmal als für die Edition maßgebliche Layer begriffen werden. Die Oberfläche, Visualisierung, die Ausgabe, die Schnittstelle, allgemein das algorithmische Erzeugnis, können über die primäre, immer aber reduzierte und mit Blick auf die kombinatorischen Möglichkeiten ausschnitthafte Darstellungsfunktion hinaus ihrerseits eigenständige Produkte bzw „Interpretationen“ sein, vgl. (Zundert 2018). Die Edition selbst sind sie aber nicht, denn eine digitale Edition kann man, streng genommen, nicht sehen. Entsprechend sind zum einen eine Reihe von Text- bzw. Dokumentelemente wie das Layout als eigenständiges bedeutungstragendes oder zumindest -beinflussendes Phänomen als digitale “Textästhethek” und als ein Ergebnis einer Funktion mit vielfältigen Parametern neu zu interpretieren (Stäcker 2019), zum anderen verändern sich Nutzungsszenarien etwa bei der Archivierung und Zitierbarkeit von Editionen, denn wenn die Oberfläche eine von mehreren Möglichkeiten ist, kann sie nicht ohne weitere Vorkehrung Gegenstand des Zitierziels sein.  Auf anderer Ebene bedeutet es, dass der/die Autor/Autorin oder, vermutlich genauer, das Autorenteam ein genaues Verständnis auch der technischen Dimension des digitalen Textes haben muss, um seinen nicht nur natürlichen, sondern auch maschinellen „Leser“ zu erreichen, oder aber, dass die Autorintention die Schaffung von Möglichkeiten der digitalen Hermeneutik und Analyse einschließen muss. Eine wesentlich Dimension der digitalen Edition ist ferner ihre Verankerung im “Netz”. Daraus ergeben sich generell Anforderungen an ihre “Hypertextualität”, ihre “Schnittstellen”  (Zundert 2018; Stäcker 2019) und Fähigkeit, sich in das “semantic web” zu integrieren (Ciotti/Tomasi 2016). Dazu zählt auch die unmittelbare Integration der genutzten „Forschungsdaten“, etwa der digitalen Faksimiles, die im Rahmen der 
                    recensio gesammelt und gesichtet wurden, so dass auf der Oberfläche ein transkludentes Ensemble entsteht, das auf 
                    hypertextuellen Strukturen aufbaut.
                
            Es besteht die Hoffnung, dass mit dem Blickwechsel von dem zweidimensionalen sichtbaren Ergebnis auf die unsichtbare Potentialität der Edition sich das eher Proteushafte der Oberfläche der digitalen Edition auflöst und auch für die schon lange gärende Frage nach deren Persistenz und Nachhaltigkeit ein zufriedenstellender Ansatz gerade in ihrer, mit dem Motto der Tagung gesprochen: Multimodalität, gefunden werden kann. Der Beitrag möchte diesen Gedanken anhand von Beispielen weiter ausführen, um einen tragfähigen Begriff von einer persistenten digitale Edition als einem funktionalen und organischen Ensemble von exakt definierbaren Komponenten zu entwickeln.
         
      
      
         
            
               Bibliographie
               
                  Andrews, Tara L. / Zundert, Joris J. van (2018):
                  What Are You Trying to Say? The Interface as an Integral Element of Argument, 
                        in: 
                        Roman Bleier / Martina Bürgermeister / Helmut W. Klug /Frederike Neuber / Gerlinde Schneider (eds. / hrsg.): 
                  Digital Scholarly Editions as Interfaces.
                    
               Norderstedt 3-33. 
                        
                     urn:nbn:de:hbz:38-91064
                  
               
               
                  Apollon, Daniel / Claire Bélisle / Philippe Régnier (eds.) (2014):
                  Digital critical editions. Topics in the digital humanities. 
                        Urbana: University of Illinois Press.
                    
               
                  Boot, Peter/ Zundert, Joris van (2011):
                  The Digital Edition 2.0 and The Digital Library: Services, not Ressources, 
                        in: Digitale Edition und Forschungsbibliothek. (Beiträge der Fachtagung im Philosophicum der Universität Mainz am 13. und 14. Januar 2011). Harrassowitz 2011: 141–52 (Bibliothek und Wissenschaft, 44) 
                        
                     http://hdl.handle.net/20.500.11755/c9e80904-8def-438e-a82b-80d4107b36ed
                  
               
               
                  Ciotti, Fabio / Tomasi, Francesca (2016):
                  Formal Ontologies,  Linked Data, and TEI Semantics, 
                        in: Journal of the Text Encoding Initiative. 
                        
                     10.4000/jtei.1480
                  
               
               
                  Cummings, James (2017):
                  Slides zum Vortrag gehalten auf der DH2017: 
                        
                     https://slides.com/jamescummings/teimyths
                  . Erscheint in Kürze in DSH.
                    
               
                  Dekker, Ronald / David J. Birnbaum (2017):
                  It's more than just overlap: Text As Graph. 
                        Presented at Balisage: The Markup Conference 2017, Washington, DC, August 1 - 4, 2017, in Proceedings of Balisage: The Markup Conference 2017. Balisage Series on Markup Technologies, vol. 19. 
                        
                     https://doi.org/10.4242/BalisageVol19.Dekker01
                  
               
               
                  Dekker, Ronald / Elli Bleeker / Bram Buitendijk, Astrid Kulsdom / David J. Birnbaum (2018):
                  TAGML: A markup language of many dimensions. 
                        Presented at Balisage: The Markup Conference 2018, Washington, DC, July 31 - August 3, 2018, in Proceedings of Balisage: The Markup Conference 2018. Balisage Series on Markup Technologies, vol. 21. 
                        
                     https://doi.org/10.4242/BalisageVol21.HaentjensDekker01
                    
                    
               
                  DeRose, Steven J. / Durand, David G. / Mylonas, Elli / Renear, Allen H.:
                  What Is Text, Really?, 
                        in: Journal of Computing in Higher Education 1, Nr. 2 (Dezember 1990) 3–26.
                        
                     https://doi.org/10.1007/BF02941632
                  .
                    
               
                  DeRose, Steven J. (2004):
                  Markup overlap: a review and a horse, 
                        in: Proceedings of Extreme Markup Languages.
                        
                     http://xml.coverpages.org/DeRoseEML2004.pdf
                  
               
               
                  Deutsche Forschungsgemeinschaft (2015):
                  Förderkriterien für wissenschaftliche Editionen in der Literaturwissenschaft. 
                        Merkblatt der DFG. 2015. 
                        
                     http://www.dfg.de/download/pdf/foerderung/antragstellung/forschungsdaten/foerderkriterien_editionen_literaturwissenschaft.pdf
                    
                    
               
                  Driscoll, Matthew James/ Elena Pierazzo (eds.) (2016):
                  Digital scholarly editing: Theories and practices. 
                        Cambridge, UK. 
                        
                     https://www.openbookpublishers.com/reader/483
                  .
                    
               
                  Gabler, Hans Walter (2010):
                  Theorizing the Digital Scholarly Edition, 
                        in: Literature Compass 7, Nr. 2 (Februar 2010) 43–56.
                        
                      https://doi.org/10.1111/j.1741-4113.2009.00675.x
                  .
                    
               
                  W3C (2017):
                  HTML 5.2 Recommendation (14 December 2017): §9: XML Syntax. 
                        
                     https://www.w3.org/TR/html52/xhtml.html#xhtml
                  
               
               
                  Kuczera, Andreas (2016):
                  Digital Editions beyond XML – Graph-based Digital Editions, 
                        in: Proceedings of the 3rd HistoInformatics Workshop on Computational History (HistoInformatics 2016). ceur-ws.org/Vol-1632/paper_5.pdf 
                    
               
                  McCarty, Willard (2005):
                  Humanities computing. 
                        Basingstoke, Hampshire [u.a.].
                    
               
                  McGann, Jerome J. (2014):
                  A new republic of letters: memory and scholarship in the age of digital reproduction. 
                        Cambridge, Mass. [u.a.]: Harvard Univ. Press.
                    
               
                  Piez, Wendell (2012):
                  Luminescent: parsing LMNL by XSLT upconversion. 
                        Presented at Balisage: The Markup Conference 2012, Montréal, Canada, August 7 - 10, in Proceedings of Balisage: The Markup Conference 2012. Balisage Series on Markup Technologies, vol. 8. 
                        
                     https://doi.org/10.4242/BalisageVol8.Piez01
                  
               
               
                  Renear, Allen / Mylonas, Elli / Durand, David (1993):
                  Refining our Notion of What Text Really Is: The Problem of Overlapping Hierarchies. 
                        Final version, January 6. 
                        
                     http://cds.library.brown.edu/resources/stg/monographs/ohco.html
                  
               
               
                  Robinson, Peter (2003):
                  WHERE WE ARE WITH ELECTRONIC SCHOLARLY EDITIONS, AND WHERE WE WANT TO BE, 
                        in: Jahrbuch für Computerphilologie. 
                        
                     http://computerphilologie.uni-muenchen.de/jg03/robinson.html
                  
               
               
                  Sahle, Patrick (2013):
                  Digitale Editionsformen : zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. 
                        Schriften des Instituts für Dokumentologie und Editorik. Bd. 1-3. Norderstedt.
                    
               
                  Scheuermann, Leif (2016):
                  Die Abgrenzung der digitalen Geisteswissenschaften, 
                        in: Digital Classics Online vol. 2. 
                        
                     https://journals.ub.uni-heidelberg.de/index.php/dco/article/viewFile/22746/21865
                  
               
               
                  Shillingsburg, Peter L. (2006):
                  From Gutenberg to Google: electronic representations of literary texts, 
                        Cambridge.
                    
               
                  Speerberg-McQueen, C. M. (2007):
                  Representation of overlapping structures: Proceedings of the 2007 Extreme Markup Languages conference. 
                        
                     http://conferences.idealliance.org/extreme/html/2007/SperbergMcQueen01/EML2007SperbergMcQueen01.html#id96245
                  
                   
               
               
                  Stäcker, Thomas (2018):
                  ‚Von Alexandria lernen‘. Die Forschungsbibliothek als Ort digitaler Philologie. 
                        In Frauen – Bücher – Höfe: Wissen und Sammeln vor 1800 Women – Books – Courts: Knowledge and Collecting before 1800. Essays in honor of Jill Bepler. Hrsg. von Volker Bauer, Elizabeth Harding, Gerhild Scholz Williams und Mara R. Wade. Wiesbaden: Harrassowitz 93–103. 
                        
                     http://nbn-resolving.de/urn:nbn:de:tuda-tuprints-75938
                  
               
               
                  Stäcker, Thomas (2019):
                  Literaturwissenschaft und Bibliothek – eine Beziehung im digitalen Wandel, 
                        in: Digitale Literaturwissenschaft. Metzler/Springer (im Druck, voraussichtl. 2019).
                    
               TEI Guidelines: 
                        
                     Chapter 20: Non-hierarchical Structures.
                     http://www.tei-c.org/release/doc/tei-p5-doc/en/html/NH.html
                  
               
               
                  Turska, Magdalena / Cummings, James / Rahtz, Sebastian (2016):
                  Challenging the Myth of Presentation in Digital Editions, 
                        in:  Journal of the Text Encoding Initiative 9 (2016-2017). 
                        
                     http://journals.openedition.org/jtei/1453
                     
                    
            
         
      
   



      
         
            Einführung
            In diesem Beitrag wird eine domänenspezifische Anwendung von Linked Open Data und Ontologien vorgestellt, die als Paradigma für den Umgang mit digitalen Corpora in der Archäologie dienen kann. Zunächst wird die Entwicklung des Projektes Nomisma.org sowie dessen Anwendung für die Verlinkung von digitalen Datenbeständen erläutert. In einem zweiten Teil wird ein Pilotprojekt vorgestellt, das darauf abzielt, textbasierte Münzpublikationen als RDF zur Verfügung zu stellen und somit prüft, inwieweit das Vokabular und die Ontologie von Nomisma.org eingesetzt werden können.
            Das 2010 von der American Numismatic Society, New York, initiierte Projekt 
                Nomisma.org1 definiert und stellt stabile digitale Repräsentationen numismatischer Konzepte nach den Prinzipien von Linked Open Data zur Verfügung. Sie werden als http-URIs 
                veröffentlicht2, die Zugang zu weiterverwertbaren Informationen zu den Konzepten liefern, dazu noch Links zu weiteren Linked Open Data-Ressourcen (Getty Vocabularies, wikidata, viaf, GND, u.v.m.). Ferner wurde eine numismatische 
                Ontologie3 entwickelt, die den Bedürfnissen und Arbeitsweisen der Numismatik gerecht ist und eine unkomplizierte Modellierung ermöglicht. Das kanonische Format von Nomisma.org ist RDF/XML, aber auch weitere Formate wie JSON-LD (für Geodaten geoJASON-LD), Turtle, KML und HTML5+RDFa 1.1. werden bedient.
                
         
         
            Virtuelle Sammlungen
            Zunächst lag der Schwerpunkt der Arbeit von Nomisma.org bei der römischen Numismatik und bis 2016 erfolgte die Onlinestellung der Linked Open Data-Ressourcen 
                    Coinage of the Roman Republic Online 
                    (CRRO)4 und 
                    Online Coins of the Roman Empire 
                    (OCRE)5 als virtuelle Münzsammlungen für die Prägungen der römischen Republik und der Kaiserzeit. Die Abfrage erfolgt über RDF Dumps der Projektpartner, die zentral gehalten werden; die dazugehörigen Bilder werden on-the-fly von den Servern der Partner geholt, wodurch nicht nur das Volumen der zentralen Datenhaltung gering gehalten und die Abfragegeschwindigkeit erhöht, sondern auch eventuelle rechtliche Probleme bei der Lizenzierung der Bilder vermieden werden.
                
            Mittlerweile befassen sich erste Projekte auch mit dem disparaten und weitaus komplexeren Stoff der griechischen 
                Welt,6 z.B. 
                    PELLA für die Prägungen der makedonischen Dynastie der Argeaden, 
                    Corpus Nummorum Thracorum für Thrakien, oder 
                    Seleucid Coins Online für die Prägungen der Seleukiden. Des Weiteren haben Arbeitsgruppen bereits angefangen, die notwendigen Konzepte für die keltische und islamische Numismatik sowie für das Mittelalter zu definieren.
                
            Fast 200.000 antike Münzen von insgesamt 39 Institutionen werden heute in auf Nomisma.org basierenden Online-Ressourcen veröffentlicht.7 Aktiv an der Entwicklungen beteiligt sind u.a. das Institute for Studies of the Ancient World, New York, das Deutsche Archäologische Institut sowie vier der weltweit bedeutendsten Münzsammlungen: Die American Numismatic Society, das British Museum, die Bibliothèque nationale de France und das Münzkabinett der Staatlichen Museen zu Berlin. Auch eine Reihe kleinerer Sammlungen sind dabei ihre Bestände in Portalen wie OCRE und CRRO online zu stellen. Darunter schon acht der 34 deutschen universitären Sammlungen, die Mitglieder im 
                    NUMiD-Verbund 
                    (Netzwerk universitärer Münzsammlungen in Deutschland)8 sind.
                
         
         
            Die verlinkte Antike
            Nomisma.org und die darauf bauenden Projekte sind in die Linked Open Data-Welt der Antike fest integriert. Mit Nomisma.org verlinkte, bzw. gemappte Münzen werden bei Ressourcen wie 
                    Pleiades
               9 und 
                    Pelagios
               10 angezeigt und umgekehrt können Einträge in den virtuellen Sammlungen mit kontextvermittelnden Daten aus anderen Quellen angereichert werden. Beispielsweise: Wird Literatur in der Form von iDAI.bibliographie / 
                    ZENON11 URIs zitiert, können umgekehrt in Zenon Verweise auf Münzen beim entsprechenden Titel angezeigt werden. Im Abschlussbericht des Work Package 15 des EU-FP7 Projektes ARIADNE wurde Nomisma.org mehrfach als führendes Beispiel für die fachspezifische Anwendung von Linked Open Data zitiert (Geser 2016).
                
            Nomisma.org hat numismatische Daten auf eine Weise digital zur Verfügung gestellt, die vor 10 Jahren kaum vorstellbar gewesen wäre und macht einen bedeutenden Bestand an Material der Forschung leichter zugänglich. Durch die Verlinkung mit anderen LOD-Ressourcen sind diese Daten auch deutlich sichtbarer und stehen damit für Forschungsvorhaben in anderen Disziplinen vermehrt zur Verfügung. So kann Nomisma.org beispielsweise einen wichtigen Beitrag zur Aufhebung der immer noch weit verbreiteten Isolation der Numismatik in den Altertumswissenschaften leisten, eine Isolation, die in der (bisher oft fehlenden) Zusammenarbeit mit der Archäologie besonders deutlich erkennbar ist.
         
         
            Fundmünzen und Archäologie
            Konzentrierten sich Projekte wie OCRE und PELLA zunächst auf die Präsentation von Münzen aus öffentlichen Sammlungen, richtet sich der Blick mittlerweile zunehmend auf Münzen im archäologischen Kontext. Nomisma.org kompatible Standards für die Aufnahme und Veröffentlichung von Münzen aus Ausgrabungen wurden bei einem Treffen an der University of Oxford im September 2018 vereinbart, um Re-use und Interoperabilität zu 
                ermöglichen.12
            
            Nationale Projekte wie 
                    NUMIS
               13 für die Niederlande oder das 
                    Inventar der Fundmünzen der Schweiz
               14 veröffentlichen umfangreiche Bestände an Fundmünzen im Web, die aber nur über das jeweilige nationale Portal zugänglich sind. Eine länderübergreifende Abfrage ist nicht möglich. Eine Ausnahme bietet das Projekt
                     Antike Fundmünzen in Europa
               15, das die Datenbanken der Römisch-Germanischen Kommission in Frankfurt und des Unternehmens 
                    Finds of Roman Coins in Poland auf das Vokabular und die Ontologie von Nomisma.org mappt und mittels eines D2R-Servers als RDF online stellt. Ein SPARQL-Endpoint ermöglicht die gemeinsame Abfrage der beiden Datenbestände und erleichtert damit die Analyse der Fundmünzen aus einem von der Nordsee bis zur Ukraine reichenden Raum.
                
         
         
            Text zu RDF: Erster Versuch einer Digitalisierung
            Bisher lag der Schwerpunkt der Verlinkung mit bzw. Mapping auf Nomisma.org auf in Datenbanken gehaltene Daten. In diesem Beitrag soll abschließend ein Pilotprojekt vorgestellt werden, das sich mit den Möglichkeiten der Anwendung von Nomisma.org auf textbasierte Datenbestände beschäftigt.
            
               Grundlage
               Das Projekt „Fundmünzen der römischen Zeit in Deutschland“ (FMRD) brachte 48 Bände mit weit über 300.000 erfassten Fundmünzen heraus. Im Rahmen eines Praxisprojektes im Studiengang „Digitale Methodik in den Geistes- und 
                    Kulturwissenschaften“16 wurde als Beispieldatensatz aus FMRD der Fundkomplex Domgrabung/Liebfrauen-Areal in Trier mit 1.157 Münzen digitalisiert (M. Radnoti-Alföldi 2006: 119–206).
                    
            
            
               Das Verfahren
               Die Daten zu den Münzen wurden aus dem PDF in CSV und XML extrahiert und in ein RDF-Dokument umgewandelt, um die Vernetzung und Visualisierung zu ermöglichen (Abb. 1).
               
                  
                  
                     Abbildung 1. Verlaufsplan.
               
               Für die Konvertierung aus dem PDF und die Bearbeitung wurde 
                        Adobe Acrobat Pro DC
                  17, der 
                        Oxygen
                  18 Editor und 
                        Libre Office
                  19 verwendet. Die extrahierten Daten wurden anhand der CSV-Extraktion in ein einheitliches Spaltensystem gebracht. Über das XML-Dokument wurden anhand von regulären Ausdrücken Konvertierungsfehler etc. ausgebessert. Am Ende stand eine einzelne TEI-konforme Tabelle. Der nächste Schritt bestand darin, ein Linked-Open-Data-Modell anhand der Ontologie von Nomisma.org (Abb. 2) zu entwickeln.
                    
               
                  
                  
                     Abbildung 2. RDF-Modell.
               
               Um dieses Modell automatisiert auf alle 1.157 Münzen der Trierer Domgrabung übertragen zu können, wurde der Webdienst 
                        XTriples verwendet
                     20
                  , der es erlaubt aus XML ein RDF-Dokument zu erzeugen.
                    
               
                  
                  
                     Abbildung 3. Webservice XTriples.
               
               Das RDF-Dokument wurde auf einen Triple Store abgelegt, der über ein RDF4J-Framework verfügt und SPARQL-Abfragen erlaubt. Ein weiterer Webdienst, 
                        sgvizler,
                     21
                   ermöglicht es im Anschluss die Daten zu visualisieren und zu analysieren (Abb. 4).
                    
               
                  
                  
                     Abbildung 4. Visualisierung über sgvizler zu den Fundmünzen der Trierer Domgrabung.
               
            
            
               Ausblick
               Das oben beschriebene Verfahren wurde dann erweitert, um eine Pipeline aufzubauen, mit der alle Bände der FMRD-Reihe digitalisiert werden können. Auf manuelle Schritte und proprietäre Software soll dabei möglichst verzichtet werden.
            
            
               Tabellenextraktion
               In einem ersten Schritt wurden aktuelle Open-Source-Angebote verglichen, die es ermöglichen, aus der PDF-Struktur Tabellen in CSV- bzw. XML-Dateien zu extrahieren. Die Daten liegen jedoch so inhomogen vor, dass bisher erschienene Programme und Skripte/Bibliotheken schnell an ihre Grenzen stoßen. Als Beispiel soll hier das browserbasierte Tool 
                        Tabula
                  
                     22
                   dienen, welches Tabellen im PDF automatisch erkennt und anschließend als CSV ausgeben kann. Im Verlauf der Erprobung stellte sich 
                        Tabula als ungeeignet heraus, da immer noch zu viele manuelle Eingriffe vonnöten sind. So erkennt es manche Tabellenteile nicht (Abb. 5) und die Spalteninhalte im erzeugten CSV werden nicht richtig zugeordnet.
                    
               
                  
                  
                     Abbildung 5. Tabula: Fundmünzen Nummer 1 bis 4 werden nicht erkannt.
               
            
            
               pdftohtml
               Eine andere Herangehensweise erzeugt als Zwischenschritt XML, bevor dieses in CSV umgewandelt werden kann. Für die Erzeugung des XML wird das Tool 
                        pdftohtml verwendet.
                     23
                   Das Ergebnis lässt sich über den 
                        pdf2xml-viewer
                  24 überprüfen (Abb. 6).
                    
               
                  
                  
                     Abbildung 6. OCR-Ergebnis.
               
               Die Normalisierung der Spalten ist hier leichter, da auch leere Tabellenspalten erkannt werden. Jedoch wird auch hier der Spalteninhalt nicht überall korrekt erkannt (Münze 18). Die XML-Struktur erlaubt es über die Tags zu navigieren und diese mit X-Technologien zu manipulieren. Doch ist der Anteil der nicht korrekt erkannten Spalteninhalte immer noch zu hoch, um die Daten wie im Druckband abzubilden.
            
            
               Reguläre Ausdrücke
               Eine andere Möglichkeit an die Tabellendaten zu gelangen sind reguläre Ausdrücke. Mithilfe eines Pythonskriptes wurden diese separiert und als CSV ausgegeben. Dafür wurden beispielhaft verschiedene Münzkomplexe herangezogen und die regulären Ausdrücke um die pro Komplex neu auftretenden Sonderfälle modifiziert. Dieses Verfahren erzeugte gute Ergebnisse für die behandelten Komplexe. So waren nur noch kleinere manuelle Nachbesserungen nötig. Im Vergleich erzeugte dieses Verfahren das beste Ergebnis.
            
         
         
            Fazit
            Die Digitalisierung von „Fundmünzen der römischen Zeit in Deutschland“ erweist sich als anspruchsvoll. Der Ansatz, die Daten über reguläre Ausdrücke zu gewinnen und zu extrahieren, erscheint bisher am vielversprechendsten. Die dabei auftretenden Abweichungen zwischen den Münzfundkomplexen lassen sich durch auf die FMRD-Struktur zugeschnittene Ausdrücke ausbessern. So erhoffen wir uns, die bisher über Nomisma.org veröffentlichten Datenbestände durch wichtige archäologische Kontexte zu erweitern und zu bereichern.
         
      
      
         
             http://nomisma.org/; Hinweis: Alle im Beitrag erwähnten URLs wurden zuletzt am 14.10.2018 überprüft.
             z.B. http://nomisma.org/id/sestertius
             http://nomisma.org/ontology
             http://numismatics.org/crro/
             http://numismatics.org/ocre/
             https://www.greekcoinage.org/portals.html
             http://nomisma.org/datasets
             http://www.numid-verbund.de/
             https://pleiades.stoa.org/
             http://peripleo.pelagios.org/
             http://peripleo.pelagios.org/
             https://www.greekcoinage.org/coins-in-context.html
             https://www.dnb.nl/en/about-dnb/nationale-numismatische-collectie/numis/numis-database/index.jsp
             https://www.fundmuenzen.ch/
             http://afe.fundmuenzen.eu/
             https://www.digitale-methodik.uni-mainz.de/
             https://acrobat.adobe.com/de/de/acrobat/acrobat-pro.html
             https://www.oxygenxml.com/
             https://de.libreoffice.org/
             http://xtriples.spatialhumanities.de/index.html
             http://mgskjaeveland.github.io/sgvizler/
             https://tabula.technology/
             https://poppler.freedesktop.org/
             https://github.com/WZBSocialScienceCenter/pdf2xml-viewer
         
         
            
               Bibliographie
               
                  G. Geser (2016):
                  ARIADNE WP15 Study: Towards a Web of Archaeological Linked Open Data 
                        (www.ariadne-infrastructure.eu/.../ARIADNE_archaeological_LOD_study_10-2016.pdf, 31.10.2016).
                    
               
                  E. Gruber:
                  Numishare Blogspot: 
                        http://numishare.blogspot.com/.
                    
               
                  E. Gruber / G. Bransbourg / S. Heath / A. Meadows (2014):
                  Linking Roman Coins: Current Work at the American Numismatic Society, 
                        in: 
                        G. Earle et al. (eds):
                  Archaeology in the Digital Era: 
                        Papers from the 40th Annual Conference of Computer Applications and Quantitative Methods in Archaeology (CAA), Southampton, 26-29 March 2012 (Amsterdam) 249–258.
               
                  E. Gruber / S. Heath / A. Meadows / D. Pett / D. Wigg-Wolf (2014):
                  Semantic Web Technologies Applied to Numismatic Collections, 
                        in: 
                        G. Earle et al. (eds):
                  Archaeology in the Digital Era: 
                        Papers from the 40th Annual Conference of Computer Applications and Quantitative Methods in Archaeology (CAA), Southampton, 26-29 March 2012 (Amsterdam) 264–274.
               Radnoti-Alföldi, M. (2006): Die Fundmünzen der römischen Zeit in Deutschland IV 3/2. Stadt und Reg.-Bez. Trier. Die Sog. Römerbauten (Mainz).
               
                  
                  K. Tolle / D. Wigg-Wolf (2015):
                  Uncertainty Handling for Ancient Coinage, 
                        in: 
                        F. Giligny et al. (eds):
                  CAA2014. 21st Century Archaeology. Concepts, Methods and Tools. 
                        Proceedings of the 42nd Annual Conference on Computer Applications and Quantitative Methods in Archaeology (Oxford) 171–178.
                    
               
                  K. Tolle / D. Wigg-Wolf (2016):
                  How to Move from Relational to 5 Star Linked Open Data – A Numismatic Example, 
                        in: 
                        S. Campana et al. (eds):
                  CAA2015. Keep the Revolution Going. 
                        Proceedings of the 43rd Annual Conference on Computer Applications and Quantitative Methods in Archaeology (Oxford 2016) 275–281.
                    
               
                  D. Wigg-Wolf / F. Duyrat (2017):
                  La révolution des Linked Open Data en numismatique: Les exemples de nomisma.org et Online Greek Coinage. 
                        Archéologies numériques 1.1, 2017.
                    
            
         
      
   



      
         
            Einleitung
            Wir präsentieren 
                WebAnno-MM1, eine Erweiterung des populären web-basierten Annotationstools 
                WebAnno2 (Yimam et al., 2013; Eckart de Castilho et al., 2014), welches die Annotation transkribierter Daten ermöglicht und dabei parallel synchronisierte Ansichten auf die Daten zur Verfügung stellt. Die Erweiterung wurde im Rahmen eines Projekts zur Erarbeitung und Erprobung innovativer Lehrmethoden entwickelt (vgl. Remus et al, 2018), um Annotation von Videodaten als Unterstützung für die Analyse und Reflexion authentischer Diskursbeispiele einsetzen zu können. Eine entscheidende Rolle spielen dabei das kollaborative Annotieren und die systematische Überprüfung der Übereinstimmung – einerseits zwischen Studierenden, als Diskussionsgrundlage im Seminar, und anderseits durch einen Studierenden zu verschiedenen Zeitpunkten, um so Einblicke in die Aneignung neuer Konzepte zu bekommen. Die Auswertung von Inter- und Intra-Annotator-Agreement wird durch die elaborierte Nutzerverwaltung und das bereits existierende Kurationsmodul von WebAnno ermöglicht. Ein weiterer Vorteil von WebAnno in Hinblick auf die Diskursannotation ist der bessere Überblick über größere Abschnitte des Diskurses verglichen mit gängigen Werkzeugen, die in erster Linie für die Transkription und Annotation auf Äußerungsebene entwickelt worden sind, wie z.B. der EXMARaLDA Partitur-Editor (Schmidt und Wörner, 2014) oder ELAN (Sloetjes, 2014). Auch hinsichtlich der Struktur der Annotationen bietet WebAnno große Flexibilität, da neben einfacher Spannenannotation auch relationale und feature-basierte Annotationen möglich sind. Ein wichtiger Aspekt für den Einsatz in der Lehre ist zudem die einfache Handhabung einer web-basierten Plattform.
                
         
         
            WebAnno-MM Plugin
            WebAnno bietet in der Basisversion nur geringe Funktionalität für transkribierte gesprochene Daten. WebAnno-MM bietet daher nun als Plugin für WebAnno sowohl die Möglichkeit, alignierte Mediendateien abzuspielen, als auch die Transkriptionsdaten nach bestimmten Konventionen und dem vorgesehenen Layout für die qualitative Analyse zu visualisieren. Durch ein Open-Source-Projekt wie WebAnno wird nicht nur der Einsatz der Lehrmethode und der neuen WebAnno-Version in verschiedenen Kontexten ermöglicht, sondern auch die Ergänzung und Weiterentwicklung zusätzlicher Funktionalitäten für ähnliche Vorhaben. In Hinblick auf Interoperabilität und Nachhaltigkeit der Lösung wurde dementsprechend der auf den 
                TEI-Richtlinien3 basierende ISO-Standard „Transcription of Spoken 
                Language“4 als Grundlage gewählt, für die eine Konvertierung aus gängigen Transkriptionswerkzeugformaten bereits möglich ist (vgl. Schmidt et al., 2017).
                
         
         
            Datenmodellierung in WebAnno
            Die Grundlage in WebAnno‘s Backend ist UIMA (Unstructured Information Management 
                Architecture)5 (Ferrucci und Lally, 2004). UIMA speichert Textinformationen, d.h. den Text selbst und die Annotationen, in sogenannten CASs (Common Analysis Systems). Eine große Herausforderung ist die Darstellung von Transkriptionen und den (zeitlichen) Annotationen basierend auf mehreren Sprechern. Die Textsequenz soll dabei die Wahrnehmung einer Konversation möglichst wenig behindern und die einzelnen segmentierten Äußerungen der Sprecher und ihrer kontinuierlichen Annotationen beibehalten. Dazu parsen wir ISO/TEI-XML-Inhalte und speichern Äußerungen einzelner Sprecher in verschiedenen sogenannten Views (verschiedene CASs des gleichen Dokuments) und behalten Zeitstempel als Metadaten innerhalb einer CAS. Wir verwenden das jeweils auf einzelnen Sprecherbeiträgen basierende 
                    annotationBlock-XML-Element als eine unterbrechungsfreie Einheit, da wir in diesem Fall davon ausgehen können, dass einzelne Annotationen innerhalb der Zeitgrenzen des Sprecherbeitrags liegen. Äußerungen und vorhandene Spannenannotationen werden auf diese Weise in die vorhandene Annotationsansicht von WebAnno übernommen. Eigenständige Ereignisse wie beispielsweise non-verbale Äußerungen hingegen, die als sogenannte 
                    Incidents auch über andere, sprecherabhängige Äußerungen hinweg auftreten können, werden nicht in die sequentielle WebAnno Annotationsansicht konvertiert, sondern nur in der zusätzlichen Partituransicht dargestellt.
                
         
         
            Visualisierung der Transkriptionsdaten
            Die neu integrierte Partituransicht basiert auf einer Visualisierung von Äußerungen und Annotationen im bekannten 
                Partiturlayout6. Abbildung 1 (rechts) zeigt einen Screenshot von der Partituransicht. Die Annotationsansicht und die Partituransicht sind synchronisiert, d.h. durch Klicken auf die entsprechenden Zeitmarkierungen im jeweiligen Browserfenster ändert sich der Fokus in dem anderen. Durch Klicken wird auch die Medienwiedergabe gestartet oder pausiert. Darüber hinaus kann in der Partituransicht unter den verknüpften Medienformaten die aktuell abzuspielende Datei festgelegt und die Breite der Partiturflächen nach Bedarf eingestellt werden. Es stehen außerdem die Metadaten der Aufnahme und den Sprechern zur Verfügung.
                
            
               
               
                  
                  Abbildung 1. Annotationsansicht (links), Partituransicht (rechts)
               
            
         
         
            Präsentation
            Wir stellen auf der Grundlage unseres Posters die theoretischen Hintergründe sowie die technische Implementierung der Erweiterung vor und zeigen das Plugin und seine Anwendung in einer Live-Demonstration.
         
      
      
         
             https://github.com/webanno/webanno-mm
                    
             https://webanno.github.io
                    
             http://www.tei-c.org/guidelines/p5/
                    
             http://www.iso.org/iso/catalogue_detail.htm?csnumber=37338
                    
             https://uima.apache.org/
                    
             Beispiel der Partituransicht: http://hdl.handle.net/11022/0000-0000-4F70-A
                    
         
         
            
               Bibliographie
               
                  Eckart de Castilho, Richard / Biemann, Chris / Gurevych, Iryna / Yiman, Seid Muhie (2014):
                  WebAnno: a flexible, web-based annotation tool for CLARIN, 
                        in: Proceedings of the CLARIN Annual Conference 2014 1-3.
                    
               
                  Ferrucci, David / Lally, Adam (2004):
                  UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment, 
                        in: Natural Language Engineering, 10(3–4): 327-348.
                    
               
                  Remus, Steffen / Hedeland, Hanna / Ferger, Anne / Bührig, Kristin / Biemann, Chris (2018):
                  WebAnno meets EXMARaLDA, 
                        in: Selected papers from the CLARIN Annual Conference 2018, Pisa, 8–10 October 2018. Linköping: University Electronic Press.
                    
               
                  Schmidt, Thomas / Wörner, Kai (2014):
                  EXMARaLDA, 
                        in: 
                        Durand, Jacques / Gut, Ulrike / Kristoffersen, Gjert (eds.): 
                  Handbook on Corpus Phonology. Oxford: University Press 402-419.
                    
               
                  Schmidt, Thomas / Hedeland, Hanna / Jettka, Daniel (2017):
                  Conversion and annotation web services for spoken language data in CLARIN, 
                        in: Selected papers from the CLARIN Annual Conference 2016, Aix-en-Provence, 26–28 October 2016. Linköping: University Electronic Press 113–130.
                    
               
                  Sloetjes, Han (2014):
                  ELAN: Multimedia annotation application, 
                        in: 
                        Durand, Jacques / Gut, Ulrike / Kristoffersen, Gjert (eds.): 
                  Handbook on Corpus Phonology. Oxford: University Press 305–320.
                    
               
                  Yimam, Seid Muhie / Gurevych, Iryna / Eckart de Castilho, Richard / Biemann, Chris (2013):
                  WebAnno: A flexible, web-based and visually supported system for distributed annotations, 
                        in: Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations  , Sofia, Bulgaria 1-6.
                    
            
         
      
   



      
         
            Das Projekt
            
               Gutenberg Biographics
               ist ein webbasiertes System zur schwerpunktmäßigen Erfassung und Präsentation der Biographien von Mainzer Gelehrten in der Zeit von 1477-1798/1802 und seit der Wiedergründung der Johannes Gutenberg-Universität 1946 bis in die Gegenwart. Das Digital Humanities Projekt wird in Kooperation der Universitätsbibliothek Mainz, des Instituts für Geschichtliche Landeskunde an der Universität Mainz e.V sowie der Akademie der Wissenschaften und der Literatur | Mainz durchgeführt. Die Schwerpunkte liegen hierbei in der quellenbasierten Erschließung, der Forschungsdatenmodellierung im Bereich der Digitalen Biographik sowie im Aufbau und der Präsentation der webbasierten Plattform. Seit 2013 wird das Projekt vom Forschungsverbund Universitätsgeschichte finanziell unterstützt.
            
            
               Gutenberg Biographics
               verzeichnet die Personen in ihren Rollen als Handelnde innerhalb unterschiedlicher historischer Ereignisse. Die Biographien der einzelnen Gelehrten werden als Summe ihrer Ereignisse erfasst, sodass ein Zugang sowohl über die Person selbst als auch über die Ereignisse ermöglicht wird. Die stark strukturierten Daten der einzelnen Professoren_innen und Lehrenden der Universität Mainz dienen hierbei als Basis für eine Vielzahl von interdisziplinären Fragestellungen (Vgl. Auge 2013): Welche Karrierewege sind typisch oder treten gehäuft auf? Lässt sich ein Karrieremuster feststellen? Welche wissenschaftlichen oder verwandtschaftlichen Netzwerke innerhalb der einzelnen Fachbereiche und/oder auf universitärer Ebene lassen sich identifizieren? Durch welche Mitgliedschaften in Vereinen, Interessensverbänden oder politischen Parteien nehmen die Professoren_innen Einfluss auf die Gesellschaft jenseits des akademischen Feldes (Beispielhaft hierfür Göllnitz 2013)? Durch die ereignisbasierte Modellierung der Personendaten profitieren also nicht nur die regionalgeschichtliche und biographische Forschung, sondern auch die Universitäts- und Wissenschaftsgeschichte von 
               Gutenberg Biographics.
                
         
         
            Datenstruktur 
            
               Verwendet wird das Cultural Heritage Framework der Akademie der Wissenschaften Mainz (Schrade 2017). Dieses ermöglicht eine hohe Flexibilität der Datenmodellierung, sodass es auch an zukünftige Fragestellungen oder Ereignisse angepasst werden kann. Für die insgesamt knapp 1650 
                    Personendatensätze
               1
                 (vgl. Eckert 2017) werden zum einen die Stammdaten bestehend aus einer System-ID, dem Namen und Namensvarianten, Lebensdaten, Konfession und Fachgebieten erfasst. Zum anderen wird jeder Personendatensatz sowie alle Orte und Körperschaften mit der jeweiligen eindeutigen Identifikationsnummer verknüpft. Personen werden zusätzlich auch über die VIAF identifiziert. Die einzelnen Ereignisse werden nach der Ereignisart, dem Datum und einer zugehörigen Rolle, welche der Handelnde einnimmt, strukturiert. Ergänzt werden diese Informationen um in Beziehung stehende Entitäten, Orte, Personen und Fachbereiche bzw. Institute sowie um archivische Quellen und Literatur. Durch die Verwendung einer 
                    GND-BEACON-Datei
               2
                werden verschiedene Projekte und bibliographische Informationen miteinander verknüpft und aufeinander bezogen. Die verwendeten Personendatensätze der GND werden zu Tp-Sätzen des Kategorisierungslevels 1 aufgearbeitet und fehlende Einträge innerhalb des Normdatenkatalogs ergänzt. So verfügen alle verzeichneten Professoren nach Abschluss des Projekts über eine eindeutige standardisierte ID, die im Rahmen von Semantic Web und Linked Open Data weiterverwendet werden kann.
            
         
         
            FAIRness
            
               Gutenberg Biographics
               legt großen Wert auf Datenstandards und Normdaten sowie auf die Offenheit und gute Nachnutzbarkeit aller im Projekt erschlossenen Forschungsdaten. Hierzu werden die Daten in standardkonformen Formaten über mehrere Schnittstellen (BEACON, TEI/XML, OpenSearch) zur freien Nachnutzung angeboten und somit die 
                    FAIR-Prinzipien
               3
                angewendet. 
            
            
               F
               INDABLE:
            
            
               Dank persistenter Identifikatoren und Permalinks können die einzelnen Uniform Resource Identifiers (URI) der Datensätze von 
               Gutenberg Biographics
                sowohl von Maschinen als auch durch den Menschen langfristig im Web gefunden werden. Zur besseren Auffindbarkeit von Ressourcen wird eine BEACON-Datei, welche mit den Normdaten der Gelehrten versehen ist, verwendet.
            
            
               A
               CCESSIBLE: 
            
            
               Gutenberg Biographics
               besitzt verschiedene offene Schnittstellen, über welche die Forschungsdaten bereitgestellt werden. Neben einer 
               
               RESTful-API steht eine Open Search-Schnittstelle zur Verfügung.
               4
            
            
               I
               NTEROPERABLE:
            
            
               Die Forschungsdaten liegen zum Austausch in standardkonformen TEI/XML vor. Auch die Präsentationsschicht der Anwendung bietet die Forschungsdaten in strukturierter Form unter Zuhilfenahme der 
                    schema.org
               5
                Ontologie an. Dank der Verwendung sowohl von Normdaten in Form von GND und VIAF als auch einer BEACON-Datei, ist 
               Gutenberg Biographics
                zu anderen Professorenkatalogen, Lexika oder bibliographischen Katalogen kompatibel. 
            
            
               R
               EUSABLE:
            
            
               Abgesehen von den verwendeten Fotographien stehen alle Daten unter der Creativ Commons Lizenz CC-BY 4.0. Die offenen Daten können somit unter Nennung des Urhebers durch Dritte weiterverwendet und bearbeitet werden.
            
         
         
            Perspektiven
            
               Geplant, aber bisher noch nicht umgesetzt, ist ein weiterer Zugang mittels Einbindung von Geoinformationssystemen und historischen Kartenmaterialien, der eine weitere Herangehensweise zur Beantwortung von historischen Fragestellungen mit digitalen Methoden ermöglicht. Weiterhin ist angedacht, die Forschungsdaten zukünftig auch in einer CIDOC CRM Modellierung zur Verfügung zu stellen 
                    (unter Verwendung des Erlangen CRM/OWL)
               6. 
                    Gutenberg Biographics
               leistet somit einen wertvollen Beitrag als Provider von strukturierten (Forschungs-) Daten für die Geschichts- und Kulturwissenschaften im Sinne der Digitalen Geisteswissenschaften. Ein nächster und notwendiger Schritt wäre sicherlich, möglichst viele digitale Gelehrtenverzeichnisse im Rahmen einer gemeinsamen Plattform zusammenzuführen. Hierfür muss jedoch eine einheitliche Basis der Modellierung bspw. in Form von katalogübergreifenden Standards und Schnittstellen gefunden werden, welche eine Zusammenführung der unterschiedlichen methodischen Grundlagen der einzelnen Kataloge (vgl. Schrade 2016: 19) ermöglicht. Durch einen interoperablen Zusammenschluss aller Professorenkataloge kann das Potential der in ihnen enthaltenen Biographien noch besser für die (historische) Forschung genutzt werden.
            
         
      
      
         
             Die Zahl setzt sich zusammen aus rund 750 Professoren_innen, die bis 1973 an die Johannes Gutenberg Universität Mainz berufen worden sind und weiteren rund 900 Professoren aus der frühneuzeitlichen Mainzer Universität (1477-1798/1823). Bis Januar 2019 konnten bereits mehr als 700 Datensätze veröffentlicht werden.
            
               https://github.com/digicademy/beaconizer
            
            
               https://www.force11.org/group/fairgroup
            
            
               http://gutenberg-biographics.ub.uni-mainz.de/daten.html
            
            
               https://schema.org/
            
            
               http://erlangen-crm.org/
            
         
         
            
               Bibliographie
               
                  Auge, Oliver / Piotrowski, Swantje (2013):
                  Professorenkataloge 2.0 – Ansätze und Perspektiven webbasierter Forschung in der gegenwärtigen Universitäts- und Wissenschaftsgeschichte, 
                        in: Jahrbuch für Universitätsgeschichte 16: 143-339.
                    
               
                  Eckert, Karin / George, Christian / Hüther, Frank (2017):
                  Gutenberg Biographics: Eine biographische Online-Datenbank zur Mainzer Universitätsgeschichte, 
                        in: ABI Technik – Zeitschrift für Automation, Bau und Technik im Archiv-, Bibliotheks- und Informationswesen 37, 3:  171-178 
                        
                     https://doi.org/10.1515/abitech-2017-0041
                   [letzter Zugriff 08. Januar 2019].
                    
               
                  Göllnitz, Martin (2013):
                  Das ‚Kieler Gelehrtenverzeichnis‘ in der Praxis: Karrieren von Hochschullehrern im Dritten Reich zwischen Parteizugehörigkeit und Wissenschaft, 
                        in: 
                        Auge, Oliver / Piotrowski, Swantje (eds.):
                  Professorenkataloge 2.0 – Ansätze und Perspektiven webbasierter Forschung in der gegenwärtigen Universitäts- und Wissenschaftsgeschichte. 
                        Jahrbuch für Universitätsgeschichte 16. Stuttgart: Franz Steiner Verlag 291-312.
                    
               
                  Pfeiffer, Barbara (2013):
                  Über Zweck und Nutzen der Gemeinsamen Normdatei, 
                        in: 
                        Auge, Oliver / Piotrowski, Swantje (eds.):
                  Professorenkataloge 2.0 – Ansätze und Perspektiven webbasierter Forschung in der gegenwärtigen Universitäts- und Wissenschaftsgeschichte. 
                        Jahrbuch für Universitätsgeschichte 16. Stuttgart: Franz Steiner Verlag 251-259.
                    
               
                  Schrade, Torsten (2016):
                  Deutsche Professorenkataloge. Perspektiven, Möglichkeiten, Potentiale zur Interoperabilität
                  
                     https://digicademy.github.io/presentation-catalogus-professorum/#/step-1
                   [letzter Zugriff 08. Januar 2019].
                    
               
                  Schrade, Torsten (2017):
                  Sammlungs- und Editionsportale mit dem Cultural Heritage Framework der Digitalen Akademie. Ein Werkstattbericht.
                  
                     https://digicademy.github.io/2017-editionsportale-jena/#/step-1
                   [letzter Zugriff 08. Januar 2019].
                    
            
         
      
   



      
         „I meant my notebooks to be a storehouse of materials for future use and nothing else” (Maugham 1949: xiv) schreibt der englische Dramatiker William Somerset Maugham in seinem Vorwort 
                A Writer’s Notebook über den Zweck seiner Notizbücher. 
                Notizbücher werden in unterschiedlichen Disziplinen verwendet: Schriftsteller, Künstler und Wissenschaftler halten ihre unmittelbaren Eindrücke und Ideen in Form von schriftlichen Notizen und flüchtigen Skizzen fest (Radecke 2013: 161). Diese werden zu einem späteren Zeitpunkt zur Ausführung eines Werkes herangezogen oder erweisen sich als Sackgasse und werden wieder verworfen. 
            
         Obwohl Notizbücher zum einen eine reiche Quelle an Informationen zur Entstehungsgeschichte von Werken liefern und zum anderen in ihrer Objekthaftigkeit als eigenständiges Meta(kunst)werk betrachtet werden können, gibt es nur wenige Editionsvorhaben – gedruckt oder digital – die sich der Herausforderung Notizbuch stellen. Zu den wenigen Projekten, die sich ausschließlich den Notizen eines Akteurs widmen, zählen die Hybrid-Edition von 
                Theodor Fontanes Notizbüchern (Radecke 2013) oder 
                Paul Klee - Bildnerische Form- und Gestaltungslehre (Eggelhöfer & Keller 2012). Als Beispiele für die Berücksichtigung von Notizen als Teil von umfassenderen Editionsprojekten seien hier stellvertretend die digitale Faksimile-Ausgabe Nietzschesource (D’Iorio 2009), die 
                Digital Edition of Fernando Pessoa (Sepúlveda & Henny-Krahmer 2017), die 
                Beckett-Edition (Van Hulle & Nixon 2018) oder das digitale Archiv 
                eMunch (Munch Museum 2011-2015) genannt.
            
         Das hier verwendete Korpus entstand als Teil einer Dissertation zur Erschließung und Untersuchung der Notizbücher des österreichischen bildenden Künstlers Hartmut Skerbisch (1945-2009) mit digitalen Methoden. Dabei handelt es sich um 35 Notizbücher, die knapp 40 Jahre künstlerischer Tätigkeit zwischen 1969 und 2008 auf insgesamt 2200 Seiten dokumentieren. Die im Zuge der Forschungsarbeit entstandene digitale Edition steht – derzeit in einer Betaversion – unter 
                
               https://gams.uni-graz.at/skerbisch
             im Geisteswissenschaftlichen Asset Management System (GAMS) des Zentrums für Informationsmodellierung der Universität Graz zur freien Verfügung und soll im Laufe des Jahres 2019 finalisiert werden.
            
         GAMS ist ein OAIS-konformes Asset-Management-System zur Verwaltung, Publikation und Langzeitarchivierung digitaler Ressourcen das sich an den FAIR Data Principles orientiert (Stigler & Steiner 2018: 209-211). 
                Damit wird eine gleichermaßen künstlerische und wissenschaftliche Ressource digital erschlossen und verfügbar gemacht, die bislang für die Öffentlichkeit unzugänglich war.
            
         Skerbisch befasste sich mit konzeptioneller Kunst, Medienkunst und Objektkunst und entzieht sich einer eindeutigen Zuordnung zu einer bestimmten Kunstrichtung. Seine 35 Notizbücher sind jedoch ohne Zweifel 
                konzeptionell (Sol LeWitt 1967; Kosuth 1969): Diese nutzte er für die Konzeption und Entwicklung seiner künstlerischen Ideen, seiner Gedankenexperimente, seines allgemeinen Verständnisses für künstlerische Konzepte und die Detailplanung seiner ausgeführten Kunstwerke, sowohl in textueller als auch grafischer Form. 
            
         Das Werk von Skerbisch reflektiert neben der grundsätzlichen Faszination für Technik und Wissenschaft zeitlebens Einflüsse aus Literatur, Musik und bildender Kunst. Er stellte die Materialität und die Weiterentwicklung des Raumbegriffs gegenüber der ästhetischen Hülle in den Vordergrund (Holler-Schuster 2015: 170; Scholger 2015: 83-86).
         Im Zusammenhang mit einer digitalen Repräsentation der Notizbuchinhalte stellen sich folgende Forschungsfragen: 
         
            a) Wie kann eine digitale Aufbereitung der Notizbücher zum Erkenntnisprozess über das Kunstwerk beitragen? 
            Besonders für die Rezeption und die Vermittlung zeitgenössischer Kunst erweisen sich Notizen als wertvolle Quelle: Auf Arbeiten mit neuen Medien und Materialien, in denen die Ästhetik des Werks nicht im Vordergrund steht, reagiert das Publikum oft verunsichert und verständnislos. Eine editorische Aufbereitung von Notizbüchern vermag jedoch Antworten und Erklärungen zugänglich zu machen, die das Kunstwerk bzw. der Künstler / die Künstlerin zum Zeitpunkt der Aus- oder Aufführung – mitunter auch durchaus bewusst – schuldig bleibt.  
            
         
            b) Wie können Notizbücher dazu beitragen, eine Werksidee, die nie zur Ausführung gekommen ist, oder eine Installation, die verloren gegangen ist, zu rekonstruieren? 
            Eine weitere Funktion von Notizbüchern gerade im Kontext zeitgenössischer Kunst ist jene der Dokumentation, insbesondere bei temporären und performativen Kunstwerken: Möglich wird damit eine – wenn auch nicht lückenlose – Rekonstruktion nicht zur Ausführung gelangter Werksideen und nicht mehr in ihrer Gesamtheit erhaltener Kunstwerke, deren Einzelteile oftmals für andere Installationen weiterverwendet wurden, oder die sogar nur noch durch Fotos, Berichte und Aussagen von Zeitgenossen dokumentiert sind.
            
         
            c) Welche Assoziationsprozesse fließen in die Werkkonzeption ein?
            Die Notizbücher enthalten eine Reihe an Querverweisen untereinander sowie Referenzen auf Kunstwerke, Personen, Literatur und Musik in Form von direkten oder indirekten Nennungen oder Zitaten. Mit dem Einbringen zusätzlichen Wissens durch Markup wird die Ressource inhaltlich erschlossen und stellt damit ein Werkzeug bereit, das den systematischen Interpretationsvorgang erleichtert und es ermöglicht, die Genese von Ideen, Assoziationen und Konzepten nachzuzeichnen. 
            
         Der Beitrag widmet sich den Resultaten dieser Forschungsfragen an die digital edierten Notizbücher von Hartmut Skerbisch und 1) präsentiert auf die Notizbücher angewandte Methoden, 2) zeigt die daraus gewonnenen Erkenntnisse und Ergebnisse und 3) versucht eine generelle Ableitung der verwendeten Methoden und Anwendbarkeit auf vergleichbare Quellen. Schließlich sollen die aus der digitalen Edition gewonnenen Erfahrungen in Bezug auf die digitale Repräsentation thematisiert werden: Welche Methoden erwiesen sich als effektiv, oder aber als wenig erkenntnisbringend in Relation zum geleisteten Ressourcenaufwand.
         Methoden
         Textkodierung/Textrepräsentation mit TEI
         Die angemessene digitale Repräsentation der textuellen Notizbucheinträge, die in etwa zwei Drittel des Korpus ausmachen, erfordert ein geeignetes Textmodell. Dieses Textmodell muss dem der Quelle inhärenten pluralistischen Charakter (Sahle 2013: 45-49) der Textgattung Notizbuch gerecht werden. So gilt es die Notizen inhaltlich, strukturell, materiell und visuell zu erfassen. Diese Anforderungen konnten weitgehend mittels des Standards der Text Encoding Initiative (TEI) abgedeckt werden, insbesondere mit dem Modul 11 zur Repräsentation von Primärquellen (TEI Consortium 2018: „Representation of Primary Sources“), das Empfehlungen zur Verzeichnung von Faksimiles, zur Verknüpfung von Text und Faksimile sowie dem Umgang mit einfachen und erweiterten Elementen für die Transkription zur Kodierung von Interventionen sowohl am Text als auch am Textträger (Streichungen, Hinzufügungen, unklare Stellen, Beschädigungen, Textumstellungen etc.) gibt. Die Eingriffe am Text lassen sich in Anlehnung an die Methode der 
                critique génétique weitgehend in TEI erfassen (Burnard et al. 2008; Brüning et al. 2013). Gerade bei Künstlernotizen steht jedoch nicht die Textgenese, sondern vielmehr die Genese einer künstlerischen Idee, eines Konzepts, im Fokus.
            
         Kodierung von Skizzen in TEI und RDF
         Neben dem Text sind es besonders Skizzen, die von Skerbisch zur Konzeption und Planung von Kunstwerken als Ausdrucksform eingesetzt wurden. Diese reichen von flüchtigen Skizzen bis hin zu detaillierten Zeichnungen mit Materialangaben und Abmessungen zur Ausführung. Sie nehmen in etwa ein Drittel der Notizbücher ein und bedürfen daher einer besonderen Betrachtung. Aus diesem Grund braucht es neben dem Textmodell ein Modell zur form- und inhaltsbezogenen Beschreibung von Skizzen, das die grafische Komponenten, die Textfunktionen und die Interpretationsebene berücksichtigt (siehe Abb. 1). 
         
            
            
               Abbildung 1. Modell zur Beschreibung von grafischen Komponenten
				
         
         Für die formale Beschreibung der grafischen Komponenten ist es nicht ausreichend, lediglich die Existenz von Skizzen im Text zu verzeichnen oder als illustrativen Zusatz zum Text zu verstehen. Vielmehr müssen die Skizzen sowohl als solitäre Einheit als auch in Verbindung mit dem Text stehend beschrieben werden können. Dazu wurden die Möglichkeiten der TEI und jene der Beschreibung von Ressourcen über das Resource Description Framework (RDF) kombiniert und ein eigenes Modell im Rahmen des Editionsprojektes entwickelt. Es beschreibt die grafischen Komponenten (Typ, Ansicht, Zeichenwerkzeug, Form, Datierung, Informationsträger), die Textfunktionen (Beschriftung, Titel, Beschreibung, Abmessung, Materialbezeichnung) und die Interpretationen (Kommentar, Bildgenese sowie Relationen zu Notizbucheinträgen, externen Referenzen, Manifestationen, Konzepten).
         Entitäten über RDF beschreiben
         Um die zahlreichen Relationen zwischen den Notizen untereinander aber auch von einzelnen Einträgen zu Entitäten in der real existierenden Welt – wie zu Personen, Literatur, Tonträgern und Kunstwerken – zu verzeichnen braucht es ein Modell, das diese Beziehungen abbildet und operationalisierte Abfragen auf diese Wissensbasis zulässt. Durch die Verknüpfung der Notizbucheinträge mit sachbezogenen Zusatzinformationen in Registern und Thesauri entsteht ein Informationsnetzwerk, das die individuellen Einträge in einen breiteren Kontext einbindet (Vogeler 2015). Diese Aussagen werden als RDF in einem Triple Store gespeichert und über SPARQL-Anfragen die Beziehungen zwischen Einheiten sichtbar gemacht. Neben der Anwendung des Referenzmodells CIDOC-CRM (Le Boeuf et al. 2018) werden, wo möglich, bestehende Authority Files wie die GND (Deutsche Nationalbibliothek 2012-2019), VIAF (OCLC 2012-2019), der Getty Art and Architecture Thesaurus (Getty Research Institute 2019) etc. integriert, um Konzepte formal zu beschreiben und dem Prinzip des 
                Linked Open Data zu folgen. 
            
         Ergebnisse
         Dieser Beitrag kombiniert methodische Ansätze – gewonnen aus der konkreten Arbeit an der digitalen Edition der Notizbücher von Hartmut Skerbisch – und versucht diese zu allgemeinen Aussagen zu formulieren, die für die Anwendung auf ähnliche Quellen herangezogen werden können. 
         Textgenese auf Dokumentebene
         Obwohl die Textkonstitution der Notizbucheinträge meist eine untergeordnete Rolle spielt, hat der Blick auf spezifische Editionsmethoden – wie jene der critique génétique – gezeigt, wie sich bestimmte Textsequenzen durch nachträgliche Interventionen durch den Künstler sinngemäß verändern.
         Text- und Ideengenese auf Korpusebene
         Textentwicklungen können bei Skerbisch nicht nur über direkte Textinterventionen am Dokument beobachtet werden, sondern auch über mehrere Notizbuchseiten innerhalb eines Heftes und sogar über das gesamte Korpus hinweg. Seine Methode sich einem Thema anzunähern, war auffallend oft eine Vielzahl an Wiederholungen, die er in den Notizbüchern verzeichnete (siehe Abb. 2). 
         
            
            
               Abbildung 2. Wiederholung und Modifikation einer Phrase in mehreren Notizbüchern
				
         
         Entwicklung einer Idee
         Eine zentrale Rolle in der Auswertung und Interpretation der Notizbücher spielt die Entwicklung einer Idee. Skerbischs Werke entstanden über einen langen Zeitraum intensiver Beschäftigung mit den Themen, die er vermitteln wollte. Hier entsteht ein Informationsnetzwerk aus Notizbucheinträgen, künstlerischen Manifestationen, physischen Objekten, externen Einflüssen aus der Literatur und werkübergreifenden künstlerischen Konzepten, das nur über die elektronische Erschließung des Materials zusammengefügt werden kann (siehe Abb. 3).
         
            
            
               Abbildung 3. Entwicklung einer Idee
				
         
         Das Editionsmodell eignet sich für Textgattungen mit ähnlichen materiellen, formalen und inhaltlichen Eigenschaften, wie a) fragmentarische Einträge, b) grafische Darstellungen, die als Bedeutungsträger einer gesonderten Betrachtung bedürfen, c) Referenzen auf externe Werke aus Literatur, Kunst und Musik, d) Namens- und Ortsnennungen, e) Verzeichnung von Zitaten, f) Textinterventionen sowie g) Text- und Ideengenese.
         Die Zusammenführung unterschiedlicher Methoden (Textrepräsentation mit TEI, Formalisierung der Skizzen mit TEI und RDF, Beschreibung von Entitäten mit RDF) ermöglicht es die Entwicklung spezifischer Ideen nachzuvollziehen. Das Ergebnis ist eine umfangreiche Dokumentation des kreativen Prozesses, von der konzeptionellen Notiz zur künstlerischen Manifestation.
      
      
         
            
               Bibliographie
               
                  Allemang, Dean / Hendler, James (2011): 
                  Semantic Web for the Working Ontologist. Effective Modeling in RDFS and OWL. Oxford: Elsevier.
                    
               
                  Burnard, Lou / Jannidis, Fotis / Pierazzo, Elena / Rehbein, Malte (2008-2013):
                  An Encoding Model for Genetic Editions. 
                        http://www.tei-c.org/Activities/Council/Working/tcw19.html 
                    
               
                  Brüning, Gerrit / Henzel, Katrin / Pravida, Dietmar (2013):
                  Multiple Encoding in Genetic Editions: The Case of ‘Faust’, 
                        in: Journal of the Text Encoding Initiative 4 http://jtei.revues.org/697
                    
               
                  Deutsche Nationalbibliothek (2012–2019): 
                  GND – Gemeinsame Normdatei, https://www.dnb.de/gnd 
                    
               
                  D’Iorio, Paolo (2009): 
                  Nietzschesource. 
                        http://www.nietzschesource.org/
               
               
                  Eggelhöfer, Fabienne / Keller Tschirren, Marianne (2012):
                  Paul Klee – Bildnerische Form- und Gestaltungslehre. 
                        
                     http://www.kleegestaltungslehre.zpk.org
                  
               
               
                  Getty Research Institute (2018): 
                  Art & Architecture Thesaurus (AAT). 
                        https://www.getty.edu/research/tools/vocabularies/aat/index.html
               
               
                  Holler-Schuster, Günther (2015):
                  Hartmut Skerbisch’s work between media art and land art, 
                        in: 
                        Verein der Freunde von Hartmut Skerbisch (ed.):
                  Hartmut Skerbisch. Life and Work. Present as Present. 
                        Wien: Verlag für moderne Kunst 142-171.
                    
               
                  Kosuth, Joseph (1969a):
                  Art after philosophy I, 
                        in: Studio International (915): 134-37.
                    
               
                  Le Boeuf, Patrick / Doerr, Martin / Emil Ore, Christian / Stead, Stephen (2018):
                  Definition of the CIDOC Conceputal Reference Model. 
                        Version 6.2.4. 
                        http://www.cidoc-crm.org/sites/default/files/2018-10-26%23CIDOC%20CRM_v6.2.4_esIP.pdf
               
               
                  LeWitt, Sol (1967):
                  Paragraphs on Conceptual Art, 
                        in: Artforum 5 (10): 80-84.
                    
               
                  Maugham, William Somerset (1949): 
                  A writer's notebook, Melbourn-London-Toronto: William Heinemann.
                    
               
                  Munch Museum (2011-2015): 
                  eMunch. Edvard Munchs Tekster. Digitalt Arkiv. 
                        http://www.emunch.no
               
               
                  OCLC - Online Computer Library Center (2010–2016): 
                  VIAF – Virtual International Authority File, https://viaf.org/.
                    
               
                  Radecke, Gabriele (2013):
                  Notizbuch-Editionen. Zum philologischen Konzept der Genetisch-kritischen und kommentierten Hybrid-Edition von Theodor Fontanes Notizbüchern, 
                        in: editio 27 149-172. 10.1515/editio-2013-010
                    
               
                  Sahle, Patrick (2013):
                  Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. Teil 3: Textbegriffe und Recodierung, 
                        in: Schriften des Instituts für Dokumentologie und Editorik 9. Norderstedt: Books on Demand.
                    
               
                  Scholger, Martina (2015):
                  Tracing the association processes of the Artist – The artwork as a commentary, 
                        in: 
                        Verein der Freunde von Hartmut Skerbisch (ed.): 
                  Hartmut Skerbisch. Life and Work. Present as Present. Wien: Verlag für moderne Kunst 305-3.
                    
               
                  Sepúlveda, Pedro / Henny-Krahmer, Ulrike (2017): 
                  Fernando Pessoa – Digitale Edition. Projekte und Publikationen. 
                        http://www.pessoadigital.pt
               
               
                  Stigler, Johannes Hubert / Steiner, Elisabeth (2018):
                  GAMS – Eine Infrastruktur zur Langzeitarchivierung und Publikation geisteswissenschaftlicher Forschungsdaten, 
                        in: Mitteilungen der Vereinigung Österreichischer Bibliothekarinnen und Bibliothekare 71 (1), 207-216. 
                        
                     https://doi.org/10.31263/voebm.v71i1.1992
                  
               
               
                  TEI Consortium (2018): 
                  TEI P5: Guidelines for Electronic Text Encoding and Interchange. Version 3.4.0.
                  http://www.tei-c.org/Vault/P5/3.4.0/doc/tei-p5-doc/en/html/
               
               
                  Van Hulle, Dirk / Nixon, Mark (2019):
                  Samuel Beckett Digital Manuscript Project.
                  
                     http://www.beckettarchive.org/
                  
               
               
                  
                  Vogeler, Georg (2015):
                  Warum werden mittelalterliche und frühneuzeitliche Rechnungsbücher eigentlich nicht digital editiert?, 
                        in: Grenzen und Möglichkeiten der Digital Humanities, edited by Constanze Baum and Thomas Stäcker. Sonderband der Zeitschrift für digitale Geisteswissenschaften 1. 10.17175/sb001_007
                    
            
         
      
   



      
         Dieses Poster stellt ein Annotationssystem für graphische Narrative vor. Mittels einer auf Java basierten graphischen Oberfläche sind Annotator_innen in der Lage, graphische Narrative und andere statische Bild/Text-Medien in XML zu annotieren. Grundlage für die hier verwendete XML-basierte Annotationssprache, „Graphic Novel Markup Language“, kurz GNML, ist die von John Walsh entwickelte, auf TEI basierende, „Comic Book Markup Language“, kurz CBML. Abbildung 1 zeigt einen Überblick über die in GNML darstellbaren Objekt-Typen und deren Beziehungen.
         Das System ermöglicht Annotator_innen die Annotation verbaler und visueller Repräsentation auf der Comicseite; dazu zählen unter anderem Panels, Charaktere, Sprechblasen, Captions, Erzähltext, Sprechtext, Onomatopoeia und diegetischer Text. Figurenkonstellationen können über verschiedene spezialisierte Interaktionsmuster festgehalten werden. Darüber hinaus erfassen narratologische Tools eine Übersicht über subjektive Filterung einer Erzählsituation (Fokalisierung), Diegese einer Erzählsituation sowie die Hierarchisierung von Erzählwelten.
         
            
            Abbildung 1. Überblick über die verschiedenen Objekt-Typen in GNML
         
         Features
         Das Annotieren wird Nutzern durch die Automatisierung einiger Prozesse erleichtert. So lokalisiert das Annotationssystem auf Basis von Verfahren, wie z.B. dem aus der Computergraphik bekannten Marching Squares Algorithmus, die Konturen der Panels. Einfachere Strukturen wie z.B. Sprechblasen oder Captions müssen vom Nutzer nur durch einen einfachen Klick auf den Hintergrund selektiert werden, so dass danach mit Hilfe eines Floodfill-Verfahrens die komplette Sprechblase erkannt wird. Komplexere Repräsentationen wie Charaktere können mithilfe einer auf der Livewire Segmentation (Mortensen & Barrett, 1995), die beim ungenauen Umranden eines Charakters sich an dessen kontrastreiche Kanten heftet und so die Kontur des Charakters genau erfasst, effizient graphisch annotiert werden. Eine automatische Texterkennung ist derzeit noch nicht integriert, da diese aufgrund der schwer zu erfassenden, oft verwendeten pseudo-handschriftlichen Fonts noch keine befriedigenden Ergebnisse liefert. Dieser wird aber hoffentlich für spätere Versionen in einer ausreichenden Qualität entweder direkt integriert in den Editor oder als optionaler Vorverarbeitungsschritt verfügbar sein. Um aber dennoch die Fehlerrate bei der manuellen Eingabe der Texte und Charakter-Namen möglichst gering zu halten, sind Mechanismen wie eine automatische Rechtschreibprüfung oder Autovervollständigung eingebaut.
         Die Beta-Version des Annotationssystems wurde zusammen mit einer Gruppe von Studierenden am „Graphic Narrative Corpus“ (GNC), dem ersten digitalen Korpus für englischsprachige graphische Narrative, getestet (Dunst, Hartel, & Laubrock, 2017). Eines der Hauptziele des Annotationssystems ist die quantitative Analyse, welche auch und vor allem etablierte narratologische Terminologie auf empirische Evidenzen hin prüft. Die Operationalisierung bestehender narratologischer Diskurse in einer für digitale und quantitative Forschung optimierten Testumgebung erfordert die umfassende Gestaltung einer narratologischen Annotationsebene. Grundlage dieser Annotationsebene sind sowohl Theorien der kognitiven und transmedialen Narratologie als auch empirische Daten, die mithilfe des vorgestellten Annotationssystems erhoben und anschließend analysiert wurden. 
         Abbildung 2 zeigt die Oberfläche des Annotationssystems mit einer Beispielannotation.
         
            
            Abbildung 2. Graphische Oberfläche und Beispielannotation des Comics „Pepper & Carrot“ (Revoy, 2017)
         
         Ausblick
         Durch die quantitative Operationalisierung etablierter qualitativer narratologischer Terminologie fördert das Annotationssystem die Erweiterung digitaler und quantitativer Methodologien in den Geisteswissenschaften. Komplexe Text-Bild-Interaktionen in graphischen Narrativen werden so mithilfe des Annotationssystems der Analyse von größeren Korpora eröffnet und ermöglichen eine weitreichende digitale Analyse multimodaler Narrative. Gleichzeitig trägt die evidenzbasierte Untersuchung dieser Elemente in zweierlei Hinsicht zu einem besseren Verständnis theoretischer Modelle bei: Zum einen zeigen die annotierten Daten die Möglichkeiten und Grenzen traditioneller Begrifflichkeiten an konkreten Analysebeispielen auf; zum anderen bietet die Auseinandersetzung von Annotator_innen mit komplex erzählten graphischen Narrativen (beispielsweise im Seminarkontext) einen hohen didaktischen Wert insofern, als dass sich Annotator_innen anhand von Fallbeispielen mit narratologischen Konzepte und Problemen kritisch auseinandersetzen.
         Die letzte Version des Annotationssystems ist frei verfügbar unter:
         
            http://graphic-literature.upb.de/?page_id=3592
         
         Eine FAQ zum Annotieren mit dem Programm ist zu finden unter:
         
            http://graphic-literature.upb.de/?page_id=4123
         
      
      
         
            
               Bibliographie
               
                  Dunst, A. / Hartel, R. / Laubrock, J. (2017): The Graphic Narrative Corpus (GNC): Design, Annotation, and Analysis for the Digital Humanities. 2nd International Workshop on coMics Analysis, Processing, and Understanding, 14th IAPR International Conference on Document Analysis and Recognition. Kyoto, Japan.
                    
               
                  Mortensen, E. / Barrett, W. (1995): Intelligent scissors for image composition. Proceedings of the 22nd annual conference on Computer graphics and interactive techniques (pp. 191-198). ACM.
                    
               
                  Revoy, D. (2017): Pepper & Carrot. Episode 21: The Magic Contest. (Pepper and Carrot) Retrieved 2018 10, from https://www.peppercarrot.com/en/article400/episode-21-the-magic-contest
               
            
         
      
   



      
         
            Das Abstract im Kontext
            Die Autoren haben auf der DHd2019 einen Hackathon zum Book of Abstracts durchgeführt, in dessen Rahmen sich die Teilnehmenden nicht nur mit der digitalen Publikation, sondern auch mit der Normierung der biobibliographischen Angaben und den Potentialen einer inhaltlichen Analyse der Abstracts auseinander gesetzt haben. (Andorfer et al. 2019) Dieser Beitrag greift die Erkenntnisse der Veranstaltung auf und soll sowohl die konzeptuelle Auseinandersetzung als auch die konkrete Implementierung weiterführen sowie die im Nachgang des Workshops erfolgten Arbeiten präsentieren. Dabei werden im Sinne des Konferenzthemas insbesondere experimentelle Ansätze hervorgehoben. Zwei Aspekte der Konferenzabstracts stehen im Fokus des Beitrags: 1.) das Abstract als eigenständige und reputierliche Publikation und 2.) die Abstracts als Datenquelle selbstreflexiver Untersuchungsansätze in den DH. Die wissenschaftliche Relevanz der Book of Abstracts der DHd-Jahrestagung bekräftigte zuletzt noch einmal Sahle in seiner Einführung des letzten Konferenzbandes (Sahle 2019, S.): „Books of Abstracts als durch peer review-Verfahren gefilterte und qualitätsgesicherte Summen der aktuellen Forschungen definieren das Feld, sind ein äußerst nützliches Instrument der Fachkommunikation und wertvolle Dokumente zum Beleg der Entwicklung über die Zeit.“
         
         
            Das Abstract als Publikation
            Der Anspruch an die Veröffentlichung der Konferenzbeiträge wird in
dem bereits 2018 konstatierten „Status einer wissenschaftlich
nutzbaren Publikation“ (Vogeler 2018) deutlich und mit dem letzten
Band der DHd 2019 noch einmal unterstrichen (Sahle 2019): „Um das Ziel
ganz klar zu formulieren: die hier vorgelegten Abstracts sind
wissenschaftliche Texte eigenen Rechts, die auch bibliografisch
fassbar sein sollen, um die eigenen Forschungsgebiete und die
gewonnenen Erkenntnisse sichtbar machen zu können.“ Auf dem Weg zu
einer digitalen Publikation, die sowohl informationswissenschaftliche
Standards erfüllt als auch informationstechnologische Potentiale
ausreizt, ergeben sich zum jetzigen Stand noch viel Raum für
Entwicklung der Konferenzbeiträge (Cremer 2018). Die Book of Abstracts
werden als Gesamtband in einer Druckfassung sowie als PDF
publiziert. Daneben werden einzelne Beiträge von den Vortragenden auf
verschiedenen Repositorien oder Webseiten unsystematisch
veröffentlicht, darunter einzelne Abstracts, Poster, Präsentationen
oder zugrundeliegende Daten. Der Beitrag evaluiert die Möglichkeiten und Voraussetzungen für eine eigenständige Publikation der einzelnen Abstracts mit persistenter Speicherung, zitationsfähiger Adressierung, bibliografischer Erfassung und multipler Repräsentationsform (PDF, HTML, TEI). Dabei werden auch dezentrale (z.B. Zenodo-Community) und zentrale Ansätze (z.B. zentrale Redaktion, eigene Infrastruktur) verglichen. Gegenüber traditionellen Formaten und Infrastrukturkomponenten im Sinne einer reputierlichen und zitierfähigen Publikationsform sollen auch experimentelle Repräsentationsformen betrachtet werden, um die vorhandenen Spielräume digitaler Publikationen auszuloten. Als Ausgangspunkt ist hier die im Rahmen des Hackathons entwickelte Präsentationsschicht zu nennen.
            
         
         
            Das Abstract als Daten
            
Die Konferenzabstracts als TEI-basierte Veröffentlichungen demonstrieren ihr Potential als Untersuchungsgegenstand innerhalb des eigenen Faches (Sahle/Henny-Krahmer 2018; Hannesschläger/Andorfer 2018; Hoenen 2019; Kiefer 2019). Ein Desiderat der Untersuchungen bis dato ist die Betrachtung und Auswertung der in den Abstracts zitierten Literatur. Die Bibliographie wissenschaftlicher Artikel dient in der geisteswissenschaftlichen Forschung neben dem Nachweis der zitierten Literatur auch als Ressource für Recherche und Kontextualisierung (Andorfer, DWP 14, S. 24-25) sowie als Datenquelle für die Analyse von Publikations- und Zitationspraktiken (Nyhan/Duke-Williams 2014). Gerade in den Digital Humanities eröffnen sich durch die Verbindung mit Methoden der Netzwerkanalyse neue Untersuchungsansätze (Gao et al. 2018). Im Rahmen des Hackathons wurden die eingereichten Abstracts über Skripte automatisiert mit zusätzlichen Informationen angereichert sowie über manuelle Arbeiten in ihren Metadaten vereinheitlicht.
Die bibliographischen Angaben in den Abstracts lagen jedoch in zu heterogenen Formen vor, so dass Auswertungen und Visualisierungen nicht möglich waren. Für das Poster werden diese Daten mit Unterstützung der DHd-AG Digitales Publizieren vereinheitlicht und in der Folge durch die Autoren in ersten Analyseergebnissen und Visualisierungen ausgewertet. Die aufgezeigten Potentiale ließen sich zudem multiplizieren, wenn auch die Konferenzabstracts früherer und folgender DHd-Tagungen aufbereitet werden können, um so auch Entwicklungen und Tendenzen eruieren zu können.

         
         
            Das Abstract in der Diskussion
            Viele Jahre nach Christines Borgmans “Call to Action for the Humanities” (Borgman 2010), der auch das digitale Publizieren jenseits der simplen Konversion der Papiermedien in das PDF-Format inkludierte, werden auch in den Digital Humanities die Möglichkeiten nicht ausgeschöpft und traditionelle Praktiken gepflegt (Kaden/Kleineberg 2017) – von dem Wechsel einer layoutbasierten zu einer strukturbasierten Publikationstechnik ganz zu schweigen (Stäcker 2013). Die DHd-Konferenzabstracts bergen dabei das Potential dieses Paradigma zu durchbrechen: die XML-basierte Einreichung, die datengestützten Analysemethoden und selbstreflexiven Ansätze des Faches, die technische Expertise der Einreichenden, die Kürze der Beiträge und die enge Vernetzung mit Infrastruktureinrichtungen. Das Poster soll auf die bisher erfolgten Arbeiten und die erzielten Ergebnisse aufmerksam machen sowie vor Ort die Diskussion um Möglichkeiten und Ressourcen sowie Relevanz und Reputation einer „erweiterten Publikation“ der DHd-Abstracts weiterführen. Die Autoren werden im Vorfeld der DHd2020 mit dem Organisationskomitee zur Anreicherung der diesjährigen Abstracts sowie Nutzung der HTML-Präsentationsschicht in Kontakt treten.
         
      
      
         
            
      DHd-Community bei Zenodo: 
            
            DHd 2019 Book of Abstracts Hackathon: 
    https://dhd-boas-app.acdh-dev.oeaw.ac.at
            
            
      Die aufbereiteten Daten sind zu finden unter: https://github.com/csae8092/dhd-boas-data.
    
         
         
            
               Bibliographie
               
                   Andorfer, Peter  (2015): 
  Forschungsdaten in den (digitalen) Geisteswissenschaften. Versuch einer Konkretisierung, DARIAH-DE Working Papers, 14 
  urn:nbn:de:gbv:7-dariah-2015-7-2[letzter Zugriff 30.08.2019].

               
                   Andorfer, Peter / Cremer, Fabian / Steyer, Timo  (2019): DHd 2019 Book of Abstracts Hackathon, in: 
  DHd 2019 Digital Humanities: multimedial & multimodal. Konferenzabstracts 
                  [letzter Zugriff 30.08.2019].

               
                   Borgman, Christine L.  (2010): The Digital Future is Now: A Call to Action for the Humanities, in: 
  Digital Humanities Quarterly 003, Nr. 4.

               
                   Cremer, Fabian  (2018): Nun sag, wie hältst Du es mit dem Digitalen Publizieren, Digital Humanities?, in: 
  Blog. Digitale Redaktion (blog)
                  [letzter Zugriff 30.08.2019].

               
                   Gao, J. / Duke-Williams, O. / Mahony, S. / Ramdarshan Bold, M. / Nyhan, J.  (2017):The Intellectual Structure of Digital Humanities: An Author Co-Citation Analysis, in: 
  Digital Humanities 2017
                   [letzter Zugriff 30.08.2019].

               
                   Hall, Mark  (2019): DH is the Study of dead Dudes, in:
  Hd 2019: multimedial & multimodal Konferenzabstracts: 111-113
  [letzter Zugriff 30.08.2019].

               
                   Hannesschläger, Vanessa / Andorfer, Peter  (2018): 
  Menschen gendern? Datenmodellierung zur Erhebung von Geschlechterverteilung am Beispiel der TEI2016 Abstracts App
                  [letzter Zugriff 30.08.2019].

               
                   Henny-Krahmer, Ulrike / Sahle, Patrick  (2019): Einreichungen zur DHd 2018, in: 
  DHd-Blog (blog), 29. März 2019 
  [letzter Zugriff 30.08.2019].

               
                   Hoenen, Armin  (2019): Einreichungen zur DHd 2019 II, in: 
  DHd-Blog (blog), 29. März 2019 
  [letzter Zugriff 30.08.2019].

               
                   Kaden, Ben / Kleineberg, Michael  (2019): Zur Situation des digitalen geisteswissenschaftlichen Publizierens – Erfahrungen aus dem DFG-Projekt ‚Future Publications in den Humanities‘, in: 
  Bibliothek Forschung und Praxis 41, Nr. 1 (2017): 7–14 
  [letzter Zugriff 30.08.2019].

               
                   Kiefer, Katharina  (2019): Einreichungen zur DHd 2019, in: 
  DHd-Blog (blog), 29. März 2019 
  [letzter Zugriff 30.08.2019].

               
                   Nyhan, Julianne / Duke-Williams, Oliver  (2014): Joint and Multi-Authored Publication Patterns in the Digital Humanities, in:
  Literary and Linguistic Computing 29, Nr. 3 (1. September 2014) 
  [letzter Zugriff 30.08.2019].

               
                   Sahle, Patrick (ed.)  (2019): 
  DHd 2019 Digital Humanities: multimedial & multimodal. Konferenzabstracts. Frankfurt am Main: Zenodo, 2019 
  [letzter Zugriff 30.08.2019].

               
                   Stäcker, Thomas  (2013): Wie schreibt man Digital Humanities richtig?, in: 
  Bibliotheksdienst 47, Nr. 1 
  [letzter Zugriff 30.08.2019].

               
                   Vogeler, Georg (ed.)  (2018): 
  DHd 2018: Kritik der digitalen Vernunft. Konferenzabstracts. Köln, Universität zu Köln, 2018.

            
         
      
   



      
         
            Die Infrastuktur und das Projekt
            Seit 2010 kooperieren das Wittgenstein Archiv der Universität Bergen und das Centrum für Informations- und Sprachverarbeitung der Ludwig-Maximilians Universität München in der Forschungsgruppe „Wittgenstein Advanced Search Group“ (WAST). Die Forschungsgruppe entwickelt Web-Frontends (FinderApps) und spezielle Suchwerkzeuge, die sich gut für die Forschung und Lehre im Bereich der Digital Humanities eignen. Ihre erste Suchmaschine, die FinderApp WiTTFind (wittfind.cis.lmu.de, siehe Abb. 1), die den von der UNESCO zum Weltkulturerbe (im Jahr 2017) erhobenen (Schmidt 2018) Nachlass von Ludwig Wittgenstein durchsucht, gewann im Jahre 2014 der EU-Open-Humanity Award. Der Preis zeichnet Gruppen aus, die herausragende Technologie im Bereich der Humanities entwickelt haben. Die in der Forschergruppe programmierte FinderApp WiTTFind erlaubt es, mit hochqualifizierten, computerlinguistisch orientierten Suchwerkzeugen Nachlasstrans-kriptionen zu durchsuchen. Die Transkriptionen entstammen der 
                    Bergen Normalized Edition, die die Grundlage der Wittgenstein Edition bildet. Neben den gefundenen Treffern der Suchmaschine, werden in den Suchergebnissen von WiTTFind die Faksimile-Extrakte aus den Originaldokumenten angezeigt. So kann der Nutzer die „Aura“ der gefundenen Textstelle im Original studieren und nicht nur den transkribierten Text sehen.
                
            
               
                  
                   Abbildung 1: WiTTFind (http://wittfind.cis.lmu.de)
               
            
            Damit derNutzer auch den seitenweisen Kontext des Suchtreffers im Original studieren kann, wurde am CIS eine weitere WEB-Applikation entwickelt, der doppelseitige Reader. Dieser Reader ermöglicht es, vom Suchtreffer direkt an die entsprechende Stelle im entsprechenden Dokument des Originals zu springen. Im doppelseitigen Lesemodus kann der Nutzer in den Faksimile des originalen Dokuments blättern. Eine symmetrische Autovervollständigung gibt während der Suchanfrage einen statistischen und lexikalischen Zugang zu den Wörtern, die in der Edition vorkommen. Im Zentrum der Suche steht die selbstprogrammierte C++ Suchmaschine wf, die mit Hilfe von Vollformlexika (WiTTlex), verbessertem POS-Tagging und weiteren Metainformationen regelbasiertes Suchen erlaubt. Zum Aufspüren semantisch ähnlicher Textpassagen in der Edition gibt es das NLP-Tool WiTTSim.
            Die thematisch getrennten Aufgaben innerhalb der Infrastruktur der WAST-Tools (siehe Abb. 2) werden über REST-API’s von einzelnen Microservices realisiert, deren zentrale Datenhaltung über eine mongo Datenbank realisiert wird. Die Oberflächen der FinderApps werden mit HTML5, Javascript und Bootstraptechniken für WEB-Browser programmiert und möglichst browserunabhängig gehalten. 
            
               
                  
                   Abbildung 2: Infrastruktur der WAST-Tools (http://gitlab.cis.lmu.de)
               
            
            Alle Programme, Schnittstellen und Entwicklungen werden dokumentiert (siehe Abb. 3) und Tutorials für Anschlussprojekte entwickelt. So ist gewährleistet, dass die Tools und Suchmaschinen nachhaltig verwendet und auch für die Forschung und Lehre eingesetzt werden können. Als Versionskontrollsystem wird git verwendet.
            
               
                  
                   Abbildung 3: Dokumentation der WAST-Tools: http://wittfind.cis.uni-muenchen.de/wast/infrastruktur/index.html
                  
               
            
            Bei der Entwicklung der Infrastruktur der WAST-Tools wurden die strengen Vorgaben des EU-Open-Humanity Awards eingehalten: Forderungen nach Open-Source, interdisziplinäre Öffnung und Nachhaltigkeit. Diese Offenheit ermöglichte es weitere FinderApps für andere Wissenschaftsbereiche zu implementieren: GoetheFind (Faust-I und Faust-II Edition, Deutsches Textarchiv Berlin (XML-TEIP5, DTA Basis Format)), HistoFind (Briefwechsel Erzherzog Leopold Wilhelms an Kaiser Ferdinand III. aus dem Reichsarchiv Stockholm; Kooperation mit Historikern) und den OdysseeReader (Schreibprozess der zur Logisch-Philosophischen-Abhandlung führte; Kooperation mit Philosophen).
            In diesem Workshop werden die verwendeten Softwaretechnologien und computerlinguistischen Methoden im konkreten Einsatz vorgestellt. Den Teilnehmer*innen wird ein Debian-10 Container mit allen notwendigen Programmen, Tools und Dokumentation der gesamten Softwareinfrastruktur zur Verfügung gestellt. Innerhalb dieses Containers können die Teilnehmer*innen die einzelnen Tools der WAST-Projektgruppe kennenlernen und bekommen von den Projektmitarbeiter*innen kleine Aufgaben gestellt, die sie dann mit ihnen bearbeiten. So können sie die Arbeitsweise der WAST Infrastruktur konkret kennenlernen.
         
         
            Im Workshop werden folgende Datenformate, Tools und Programmierkonzepte vorgestellt und geübt
            Gitlab Projektmanagement und Continuous Integration, XML TEI-P5 Edition CISWAB, Faksimilestrukturierung und Texterkennung, lexikalische Arbeit, WEB-Oberfläche der FinderApps und Einsatz mit Micorservices, doppelseitiger Faksimilereader mit MongoDB, NLP-Tools zur semantischen Ähnlichkeitssuche, Vorstellung und Programmierung einer regelbasierten Suchmaschine und die Erstellung eines Dokumentationssystems mit Sphinx.
            
               Voraussetzungen an die Kursteilnehmer*innen
               Programmierkenntnisse (Grundkenntnisse): LINUX (Arbeit mit der UNIX-Shell), Python, XML, HTML, git, javascript, POS-Tagging.
               Da beim Workshop einige Entwickler der WAST-Tools anwesend sein werden, gibt es die Möglichkeit auch vertieft in die jeweilige Thematik einzusteigen.
            
            
               Gitlab Projektmanagement und Continuous Integration (Hadersbeck, Still)
               Im gesamten Projekt wird als Versionierungssystem git verwendet. Die Projektrepositories werden auf zwei unterschiedlichen Rechnern ausgerollt: Dem preview-Server für Tests und einem Projektserver für die offizielle Onlineversion. Es wird das in der Praxis bewährte „git branching model“ kombiniert mit einer „continuous integration“ Technik eingesetzt. Mit einer Feedbackapp können Nutzer Fehler melden oder Implementierungswünsche äußern, die in Issues innerhalb der Projektrepositories bearbeitet werden.
            
            
               XML TEI-P5 Edition CISWAB (Hadersbeck)
               Als Datenbasis für das WiTTFind Projekt wird die „Bergen Nachlass Edition“ (BNE) verwendet, die sich an den Richtlinien der Text Encoding Initiative (TEI-P5) orientiert. Im Workshop werden die wichtigen TEI-XML-Elemente der BNE vorgestellt.
                  
            
            
               Faksimilestrukturierung und Erkennung (Eisterhues, Landes)
               Da in den FinderApps neben den gefunden Textstellen auch die zugehörigen Faksimileextrakte aus der Edition dargestellt werden, sind Kenntnisse der Bildkoordinaten der Textstellen nötig. Diese Koordinaten werden mit Hilfe einer Kette von Bildverarbeitungstools ermittelt. Da bei Manuskripten und bei manuellen Änderungen in Dokumenten die automatische Zeichenerkennung unbrauchbare Ergebnisse liefert, wurden eigene Strategien entwickelt, die die Informationen aus der BNE nutzen. Im Workshop werden die eingesetzten Tools und Optimierungsstrategien vorgestellt.
            
            
               Lexikalische Arbeit (Lokale Grammatiken, Semantik) (Röhrer)
               Zur lemmatisierten Suche, Partikelverberkennung und semantischen Wortfeldern wurden spezielle Projektlexika entwickelt (Röhrer 2017). Die Lexika enthalten alle Wörter der zu durchsuchenden Edition und sind mit grammatischen Angaben und zum Teil mit zusätzlichen semantischen Informationen versehen. Diese Lexika und ein nachgestelltes optimiertes Part-of-Speech Tagging ist die Grundlage für die computerlinguistischen Methoden, die bei der regelbasierten Suche im Nachlass von Ludwig Wittgenstein eingesetzt werden.
            
            
               Regelbasierte Suchmaschine (Babl)
               Im Zentrum der FinderApps steht die Suchmaschine wf, ein multithreaded C++ Programm, das viele Anfragemöglichkeiten zur Suche implementiert: Einwort und Mehrwortsuche (mit internem Rankingverfahren) und reguläre Ausdrücke kombiniert mit linguistischen Anfragen (Morphologische Eigenschaften, POS-Tags, semantische und syntaktische Tags). Für das Rankingverfahren wird für jeden Suchtreffer die Relevanz zur Suchanfrage berechnet. Die Qualität für jeden Suchtreffer, die Distanz zwischen den einzelnen Wörtern und unterschiedlichen Belohnungs- und Bestrafungsparametern, gehen in die Berechnung der Relevanz ein. Die Treffer werden dann nach dieser sortiert und auf der Website ausgegeben. Durch dieses neuartige Ranking kann nun auch nach verschiedenen Wörtern gesucht werden, die im Text nicht direkt hintereinander stehen müssen.
            
            
               NLP-Tool Semantische Ähnlichkeitssuche (Ullrich)
               Zur Extraktion von semantisch ähnlichen Bemerkungen wurde das Analysetool WiTTSim (Ullrich 2018) entwickelt, welches anhand von semantischen und syntaktischen Features ähnliche Texte identifiziert. Da die enorm hohe Anzahl von etwa 100.000 Features in Kombination mit den zu vergleichenden 54.000 Bemerkungen eine effiziente Suche unmöglich macht, wurde ein semantisches Clustering-Verfahren vorgeschaltet (Ullrich 2019), welches durch Dimensionsreduktion und Gruppierung der Texte die Rechenzeit der Ähnlichkeitssuche um den Faktor 100 beschleunigt.
            
            
               WEB-Oberfläche der FinderApps und Micorservices (Hadersbeck, Still)
               Zur Arbeit mit WiTTFind wird dem User eine WEB-basierte FinderApp zur Verfügung gestellt, die über REST-APIs und „internet microservices“ mit den WAST-Tools kommuniziert. HTML5, Javascript und Bootstrap-css erlauben den Aufbau der WEB-page, die nahezu browserunabhängig die Schnittstelle zum Anwender darstellt. 
            
            
               Doppelseitiger Faksimilereader und MongoDB (Lindinger)
               Der doppelseitiger Faksimilereader ist eine komplett eigenständige Anwendung mit Suchschlitz und Investigate Mode zur gleichzeitigen Betrachtung von Faksimile und Transkription. Außerdem gibt es zahlreiche weitere Features, die es den Nutzern sehr bequem erlauben, die gefunden Treffer der Suchmaschine im Kontext einer doppelseitigen Darstellung der Faksimile zu sehen und gleichzeitig durch die Dokumente der Forschungsdomäne zu blättern. Sämtliche Informationen bzgl. Edition und Faksimile sind in einer MongoDB gespeichert und werden über HTTP-Schnittstellen abgefragt.
            
            
               Dokumentationssystem Sphinx (Babl) (siehe Abb.2)
               Für jedes Teilprojekt der Wittgenstein Advanced Search Tools (WAST) wird im entsprechenden Gitlab Ordner eine README.md Datei erstellt, das in einer Dokumentation, die alle Projekte umspannt mithilfe der Software Sphinx zusammengefasst und online auf ansprechende Art und Weise darstellt. Die Dokumentation hilft, neuen Studierenden einen schnelleren Einstieg in das Projekt zu finden und ermöglicht es, das gesamte WAST-Projekt schnell nach bestimmten Fachbegriffen zu durchsuchen. 
            
         
         
            Programm des Workshops (ganztages Workshop)
            
               Überblick/Einführung/Vorstellungsrunde 
               Digitaler Zugang zum Nachlass von Ludwig Wittgenstein, das Projekt WAST (Dr. Max Hadersbeck)
               Fragen/ Diskussion/ gewünschte Schwerpunkte der Teilnehmer*innen des Workshops
            
            
               
                  WAST-Spezialthemen (jeweils ca. 15 Min. Theorie / 20 Min. Praxis)
               
                   Gitlab Projektmanagement und Continuous Integration mit git production / testing server (Hadersbeck, Still) 
      
                   XML TEI-P5 Edition CISWAB (Hadersbeck): Bergen Normalized Edition und xslt-Transformationen und Investigate-Mode von WiTTFind
                   Faksimilestrukturierung und OCR Erkennung (Eisterhues, Landes) 
                   Lexikalische Arbeit (Röhrer): Lemmatisierte Suche, Lexika, Lokale Grammatiken, Query Beispiele
                   WEB-Oberfläche der FinderApps und Microservices (Hadersbeck, Still): Flask server, Javascript
                   Doppelseitiger Faksimilereader und mongodb (Lindinger) 
                   NLP-Tool Semantische Ähnlichkeitssuche (Ullrich): NLP-Python Libraries, Funktionalitäten
                   Regelbasierte Suchmaschine (Babl): Programmierung C++, make/cmake, client-server Programmierung mit C++
                   Dokumentationssystem Sphinx (Babl): Markdown, Sphinx Installation, 2HTML, 2PDF
               
            
            
               Arbeitsgruppen: Diskussionen/Spezialfragen
               Je nach Interesse der Teilnehmer*innen unter der Leitung der einzelnen Dozent*innen.
            
         
         
            Kurzbiographie der Dozent*innen
            
               Florian Babl (CIS)
               Bachelorarbeit: Entwicklung eines Rankingverfahrens der Suchtreffer für die FinderApp WiTTfind im Nachlass Ludwig Wittgensteins 
               Forschungsschwerpunkte: verschiedene Rankingalgorithmen und ihre Funktionalität mit dem Ziel der Rankingverbesserung.
            
            
               Marcel Eisterhues (CIS)
               Forschungsschwerpunkte: Der momentane Forschungsschwerpunkt ist die automatische Seitensegmentierung von handgeschriebenen Texten.
            
            
               Max Hadersbeck (CIS)
               Projektleiter und Dozent am CIS
               Forschungsschwerpunkte: Digitaler Zugang zum Nachlass von Ludwig Wittgenstein, FinderApp WiTTFind, Wittgenstein Advanced Search Tools, Programmierung: C++, Python, XML
            
            
               Florian Landes (Kommission für bayerische Landesgeschichte bei der Bayerischen Akademie der Wissenschaften) 
               Bachelorarbeit: Optical Character Recognition (OCR) – Optische Zeichenerkennung (OZE) Ein Werkzeug zur Verknüpfung von digitaler Edition und Faksimile? Semiautomatische Ermittlung von Bildkoordinaten für WiTTFind
               Forschungsschwerpunkte: OCR, OZE, Bavarikonprojekt Ortsnamen des Regierungsbezirks Schwaben
                  
            
            
               Ines Röhrer (CIS)
               Masterarbeit: Lexikon, Syntax und Semantik - computerlinguistische Untersuchungen zum Nachlass Ludwig Wittgensteins
               Forschungsschwerpunkte: Digitales Speziallexikon WiTTLex für den Nachlass von Ludwig Wittgenstein
            
            
               Sebastian Still (CIS)
               Masterarbeit: Ludwig Wittgenstein: 100 Jahre Traktatus. Der Odyssee-Reader, ein web-basiertes Tool zur textgenetischen Suche im Traktatus
               Forschungsschwerpunkte: moderne Frontend Programmierung, NLP (Backend)
            
            
               Sabine Ullrich (CIS)
               Masterarbeit: Clustering zur Verbesserung der Performanz einer Ähnlichkeitssuche
               Forschungsschwerpunkte: Natural Language Processing, Data Mining, semantische Ähnlichkeitserkennung im Nachlass von Ludwig Wittgenstein
            
         
      
      
         
            
               Bibliographie
               
                  Babl, Florian (2019): 
                        Entwicklung eines Rankingverfahrens der Suchtreffer für die FinderApp WiTTFind im Nachlass Ludwig Wittgensteins. Bachelor‘s thesis. LMU.
                    
               
                  Landes, 
                  Florian (2019): 
                        Optical Character Recognition (OCR) – Optische Zeichenerkennung (OZE). Ein Werkzeug zur Verknüpfung von digitaler Edition und Faksimile? Semiautomatische Ermittlung von Bildkoordinaten für WiTTFind, Bachelorarbeit, LMU.
                    
               
                  Lindinger, Matthias (2013): 
                        Highlighting von Treffern des Suchmaschinentools 
                  WiTTFind im zugehörigen Faksimile. Bachelor‘s thesis, LMU.

               
                  Lindinger, 
                  Matthias (2015): 
                        Entwicklung eines WEB-basierten Faksimileviewers mit Highlighting von Suchmaschinen-Treffern und Anzeige der zugehörigen Texte in unterschiedlichen Editionsformaten. Master's thesis, LMU.
                    
               
                  Pichler, Alois (2017): 
                        Wittgenstein Archives at the University of Bergen (WAB): Open Access to Wittgenstein's Nachlass. XML based Interactive Dynamic Presentation (IDP) of WAB's Nachlass transcriptions. 16. Mai 2017. http://wab.uib.no/transform/wab.php?modus=opsjoner [letzter Zugriff 20.09.2019].
                    
               
                  Hadersbeck, 
                  Maximilian / 
                  Pichler, 
                  Alois / 
                  Fink, 
                  Florian /
                   Gjesdal, 
                  Øyvind L. (2014): „Wittgenstein's Nachlass: WiTTFind and Wittgenstein advanced search tools (WAST)“
                        , in: 
                        Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage, 91-96. ACM.
                    
               
                  Hadersbeck , 
                  Maximilian /
                   Pichler, 
                   Alois /
                   Bruder, Daniel / Schweter, Stefan (2016): 
                        New (re)search 
                  possibilities for Wittgenstein's Nachlass II: Advanced Search, Navigation and Feedback with the FinderApp WiTTFind. http://wab.uib.no/alois/Hadersbeck_Pichler%20Kirchberg2016.pdf [letzter Zugriff 20.09.2019].
                    
               
                  Röhrer, 
                  Ines / 
                  Ullrich, 
                  Sabine / 
                  Hadersbeck, 
                  Maximilian (2019): 
                        Weltkulturerbe international digital: Erweiterung der Wittgenstein Advanced Search Tools durch Semantisierung und neuronale maschinelle Übersetzung. multimedial multimodal. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 25. - 29.03.2019 an den Universitäten zu Mainz und Frankfurt.
                    
               
                  Röhrer, 
                  Ines (2017): 
                        Musik und Ludwig Wittgenstein: Semantische Suche in seinem Nachlass. Bachelor‘s thesis, LMU.
                    
               
                  Schmidt, 
                  Alfred (2018): „Ludwig Wittgenstein’s Nachlass in the UNESCO Memory of the World register.“, in: 
                        Nordic Wittgenstein Review 7(2):209–213.
                    
               
                  Ullrich, 
                  Sabine /
                   Bruder, 
                  Daniel /
                   Hadersbeck, 
                  Maximilian (2018): Aufdecken von „versteckten" Einflüssen: Teil-Automatisierte Textgenetische Prozesse mit Methoden der Computerlinguistik und des Machine Learning. Kritik der digitalen Vernunft. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 26.02.-02.03. 2018 an der Universi
                        tät zu Köln, veranstaltet vom Cologne Center for eHumanities (CceH).
                    
               
                  Ullrich, Sabine (2019):
                         Boosting Performance of a Similarity Detection System using State of the Art Clustering Algorithms. Master‘s thesis. LMU.
                    
            
         
      
   



      
         
            Einleitung
            
  Seit 2010 kooperieren das Wittgenstein Archiv an der Universität Bergen (WAB, Alois Pichler) und das Centrum für Informations- und Sprachverarbeitung der Ludwig-Maximilians Universität München
  (CIS, Max Hadersbeck et. al.) in der Forschungsgruppe „Wittgenstein Advanced Search Tools” (WAST). Die WAST-Projektgruppe entwickelt die web-basierte 
  FinderApp WiTTFind (), die einen computerlinguistisch gestützten digitalen Zugang zu WABs Wittgenstein-Edition erlaubt. Nach einer kompletten Neuscannung des Nachlasses und intensiven Verhandlungen des WAB mit den Rechteinhabern, dürfen seit 2018 WABs Edition auf der WiTTFind-Webseite durchsucht und Faksimileextrakte dargestellt werden. Nun konnten wir uns einer zentralen Frage der Wittgensteinforscher widmen: Wo finden sich in seinem Nachlass semantisch ähnliche Bemerkungen und, retroperspektivisch betrachtet, wann fanden diese Änderungen statt? 

            Wir entwickelten das Analysetool WiTTSim (Ullrich, 2018), das semantisch ähnliche Bemerkungen in der Edition aufspürt, zusammen mit einem vorgeschalteten semantischem Clusterverfahren (Ullrich, 2019), welches die Rechenzeit der Ähnlichkeitssuche um den Faktor 100 verkürzte. Zur retroperspektivischen Analyse der Edition entwickelten wir ein zeitorientiertes, textgenetisches Datenmodell, das die Spielräume der Interpretation der bisher dokumentorientierten Edition auf zugelassene Lesarten reduziert.
            In unserem Vortrag stellen wir die Verfahren unserer Ähnlichkeitssuche mit vorgeschaltetem semantischen Clustering und ein neues mehr textgenetisch- als dokumentorientiertes Modell einer Edition vor, das im Web-Frontend des OdysseeReaders (www.odysseereader.wittfind.cis.lmu.de) implementiert ist und auch die Frage beantwortet: „Wann gibt es semantisch ähnliche Bemerkungen“.
         
         
            Die Datenbasis: Dokument- und Zeitorientierte Modelle
            Die bei uns verwendete Datenbasis BNE 2015- und IDP 2016-, die am Wittgensteinarchiv an der Universität Bergen (Pichler, WAB) erstellt werden, enthalten Faksimile und Transkriptionen (auf der Basis von XML-TEI-P5) des Nachlasses von Ludwig Wittgenstein. Dieser Nachlass umfasst ca. 20.000 Seiten, welche vom WAB in Dokumente und diese wiederum in logische Textabschnitte unterteilt sind. Jeder der 54.930 Textabschnitte – eine sogenannte Bemerkung – wird mit einer eindeutigen Bezeichnung, dem sogenannten Siglum, versehen und wird in unserer Ähnlichkeitssuche als einzelnes Textobjekt definiert und semantisch analysiert. 
            Betrachtet man die Annotationen der BNE unter dem Aspekt der Retroperspektive, taucht folgendes Problem auf: Die BNE liefert nur auf der Ebene der Bemerkungen Informationen über ihren Erstellungszeitpunkt bzw. -zeitrahmen. Die Änderungen auf Wort und Zeichenebene sind zwar akribisch annotiert, allerdings fehlt die zeitliche Information wann diese Änderungen vorgenommen wurden. Um textgenetische Metainformationen auf Wort- bzw. Zeichenebene in das “ordered hierarchy of content objects model data” (OHCO) einer XML-Edition, wie das der BNE zu integrieren, schlägt das TEI-P5 Konsortium Fragmentierungs-, Milestone oder Standoff-Markup Annotationen vor (Jörg Hornschemeyer, 2013), die am WAB bisher nicht durchgeführt wurden. Von Geisteswissenschaftlern, deren wissenschaftliches Kerngebiet im Allgemeinen weit entfernt von der XML-Programmierung liegt, würde großer programmtechnischer Editionsaufwand verlangt. Eine Folge ist, dass von „Nachverwertern“ der Edition zur Generierung der textlichen Varianten algorithmisches Ausmultiplizierten der annotierten Varianten implementiert wird, was z.B. in der Wittgenstein-Edition bei einzelnen Bemerkungen eine vierstellige Anzahl von Lesarten generiert. Betrachtet man die so automatisch generierten Lesarten, sind die meisten syntaktisch und semantisch falsch, was fatale Auswirkungen auf semantische Analysen der Textobjekte hat. Ohne zusätzliche, fein granulierte Metainformation in den annotierten Varianten sind die Spielräume der automatisierten Lesartengenerierung jedoch nicht einzugrenzen.
            Im Umfeld der Wittgensteinforschung gibt es eine Edition, die bis auf Zeichenebene zeitliche Informationen zur Textgenese liefert: Die Prototractatus-Tools (PTT 2016) von Martin Pilch (Pilch 2018). Sie dokumentieren den 
Nutzern Ludwig Wittgensteins Schreibprozess, beginnend mit einem leeren Notizbuch im Jahre 1915 und bis zum endgültigen Diktat des Ts-204 im Sommer 1918, das zu seiner einzigen philosophischen Veröffentlichung zu Lebzeiten, der „Logisch-Philosophischen Abhandlung“ führte. Leider konnten wir die Daten und Metainformationen der PTT-Edition in unserer FinderApp Infrastruktur nicht direkt analysieren, da unsere WiTTFind Infrastruktur zum einen auf das dokumentorientierte XML-TEI-P5 Datenformat aus Bergen zugeschnitten ist, und zum anderen die PTT-Edition im inkompatiblen Microsoft Word-97 Format vorliegt. Alle verfügbaren XML-TEI Importtools erfassten nur Bruchteile der Annotationen, sodass z.B. die Zeitinformationen der PTT überhaupt nicht erkannt und transformiert wurden. Um möglichst viel von der PTT-Textedition weiterzuverwenden, und damit der PTT-Hg. die Edition in seiner gewohnten Microsoft-Office Umgebung weiter optimieren kann, entwickelten wir eine mit Microsoft EXCEL leicht zu bedienende mehrdimensionale Tabellenstruktur. Die Editionsdaten und Metainformation der Word-97 Edition konnten wir größtenteils mit eigenen Programmen und Office-Macrotechniken transferieren. Zur Integration der Tabellen in die Infrastruktur unserer FinderApp verwendeten wir LibreOffice-Tools und selbst geschriebene Python Programme, die die Daten, sobald sie in das git-Repository des Projekts kopiert werden, mit Hilfe der continuous Integration automatisch transformieren und importieren. Zur Web-Präsentation werden sie an unsere neu entwickelte FinderApp, den 
OdysseeReader (siehe Abb. 1,
odysseereader.wittfind.cis.lmu.de), übergeben. Dieses Vorgehen trennt zwar das Daten- und Repräsentationsmodell, jedoch entwickelten wir ein positionsinvariantes Siglensystem, bestehend aus dem Tupel (Zeitstempel, Dokument, Seite, Zeile, Zeichenposition), das die beiden Modelle eineindeutig verknüpft. Diese bijektive Relation zwischen den beiden Modellen definiert dem Hg., wo er in seinem Datenmodell Änderungen vornehmen muss um sie an eine bestimmte Stelle, zu einem bestimmten Zeitpunkt im Repräsentationsmodell zu platzieren.

            
               
                  
                   Abbildung 1: Der OdysseeReader odysseereader.wittfind.cis.lmu.de
                  
               
            
         
         
            Ähnlichkeitssuche mit vorgeschaltetem Semantic Clustering
            Die Ähnlichkeitssuche WiTTSim berechnet mit Hilfe
computerlinguistischer Methoden für jede Bemerkung einen
„charakteristischen” Vektor, oder, intuitiv gesprochen: Man bestimmt
einen “Fingerabdruck”. Dieser automatisierte Prozess wird unabhängig
im Voraus berechnet, was spätere Prozesse vereinfacht und
beschleunigt. Dieser „Fingerabdruck“ beinhaltet linguistische
Informationen, wie beispielsweise Wörter, deutsche und englische
Synonyme (aus Germanet und Wordnet), Wortarten (Treetagger) und
Lemmata  (WiTTLex, Röhrer 2019). Diese Informationen werden in binäre Vektoren übersetzt, welche insgesamt etwa 115.000 Features umfassen. Zusätzlich zur Datenbasis wurden 471 Bemerkungen bereits gruppiert, also mit Ground Truth Labels versehen. Die Gruppen bestehen dabei aus 2-15 Bemerkungen und das gelabelte das Korpus umfasst 1.670 Bemerkungen, was ca. 3% des gesamten Nachlasses entspricht.

            Zur Semantischen Ähnlichkeitsberechnung ist allerdings eine Reduktion des Feature Raumes zwingend nötig, da die Vektoren mit so hoher Dimensionalität semantisch „weit voneinander entfernt“ sind und keine semantischen Gruppierungen auszumachen sind. Dieses Phänomen ist auch bekannt als 
Curse of Dimensionality. Daher werden die Vektoren zunächst auf eine angemessene Anzahl von Features skaliert, um sie anschließend clustern zu können. Verwendete Reduktionstechniken umfassen Singular Vector Decomposition (SVD), Principal Component Analysis (PCA), Sparse Random Projection (SRP) und Uniform Manifold Approximation and Projection (UMAP). Auf unseren Daten zeigte eine SVD Reduktion zu 1.600 Dimensionen die besten Ergebnisse, zusammen mit UMAP, welches darüber hinaus die Daten im zweidimensionalen Raum klar gruppiert. Letzteres erlaubt nur eine Zieldimension von 2 bis 100 Dimensionen, weshalb zum Erhalt der Varianz die maximale Dimensionsanzahl von 100 gewählt wurde, um einen bestmöglichen Erhalt der gespeicherten Information zu gewährleisten.

            Nach erfolgter Reduktion der Dimension können die Datenpunkte, also alle Bemerkungen, geclustert werden. Verwendete Clustering Techniken umfassen den klassischen K-Means Ansatz (Mac-Queen 1967, Ball and Hall 1956, Lloyd 1982, Steinhaus 1955), aber auch Dichte-basierte Ansätze wie Mean-Shift (Duda und Hart 1973) und DBSCAN (Ester et al. 1996), das statistische Gaussian Mixture Modell (Redner und Walker 1984) und das hierarchische Ward Clustering (Ward 1963). Beste Ergebnisse konnten mit einer Kombination von SVD und K-Means mit einer Anzahl an k=150 Clustern erzielt werden. Evaluiert wurde anhand der drei unüberwachten Metriken Silhouette Score, Davies Bouldin Index, und Calinski-Harabasz Index. Zusätzlich konnte durch die verfügbaren Ground Truth Labels auch der Recall berechnet werden, welcher in den Experimenten einen maximalen Wert von 1,0 erreicht. Dies zeigt, dass alle der gelabelten Daten richtig zugeordnet werden konnten. Wird eine Suchanfrage zum Auffinden ähnlicher Bemerkungen gestartet, muss nur der charakteristische Vektor der eingegebenen Bemerkung berechnet werden und das nächstliegende Cluster bestimmt werden. Letzteres erfolgt durch eine Bestimmung des am nächsten gelegenen Cluster Mittelpunkts (Zentroids). Anschließend werden die Abstände zu allen Bemerkungen des bestimmten Clusters gemessen, welche zuletzt dem Philologen zur genaueren Prüfung „gerankt“ vorgeschlagen werden.
         
         
            Zusammenfassung und Ausblick
            Unsere zeitgesteuerte textgenetische Edition kann von einem Wissenschaftler ohne XML Kenntnisse innerhalb einer Office Umgebung erstellt werden. Das continuous Integration System von git transferiert die Edition automatisch in unser WEB-basiertes Repräsentationssystem, den OdysseeReader. Über das von uns entwickelte eineindeutige Siglensystem verliert der Hg. niemals den klaren Zusammenhang zwischen Editions- und Präsentationsmodell.

            Das von uns entwickelte Ähnlichkeitstool mit vorgeschaltetem Semantic Clustering könnte auch zur Ähnlichkeitsbestimmung zwischen zwei gegebenen Texten verwendet werden: Der Nutzer könnte einen Text eingeben, und es werden potentiell ähnliche Textpassagen in einer Sammlung von Texten gesucht, die dann „gerankt“ nach Ähnlichkeiten in einer Art Hitliste ausgegeben werden. Eine derartige Sortierung nach Textähnlichkeiten könnte es dem Philologen zum Beispiel besonders erleichtern, potentielle Zitate, Einflüsse und Verweise eines Autors innerhalb seines Werkes und im Bezug auf die Literatur seiner Zeit aufzuspüren.
         
      
      
         
            
               Bibliographie
               
                  Ball, 
                  Geoffrey H.
                   / Hall 
                  David J. (1965): 
                        Isodata, a novel method of data analysis and pattern classification. Technical report, Stanford research inst Menlo Park CA.
                    
               
                  Duda, Richard
                   O. / Hart, 
                  Peter E. (1973): "Pattern analysis and scene classification." 
                        J. Wiley 1:73.
                    
               
                  Ester, 
                  Martin
                   / Kriegel 
                  Hans-Peter /
                   Sander, 
                  Jörg
                   / Xu, 
                  Xiaowei
                   et al. (1996): „A density-based algorithm for discovering clusters in large spatial databases with noise.“, in 
                        KDD, volume 96, pages 226–231.
                    
               Hadersbeck, Maximilian / Pichler, Alois / Fink, Florian / Gjesdal, Oyvind (2014): Wittgenstein’s Nachlass: WiTTFind and Wittgenstein Advanced Search Tools (WAST), DATeH, Madrid.
               
                  Hadersbeck, 
                  Maximilian
                   / Still, 
                  Sebastian
                   (2018): 
                  Investigating Wittgenstein’s Nachlass: WiTTFind, WiTTReader, OdysseeReader and Wittgenstein Advanced Search Tools, im Katalog zur Ausstellung „DIE TRACTATUS ODYSSEE“ S.127-137, Wittgenstein Initiative, Wien.
                    
               
                  Lloyd, 
                  Stuart P. (1982): „Least squares quantization in pcm“, in: 
                        IEEE transactions on information theory, 28(2):129–137.
                    
               
                  MacQueen, 
                  J. B. (1967): „Some methods for classification and analysis of multivariate observations.“, in: 
                        Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281–297. Oakland, CA, USA, 1967
                    
               
                  Pichler, Alois / Krüger, Heinz W. / Smith, D. / Bruvik, Tone / Lindebjerg, Anne / Olstad, Vemund (Hrsg.) (2009): Wittgenstein Source Bergen Facsimile (BTE). Wittgenstein Source Bergen.
                    
               
                  Redner, 
                  Richard A. /
                   Walker, 
                  Homer F. (1984): Mixture densities, maximum likelihood and the em algorithm. SIAM review, 26(2):195–239.
                    
               
                  Röhrer, 
                  Ines / 
                  Ullrich, 
                  Sabine / 
                  Hadersbeck, 
                  Maximilian (2019): 
                        Weltkulturerbe international digital: Erweiterung der Wittgenstein Advanced Search Tools durch Semantisierung und neuronale maschinelle Übersetzung. multimedial multimodal. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 25. - 29.03.2019 an den Universitäten zu Mainz und Frankfurt.
                    
               
                  Steinhaus, 
                  Hans (1955): Quelques applications des principes topologiques à la géométrie des corps convexes. Fund. Math, 41:284–290.
                    
               
                  Ullrich, Sabine /
                   Bruder, Daniel /
                   Hadersbeck, Maximilian (2018): “Aufdecken von “versteckten” Einflüssen: Teil-Automatisierte Textgenetische Prozesse mit Methoden der Computerlinguistik und des Machine Learning”, 5. Tagung Digital Humanities im deutschsprachigen Raum 26.2.-2.3. (Köln).
                    
               
                  Ullrich, Sabine (2019):
                         Boosting Performance of a Similarity Detection System using State of the Art Clustering Algorithms. Master‘s thesis. LMU.
                    
               
                  Pilch, Martin (2018): 
                        Frontverläufe im Prototractatus – Zur gedanklichen Entwicklung von Krakau bis Sokal (1914/1915), Wittgenstein-Studien 9 (S.101-154), Internationale Ludwig Wittgenstein Gesellschaft (ILWG).
                    
               
                  Still, Sebastian (2018): 
                        Ludwig Wittgenstein: 100 Jahre Traktatus. Der Odyssee-Reader, ein web-basiertes Tool zur text-genetischen Suche im Traktatus, Masterthesis, Ludwig-Maximilians-Universität München.
                    
               
                  Feldweg, Birgit /
                   Feldweg, Helmut (1997): „GermaNet - a Lexical-Semantic Net for German.", in: 
                        Proceedings of the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications. Madrid.
                    
               
                  Henrich, Verena / Hinrichs, Erhard (2010): „GernEdiT - The GermaNet Editing Tool", in: 
                        Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010). Valletta, Malta, pp. 2228-2235.
                    
               
                  Hörnschemeyer, Jörg / Thaller, Manfred / Förtsch, Reinhard (2017): 
                        Textgenetische Prozesse in Digitalen Editionen, Köln Universitäts- und Stadtbibliothek Köln 2017, https://www.worldcat.org/title/textgenetische-prozesse-in-digitalen-editionen/oclc/1002260195
                    
               
                  Schmidt, 
                  Alfred (2018): „Ludwig Wittgenstein’s Nachlass in the UNESCO Memory of the World register.“, in: 
                        Nordic Wittgenstein Review 7(2):209–213.
                    
               
                  UNESCO (2017): UNESCO-Weltdokumentenerbe - Zwei
  Neuaufnahmen. URL:
  https://www.unesco.at/presse/artikel/article/unesco-weltdokumentenerbe-zwei-neuaufnahmen/
  [letzter Zugriff 19. Juni 2018].

               
                  Ward, John H. (1963): „Hierarchical grouping to optimize an objective function." 
                        Journal of the American statistical association 58.301: 236-244.
                    
            
         
      
   



      
         
            Digitale Edition für Born-Digital-Texte?
            Kulturelles Erbe wird zunehmend vielfältig in digitalen Formen hervorgebracht. Für einen stetig wachsenden Teil dieses Born-Digital-Materials werden besondere Zugangsarten benötigt: Lesegeräte für veraltete Medienspeicher müssen konserviert und bereitgehalten werden, Emulatoren für nicht mehr unterstützte Betriebssysteme entwickelt und Daten in aktuellere Formate übertragen werden – was manchmal nur noch mithilfe der Computerarchäologie gelingt.
            Der Erhalt digitalen Kulturerbes wird jedoch nicht nur durch deren technische Verfügbarkeit garantiert, sondern bedarf zusätzlich einer kritischen Beleuchtung und wissenschaftlich fundierten Kommentierung. Im Bereich des “analogen” kulturellen Erbes werden beispielsweise jahrhundertealte, zerfallende Handschriften für die Wissenschaft und für die Öffentlichkeit in textkritischen (digitalen, analogen und hybriden) Editionen aufbereitet und verfügbar gemacht: Dies müsste mit denselben Konsequenzen auch für jedwede Überlieferung relevanten kulturellen Erbes in digitaler Form gelten. Benötigen wir also (digitale) textkritische Editionen von Born-Digital-Texten?
         
         
            Digitale Fachmagazine der 8-Bit-Ära
            Ein Beispiel, das konservatorische und textkritische Aspekte verbinden kann, ist in digitalen Fachmagazinen überliefert, die in den späten 1980er und frühen 1990er Jahren auf 5¼-Zoll-Disketten (“Floppy Disk”) publiziert wurden. Ein beachtenswerter Teil dieser sogenannten “Diskmags” richtete sich an Nutzer des 8-Bit-Rechners 
Commodore 64 (kurz: C64) und bediente primär eine frühe User- und Gamer-Community. Diskmags enthielten einerseits benutzbare Software (meist Public Domain) und andererseits Besprechungen neu erschienener Software, Rezensionen aktueller Spiele, Hardwaretipps und -bauanleitungen, Editorials, Leserbeiträge und mehr. Schätzungsweise existierten über 30 Diskmags (vgl. Diskmag-Archiv o. D.), unter denen im deutschsprachigen Raum die Titel Magic Disk 64 (1987–1993) und 
Game On (1988–1995)  zweifelsohne zu den bekanntesten gehörten.
            
            Die Überlieferungssituation der Diskmags ist prekär. In öffentlichen Institutionen sind kaum Bestände erhalten, und viele Exemplare werden nur noch in Privatsammlungen konserviert. Zudem drohen die Datenträger ihre Inhalte zu verlieren,  und inwieweit die gedruckten Titelseiten, mit denen die Diskmags ausgeliefert wurden, überhaupt noch greifbar sind, ist bislang nicht erforscht. 
                    Es ist einer sehr lebendigen 8-Bit-Szene zu verdanken, dass zumindest ein Teil der Binärdaten noch verfügbar ist; deren Erschließung allerdings vorrangig die Interessen der Fangemeinde bediente und bislang höchstens ansatzweise unter wissenschaftlichen Vorzeichen geschah. Bei anderen “professionellen” Bewahrern ist die Diskmagazin-Thematik offenbar noch nicht weiter ins Bewusstsein gerückt.
            
         
         
            Computerspielekritik als Forschungsobjekt
            Diskmags sind unter anderem für die Kulturwissenschaft interessant, da die Spiele-Rezensionen (auch “Spieletests”) unmittelbar das zeitgenössische Erleben von Computerspielen widerspiegeln. Die damaligen Kriterien hinsichtlich Konzept, Ästhetik (Grafik, Animationen, Sound, Musik) und Spielerfahrung sind aus heutiger Sicht kaum noch nachvollziehbar und können allenfalls von Zeitzeugen mit überzeugender Authentizität geschildert werden: Die C64-Hardware war mit 320x200 Bildpunkten, 16 Farben, 64KiB RAM, einer Rechenleistung von ca. 1MHz  und einem dreistimmigen SID-Soundchip
 “state of the art”, und bot Programmierern sowie Konsumenten völlig neue Nutzungsmöglichkeiten und Erlebnisräume. Und das sogar mit “Haushaltsgeräten”: Es war gängig, anstelle eines Monitors den Fernseher als Bildschirm zu verwenden, und die Mono-Tonausgabe über die eigene Soundanlage laufen zu lassen. Über den kreativen Umgang mit der technischen Grundausstattung des Commodore 64 und der Auslotung ihrer Grenzen wurde in der Fachcommunity ausführlich diskutiert. Beispielsweise konnte der Soundchip dazu gebracht werden, Sprachausgaben zu erzeugen (z. B. im Spiel “Ghostbusters”, vgl. Rettinghaus 2018), und man fand heraus, dass durch eine besondere Behandlung der sogenannten “Sprites” (vgl. Morrow 2019) bildschirmfüllende Animationen kreiert werden konnten (z. B. im Spiel “Katakis”, C64 Wiki 2019c, Abschnitt “Katakis-Entwicklungs-System”). Die in den Diskmags (und Printmagazinen, z.B. 64er
               ) überlieferten Rezensionen sind für Studien im Gebiet der 8-Bit-Ära eine unersetzliche Quelle.
                
            Ein Blick in die 
                    Magic Disk 64 veranschaulicht deren Potenzial als Quelle kulturgeschichtlicher Untersuchungen. In frühen Ausgaben sind die Spieleberichte sehr kurz und lesen sich mehr wie Teasertexte, weshalb die Bezeichnung “Rezension” im Sinne einer kritisch-reflexiven Betrachtung noch nicht angemessen ist (z. B. in “The Last Ninja”, Magic Disk 64 1987a). Die Texte vermitteln die Thematik und die Atmosphäre eines Spiels und übernehmen die Aufgabe, in der 8-Bit-Umgebung eine Vorstellungswelt zu stimulieren. In späteren Ausgaben werden die Texte ausführlicher und kritischer, und ein Bewertungsraster tritt hinzu. Trotz dieser Objektivierungstendenz bleibt das subjektive Spielerlebnis – so lassen die bislang gesichteten Texte vermuten – der entscheidende Faktor für die Bewertung. Anhand der Entwicklung der Spieleberichte über einen längeren Zeitraum (und den Vergleich mit weiteren Textkorpora) ließen sich sowohl die Herausbildung einer 8-bit-spezifischen Spieleästhetik als auch eine Genese der Computerspielekritik beobachten und nachvollziehen.
                
         
         
            Herausforderungen einer digitalen Edition
            Als Quellenmaterial stellen Diskmags eine Herausforderung dar. Die einerseits historische und andererseits technische Distanz sprechen für eine zeitgemäß aufbereitete (digitale) Präsentation. 
                    Im Unterschied zu analogem Material stellt beim Diskmag die Medialität eine besondere Herausforderung dar, zumal es für diese keine editionswissenschaftlichen Standards gibt. 
                    Die Medialität des Editionsobjekts zwingt den Bearbeiter förmlich dazu, sich mit der spezifischen Benutzungsweise auseinanderzusetzen und diese als “Erlebensparameter” in die Edition mit einzubeziehen – und auch Vorzüge der Emulation gegenüber der Edition abzuwägen.
                
            Ein Versuch, die Texte zugänglich zu machen, wurde bereits durch ein anonymes Underground-Portal unternommen (
                    http://magicdisk.untergrund.net, keine autoritative Quelle). Die dort wiedergegebenen Texte wurden aus Originaldaten generiert, welche der Beschreibung nach im Floppy-Image-Format “.d64” vorlagen.  Die Übertragung offenbart jedoch einige Desiderata in text- und medienkritischer Hinsicht:
                
            
               Wie waren die Texte insgesamt zu einem Magazin angeordnet? Wie war das Layout der einzelnen Texte gestaltet? Wie wurden Grafiken eingebunden? Könnten exemplarische Screenshots helfen, um die Originaldarstellung nachvollziehen zu können? Und sind die einzelnen “Screens” statisch (wie eine konventionelle Seite) oder sind sie scrollbar (ähnlich einer Website)?
               Das originale Diskmag zeichnete sich durch eine animierte Menüführung und eine Begleitmusik aus, die im Hintergrund abgespielt wurde.
 Beides wurde nicht abgebildet. Wie könnten diese aber in einer Edition präsentiert werden?

               Ferner ist das Layout von einem Zeichenraster abhängig, das aus 25 Zeilen zu 40 PETSCII-Zeichen besteht. Sollten die Texte also mit einer dicktengleichen Schriftart dargestellt werden? Wie können die originalen Zeilenumbrüche dokumentiert und berücksichtigt werden, zumal sie manchmal für die korrekte Darstellung von Rastergrafiken (z.B. Magic Disk 64 1987b) notwendig sind?
               
                  Hinzu kommt die Tatsache, dass der Grafikchip des C64 für eine analoge Bildschirmausgabe entwickelt wurde und eine (wie auch immer geartete) Farbtreue im Digitalen nur schwer zu erzielen ist (vgl. Pepto 2017).
                    
               
                  Der teilweise sehr spezielle Fach- und Szenejargon bedarf einer historischen Erläuterung. Zudem fehlt jeglicher Kommentar zu offensichtlichen sachlichen Fehlern im Text. Beispielsweise wurde bei einer Besprechung des sehr erfolgreichen Spiels “Pirates!” der englische Ausdruck “Dutch” mit “Deutsch” verwechselt (vgl. Magic Disk 64 1987a). Spätestens an einer solchen Stelle ist ein textkritischer Kommentar notwendig.
                    
            
            Des Weiteren stellen sich allgemeine methodische Fragen:
            
               In welches Format ließen sich die Texte generell sinnvoll übertragen, um sie langfristig zu erhalten und darauf aufbauende Studien zu ermöglichen? Finden sich beispielsweise in TEI bereits Lösungen dafür, oder bedarf die “neodigitale” Form neuer Elemente? 
                        An einem Versuch ließ sich feststellen, dass zwar keine grundsätzlichen neuen Elemente notwendig sind, sich aber teilweise die Begrifflichkeiten verschieben: So ist z.B. die “Bildschirmzeile” (die auch leer sein kann) von der “Textzeile” abzugrenzen.
                    
               Wie können Diskmag-spezifische interaktive oder mediale Elemente, z.B. Menüführung, Animationen und Hintergrundmusik nachnutzbar dokumentiert werden? Sollte z.B. die Musik vom SID-Format nach MEI übertragen werden, bzw. welche anderen nicht-binären Formate bieten sich an?
               
               Wie ist das Verhältnis zur mitgelieferten Software zu gestalten? Gibt es außerdem Printanteile, die den Magazinen beilagen (Intertextualität)?
               
               Ist es möglich – und wenn ja, wieweit sinnvoll – die ursprüngliche Nutzererfahrung nachzubilden? Möchte der heutige Nutzer beispielsweise die ursprünglich langen Ladezeiten nachvollziehen können?
               Welche juristischen Modelle könnten im Hinblick auf die Urheberschaft und die Verwertungsrechte von Texten, Musik und Grafiken, die noch in persönlicher bzw. privatwirtschaftlicher Hand liegen, zur Anwendung kommen?
            
         
         
            Zusammenfassung
            Für eine kritische digitale Edition von Diskmags sprechen sowohl die kulturgeschichtliche Relevanz der Texte, gerade im Hinblick auf die wenig erforschte Entwicklung einer Ästhetik und professionellen Software- und Spielekritik, als auch die Notwendigkeit einer Vermittlung des Materials, das hinsichtlich Inhalte und (8-Bit-)Medium als historisch anzusehen ist. Der Vortrag präsentiert ausgewählte Diskmags im Original sowie eine exemplarische digitale Diskmag-Edition (TEI mit HTML-Ausgabe), anhand derer die aufgezeigten Phänomene demonstriert und die verwendeten Methoden zur Diskussion gestellt werden. Welche Grenzen weisen die gewählten Verfahren auf, und welche Aspekte – analog zur unersetzlichen Begutachtung eines echten Manuskripts – kann wiederum nur die Emulation abdecken? Welche methodischen Aspekte gelten allgemein für Born-Digital-Material, welche sind materialspezifisch für Diskmags? Bedarf es schließlich einer Erweiterung der “textkritischen Edition” zu einer “medienkritischen Edition”?
                
         
      
      
         
             Magic Disk 64 1987–1993. Erscheinungsjahre bei Wikipedia 2019 und C64 Wiki 2019a.
             Game On 1988–1995. Erscheinungsjahre bei Wikipedia 2018 und C64 Wiki 2019b.
             Eine systematische Auswertung von Auflagen, Verkaufzahlen und Verbreitung steht aus. Zudem ist eine Sekundärleserschaft zu berücksichtigen: Diskmags wurden mehrfach weitergegeben und auch durch Kopien vervielfältigt (sofern kein Kopierschutz dies verhinderte).
             In der Zeitschriftendatenbank (ZDB) sind von den Diskmags
                            Magic Disk 64 und 
                            Game On lediglich einige Ausgaben aus den Jahren 1992 und 1993 in München nachgewiesen. Wenigstens von der 
                            Golden Disk 64 sind Exemplare aus den Jahren 1993 bis 1996 in München greifbar.
                        
             Magnetische Aufzeichnungen erleiden mit der Zeit eine sogenannte “data degradation”. Speziell zur Floppy Disk vgl. die Ausführungen in National Semiconductor Corporation 1989, 5-30.
            
               Eine konkrete Anfrage beim Computerspielemuseum Berlin blieb bislang leider unbeantwortet.
                    
             Abhängig vom verwendeten Farbkodiersystem (NTSC/PAL).
             Zu technischen Aspekten und der kulturgeschichtlichen Bedeutung des SID vgl. Rettinghaus 2018.
             Der C64 hatte einen eingebauten HF-Modulator, sodass man das Bild über ein Antennenkabel (Koaxialkabel) in einen Fernseher einspeisen konnte.
             64er 1984–1996. Digitalisate des Magazins sind unter anderem im Internet Archive 2013 verfügbar.
             Dies impliziert nicht, dass dies angesichts der grafischen und auditiven Möglichkeiten des Commodore 64 notwendig war. Selbst rein textbasierte Spiele (Textadventures) oder Spiele mit extrem einfacher Grafik können eine hohe Spielqualität aufweisen.
             Das Schema umfasste die Kategorien Grafik, Musik [vermutlich auch Soundeffekte], Motivation [gemeint: “Wiederholungsdrang”], Preis/Leistung und Overall [Gesamtbewertung]; vgl. Magic Disk 64 1993: “Game-Test: First Samurai”.
             Eine detaillierte Beschreibung des d64-Image-Formats (entspricht dem 1541 Floppy Disk Format) ist unter
                            verfügbar.
                        
             Die Musik wurde zumeist von Thomas Detert beigesteuert und ist in der High Voltage SID Collection (HVSC) erhalten. Das Sammlungsprojekt versteht sich als “attempt to accurately archive the most popular C64 SIDs into one complete collection”. Hinter SID verbergen sich die Musikdateien, die in den zwei Datenformaten PSID und RSID vorliegen, vgl. die Dokumentation bei HVSC 2019.
             Unberücksichtigt bleibt hier der Horizont technischer Reproduzierbarkeit des Klangerlebnisses, vgl. Rettinghaus 2018, hier bes. S. 275.
             Aufgrund des begrenzten Arbeitsspeichers waren Printbeilagen eine häufige Ergänzung, teilweise wurden z. B. (wie auch bei Printmagazinen) Poster beigelegt.
         
         
            
               Bibliographie
               
                  64er (1984–1996): 
  64er – Das Magazin für Computerfans, Haar (bei München): Markt+Technik Verlag. (ZDB: 
  50387-3)

               
                  C64 Wiki (2019a): “Magic Disk 64”, Website: 
  C64 Wiki, 17. März 2019, 21:11 Uhr. URL: 
  
               
               
                  C64 Wiki (2019b): “Game On”, Website: 
  C64 Wiki, 28. März 2019, 21:28 Uhr. URL: 
  
               
               
                  C64 Wiki (2019c): “Katakis”, Website: 
  C64 Wiki, 29. August 2019, 18:25 Uhr. URL: 
  
               
               
                  C64 Wiki (2019d): “Ghostbusters”, Website: 
  C64 Wiki, 4. September 2019, 10:45 Uhr. URL: 
  
               
               
                  Game On (1988–1995): 
  Game On. Das C64-Spielemagazin auf Diskette, Nürnberg: CP Computer Publications, 1988–1995. (ZDB: 
  1276125-4)

               
                  HVSC (2019): 
  The High Voltage SID Collection. Commodore 64 music for the masses, URL: 
  
               
               
                  Internet Archive (2013): “64’er Magazine”, Website: 
  archive.org, 9. Januar 2013. URL: 
  
               
               
                  Magic Disk 64 (1987–1993): 
  Magic Disk 64. Das C64-Magazin auf Diskette, Nürnberg: CP Computer Publications. (ZDB: 
  1275979-X)

               
                  Magic Disk 64 (1987a): “3.1 Software” [Autor: Alexander Wiederhold?], 
  Magic Disk 64 12/1987. (Textversion abrufbar auf: 
  magicdisk.untergrund.net,
  zuletzt geändert: 15. August 2008, 12:08 Uhr, archiviert unter: 
  Internet Archive Wayback Machine)

               
                  Magic Disk 64 (1987b): “Intern” (= Rubrik 8.1), 
  Magic Disk 64 12/1987. (Textversion abrufbar auf: 
  magicdisk.untergrund.net, zuletzt geändert: 15. August 2008, 12:08 Uhr, archiviert unter: 
  Internet Archive Wayback Machine)

               
                  Magic Disk 64 (1993): “Game-Test: First Samurai” [Autor: Florian Brich?], 
  Magic Disk 64 (02/1993). (Textversion abrufbar auf: 
  magicdisk.untergrund.net,
  zuletzt geändert: 15. August 2008, 12:03 Uhr, archiviert unter: 
  Internet Archive Wayback Machine)

               
                  Steve Morrow (2019):  “C64 Sprites Defined”, Website: 
  C64 Brain, 4. Februar 2019. URL: 
  
               
               
                  National Semiconductor Corporation (1989): Mass Storage Handbook, Section 5: Floppy Disk Controller. Santa Clara: National Semiconductor. (archiviert unter: 
  Internet Archive)

               
                  
                  Philip „Pepto“ Timmermann: (2017): “Calculating the color palette of the VIC II”, Februar 2017. URL: 
  ; siehe auch die Fassung von 2001: “Commodore VIC-II Color Analysis (Preview)”. URL: 
  
               
               
                  Klaus Rettinghaus (2018): “Sidology. Zur Geschichte und Technik des C64-Soundchips”, in: 
  Digitale Spiele. DOI: 
  10.14361/9783839440025-018
               
               
                  Wikipedia (2018): “Game On”, Website: 
  Wikipedia. Die freie Enzyklopädie, 2. Dezember 2018, 18:35 Uhr. URL: 
  
               
               
                  Wikipedia (2019): “Magic Disk 64”, Website: 
  Wikipedia. Die freie Enzyklopädie, 9. Mai 2019, 21:06 Uhr. URL: 
  
               
               
                  www.c64.at (o. D.): “Diskmag-Archiv”, Website: 
  www.c64.at. Das Verzeichnis von deutschsprachigen Magazinen für den C64, ohne Datum. URL: 
  
               
            
         
      
   



      
         
            Ziele und technische Grundlagen des Infrastrukturprojektes „Deutsches Zeitungsportal“
            Historische Zeitungen wurden in den letzten Jahren von deutschen Kulturerbe-Einrichtungen verstärkt digitalisiert und zugänglich gemacht. Dadurch stehen der Forschung hunderte Millionen digitalisierter Zeitungsseiten zur Verfügung – ein Reichtum an Primärquellen, dem man mit den herkömmlichen geisteswissenschaftlichen Forschungsmethoden („Close Reading“) kaum gerecht werden kann. Zunehmend gewinnen daher Analysemethoden der Digital Humanities, wie z.B. „Distant Reading“ (Burckhardt u.A. 2018), an Bedeutung, um die bei der Massendigitalisierung entstandenen Daten auswerten zu können. 
            Die Aufgabe eines nationalen Zeitungsportals ist es jedoch, nicht nur für Forschende, sondern auch für eine interessierte Öffentlichkeit einen niedrigschwelligen Zugang zu entwickeln. Während mit ANNO, Delpher oder dem British Newspaper Archive im europäischen Raum mehrere Projekte – teilweise in Kooperation mit kommerziellen Partnern – entstanden sind, die große digitale Zeitungsbestände in einem nationalen Portal verbinden, existieren in Deutschland bislang nur lokale und regionale Portale einzelner Bibliotheken oder Regionen. Ein übergreifendes Portal, das einen zentralen Zugriff bietet, ist bislang noch ein Forschungsdesiderat (Blome 2018: B.6-33). Dank verschiedener DFG-Förderinitiativen zur Digitalisierung historischer Zeitungen sowie der Förderung der Errichtung eines nationalen Zeitungsportals wird sich das nun ändern. Anfang 2019 wurde das Projekt „Deutsches Zeitungsportal“ gestartet. Es wird im Rahmen der Deutschen Digitalen Bibliothek (DDB) umgesetzt. Ende 2020 soll das Portal, an dessen Aufbau vier Projektpartner beteiligt sind, online gehen.
                
            Das entstehende Portal wird eine auf die Besonderheiten von Zeitungen zugeschnittene zentrale Präsentations- und Rechercheoberfläche bieten und die folgenden Kernfunktionalitäten umsetzen:
            
               übergreifende Volltextsuchen in den digitalisierten Zeitungsbeständen
               unterschiedliche Einstiegspunkte, z.B. über Kalender und Zeitungstitel
               einen unmittelbar in die Portalumgebung integrierten Viewer für Volltexte und Images
               
                  persistente Referenzierbarkeit und damit Zitationsfähigkeit der digitalen Zeitungsobjekte
               
            
            Das Deutsche Zeitungsportal ist ein Infrastrukturprojekt, das auf den bestehenden Netzwerken, Techniken und Workflows der DDB aufbaut. Das heißt, die Zeitungen, die im Zeitungsportal zur Verfügung gestellt werden, stammen aus verschiedenen Einrichtungen, zumeist Bibliotheken, die Erschließungsinformationen, Bilddateien und Volltexte der Zeitungen aus ihren Beständen an das Zeitungsportal liefern. In der ersten Ausbaustufe des Zeitungsportals stellen Bibliotheken ihre Metadaten im METS/MODS-Format bereit, sodass über Verlinkungen in der Datenstruktur sowohl die Bilddateien (i.d.R. im jpg-Format) als auch die Volltexte (i.d.R. im ALTO-xml-Format) in das Portal übernommen werden. Dort werden diese Bestände zusammengeführt und können über einen Suchschlitz durchsucht werden. Basis der Suche bilden die Volltexte, welche vollständig in einen zentral vorgehaltenen SOLR-Index aufgenommen werden. Durch die standardisierten, maschinennutzbaren Inhalte des Zeitungsportals, die über eine Schnittstelle (API) heruntergeladen werden können, sollen neue Nutzungsszenarien, wie z.B. Big-Data-Analysen, ermöglicht werden (Altenhöner 2018: 146). Da es sich bei den Zeitungstexten und -bildern ausschließlich um rechtefreies Material handelt, können die Dokumente in der eigenen Forschung nachgenutzt und weiterverarbeitet werden. 
            Zum Start des Portals werden Bestände aus mindestens sechs deutschen Bibliotheken verfügbar sein; vorsichtigen ersten Schätzungen zufolge wird es sich um 250 verschiedene Zeitungstitel im Umfang von ca. 15 Mio. Zeitungsseiten handeln. Nach der Inbetriebnahme des Portals sollen die Inhalte beständig wachsen, um möglichst viele historische Zeitungen in diesem Portal zu vereinen. Das Fernziel des Unterfangens ist es also, den Forschenden (und der allgemeinen Öffentlichkeit) einen einheitlichen und stabilen Zugang zu vielen (wenn möglich: allen) historischen Zeitungen aus deutschen Kulturerbe-Einrichtungen zu geben. Zudem wird angestrebt, in einer zweiten Ausbaustufe ab 2021 das Zeitungsportal für andere Datenformate wie TEI-XML zu öffnen, um z.B. digitale Editionen und Annotationen in den Korpus aufnehmen zu können. 
                
            Um Zeitungen aus vielen unterschiedlichen Quellen zusammenzuführen, sind bei der Aufbereitung der (Meta-)Daten vielfältige Standardisierungsprozesse notwendig. So wird im Rahmen des Projekts ein zeitungsspezifisches Anforderungsprofil für das METS/MODS-Metadatenformat entwickelt und eine Verknüpfung der Metadaten mit der Zeitschriftendatenbank (ZDB) umgesetzt, deren Identifier für die eindeutige Identifizierung der Zeitungstitel benutzt werden.
            Durch den Aufbau der Datenverarbeitungsstrukturen für Zeitungen und die dafür nötigen Standardisierungsprozesse sollen auch Impulse für die Digitalisierung, Erschließung und Referenzierung der Zeitungsbestände der kooperierenden Datenpartner ausgehen, ein Effekt, der sich bereits beim Aufbau der Deutschen Digitalen Bibliothek gezeigt hat. Darüber hinaus soll der Aufbau eines Zeitungsportals Kulturerbe-Einrichtungen dazu anregen, weitere Digitalisierungsvorhaben von historischen Zeitungen anzugehen. Um ein Portal mit für die Zukunft gerüsteten offenen Schnittstellen zu schaffen, ist die perspektivische Integration der IIIF-Technologie sowie eine Anbindung an die Europeana Newspapers Collection geplant.
         
         
            Ein Zeitungsportal für die Wissenschaft und die interessierte Öffentlichkeit
            Das Medium Zeitung zeichnet sich dadurch aus, dass es alle Bereich des Lebens abdeckt. Digitalisierte historische Zeitungen bieten den (Geistes-)Wissenschaften die Möglichkeit, eine Vielzahl von Forschungsfragen zu adressieren (Blome 2018). Das Interesse aus der Wissenschaft fließt beim Aufbau des Zeitungsportals auf mehreren Wegen in die Konzeption ein: So waren die oben genannten vier Kernfunktionalitäten Ergebnis eines Workshops mit WissenschaftlerInnen, der im Herbst 2014 im Rahmen des DFG-Pilotprojektes „Digitalisierung historischer Zeitungen“ in Bremen stattfand. Auch der laufende Entwicklungsprozess des Zeitungsportals wird von einem internationalen wissenschaftlichen Beirat begleitet. Im Gegensatz zu führenden Zeitungsprojekten der Digital Humanities wie Impresso, NewsEyes oder Oceanic Exchanges, welche ausschließlich ein wissenschaftliches Publikum im Blick haben, richtet sich das Angebot des Zeitungsportals auch an allgemeininteressierte NutzerInnen. Wie bereits bestehende, internationale Zeitungsportale zeigen, werden ihre Angebote sehr gut angenommen und von unterschiedlichsten Nutzergruppen besucht (für Nutzergruppen des österreichischen Zeitungsportals ANNO vgl. z.B. Müller 2016: 86). Vor allem die Volltextsuche, also die Möglichkeit, nicht nur in den Zeitungstiteln und anderen Metadaten, sondern in den eigentlichen Zeitungstexten zu suchen, macht das Zeitungsportal zu einem sehr niedrigschwelligen Angebot: Auch ohne große Recherchekenntnisse können NutzerInnen Artikel oder Informationen zu allen vorstellbaren Themen finden – sei es zum eigenen Sportverein, zu einer berühmten Persönlichkeit oder zur Geschichte der eigenen Familie. 
                
         
         
            Anforderungen der unterschiedlichen Nutzergruppen
            Das Projekt „Deutsches Zeitungsportal“ steht somit vor der Herausforderung, Anforderungen aus der Wissenschaft und aus der allgemeinen Öffentlichkeit zu analysieren, sie soweit wie möglich zu vereinen und ihnen allen bei der Umsetzung des Zeitungsportals möglichst gerecht zu werden. 
            Einer der ersten Schritte bei der Entwicklung des Zeitungsportals war es darum, die Anforderungen von Seiten der Wissenschaft sowie von allgemeininteressierten NutzerInnen zu erheben. Dabei kamen zu Beginn des Projekts drei unterschiedliche Methoden zum Einsatz: eine webbasierte Nutzerumfrage, fünfzehn, jeweils einstündige Interviews mit ausgewählten NutzerInnen und ein zweitägiger Workshop mit dem wissenschaftlichen Beirat. 
            Die Umfrage hatte als Ziel, potenzielle Zielgruppen des Zeitungsportals und ihre Anforderungen und Erwartungen kennenzulernen. Sie umfasste 23 Fragen zu Themengebieten wie Rechercheanlässe, Funktionalitäten und thematische Interessen und wurde für fünfeinhalb Wochen über die Homepage der DDB und über 27 weitere digitale Kanäle verbreitet. Das große Interesse, das dem Zeitungsportal entgegengebracht wird, zeigte sich hierbei schon rein zahlenmäßig – mit 2.422 ausgefüllten Fragebogen.
            Die Usability-Tests mit fünfzehn ausgesuchten potenziellen NutzerInnen wurden als halbstrukturierte Interviews durchgeführt, bei denen Methoden wie Protokolle lauten Denkens, Beobachtung und Aufzeichnung des Klickverhaltens, Aufzeichnung der Gestik und Mimik zum Einsatz kamen. Der Schwerpunkt der Interviews lag auf der Interaktion mit einem Klick-Dummy des Zeitungsportals, der aufgrund gezielter Fragen und kleiner Aufgaben auf seine Verständlichkeit und Usability einerseits und die Erwartungen der Interviewten andererseits überprüft wurde.
            Die Nutzerumfrage und die Nutzerinterviews richteten sich an die allgemeine Öffentlichkeit und wurden in Zusammenarbeit mit einer Marktforschungsagentur durchgeführt, die die Ergebnisse analysiert und aufbereitet hat.
            Bei dem Workshop mit dem wissenschaftlichen Beirat handelte es sich um ein zweitägiges Treffen, bei dem im Juni 2019 die vorliegenden Konzeptpapiere und der Klick-Dummy vorgestellt und mit den WissenschaftlerInnen unter Einbeziehung anderer Zeitungsportale diskutiert wurden. Die Empfehlungen, die im Lauf des Workshops entwickelt wurden, sind ebenfalls in die Konzeption des Portals eingeflossen.
            Der Vortrag widmet sich der Auswertung dieser Erhebungen: Wo finden sich Gemeinsamkeiten? Wo gibt es Unterschiede? Was ist zu tun, wenn sich die Anforderungen aus Wissenschaft und allgemeiner Öffentlichkeit widersprechen? Welche Wünsche lassen sich überhaupt realisieren und wo liegen Grenzen, seien diese technisch oder urheberrechtlich bedingt?
            Ein Beispiel für eine übereinstimmende Anforderung ist der Wunsch nach möglichst umfassenden Inhalten. Sowohl die allgemeine Öffentlichkeit als auch die Wissenschaftscommunity wünscht sich ein Zeitungsportal, das weitreichende Bestände anbietet, sodass sich die Recherche im besten Fall über ein einziges Portal erledigen lässt. Zwar ist genau dies der Anspruch und das angestrebte Alleinstellungsmerkmal des Deutschen Zeitungsportals, aber die Umsetzung dieses Zieles kann nicht allein vom Zeitungsportal erreicht werden. Viele Faktoren und Stakeholder – wie die schiere Menge des Materials, die Zerstreuung der Bestände in viele unterschiedliche Einrichtungstypen, die von der DFG überhaupt nicht erreicht werden, und nicht zuletzt das Urheberrecht, das die Digitalisierung und Verbreitung der besonders interessanten Bestände aus dem 20. Jahrhundert einschränkt – spielen hier eine Rolle. Ein kompletter Nachweis aller deutschen digitalisierten Zeitungsbestände kann darum eher als Vision der Community der Kulturerbe-Einrichtungen und ihrer Träger beschrieben werden, denn als ein kurzfristig erreichbares Ziel (Bürger 2018: 131f.).
            Eine wichtige Erkenntnis der Diskussion war es, dass das Zeitungsportal möglichst transparent mit seinen Inhalten umgehen muss: Wenn nicht alle historischen Zeitungen zu finden sind, muss für die NutzerInnen deutlich werden, welche Inhalte verfügbar sind und nach welchen Kriterien das vorhandene Zeitungskorpus zusammengestellt wurde. 
            Unterschiedliche Erwartungen der Nutzergruppen wurden besonders im Bereich Nutzerinterface formuliert. Während die allgemeinen NutzerInnen sich hier eher eine einfach gestaltete Oberfläche wünschen und erwarten, alle Suchanfragen über einen Suchschlitz eingeben zu können, wurde in der Diskussion mit der wissenschaftlichen Begleitgruppe der Wunsch nach anspruchsvolleren Funktionen laut, z. B. nach der Möglichkeit, sich ein individuelles Zeitungskorpus zusammenzustellen und dieses gezielt durchsuchen zu können. Diese Anforderungen werden teilweise von der aus dem DDB-Hauptportal übernommenen Funktion „Meine DDB“ erfüllt, mit der sich Favoriten und Suchanfragen speichern lassen. Zudem könnten zukünftig erweiterte Funktionen implementiert werden, die, ohne die Startseite zu überladen, für WissenschaftlerInnen und erfahrene NutzerInnen einen komfortablen Zugang bieten, der komplexere Suchanfragen erlaubt. Der Suchschlitz für die Volltextsuche soll jedoch über alle Iterationen das bestimmende Element bleiben. Im Suchschlitz selber können auch komplexe Abfragen gemäß der von der Suchmaschine vorgegebenen Syntax eingegeben werden.
         
         
            Ausblick
            
               Zum Abschluss des Vortrags wird der aktuelle Projektstand vorgestellt, inklusive eines ersten Blicks auf den Prototypen des Zeitungsportals, das Ende 2020 für die Öffentlichkeit freigeschaltet werden soll. Der Prototyp ist nicht nur aus technischen Gesichtspunkten (Tests, Qualitätssicherung) wichtig, sondern soll als Grundlage für eine zweite Runde der Nutzerforschung dienen: Die Erkenntnisse, die zu Beginn des Projektes gewonnen wurden, sollen bis Ende 2020 anhand einer weiteren Nutzerbefragung überprüft werden. So ist geplant, den Prototyp sowohl von den allgemeininteressierten NutzerInnen als auch vom wissenschaftlichen Beirat evaluieren zu lassen und daraus Erkenntnisse für die Schwerpunkte der nächsten, für 2021–2022 geplanten Projektphase zu gewinnen.
            
         
      
      
         
            
      Hervorzuheben sind insbesondere das Portal der Staatsbibliothek zu Berlin ZEFYS http://zefys.staatsbibliothek-berlin.de [letzter Zugriff 05.09.19], digiPress der Bayerischen Staatsbibliothek 
      https://digipress.digitale-sammlungen.de/ [letzter Zugriff 05.09.19] und das Zeitungsportal NRW 
      https://zeitpunkt.nrw/ [letzter Zugriff 05.09.19]. 
    
            
      Deutsche Nationalbibliothek (Projektleitung), Sächsische Landesbibliothek – Staats- und Universitätsbibliothek Dresden (SLUB), Staatsbibliothek zu Berlin – Preußischer Kulturbesitz, FIZ Karlsruhe – Leibniz-Institut für Informationsinfrastruktur.
    
            
      Dabei handelt es sich um die folgenden sechs Bibliotheken, von denen die ersten fünf auch am vorgeschalteten DFG-Pilotprojekt beteiligt waren: Sächsische Landesbibliothek – Staats- und Universitätsbibliothek Dresden (SLUB), Staatsbibliothek zu Berlin – Preußischer Kulturbesitz (SBB), Bayerische Staatsbibliothek (BSB), Universitäts- und Landesbibliothek Sachsen-Anhalt (ULB), Staats- und Universitätsbibliothek Bremen (SuUB), Staats- und Universitätsbibliothek Hamburg Carl von Ossietzky (SUB).
    
            
      Mitglieder des wissenschaftlichen Beirats sind: Astrid Blome (Institut für Zeitungsforschung Dortmund), Christa Müller (Österreichische Nationalbibliothek), Claudia Resch (Österreichische Akademie der Wissenschaften), Estelle Bunout (Universität Luxemburg), Fotis Jannidis (Universität Würzburg), Günter Mühlberger (Universität Innsbruck), Jana Keck (Universität Stuttgart), Jörg Lehmann (Universität Tübingen), Marc Priewe (Universität Stuttgart), Maria Elisabeth Müller (Staats- und Universitätsbibliothek Bremen), Marian Dörk (Fachhochschule Potsdam/Urban Complexity Lab), Marten Düring (Universität Luxemburg), Pim Huijnen (Universität Utrecht), Thomas Werneke (Zentrum für Zeithistorische Forschung Potsdam).
    
         
         
            
               Bibliographie
               
                   ANNO (AustriaN Newspapers Online):
                  http://anno.onb.ac.at [letzter Zugriff 29. August 2019].

               
                  Altenhöner, Reinhard (2018): „Auf dem Weg zu einem nationalen Zeitungsportal. Eine materialspezifische Kooperation als Treiber eines neuen Dienstes für Wissenschaft und Forschung“ in: Bonte, Achim / Rehnolt, Juliane (eds.): 
  Kooperative Informationsinfrastrukturen als Chance und Herausforderung: Festschrift für Thomas Bürger zum 65. Geburtstag. Berlin, Boston: De Gruyter 144-160, DOI: 
  10.1515/9783110587524-019.

               
                  Bürger, Thomas (2016): „Zeitungsdigitalisierung als Herausforderung und Chance für Wissenschaft und Kultur“ in: 
  Zeitschrift für Bibliothekswesen und Bibliographie 63, H 3, 123-132, DOI: 10.3196/186429501663332.

               
                  Blome, Astrid (2018): „Zeitungen“ in: Busse, Laura / Enderle, Wilfried / Hohls Rüdiger u.A. (eds.): 
  Clio-Guide. Ein Handbuch zu digitalen Ressourcen für die Geschichtswissenschaften. Berlin: Clio-online und Humboldt-Universität zu Berlin (Veröffentlichungen von Clio-online, Nr. 2), B.6-1-D.6-36, DOI: 10.18452/19244.

               
                   The British Newspaper Archive:
                   [letzter Zugriff 29. August 2019]. 
               
                  Burckhardt, Daniel / 
  Geyken, Alexander / 
  Saupe, Achim / 
  Werneke, Thomas (2019): „Distant Reading in der Zeitgeschichte. Möglichkeiten und Grenzen einer computergestützten Historischen Semantik am Beispiel der DDR-Presse“ in: 
  Zeithistorische Forschungen/Studies in Contemporary History 16: 177-196, DOI: 10.14765/zzf.dok-1345.

               
                  Delpher: 
                  https://www.delpher.nl/ [letzter Zugriff 29. August 2019].

               
                  DFG-Antrag „Errichtung eines nationalen Zeitungsportals auf der Basis der organisatorischen und technischen Infrastruktur der Deutschen Digitalen Bibliothek (DDB) – DDB-Zeitungsportal. Online verfügbar unter: 
  https://pro.deutsche-digitale-bibliothek.de/node/985 [letzter Zugriff 29. August 2019].

               
                  DFG-Ausschreibung „Digitalisierung historischer Zeitungen des deutschen Sprachgebiets“ im Rahmen des LIS-Förderprogramms von 2018
  ( ) [letzter Zugriff 10. September 2019].
               
                  DFG-Ausschreibung „Digitalisierung historischer Zeitungen des deutschen Sprachgebiets“ im Rahmen des LIS-Förderprogramms von 2019(
  https://www.dfg.de/download/pdf/foerderung/programme/lis/ausschreibung_zeitungsdigitalisierung_2019.pdf) [letzter Zugriff 10. September 2019].

               
                  digiPress: 
                  https://digipress.digitale-sammlungen.de/ [letzter Zugriff 05. September 2019].

               
                  Empfehlungen zur Digitalisierung historischer Zeitungen in Deutschland (Masterplan Zeitungsdigitalisierung):
 [letzter Zugriff 05. September 2019].
               
                  Impresso: 
                  https://impresso-project.ch/ [letzter Zugriff 29. August 2019].

               
                  Müller, Christa (2016): „ANNO – Der Digitale Zeitungslesesaal der Österreichischen Nationalbibliothek: Aktuelle und zukünftige Entwicklungen im Überblick“ in: 
  BIBLIOTHEK – Forschung und Praxis 40(1). Berlin: de Gruyter 83-89, DOI: 10.1515/bfp-2016-0012.

               
                  NewsEye: https://www.newseye.eu/ [letzter Zugriff 29. August 2019].
               
                   Oceanic Exchanges: http://oceanicexchanges.org/ [letzter Zugriff 29. August 2019].
               
                  ZDB (Zeitschriftendatenbank): 
                  https://zdb-katalog.de/ [letzter Zugriff 29. August 2019].
  
               
                  ZEFYS: 
                  http://zefys.staatsbibliothek-berlin.de [letzter Zugriff 05. September 2019]. 
  
               
                  Zeitpunkt NRW: 
                  https://zeitpunkt.nrw/ [letzter Zugriff 05. September 2019].
  
            
         
      
   



      
         
            Das Problemfeld der OCR früher Drucke
            Lange galt die automatisierte Texterkennung oder sog. Optical Character Recognition (OCR) historischer Drucke des späten Mittelalters und der Frühen Neuzeit, das heißt die Überführung des gedruckten Textes in eine maschinenverarbeitbare Form, als sehr problematisch (Rydberg-Cox 2009). Die OCR moderner Texte wird dagegen auch aufgrund technischer Innovationen wie des zeilen- statt zeichenbasierten OCR-Ansatzes (Breuel et al. 2013) weithin als informatisch gelöstes Problem angesehen. Die teils höchst komplexen Layoutstrukturen von Inkunabeln und der bis zum Ende des 18. Jahrhunderts gedruckten Werke, ihr oft schlechter Erhaltungs- und Druckzustand sowie die Vielfalt und Varianz der in ihnen verwendeten Drucktypen stellen dagegen bis heute sogar den kommerziellen State of the Art der Texterkennungssoftware wie beispielsweise ABBYY FineReader
vor erhebliche Probleme. Auch die vermeintlich einfach gedruckten Frakturromane des 19. Jahrhunderts bereiten bei ihrer Überführung in eine E-Text-Variante immer wieder große Schwierigkeiten. Trotz der durch Bibliotheken und andere öffentliche Einrichtungen bereit gestellten, wachsenden Bestände bilddigitalisierter Vorlagen dieser Epochen ist darum der Umfang digitalisierter Texte nicht annähernd im selben Maß gewachsen, obwohl in den vergangenen Jahren bereits deutliche Fortschritte für die OCR vormoderner Drucke aufgezeigt werden konnten (Springmann / Lüdeling 2017).
                
            Vor allem für die geistes- und kulturwissenschaftliche Editionsphilologie eröffnet sich auf diese Weise ein erhebliches Problemfeld, ist diese vor dem Hintergrund der Entwicklung hin zu immer mehr digitalen Editionen doch auf meist große Textmengen in digitaler Form angewiesen, die im besten Fall neben ihrer hohen Zeichengenauigkeit bereits Metainformationen über das gedruckte Ursprungsmedium aufweisen – zu denken wäre hier besonders an die Typisierung unterschiedlicher Layoutregionen (Überschriften, Marginalien, Bildbeischriften etc.) oder die Lesereihenfolge der einzelnen Layoutelemente des originalen Textes. Und auch mit Blick auf neuere Forschungsfelder innerhalb der Geisteswissenschaften und Digital Humanities (Text Mining, Sentiment Analysis usw.) sowie deren Bedarf an großen Textmengen zur Anwendung quantitativer Analyseverfahren stellt sich zunehmend die Frage nach Möglichkeiten einer OCR früher und vormoderner Drucke, die sowohl hohen Qualitätsansprüchen als auch einem entsprechenden Automatisierungsgrad genügt.
            Werkzeuge, die diese Anforderungen erfüllen, sollten zudem frei verfügbar sein, sich einfach und selbstständig von einem informatisch nicht vorgeschulten Nutzerkreis auf einer einheitlichen Benutzeroberfläche bedienen lassen und die unterschiedlichen Submodule wie beispielsweise die Vorverarbeitung von Bilddateien, Möglichkeiten der Layouttypisierung sowie die eigentliche Zeichenerkennung integrativ zu einem kohärenten OCR-Workflow zusammenführen.
            Am Lehrstuhl für Künstliche Intelligenz und Angewandte Informatik der Julius-Maximilians-Universität Würzburg wurde deshalb die OCR-Software OCR4all entwickelt, welche die genannten Notwendigkeiten in sich vereint und sich als erstes Programm überhaupt mit Blick auf die besonders herausfordernden Textgruppen direkt an Geisteswissenschaftler*innen richtet. 
                
         
         
            OCR-Workflow
            Typischerweise gliedert sich ein OCR-Workflow in vier Hauptkomponenten (s. Abbildung 1). Im sog. 
                    Preprocessing werden die Originalbilder in Vorbereitung späterer Arbeitsschritte binarisiert (Konvertierung des Ausgangsbildes in ein Schwarzweißbild) und gerade gestellt, um die nachfolgenden Arbeitsschritte zu erleichtern.
                
            
               
               Abbildung 1: Hauptkomponenten eines typischen OCR-Workflows. Von links nach rechts: Originalbild, Preprocessing, Segmentierung, OCR, Nachkorrektur. 
            
            Während der 
                    Segmentierung erfolgt die Erkennung und Typisierung der Layoutbestandteile. Dazu werden zuerst die Text- und Nicht-Textregionen (Bilder, Bordüren etc.) unterschieden, optional die Textregionen anschließend als Haupttext, Überschriften, Marginalien etc. semantisch ausgezeichnet. Abschließend werden die Textregionen zur Vorbereitung der OCR in einzelne Zeilenbilder zerschnitten. 
                
            In einem dritten Schritt, der 
                    OCR, werden die identifizierten Bildzeilen durch die Anwendung von sog. Modellen in maschinenverarbeitbaren Text umgewandelt. Je nach Material können dazu entweder sog. gemischte Modelle verwendet werden, die mithilfe einer Vielzahl ganz unterschiedlicher, jedoch epochentypischer Werke erstellt wurden. Handelt es sich bei den zu bearbeitenden Werken hinsichtlich der Vielfalt und Varianz der in ihnen verwendeten Drucktypen sowie deren Erhaltungszustand jedoch um sehr spezifische Drucke, können sog. werkspezifische Modelle für die Erkennung erstellt und verwendet werden.
                
            In der 
                    Nachkorrektur können die generierten maschinenverarbeitbaren Texte und Daten abschließend nachbearbeitet und korrigiert werden.
                
            OCR4all orientiert sich in seinem Aufbau an den beschriebenen Hauptkomponenten eines OCR-Workflows, gliedert diese jedoch noch einmal in unterschiedliche Teilmodule. Der modulare Aufbau erlaubt dabei eine Einbindung und Verwendung bereits bestehender Softwarelösungen, die gemäß ihrer Stärken zu einem kohärenten OCR-Workflow kombiniert werden.
            Grundsätzlich kann der Workflow vollautomatisch durchlaufen werden. Dennoch hat der Nutzer immer die Möglichkeit, korrigierend in jeden Teilschritt einzugreifen, um ein optimales Ergebnis zu garantieren, welches als Startpunkt des dann folgenden Teilschritts fungiert. Dafür können die für jedes Teilmodul vorgegebenen Einstellungen durch den Nutzer individuell angepasst werden.
            Das Preprocessing erfolgt in OCR4all wie oben beschrieben. Dabei werden alle gängigen Eingabeformate für Bilddateien unterstützt. Dem schließt sich die Layouttypisierung mithilfe des Segmentierungstools LAREX
 (s. Abbildung 2) an. Hier können werkspezifische Parameter zur Text- und Bildtypisierung festgelegt sowie zu erkennende Layoutregionen (Haupttext, Überschriften, Marginalien, Seitenzahlen etc.) definiert werden. Je nach Komplexität des vorliegenden Seitenlayouts ist nach einer automatischen Layouterkennung ein Eingriff in das vorliegende Ergebnis mittels unterschiedlicher Korrekturwerkzeuge möglich. Weiterhin kann in LAREX die Lesereihenfolge der Layoutbestandteile markiert werden, um den Lesefluss des Originals vorlagengetreu nachbilden zu können. Vor allem für die Verwendung des maschinenverarbeitbaren Textes in digitalen Editionen sind diese Funktionen unverzichtbar. 
                
            
               
               Abbildung 2: Im Teilmodul der Segmentierung erfolgen die Typisierung der Layoutelemente sowie die Festlegung der Lesereihenfolge. 
            
            Der Layouttypisierung folgt die Zeilensegmentierung. In dieser werden die Text beinhaltenden Layoutbestandteile in einzelne Zeilenbilder zerteilt (OCRopus), um die eigentliche OCR vorzubereiten.
                
            Anschließend wird im Erkennungsschritt aus den vorliegenden Einzelzeilen (mittels Calamari) maschinenverarbeitbarer Text generiert. Dazu können in OCR4all bereits standardmäßig integrierte gemischte Modelle für Fraktur- und Antiquaschriften unterschiedlicher Epochen genutzt werden. Es besteht die Möglichkeit, die entstandenen Texte anschließend in einem Editor komfortabel zu korrigieren (s. Abbildung 3).
                
            
               
               Abbildung 3: Im Editor kann generierter Text mithilfe eines sog. Virtual Keyboard (rechts) zeichengetreu korrigiert werden.
            
            Für die Feststellung der Fehlerrate der Zeichenerkennung kann im Evaluationsmodul der ursprünglich erkannte Text mit der durch den Nutzer vorgenommenen Korrektur verglichen werden.
            Darüber hinaus bietet OCR4all die Möglichkeit, die oben angesprochenen werkspezifischen Modelle unter Verwendung vorgenommener Textkorrekturen selbst zu trainieren, stetig zu verfeinern und anzuwenden. Besonders bei Werken mit erheblicher Typenvielfalt und -varianz, bei denen ein bestehendes gemischtes Modell keine hinreichenden Erkennungsergebnisse erzielt, können auf diese Weise dennoch sehr hohe Zeichenerkennungsraten erreicht werden.
            In der abschließenden Nachkorrektur können die generierten Texte editionsreif korrigiert und als Plain Text oder PageXML ausgegeben werden. Letzteres Format beinhaltet neben dem eigentlichen Text auch dessen Verankerung in semantischen Positionen auf den Druckseiten in Form von Koordinaten. 
                
            In Abhängigkeit des Ausgangsmaterials variiert der zum Erreichen einer sehr hohen Genauigkeit benötigte Arbeitsaufwand zwischen wenigen Minuten bei Werken mit einfachen Layoutstrukturen, für die ein passendes Modell vorliegt, und einigen Stunden bei sehr komplexen, frühen Drucken, für die werkspezifische Modelle trainiert werden müssen (Reul et al. 2019). 
         
         
            Workshopkonzeption
            Der ganztägige Workshop soll einem informatisch und technisch nicht spezifisch vorgeschulten Nutzerkreis einen nachvollziehbaren und verständlichen Einstieg in das Themen- und Problemfeld der OCR historischer Drucke bieten. Er wird dazu befähigen, mithilfe der vorgestellten Software eigenständig qualitativ hochwertige Texte aus ganz unterschiedlich anspruchsvollen Ausgangsdaten zu generieren – und dies mit zeitlich vertretbarem Aufwand. Die Konzeption erfolgt aus diesem Grund sehr praxisbezogen. Konkret bedeutet dies einen angeleiteten und individuell betreuten Durchgang durch den oben vorgestellten OCR-Workflow anhand verschiedener, nach Layoutkomplexität, Typographie, Erhaltungszustand und Entstehungszeitraum geclusterter Drucke. Dabei sollen anwendungsbezogen wichtige Grundfragen der OCR beantwortet werden: 
            
               Wie verändert sich entsprechend des Ausgangsmaterials die Anwendung der OCR-Workflows und der in ihm enthaltenen Submodule? 
               Mit welchem Aufwand ist in unterschiedlichen Bearbeitungsphasen des Materials zu rechnen? 
               Wie stark lässt sich der Workflow in Abhängigkeit des vorliegenden Materials automatisieren? 
               Wie schnell sind bei einem werkspezifischen Training welche Erkennungsraten erreichbar? 
               Welcher Aufwand ist mit Blick auf die spätere Verwendung der produzierten Texte überhaupt sinnvoll?
               …
            
            Da sich neben den oben beschriebenen, meist vormodernen Textspezifika auch eine grundlegende technische Expertise der Benutzer*innen im Bereich der OCR als eine wichtige Bedingung für die Produktion hochwertiger digitaler Texte herausgestellt hat, strebt der Workshop neben einer besonders praktischen Handlungsanleitung auch die Vermittlung der wichtigsten Funktionskonzepte der in OCR4all integrierten Submodule an. 
            Der Workshop umfasst neben den oben beschriebenen Inhalten auch Fragen der Einrichtung und Installation der Software. Zusätzlich wird eine Serverversion der Software zur Verfügung gestellt, die einen reibungslosen Ablauf gewährleistet und Trainingsprozesse werkspezifischer Modelle effizient durchführbar macht. Die max. 25 Teilnehmer*innen benötigen einen Laptop und Internetzugang. Die Verwendung einer Maus wird empfohlen. 
         
         
            Forschungsinteressen der Beitragenden
            
               Maximilian Wehner ist Wissenschaftlicher Mitarbeiter am Lehrstuhl für Künstliche Intelligenz und Angewandte Informatik sowie am Zentrum für Philologie und Digitalität „Kallimachos“ der Julius-Maximilians-Universität Würzburg. Forschungsinteressen sind die Literatur der Frühen Neuzeit, die OCR früher Drucke sowie die Entwicklung entsprechender Vermittlungskonzepte.
                
            
               Dr. Michael Dahnke arbeitet als Wissenschaftlicher Mitarbeiter am Zentrum für Informations- und Medientechnologie der Universität Siegen. Seine Forschungsschwerpunkte bewegen sich in den Bereichen digitaler Editionsphilologie, Datenmodellierung im Rahmen von TEI sowie der OCR und der Modellierung gewonnener Textdaten.
                
            
               Florian Landes ist als Wissenschaftlicher Mitarbeiter bei der Bayerischen Akademie der Wissenschaften beschäftigt. Seine Forschungsinteressen liegen in den Bereichen der OCR sowie der digitalen Rekonstruktion.
                
            
               Robert Nasarek ist Wissenschaftlicher Mitarbeiter am Lehrstuhl für Wirtschafts- und Sozialgeschichte der Martin-Luther-Universität Halle-Wittenberg sowie des Zentrums für Wissenschaftsforschung der Nationalen Akademie der Wissenschaften Leopoldina. Seine Arbeit bewegt sich im Bereich der Wirtschafts- und Sozialgeschichte, OCR und Digital Humanities.
                
            
               Christian Reul ist Kommissarischer Leiter der Digitalisierungseinheit des Zentrums für Philologie und Digitalität „Kallimachos“ der Julius-Maximilians-Universität Würzburg. Seine Forschungsschwerpunkte sind die OCR auf historischem Material sowie die Entwicklung von OCR-Software.
                
         
      
      
         
            
               
            
            
               
            
            
               
            
            
               
            
            
               
            
            
               
            
         
         
            
               Bibliographie
               
                  Breuel, Thomas M. / Ul-Hasan, Adnan / Al-Azawi, Mayce Ali / Shafait, Faisal (2013): High-Performance OCR for Printed English and Fraktur Using LSTM Networks, in:
                         12th International Conference on Document Analysis and Recognition: 683-687.
                    
               
                  Reul, Christian / Christ, Dennis / Hartelt, Alexander / Balbach, Nico / Wehner, Maximilian / Springmann, Uwe / Wick, Christoph / Grundig, Christine / Büttner, Andreas / Puppe, Frank (2019): OCR4all – An Open-Source Tool Providing a (Semi-)Automatic OCR Workflow for Historical Printings, in: 
                        ArXiv Preprints (submitted to MDPI – Applied Sciences) https://arxiv.org/abs/1909.04032.
                    
               
                  Rydberg-Cox, Jeffrey A. (2009): Digitizing Latin Incunabula: Callenges, Methods, and Possibilities, in: 
                        Digital Humanities Quarterly 3, 1 http://digitalhumanities.org:8081/dhq/vol/3/1/000027/000027.html.
                    
               
                  Springmann, Uwe / Lüdeling, Anke (2017): OCR of historical printings with an application to building diachronic corpora: A case study using the RIDGES herbal corpus, in: 
                        Digital Humanities Quarterly 11, 2 http://www.digitalhumanities.org/dhq/vol/11/2/000288/000288.html.
                    
            
         
      
   



      
         Karl Kraus' Endzeitdrama »Die letzten Tage der Menschheit«, 1919 zum ersten Mal vollständig erschienen (Buchausgabe 1922), ist in vielerlei Hinsicht inkommensurabel. Der schiere Umfang sprengt alle Gattungsnormen (638 Seiten in der »Volk und Welt«-Ausgabe von 1978). Die fünf Akte plus Vorspiel und Epilog sind in 220 Szenen unterteilt, es gibt je nach Zählweise um die 1.000 sprechende Figuren bzw. Instanzen (zum Vergleich: als nächstgrößtes deutschsprachiges Drama gilt Grabbes »Napoleon oder Die hundert Tage« von 1831 mit 259 Figuren).
         Die Zählweise ist nicht nur deshalb kontingent, weil es zahlreiche Rufe aus der Menge gibt, die sich nicht quantifizieren lassen (wozu vor allem auch das undurchsichtige Stimmengewirr im Epilog gehört), sondern auch, weil es konkrete Gruppierungen wie die ›Fünfzig Drückeberger‹ (III/26) oder ›Die zwölfhundert Pferde‹ (V/55) gibt, die man theoretisch quantifizieren könnte, auch wenn dies nicht unmittelbar sinnvoll erscheint. Insgesamt spricht man tatsächlich besser von Sprecherinstanzen, die von historischen Personen über namenlose Zwischenrufer und allegorische Figuren (etwa den »Hyänen, die Menschengesichter tragen«) bis hin zur »Stimme Gottes« reichen.
         Es ist nicht nur auf das Thema des Stücks bezogen – die Apokalypse des Ersten Weltkriegs –, sondern auch auf die Form, wenn Kraus im Vorwort schreibt: »Die Aufführung des Dramas, dessen Umfang nach irdischem Zeitmaß etwa zehn Abende umfassen würde, ist einem Marstheater zugedacht. Theatergänger dieser Welt vermöchten ihm nicht standzuhalten.« (Kraus 1978, S. 5) Die Handlung der Tragödie ist »unmöglich, zerklüftet, heldenlos« (ebd.) und erschwert jede Absicht, das Stück darzustellen, zumal vollständig. Dies betrifft sowohl Inszenierungen auf der Bühne oder als Hörspiel (obwohl es schon Kompletteinspielungen gibt) als auch digitale Modellierungen der Figurenbeziehungen.
         Es ist Konsens innerhalb des Forschungszweigs der Netzwerkanalyse dramatischer Texte, dass sich eine Einzelanalyse der verhältnismäßig übersichtlichen Figurennetzwerke selten lohnt. Das Augenmerk liegt daher normalerweise auf der Untersuchung struktureller Entwicklungen hunderter oder tausender Stücke über verschiedene historische Zeiträume (Algee-Hewitt 2017, Trilcke/Fischer 2018).
         »Die letzten Tage der Menschheit« gehören hier zu den Ausnahmen. Ziel dieses Projekts ist es, das Stück als soziales Netzwerk zu visualisieren, basierend auf Kookkurrenzen von Sprecherinstanzen in den einzelnen Szenen. Voraussetzung dafür ist eine brauchbare Formalisierung des Gesamttextes. Dieser ist einerseits bereits digitalisiert, in annehmbarer Qualität innerhalb des Projekts Gutenberg-DE (obwohl es in dieser Version kaum eine Seite ohne zumindest kleinere OCR-Fehler gibt). Andererseits gibt es noch keine digitale Fassung in einem Format, das die wissenschaftliche Auswertung ermöglicht.
         Am Beginn dieses Projekts stand daher die Herstellung einer TEI-Version des Dramas, die vor Konferenzbeginn veröffentlicht wurde und damit der wissenschaftlichen Community zum ersten Mal eine Version des Textes zur Verfügung stellt, die auf die FAIR-Prinzipien setzt (findable, accessible, interoperable, reusable). Neben einem Qualitätssprung hinsichtlich der Textbasis im Vergleich zur Gutenberg-DE-Version stand dabei die Auszeichnung der Sprecher-IDs im Mittelpunkt. Da, wie bereits angedeutet, diese Auszeichnung kontingent ist, also je nach Formalisierungsentscheidung anders aussehen kann, wird dieser Prozess offengelegt. So werden etwa die Vielzahl an Stimmen aus Menschenmengen oder die Unzahl ausrufender Zeitungsverkäufer nachvollziehbar individualisiert, speziell die Massenszenen in Wien, etwa die Geschehnisse an der Sirk-Ecke, die das Vorspiel und jeden der fünf Akte eröffnen.
         Ergebnis ist ein visualisiertes Netzwerk, das auf einem Poster im A0-Format einen Blick ins Kraus'sche »Marstheater« erlaubt, auf die schiere Masse der Auftritte und Stimmen, aus der doch eine Struktur hervorscheint, wie sie bisher im Kontext der Kraus-Forschung noch nicht visualisiert worden ist. So werden viele »innere Symmetrien« sichtbar (Matala de Mazza 2018), die das Stück strukturieren, wiederkehrende Konstellationen wie etwa die vier Offiziere am Beginn jedes Aktes oder die Szenen in der Schulklasse (I/9 und V/23).
         Deutlich wird im Netzwerkgraph auch die Diskrepanz zwischen Front und Heimat, zwei Welten für sich, wobei Kraus den Fokus auf die entlarvende Sprache von nicht direkt am Krieg beteiligten Personen legt: »Wenn nicht Krieg wär, möcht man rein glauben, es is Friede.« (Kraus 1978, S. 95)
         Da der Text nunmehr als Volltext-TEI-Dokument vorliegt, lässt sich auch der Word Space in das Netzwerk hineinmodellieren, d. h., die Anzahl der Wörter pro Sprecherinstanz. Auf diese Weise scheinen deutlich die (quantitativ gesehen) Hauptfiguren dieses »heldenlosen« Dramas auf (etwa der »Nörgler« und der »Optimist« sowie der »Patriot« und der »Abonnent«), die oft über dutzende Seiten als Zweierkonstellationen auftreten, die aber darüber hinaus, wie der Graph verdeutlicht, auch anderweitig vernetzt sind.
         Um auch komparatistische Aspekte abzudecken, werden auf dem Poster vergleichend einige Netzwerkmetriken präsentiert, um die Gigantomanie des Dramas mit Zahlen zu verdeutlichen.
         Zur Gewährleistung der Nachnutzbarkeit und Nachhaltigkeit der
Modellierung wurde das Stück auch dem German Drama Corpus hinzugefügt
(), der den Zugang zu bestimmten Formalisierungen der Textsubstanz erheblich erleichtert (Fischer et al. 2019).

      
      
         
            
               Bibliographie
               
                  
                  Algee-Hewitt, Mark
	(2017): Distributed Character: Quantitative Models
		  of the English Stage, 1550–1900. In: New Literary
		  History 48(4), 751–782. Johns Hopkins University
		  Press.
		  DOI: 
               
               
                  Fischer, Frank / Börner, Ingo / Göbel, Mathias / Hechtl, Angelika / Kittel, Christopher / Milling, Carsten / Trilcke, Peer
		  (2019): Programmable Corpora. Die digitale
		  Literaturwissenschaft zwischen Forschung und
		  Infrastruktur am Beispiel von DraCor. DHd
		  2019. Digital Humanities: multimedial &
		  multimodal. Konferenzabstracts, S. 194–197.
		  DOI: 
               
               
                  Kraus, Karl
		  (1978): Die letzten Tage der Menschheit. Tragödie in fünf Akten mit Vorspiel und Epilog. Ausgewählte Werke. Band 5,1. Berlin: Volk und Welt 1978.
		
               
                  Matala de Mazza, Ethel
		  (2018): Der populäre Pakt. Verhandlungen der Moderne zwischen Operette und Feuilleton. Frankfurt am Main: S. Fischer 2018.
		
               
                  Trilcke, Peer / Fischer, Frank
		  (2018): Literaturwissenschaft als Hackathon. Zur
		  Praxeologie der Digital Literary Studies und ihren
		  epistemischen Dingen. In: Wie Digitalität die
		  Geisteswissenschaften verändert: Neue
		  Forschungsgegenstände und Methoden. Hrsg. von Martin
		  Huber und Sybille Krämer (= Sonderband der
		  Zeitschrift für digitale Geisteswissenschaften, 3).
		  DOI: 
               
            
         
      
   



      
         
            Einleitung
            In diesem Beitrag stellen wir eine Methode vor, um Informationen über Figurenrelationen in dramatischen Texten, die innerhalb der 
dramatis personae (Figurenverzeichnis) sprachlich kodiert sind, zu extrahieren und maschinenlesbar im TEI/XML vorzuhalten. Das Figurenverzeichnis kann als Paratext (Genette 1993) dem Nebentext zugerechnet werden, ist jedoch literaturwissenschaftlich, von Einführungswerken abgesehen, noch so gut wie nicht erschlossen. Das Figurenverzeichnis steht zwar unabhängig vom eigentlichen Text am Anfang, kann jedoch bereits Figuren- bzw. Textwissen vermitteln, indem die Figuren nach sozial-politischem Stand, Familienzugehörigkeit oder nach anderen Gruppierungen geordnet sind (vgl. Abbildung 1). Häufig lässt sich an der Positionierung eines Names im Figurenverzeichnis auch die Wichtigkeit der betreffenden Figur im Drama ablesen (Pangallo 2015, 91). Durch diese Strukturierung ist es teilweise möglich, schon vorab auf zentrale Konfliktpotentiale des Textes zu schließen (Jeßing 2015, 79–80). Darüberhinaus kann das Figurenverzeichnis laut Pfister und Asmuth auch der Ort erster auktorialer Bewertungen oder Hinweise sein und dient somit nicht nur der reinen Vorstellung der Figuren und ihrer Strukturen untereinander (Pfister 2001, 95; Asmuth 2016, 85).

            
               
                  
                   Abbildung 1: Figurenverzeichnis in Die Räuber (Friedrich Schiller, 1781)
               
            
            Das Verfahren – und dessen Implementierung in einem Python-Skript – ist auch für in Zukunft digitalisierte Dramen anwendbar, und wird von uns als quelloffene Software zur Verfügung gestellt. Es ist vergleichsweise einfach auf neue Sprachstufen oder Genres anpassbar und liefert – auch bei nicht-perfekten Ergebnissen – eine gute Vorlage. Eine Evaluation des Verfahrens erfolgt auf ungesehenen Testdaten. Außerdem veröffentlichen wir einen Datensatz mit extrahierten Figurenrelationen aus deutschsprachigen Dramen, die manuell validiert und korrigiert wurden. Diese Daten werden zur einfachen und breiten Nutzung im TEI-Format in das GerDraCor eingespeist. Schlussendlich beschreiben wir beispielhaft zwei Analyseszenarien in denen die Daten neue Einblicke bieten (können).

         
         
            Automatische Extraktion von Figurenrelationen
            Unsere Methode unterscheidet zwischen sieben Kategorien von Figurenrelationen (Tabelle 1). Ausschlaggebend für die Zuordnung zu einer der Kategorien sind Signalwörter wie “Vater”, “Kammerdiener”, “Geschwister” etc. Diese Signalwörter werden in einer kontextfreien Grammatik der entsprechenden Kategorie zugeordnet.
            
                Tabelle 1: Figurenrelationen
               
                  Relationen Label
                  gerichtet/ungerichtet
                  Beschreibung
               
               
                  parent_of
                  directed
                  Eine Figur ist Elternteil einer anderen
               
               
                  lover_of
                  directed
                  Liebesbeziehungen (unverheiratet)
               
               
                  related_with
                  directed
                  Familienbeziehungen (außer Eheleute)
               
               
                  associated_with
                  directed
                  Figuren, die miteinander anderweitig verbunden sind (z.B. Diener, Kindermädchen etc.)
               
               
                  siblings
                  undirected
                  Figuren, die mindestens ein gemeinsames Elternteil haben
               
               
                  spouses
                  undirected
                  verheiratete oder verlobte Figuren
               
               
                  friends
                  undirected
                  Freundschaftsbeziehungen
               
            
            Kontextfreie Grammatiken bezeichnen in der Informatik eine Sammlung aller syntaktisch korrekten Programme einer Programmiersprache (Böckenhauer und Hromkovič 2013, 177). Die formalisierte Art, in der die Grammatik alle Regeln einer Programmiersprache enthält, erlaubt es, automatisierte Syntaxanalysen von Programmen durchzuführen (Böckenhauer und Hromkovič 2013, 177). Die Regeln werden mit Hilfe zweier Alphabete beschrieben: Das Terminalalphabet enthält alle Wörter einer Sprache, wohingegen das Nichtterminalalphabet Variablen enthält, die vorgeben, auf welche Art und Weise die Wörter kombiniert werden können (Böckenhauer und Hromkovič 2013, 178).
            Wir nutzen eine solche Grammatik, um drei verschiedene Zeilenarten im Figurenverzeichnis zu unterscheiden, bei denen es sich um Nichtterminale handelt. Alle in den Sätzen vorkommenden Tokens sind Terminale, deren Kombination und Anzahl Aufschluss darüber gibt, um was für eine Art von Zeile es sich jeweils handelt. Auf diese Weise können auch zeilenübergreifende Relationen erkannt werden.
            Zu Beginn des Programmablaufs werden die in GerDraCor vorhandenen Figuren-IDs zusammen mit dem Figurenverzeichnis ausgelesen und gespeichert. Da wir die Beziehungen zwischen den Figuren ausschließlich anhand der Angaben im Figurenverzeichnis konstruieren, muss der Dramentext nicht extra eingelesen werden. Daraus ergibt sich die Beschränkung, dass jegliche Beziehungen, die nicht im Figureverzeichnis explizit gemacht werden, vom Programm auch nicht erkannt werden können. Es geht demnach ausschließlich darum, das Personenverzeichnis maschinenlesbar und -interpretierbar zu machen. So ignoriert das Programm beispielsweise auch alle Zeilen, die eine Gruppe von Figuren als Kollektiv einführt, da diese als “Nummern oder als anonyme Angehörige von Untergruppen” (Schlaffer 1972, 11) meistens keine eigenen Namen haben und auch keine explizit gemachten Beziehungen.
            
            Anschließend werden alle Tokens jeder Zeile des Figurenverzeichnisses daraufhin untersucht, ob es sich dabei um Figurennennungen oder Signalwörter handelt und die Grammatik einem Parser übergeben, der die Zeilen des Figurenverzeichnisses in Baumstrukturen überführt (Abbildung 2).
            
               
                  
                  Abbildung 2: Zwei reduzierte Baumstrukturen für Figuren aus Nathan der Weise. 
               
            
            Aus den erstellten Baumstrukturen werden einzelne Informationen ausgelesen, die grundlegend für die Erkennung der Figurenrelationen sind. Zuerst wird überprüft, wie viele IDs sich in einer Zeile befinden. Die erste oder einzige wird zur Erstellung späterer Relationen abgespeichert. Befindet sich in einer Zeile zusätzlich zu einer ID noch ein Signalwort für eine Figurenrelation, bezieht sich die Zeile in der Regel auf die vorangegangene, wie beispielsweise in 
                    Nathan der Weise:
                
            Sultan Saladin.
            Sittah, seine Schwester.
            Die zweite Zeile enthält neben dem Namen noch das Signalwort “Schwester”, das auf die Beziehungsart siblings hinweist, eine ungerichtete Relation. Da keine zweite Figurenbezeichnung in der Zeile vorkommt, entnimmt das Programm als zweiten Part für die Geschwisterbeziehung den Namen bzw. die daraus abgeleitete ID saladin aus der vorherigen Zeile:
            
            Wenn die beiden benötigten IDs für das Erstellen der Figurenrelation feststehen, wird die Art der Relation durch das Auslesen des Signalworts aus der Baumstruktur festgestellt. Danach werden daraus die Zeilen mit den Figurenrelationen erstellt und diese anschließend in die jeweilige TEI-Version des Textes geschrieben.
            Befindet sich in einer Zeile eine zweite Figuren-ID, bezieht sich die Zeile nicht auf eine vorangegangene, sondern stellt selbst den zweite Bezugspunkt der Relation. Das ist beispielsweise bei der Figur “Camillo Rota” in 
                    Emilia Galotti der Fall:
                
            Camillo Rota, einer von des Prinzen Räten.
            Die erste erkannte ID ist camillo_rota, die zweite der_prinz, abgeleitet aus “des Prinzen”. Die IDs werden in gerichtete Relationen mit aktivem und passivem Part überführt:
            
            Das Programm arbeitet dabei ausschließlich mit den IDs. Dafür ist es nicht nötig, dass Figurennamen explizit als Namen oder Adelstitel als Titel erkannt werden. Es geht ausschließlich darum aus den einzelnen Wörtern einer Zeile im Figurenverzeichnis Namen bzw. Namensteile und Titelangaben herauszufiltern, die den IDs entsprechen, um die Zeilen einer oder mehreren Figuren zuordnen zu können.
            Um auch IDs zu erkennen, die sich geringfügig von den Namensnennungen im Figurenverzeichnis unterscheiden, überprüft das Programm pro Wort eine Reihe an Varianten. So trennt es beispielsweise vom oben gennannten Wort “Prinzen” das Suffix ab und überprüft, ob ein Artikel Teil der ID ist. So kann “des Prinzen” der ID “der_prinz” zugeordnet werden. In manchen Fällen funktioniert diese Abwandlung aber nicht so reibungslos. In 
                    Der Eheteufel auf Reisen wird eine Figur im Figurenverzeichnis mit dem Namen “Gustel” eingeführt, wohingegen die ID „gustchen“ lautet. Die ID orientiert sich hier an der Namensform, die im Stück tatsächlich verwendet wird und nicht an der Bezeichnung im Figurenverzeichnis. Das führt dazu, dass das Programm die ID “gustchen” nicht dem Wort “Gustel” zuordnen kann, da sie sich zu stark unterscheiden.
                
         
         
            Evaluation
            Um die Methode zu evaluieren, wurden die automatisch erzeugten Relationen manuell nachkorrigiert und so ein Goldstandart erzeugt. Im Schnitt bearbeiteten die Korrektoren 12 Texte pro Stunde. Beim Abgleich der automatisch erzeugten Ergebnisse mit dem Goldstandart lag der Macro-Average-Recall Wert bei 0,3 (Standardabweichung: 0,3) und der Wert von Macro-Average-Precision bei 0,55 (Standardabweichung: 0,4), was einen Macro-Average-F-Score von 0,49 (Standardabweichung: 0,25) ergibt.
         
         
            Korpus
            GerDraCor ist ein deutsches Dramenkorpus, das nach TEI-P5 Standarts kodiert ist und im Dezember 2019 474 Dramen enthält, die im Zeitraum von 1730 bis 1940 veröffentlicht wurden (Fischer u. a. 2019). Es ist Teil des größeren DraCor (Fischer u. a. 2019), das als 
                    Programmable Corpus darauf ausgelegt ist, durch Community-Anstrengungen korrigiert und verbessert werden zu können (Fischer u. a. 2019, 195). Da auf einem Fork von GerDraCor gearbeitet wurde, können die automatisch erzeugten Figurenrelationen dem Korpus unproblematisch hinzugefügt werden. Zusätzlich wurden die Relationen, wie bereits beschrieben, manuell nachkorrigiert, um eine erhöhte Qualität für die Nachnutzung zu gewährleisten.
                
            Im Rahmen der manuellen Nachkorrektur wurden außerdem interessante Fälle identifiziert. So wird etwa eine Gruppe von Figuren in dem oben abgebildeten Figurenverzeichnis von Schillers 
                    Die Räuber als “Libertiner,
		    nachher Banditen” bezeichnet, wodurch
		    Informationen aus der späteren Handlung des
		    Stückes vorweggenommen werden. Diese Art der
		    Vorwegnahme findet sich außerdem in Stücken von
		    Grabbe
		    (Herzog Theodor von
		    Gothland, Panizza
		    (Das Liebeskonzil) und
		    Uhland
		    (Ludwig der Bayer). In Kaisers 
                    Stadt und Land hingegen wird mit der Zeile “Erster Bergmann, später Michael” keine Entwicklung in der Handlung, sondern eine Veränderung der Sprecherbezeichnung markiert. Vorwegnahmen mit Bezug auf veränderliche Beziehungen zwischen Figuren konnten nicht festgestellt werden.
                
         
         
            Analyseszenarien
            Wir stellen im folgenden zwei Analysen vor, in denen von den automatisch extrahierten Relationen Gebrauch gemacht wird, sowohl eine Einzeltext- als auch eine Korpusanalyse. Diese illustrieren Möglichkeiten, die Relationen in der Textanalyse zu berücksichtigen.
            Im ersten Beispiel betrachten wir Shakespeares 
                    Romeo and Juliet in der derzeit auf dracor.org verfügbaren Fassung. Zunächst können die Relationen visualisiert werden. Abbildung 3 zeigt das Figurennetzwerk nach Kopräsenz auf der linken und das Netzwerk, das sich aus den sozialen Beziehungen ergibt auf der rechten Seite. Zur besseren Lesbarkeit wurde ein geeigneter Layout-Algorithmus angewendet. Dabei ist zunächst interessant, dass die beiden Familien keineswegs unverbunden sind: Über Mercutio (Freund von Romeo) und Paris (Verlobter von Julia) sind beide mit dem Prinzen verbunden.
                
            
               
                  
                  Abbildung 3: Figurennetzwerke nach Kopräsenz (oben) und Relationen (unten). Zur besseren Übersichtlichkeit wurden die Figuren auf feste Positionen gesetzt, die oben und unten gleich sind. Bezeichnungen werden nur gezeigt wenn der Grad groß genug ist (oben) oder sie an einer Beziehung beteiligt sind (unten). 
               
            
            Auch wenn Abbildung 3 eine gewisse Symmetrie suggeriert, ist diese keineswegs gegeben wenn wir die Redeanteile nach Familien aufschlüsseln, wie es aus den Annotationen ebenfalls direkt möglich ist. Abbildung 4 zeigt die aggregierten Redeanteile der Figuren, wobei Figuren, die durch Verwandtschaft oder Arbeitsverhältnis zu einer der Familien gehören, zusammengefasst wurden (mit Ausnahme von Mercutio und Paris, die beide mit dem Prinzen verwandt sind). Es zeigt sich, dass Angehörige der Familie Capulet etwas weniger als doppelt so viele Wörter äußern als Angehörige der Familie Montague.
            
               
                  
                  Abbildung 4:  Redeanteile nach Familie 
               
            
            
               
                  
                  Abbildung 5: Verteilung der Relationen im Gesamtkorpus
               
            
            Betrachtet man das annotierte Gesamtkorpus stellt man fest, dass die Relationen ungleich verteilt sind. Während Ehen/Verlobungen, Elternschaft und sonstige Assoziationen relativ häufig vorkommen, spielen Geliebte, sonstige Verwandtschaften, Freundschaften und Geschwister eine vergleichsweise kleine Rolle.
            
            In Abbildung 6 sehen wir die Anzahl der Relationen bestimmter Typen
ins Verhältnis gesetzt zur Großgattung (Komödie/Tragödie). Dabei
wurden die Angaben auf den Titeln der Dramen übernommen und leicht
vereinheitlicht (z.B. Bürgerliches Trauerspiel → Tragödie oder Zauberlustspiel → Komödie). Dabei ist zu konstatieren, dass Median und erstes Quartil bei 0 für alle Dramen bei 0 liegen: Viele Dramen weisen keine Beziehungsdefinition auf (oder sie konnten nicht automatisch identifiziert werden, siehe Fußnote ). Größere oder signifikante Abweichungen zwischen den Gattungen gibt es nicht, egal welche Relation betrachtet wird. Lediglich die Relation spouses scheint im Figurenverzeichnis von Komödien häufiger genannt zu werden.
            
               
                  
                  Abbildung 6:  Anzahl typisierter Relationen nach Gattung 
               
            
            
               
                  
                  Abbildung 7: Anzahl typisierter Relationen nach Autor. Zur besseren Übersicht wurden nur Autoren berücksichtigt, die mindestens durch fünf Dramen vertreten sind
               
            
            Eine Verteilung der genannten Relationen nach Autor zeichnet jedoch ein anderes Bild (Abbildung 7). Bestimmte Autoren, vor allem Ludwig Anzengruber (1839-1889) und Johann Nestroy (1801-1862), haben klare Tendenzen dazu, mehr Relationen im Figurenverzeichnis zu nennen. Beide verfassen tendenziell Possen und Komödien.
         
         
            Fazit
            Mit den von uns bereitgestellten maschinenlesbaren Informationen ermöglichen wir Analysen dramatischer Figuren, die die als bekannt vorausgesetzten Informationen im Figurenverzeichnis mit berücksichtigen können. Neben den oben skizzierten Analysen können die Informationen auch in inhaltliche Analysen einfließen und etwa die soziale Nähe mit der Bühnennähe korrelieren o.ä.
            Kontextfreie Grammatiken haben sich hier – trotz der bekannten Schwächen im Bezug auf natürliche Sprache – als effizienter Formalismus herausgestellt, um die Figurenverzeichnisse maschinenlesbar zu machen. Wir halten dieses Verfahren für geeignet, um auch in anderen Kontexten mit semi-strukturierten Textdaten zu arbeiten, wo aufgrund der begrenzten Menge ein maschinelles Lernverfahren nur bedingt zum Einsatz kommen kann.
         
      
      
         
            
      Beispielsweise  spielt das Figurenverzeichnis im kürzlich erschienenen
      (Tonger-Erk, Werber, und Baum 2018), aber auch in (Genette 1993)
      quasi keine Rolle.
    
            
               https://dracor.org
            
            
      Für mehr     Informationen vergleiche (Schlaffer 1972, 11)
    
            
               
            
            
      Die konkreten Ergebnisse wurden auf den vollautomatisch erzeugten Relationen erzielt.
    
         
         
            
               Bibliographie
               
                   Asmuth, Bernhard.  (2016). 
  Einführung in die Dramenanalyse. Stuttgart: J.B. Metzler Verlag.

               
                   Böckenhauer, Hans-Joachim /  Juraj Hromkovič  (2013): 
  Formale Sprachen: Endliche Automaten, Grammatiken, lexikalische und syntaktische Analyse. Zürich: Springer.

               
                   Fischer, Frank / Ingo Börner / Mathias Göbel /
  Angelika Hechtl  / Christopher Kittel / Carsten Milling /  Peer
  Trilcke
  (2019):  „Programmable Corpora – Die digitale Literaturwissenschaft zwischen Forschung und Infrastruktur am Beispiel von DraCor“. In 
  Proceedings of DHd. 
  .

               
                   Genette, Gérard  (1993): 
  Palimpseste. Die Literatur auf zweiter Stufe. Frankfurt am Main: Suhrkamp.

               
                   Jeßing, Benedikt (2015): 
  Dramenanalyse. Eine Einführung. Berlin: Erich Schmidt Verlag.

               
                   Pangallo, Matteo (2015):  „‚I will keep and character that name‘: Dramatis Personae Lists in Early Modern Manuscript Plays“. 
  Early Theatre 18 (2): 87–118. .

               
                   Pfister, Manfred (2001): 
  Das Drama. München: Wilhelm Fink.

               
                   Schlaffer, Hannelore (1972):
  Dramenform und Klassenstruktur. Eine Analyse der dramatis persona "Volk". Stuttgart: J.B. Metzler Verlag.

               
                   Tonger-Erk, Lily  /  Nils Werber /  Constanze Baum (Hrsg.) (2018):  „Hauptsache Nebentext. Regiebemerkungen im Drama“. 
  Zeitschrift für Literaturwissenschaft und Linguistik 48 (3).

            
         
      
   



      
         
            Historischer Raum konstituiert sich stets als Summe der dokumentierten Wahrnehmungen einer Zeit. Dabei spielen die beschriebenen Räume nicht nur eine dokumentarische Rolle, vielmehr waren sie in ihrer Zeit normative Dokumente, so dass sich mentale Räume nicht nur im Begehen entwickeln konnten, sondern auch im Erlesen oder Erblicken. Historischer Raum kann also als Bricolage aus Erfahrungen historischer AkteurInnen verstanden werden, welche diese selbst körperlich oder aber textuell bzw. bildlich vermittelt gemacht haben.
         
         
            Das durch die Österreichische Akademie der Wissenschaft geförderte Projekt „Becoming Urban – Reconstructing the city of Graz in the long 19th century (BeUrb)“ widmet sich als Gemeinschaftsprojekt der Karl-Franzens-Universität Graz, des Stadtmuseums Graz sowie des Stadtarchivs Graz der (Re-)konstruktion dieses historischen Raums am Fallbeispiel der Stadt Graz im langen 19. Jahrhundert (1789-1914). Thematisiert werden dabei gleichfalls die Veränderung der Stadt im Kontext der Urbanisierung wie auch deren Wahrnehmung in schriftlichen, bildlichen und kartographischen Quellen.
         
         
            Durch die Zusammenarbeit von Stadtarchiv, GrazMuseum und Universität konnte die Quellenbasis mit historischen Reiseberichten und -führern, Karten, Plänen und weiteren geographischen Darstellungen sowie Stadtansichten auf Fotografien, Druckgrafiken und Gemälden auf unterschiedlichsten Gattungen aufgebaut werden, was einen umfangreichen Blick auf das Verständnis von der Stadt Graz im 19. Jh. erlaubt. Diese transdisziplinäre Herangehensweise, welche Historische Geographie, Kunstgeschichte, Urbanistik und Digitale Geisteswissenschaften vereint, wird durch eine gemeinsame technische Grundlage in Form eines Geoinformationssystems ermöglicht. 
         
         
            Als erste Herausforderung steht dabei die Erarbeitung eines gemeinsamen Datenmodells, welches die quellenspezifischen Charakteristika abbildet und die aufgenommenen Quellen gleichzeitig auf einer topographischen Ebene miteinander in Beziehung setzt. Modellierung wird dabei als „a process of signification and reasoning in action“ (Ciula / Eide, 2017  i34) angesehen. Die Theorie Modelle als
            Icons
             – also als bildhafte
Vertreter anzusehen, hilft dabei Blickpunkte zu verschieben und
Interpretationsspielräumen zu eröffnen, denn: “The context of the
interpretation changes the sign but the sign also changes the context
of interpretation” (i38). Fotografien und Pläne, aber auch Texte sind
zudem an sich bildhafte Modelle, die in einem spezifischen
Ähnlichkeitsverhältnis zum Original stehen. Ziel des Projektes ist es
also, aus diesen singulären Modellen relationale, diagrammatische
Modelle in Form eigener historischer, dynamisch abfragbarer Karten zu
erzeugen, die eine, wie Kralemann und Lattmann definieren,
“interdependence between the structure of the sign and the structure
of the object” (Kralemann / Lattmann, 2013: 3408) aufweisen.
         
         Umsetzung
         
            Die Umsetzung kann in eine Datenaufbereitung und daran anschließend eine vertiefte Geoanalyse unterteilt werden. Abschließend werden beide Ergebnisse des Projektes in einem Webauftritt präsentiert und nachhaltig in einem Repositorium archiviert.
         
         a)
                    Ein Metadatenmodell wird für die bildlichen, textuellen und kartographischen Quellen in LIDO implementiert.
            
         b)
                    Historische Karten werden georeferenziert und transkribiert, d.h. die in der Entzerrung entstandenen Artefakte werden auf einen Grundplan übertragen, der auf der Basis des Franziszeischen Katasters (1824) erstellt wird. Dabei werden bauliche Veränderungen, die sich aus den späteren Kartendarstellungen ergeben, in die Grundkarte integriert. Ein in diesem Kontext aufgebauter eindeutiger Identifikator fungiert als Grundlage für die räumliche Verortung der Bild- und Textquellen.
            
         c)
                    Textuelle Stadtbeschreibungen (z.B. Reiseberichte) werden transkribiert und die räumlichen Objekte (Gebiete, Wege, Plätze, Gebäude) darin in TEI-P5 mit den Referenzen aus der Basiskarte ausgezeichnet. In den Texten implizit vorhandene ‚narrative‘ Wege bzw. Abfolgen der Nennungen der Geoobjekte finden dabei ebenfalls besondere Berücksichtigung.
            
         d)
                    Bildliche Darstellungen (z.B. Gemälde, Druckgraphiken, Fotos, Postkarten) werden in LIDO beschrieben, in ihrer zeitlichen und räumlichen Gestaltung verortet. Dabei werden nicht nur abgebildete Objekte, sondern auch die mögliche Standorte und Blickrichtungen einbezogen
            
         Auf Basis der gewonnenen Daten kann nun ein ‚Spiel mit den Räumen‘
beginnen, welches die hermeneutische Aneignung durch die Projektteilnehmer ebenso beinhaltet wie die diagrammatische Analyse (vgl. Bauer / Ernst 2010, Rau 2013). In der Analyse wird nun die sich stets im architektonischen Wandel begriffene ‚fluide‘ Stadt stehen, wie auch das Diskursfeld Graz, welche sich in den Quellen widerspiegelt. Dabei wird nicht nur die ‚dargestellte‘ Stadt im Zentrum stehen, sondern in besonderer Weise auch diejenigen Bezirke, die gerade nicht in den Quellen genannt werden und so als ‚blinde Flecken‘ im Diskursfeld der Stadt besonderer Aufmerksamkeit bedürfen.
            
         Dabei stellen sich fragen wie: In wieweit sind jene Gebäude Ereignisorte der öffentlichen, aber auch einer ganz privaten Wahrnehmung? Wie sind die Eindrücke einer Stadt für Bewohnerinnen und Bewohner? Wie im Gegensatz dazu für Reisende? Wie möchte sich die Stadt nach außen repräsentieren? Was bewirken diesbezüglich Bau- und Infrastrukturmaßnahmen? Welche Bedeutung kommt Plänen einer zukünftigen Stadt zu, die niemals umgesetzt wurden? Wo finden sich widersprüchliche Wahrnehmungen – oder Raumverzerrungen?
      
      
         
            
      Als Spielwiese dient das open-source geographisches Informationssystem ‚QGIS‘ und die ‚GAMS-Infrastruktur‘, die die Forschungsdaten (GML, TEI, LIDO) langzeitarchiviert und die Möglichkeit bietet Daten neu zu kombinieren (content models, GEOSPARQL) und zu vernetzen (RDF) und abzufragen (SPARQL, SOLR).
    
         
         
            
               Bibliographie
               
                   Bauer, Matthias / Ernst, Christoph 
	(2010): Diagrammatik. Einführung in ein kultur- und
	medienwissenschaftliches Forschungsfeld. Bielefeld:
	transcript.
      
               
                   Ciula, Arianna / Eide, Øyvind  (2017): „Modelling in digital humanities: Sings in context“ in: Digital Scholarship in the Humanities 32 (suppl_1) i33-i46 
        https://doi.org/10.1093/llc/fqw045
               
               
                   GAMS:  Geisteswissenschaftliches Asset
	Management System, 
        http://gams.uni-graz.at/
               
               
                   GEOSPARQL – A Geographic Query Language for
	RDF DATA  (2012). Version 1.0 (eds.) OGC 
        https://www.opengeospatial.org/standards/geosparql
               
               
                   GML: Geographic Markup Language  (2012) – Extended schemas and encoding rules. Version 3.3.0 (eds.) OGC 
        https://www.opengeospatial.org/standards/gml
               
               
                   LIDO – Lightweight Information Describing
	Objects  (2010). Version 1.0 (eds.) Coburn, Erin et al. 
        http://www.lido-schema.org/schema/v1.0/lido-v1.0-specification.pdf
               
               
                   Kralemann, B. / Lattmann, C.  (2013):
	„Models as icons: modeling models in the semiotic framework of Peirce’s theory of signs“ in:  Synthese 190, 3397-3420.
      
               
                   QGIS: 
                  https://www.qgis.org/de/site/
               
               
                   Rau, Susanne (2013): Räume, Wahrnehmungen, Nutzungen. Frankfurt/New York: Campus.
               
                   RDF – Resource Description Framework  (2014) (eds.) W3C 
          https://www.w3.org/RDF/
               
               
                   Scheuermann, Leif  (2014): “Thoughts on a Web Based Co-productive Spatio-Temporal Information System” in: Rau, Susanne / Schönherr, Ekkehard (eds.): Mapping Spatial Relations, Their Perceptions and Danymics. Cham / Heidelberg / New York: Springer 17-23.
               
                   SOLR: Apache Solr. (eds.)  Apache Software Foundation 
          https://lucene.apache.org/solr/
               
               
                   SPARQL 1.1 Query Language  (2013) (eds.) W3C 
  https://www.w3.org/TR/sparql11-query/
               
               
                   TEI P5: Guidelines for Electronic Text
	  Encoding and Interchange  (2019). Version 3.6.0 (eds.) the TEI Consortium 
          https://tei-c.org/release/doc/tei-p5-doc/en/Guidelines.pdf
               
            
         
      
   



      
         Am Lehrstuhl für Medieninformatik der Otto-Friedrich-Universität Bamberg wird derzeit ein Prototyp zum Management von Forschungsdaten entwickelt (FDMS), der heterogene Daten, die momentan nicht digital katalogisiert werden, speichern, beschreiben, und veröffentlichen soll. Von besonderer Bedeutung sind hierbei die FAIR-Prinzipien, wonach wissenschaftliche Daten auffindbar (Findable), zugänglich (Accessible), interoperabel (Interoperable), und wiederverwendbar (Re-usable) sein sollten (Wilkinson 2016). Es existiert bereits ein Projekt zur Einführung eines Forschungsinformationssystems (FIS) an der Universitätsbibliothek Bamberg (Franke 2019), jedoch liegt dort der Fokus auf Publikation, Forschenden, und Projekten. Forschungsdaten werden hier bisher nur am Rande als Anhang bei der Publikation von Dissertationen und anderen Arbeiten adressiert. Weitere Forschungsdaten, die mit keiner Publikation zusammenhängen, werden hingegen in diesem FIS nicht miteinbezogen. Aus diesem Grund soll ein FDMS entwickelt werden, welches genau diese Art von Forschungsdaten ohne zugehörige Publikation, sowie auch alle anderen Forschungsdaten abdeckt.
         Eine wichtige Anforderung und Herausforderung für das FDMS stellt die Heterogenität der Daten dar. Diese Heterogenität manifestiert sich durch die unterschiedlichen Fachrichtungen der Lehrstühle an der Universität Bamberg und dementsprechend in einer Vielzahl an Metadatenschemata und Datenformaten. Die Erschließung und weitere Verwendung dieser Metadaten ist von großer Bedeutung, da die effiziente Suche sowie weitere Dienste, wie Filterung oder Visualisierung, von umfangreichen Metadaten profitieren (Neuroth 2017) und zudem Fördergeber wie die Deutsche Forschungsgemeinschaft Anforderungen an ein Forschungsdatenmanagement stellen, das den FAIR-Prinzipien folgt.
         Für die erste Umsetzung wurde DSpace verwendet (Smith 2003, Donohue 2018), eine Software zur Verwaltung von digitalen Forschungs- und Lehrmaterialien. DSpace wird in Bamberg auch zur Realisierung des FIS eingesetzt, dort jedoch in der Erweiterung als DSpace-CRIS (Donohue 2019). Während der Installation und Anpassung von DSpace an die Testdaten wurde eine Einschränkung und zugleich ein Nachteil von DSpace deutlich, welche die Nutzung dieser Software für den Zweck eines FDMS ungeeignet erscheinen lassen. Der Import von unterschiedlichen Metadatenschemata für Forschungsdaten wird in DSpace zwar unterstützt, jedoch ist dieser Import in der DSpace Standard-Installation nur mit den originalen Dublin-Core Elementen eingerichtet. Außerdem können keine hierarchischen Datenelemente erzeugt werden, weswegen der Import von Daten sowie deren Metadaten, welche in hierarchischen XML-Dateien vorliegen, verhindert wird.
         Aufgrund dieser Einschränkung von DSpace, welche sich bei einer Vielzahl von Forschungsdaten als restriktiv darstellen würde und somit einer der Anforderungen an ein FAIR-basiertes FDMS widerspricht, wurde eine alternative Software benötigt und in Dataverse gefunden (Dataverse 2019a). Dataverse, als Projekt vom Harvard Institute for Quantitative Social Science, der Harvard Library, und anderen Partnern initiiert (Dataverse 2019b), ermöglicht die Modellierung der Metadaten in mehreren Varianten. Zum einen können über die Administrationsoberfläche der Dataverse Installation händisch Elemente der bestehenden Metadatenschemata angepasst werden. Zum anderen, und für das Umsetzungsszenario der Universität Bamberg passender, kann ein sogenannter Metadatenblock angelegt werden, mit dem ein komplett neues Metadatenschema für Datenobjekte in der Dataverse Installation zur Verfügung gestellt wird (Dataverse 2019c). Dieser selbst erzeugte Metadatenblock, in Form einer TSV (Tab-Separated Values) Datei, kann mithilfe eines Kommandozeilen Befehls importiert werden und steht daraufhin neben den in Dataverse bereits vorgefertigten Standard-Metadatenschemata zur Verfügung.
         
            
               
               Abbildung 1: Oberfläche des FDMS-Prototyps mit einer Suchanfrage und der Ergebnisseite. 
            
         
         Als prototypische Anwendung wurden Bilder von Grabsteinen mit weiteren zugehörigen Forschungsprimärdaten verwendet, die mehrere Eigenschaften eines typischen Use Cases für das künftige FDMS abdecken. Diese Forschungsdaten liegen zum einen als Bilder vor, die ohne zugehörige Publikation bisher nicht in einem FIS abgespeichert werden können, zum anderen sind komplexe Metadaten vorhanden, welche in DSpace, wie oben beschrieben, nur eingeschränkt darzustellen, in Dataverse jedoch besser einzufügen sind. Die Testdaten für diesen Prototyp basieren auf dem epidat Projekt des Steinheim-Instituts (Steinheim-Institut 2019), die dem Datenbestand der Professur für Judaistik der Universität Bamberg ähneln, welcher ein konkretes künftiges Anwendungsszenario darstellt.
         Auch für die Analyse der Metadatenelemente, die im FDMS verwendet werden, wurde das epidat Projekt als Testfall genutzt. Hierzu wurden die XML-Dateien, welche die Gräber einer Vielzahl von Friedhöfen in Deutschland mit Metadaten im Epidoc-TEI-Format (Elliott 2006-2017) beschreiben, analysiert und relevante Metadatenfelder identifiziert. Diese relevanten Metadatenelemente wurden als Basis für ein Dataverse im Kontext von Fotografien von Grabsteinen verwendet. Der Begriff Dataverse umfasst dabei nicht nur die Software Dataverse, sondern dient auch als Oberbegriff für eine Sammlung von Datensätzen (King 2007). Die Vorgehensweise zur Erstellung eines eigenen Metadatenschemas wurde verwendet, da das im epidat-Projekt verwendete Epidoc-Format sehr umfangreich und dementsprechend für ein initiales Anwendungsszenario zu mächtig scheint. Die Anzahl der Elemente für den Prototyp wurde daher eingeschränkt. 
         Die identifizierten Metadatenelemente, welche für die forschungsorientierte Beschreibung der Grabsteine notwendig sind, wurden mithilfe eines der oben erwähnten Metadatenblöcke modelliert. Das hiermit erstellte Metdatenschema „epitaph“ – betitelt in Anlehnung an den Fachbegriff für Grabinschriften – enthält 25 Elemente, die unter anderem den zugehörigen Friedhof, den Zustand des Grabmals, die Inschrift inklusive Übersetzung, und weitere Datenfelder umfassen. Weiterhin wurden hierarchische Beziehungen zwischen den Elementen aufrechterhalten, beispielsweise für die verstorbene Person, deren Todestag, und die erwähnten Verwandten dieser Person über die Datenfelder „person“, „personDeathdate“, und „personRelationship“.
         In Kooperation mit dem Rechenzentrum und der Universitätsbibliothek wird aufbauend auf internen Diskussionen die Weiterentwicklung des in diesem Poster vorgestellten Prototyps geplant, welche in einer größeren Testphase 2020 vorgenommen werden soll. Die Anbindung an das bereits im Betrieb befindliche FIS soll ebenso wie die Integration der lokalen Shibboleth-Authentifizierungsverfahren dort umgesetzt werden.
         Das Poster verdeutlicht die Anforderungen und Anstrengungen, die für die Konzeption eines FDMS an einer mittelgroßen Universität notwendig sind, sowie erste positive Ergebnisse. Weiterhin werden durch die Betrachtung zweier Softwarelösungen die Probleme in der Praxis, also in der Umsetzung eines derartigen Forschungsdatenmanagements, näher beleuchtet. Die Erfahrungen verdeutlichen wie wichtig eine systematische Anforderungsanalyse bei der Auswahl eines Systems zum Forschungsdatenmanagement ist. Hierbei sollten insbesondere Aspekte wie die Unterstützung unterschiedlicher Metadatenformate und deren hierarchische Ausprägungen berücksichtigt werden. Diese und weitere Erfahrungen werden am Poster geteilt und ausgetauscht.
      
      
         
            
               Bibliographie
               
                   Dataverse (2019): The Dataverse Project: Open source research data repository software. Online: https://dataverse.org/ [letzter Zugriff: 19.12.2019]
               
                   Dataverse  (2019): The Dataverse Project: About the project. Online: https://dataverse.org/about [letzter Zugriff: 20.12.2019]
               
                   Dataverse  (2019): The Dataverse Project Admin Guide: Metadata Customization. Online: http://guides.dataverse.org/en/latest/admin/metadatacustomization.html [letzter Zugriff: 19.12.2019]
               
                   Donohue, Tim  (2018): DSpace User FAQ. Online: https://wiki.lyrasis.org/display/DSPACE/User+FAQ [letzter Zugriff: 20.12.2019]
               
                   Donohue, Tim  (2019): DSpace-CRIS Home. Online: https://wiki.duraspace.org/display/DSPACECRIS/DSpace-CRIS+Home [letzter Zugriff: 19.12.2019]
               
                   Elliott, Tom / Bodard, Gabriel / Cayless, Hugh / et al.  (2006-2017): EpiDoc: Epigraphic Documents in TEI XML. Online: https://sourceforge.net/p/epidoc/wiki/Home/ [letzter Zugriff: 19.12.2019]
               
                   Franke, Fabian  (2019): Laufende Projekte unter Beteiligung der Universitätsbibliothek Bamberg. In: Projekte – Otto-Friedrich-Universtiät Bamberg. Online: https://www.uni-bamberg.de/ub/ueber-uns/projekte/ [letzter Zugriff: 19.12.2019]
               
                   King, Gary  (2007):
  An Introduction to the Dataverse Network as an Infrastructure for Data Sharing. In:
   Sociological Methods & Research 36(2): 173–199.

               
                   Neuroth, Heike  (2017): Bibliothek, Archiv, Museum. In:
   Digital Humanities: 213–222.

               
                   Smith, MacKenzie / et al. (2003): "An Open Source Dynamic Digital Repository". In:  D-Lib Magazine 9.1.

               
                   Steinheim-Institut  (2019): Datenbank: Jüdische Grabsteinepigraphik. Online: http://www.steinheim-institut.de/cgi-bin/epidat [letzter Zugriff: 20.12.2019]

               
                   Wilkinson, Mark D., et al.  (2016): 
  The FAIR Guiding Principles for scientific data management and stewardship. In:  Scientific data  3.

            
         
      
   



      
         Das sogenannte „Detmolder Hoftheater-Projekt“ hat im Laufe der letzten fünf Jahre den überlieferten Musikalienbestand des Detmolder Hoftheaters aus der Zeit von 1825–1875 einschließlich der erhaltenen Aktenmaterialien erschlossen bzw. übertragen und mit einer eigenen Software im Web präsentiert (www.hoftheater-detmold.de). Die dabei eingesetzte Software „Theatre-Tool“ wurde ausdrücklich so konzipiert, dass sie auf andere ähnliche Bestände übertragbar ist. Dieser Vortrag möchte die bisherigen Arbeitsergebnisse des Projekts zusammenfassen und die Erschließungsgrundsätze und den Aufbau und die Anforderungen der Software erläutern. Dabei werden auch die Möglichkeiten und Probleme der Übertragbarkeit und der Zusammenarbeit mit anderen bestehenden und neuen Erschließungsprojekten angesprochen.
            
         Im Bereich der digitalen Edition ist es inzwischen zum Standard geworden, Kontextmaterialien im Rahmen der Edition ebenfalls als Volltext bereit zu stellen und über Markup zu verknüpfen (vgl. die für den Musiktheaterbereich vorbildliche Präsentation https://freischuetz-digital.de/). Doch im Bereich der Erschließung von Beständen wird noch sehr viel mit proprietären Datenbanken bzw. bibliothekarischen Standards gearbeitet, die nicht ohne weiteres im Web zugänglich gemacht werden können, und werden vor allem unterschiedliche Quellentypen getrennt in je eigenen Systemen erfasst. Dies zeigt sich z. B. in der zur Zeit im Bibliotheksbereich geführten Diskussion um die Erfassung von Ephemera (siehe Literaturverzeichnis), die gerade im Bereich der für die Theaterforschung wichtigen Erschließung von Theaterzetteln zu zahlreichen Insellösungen geführt hat (vgl. z. B. 
http://digital.ub.uni-duesseldorf.de/theaterzettel, 
http://www.theaterzettel-weimar.de), was eine übergreifende Suche z. B. nach Darstellernamen unmöglich macht. 

         Das Hoftheater-Projekt hat ein Modell zur kontextuellen Erschließung der verschiedenen Materialien entwickelt, auf dem das in seiner Oberflächengestaltung zunächst noch rein funktionale Theatre-Tool aufbaut (https://hoftheater-detmold.de/47-2/das-modell/) . 

         
            
            Abbildung 1: Modell des „Theatre Tool“ 
         
         Dieses Modell basiert auf dem im Bibliotheksbereich allgemein angewandten FRBR-Modell, so dass die erfassten Daten sowohl bibliothekarische als auch wissenschaftliche Anforderungen erfüllen. Die Erschließung der Quellen erfolgt nach FRBR auf drei verschiedenen Ebenen: Die Werkdateien erfassen die Grunddaten ggf. mit dem Datum der Uraufführung und einer normierten Angabe zur Klassifikation. Die Quellendateien (entspricht der FRBR-Entität: manifestation) beschreiben die vorliegenden Quellen, die in unserem Fall zu einer „componentGroup“ zusammengefasst werden, da die Aufführungsmaterialien eine Einheit bilden. Bindeglied zwischen Werk und Quelle ist die expression-Datei, denn das jeweilige Aufführungsmaterial des Theaters in Detmold ist als Einheit eine expression des Werks, dasjenige eines anderen Theaters jedoch eine andere. Auch wäre beispielsweise eine Bearbeitung der Oper für Bläser-Ensemble wiederum eine weitere expression. Die Beziehungen zwischen den Dateien werden mit Relationen beschrieben, wie sie durch FRBR vorgegeben sind: „hasRealization“, „isEmbodimentOf“, „hasEmbodiment“, „isPartof“ etc. 
         Zusätzlich zu diesen zur Quellenerschließung notwendigen Dateien werden solche zu Personen und dramatis personae angelegt. Durch eine jeweils eindeutige ID, mit der jede Datei gekennzeichnet wird, ist bei jeglicher Wiederkehr eines Werk-, Rollen- oder Personennamens eine eindeutige Kennzeichnung möglich. 
         Handelt es sich bei diesen Dateien um typische Katalog-Erschließungen, die jedoch zumindest bei den Quellendateien weit über eine übliche bibliothekarische Erfassung hinausgehen, so werden die umfangreich überlieferten Kontextmaterialien z. T. als Regeste, überwiegend aber im Volltext erfasst. Beide Erschließungsformen basieren auf den XML-Standards TEI und MEI, so dass alle Daten durch ein Markup ausgezeichnet werden können.
         Darüber hinaus werden für Personen, Werke und ggf. Orte Normdaten (GND, VIAF, GeoNames) verwendet, so dass externe Informationen eingebunden werden können. Da aber etliche Personen und Werke wenig bekannt oder nicht eindeutig zu bestimmen sind und damit nicht eindeutig einer Normdaten-ID zugeordnet werden können, bleibt die Verwendung von eigenen IDs notwendig.
         Durch die Verknüpfung der Daten über alle Quellengrenzen hinweg ergeben sich verschiedene inhaltliche Verbindungen: So können zu den Personen des Detmolder Hoftheaters einerseits die Daten zu Gage und eventuellen Sonderzuwendungen, Beschäftigungsdauer und zusätzliche Beschäftigungen im Theaterbetrieb abgerufen werden, andererseits aber auch die Werke und sogar die Rollen, in denen sie beschäftigt waren. Zu den Aufführungsmaterialien werden aus den Akten Angaben zur Datierung und zum Schreiber verknüpft und die Einträge in den Kostüm- und Regiebüchern geben erste Hinweise auf die Darstellung einzelner Werke auf der Bühne. 
         Die Materialien (Digitalisate ausgewählter Quellen) und die XML-Dateien der Katalog- oder der Volltext-Erschließungen werden in einer Web-Präsenz zusammengefasst. 
         
            
            Abbildung 2: Startseite des Portals 
         
         Die hierfür eigens entwickelte Software „Theatre-Tool“ basiert auf XQuery und JavaScript.
         Die Darstellung der Faksimiles erfolgt mit Hilfe eines Leaflet-Plugins (https://leafletjs.com), einer Bibliothek für die Kartendarstellung im Web. 
         Die Software bietet bislang eine einfache Suche nach Personen, Rollen und Werken, die mit einem Fuse.js Plugin, einer Fuzzy basierten Bibliothek, erstellt wurde. 
         Wie in Web-Präsenzen üblich, können die Inhalte als XML-Dateien heruntergeladen werden, um weitere Arbeiten mit den Daten zu ermöglichen (Suche, Abfrage in größerem Kontext etc.). Selbstverständlich können die Daten auch als Beispiele für eine Erschließung in anderen Projekten verwendet werden. 
         Die Werke, Quellen, Personen und Rollen können mit Hilfe von Permalinks von anderen Projekten direkt referenziert werden.
         Da bislang im Detmolder Hoftheater-Projekt vor allem Materialien zum Musiktheater erschlossen worden sind, sind in die Software einige musik-spezifische Anwendungen integriert. So werden z. B. die Anfänge der einzelnen Musiknummern mit Noten-Incipits wiedergegeben, um sie rasch vergleichbar zu machen. Um dem Musikwissenschaftler auch Informationen zur originalen Partituranordnung, Schlüsselung, Schreibweise der Instrumente etc. zu geben, wird nicht nur – wie traditionell üblich – eine Stimme oder ein Klavierauszug wiedergegeben, sondern werden die ersten Takte und der Singstimmeneinsatz vollständig in Partitur wiedergegeben. Die Codierung der Incipits erfolgt mit MEI, die Darstellung mit einem Verovio-Plugin (http://www.verovio.org/index.xhtml). 
         
            
            Abbildung 3: Darstellung der Incipits 
         
         Eine weitere Besonderheit des Projekts ist die exemplarische
sog. Tiefenerschließung einiger ausgewählter Aufführungsmaterialien:
Bei diesen werden auch die Faksimiles der Quellen zur Verfügung
gestellt und zwar in einer Aufbereitung für einen taktgenauen
Zugriff. Nur durch diese Form der Erschließung ist es z. B. möglich,
Eingriffe in den Notentext nicht auf Grund der Materialität (also
z. B.: Streichung auf Bl 4v bis 5r vorletzter Takt), sondern
inhaltlich (z. B.: Streichung in Nr. 1 von T. 17–20) und damit für den
Nutzer (mit Hilfe anderer Materialien, also gedruckter oder anderer
handschriftlicher Quellen) nachvollziehbar anzugeben. Zur Erstellung
der sog. Vertaktung wird die Software „Edirom“ benutzt und zur
Darstellung ist das „Theatre Tool“ mit Edirom Online
(https://github.com/Edirom/Edirom-Online) verknüpft. Diese Software wurde zwar für die Aufbereitung von Notenmaterial entwickelt, aber es lassen sich damit auch Textquellen z. B. nach Szenen oder sogar Zeilen kartieren.
            
         Das Theatre-Tool ist für die Darstellung dieser komplexen Text- und Datenstrukturen entwickelt, kann aber leicht an andere Anforderungen angepasst werden: Bei dem im Projekt erfassten Material handelt es sich z. B. überwiegend um handschriftliches Material, weshalb die nach FRBR vorgesehene vierte Ebene, das Exemplar (item), nach der Regel der „manifestation singleton“ nicht berücksichtigt wird. Selbstverständlich wäre aber auch diese darstellbar. Da das Hauptinteresse der Erschließung auf der Arbeitsweise und dem Personal der Detmolder Hoftheater-Gesellschaft liegt, werden die erwähnten Orte zwar ausgezeichnet, gibt es für diese aber keine eigenständigen Dateien (mit der Möglichkeit zu Referenzen) und bislang keine Suchmöglichkeit. 
         Mit der zunehmenden Digitalisierung der Bestände durch die Bibliotheken könnten diese über iiiF in das Theatre Tool eingebunden werden, wodurch etliche rechtliche Probleme gelöst werden könnten. Wie damit auch eine Vertaktung verbunden werden kann, wäre zu überprüfen. 
         Weiterer Abstimmungsbedarf, an dem aber beidseitig großes Interesse besteht, ist notwendig zwischen Wissenschaft und Bibliothek. Es ist selbstverständlich, dass die Beispiele der Tiefenerschließung des Projekts ebenso wie die Erstellung z. B. von Komponisten-Werkverzeichnissen nur durch die Wissenschaft zu leisten sind. Dennoch besteht großes Interesse, diese Detailinformationen zu einzelnen Quellen auch über die besitzende Bibliotheken zugänglich zu machen Die Verwendung von Standards und Normdaten wie sie im Hoftheater-Projekt erprobt worden sind, bildet hierzu einen erster Schritt, doch muss sicherlich auch verstärkt über Schnittstellen für den Datenaustausch nachgedacht werden. 
      
      
         
            
               Bibliographie
               
                   Kamzelak, Roland S. (2016): „Digitale Editionen im semantic web. Chancen und Grenzen von Normdaten, FRBR und RDF“ in: Richts, Kristina / Stadler, Peter (eds.): 
	„ei, dem alten Herrn zoll ich Achtung gern“. Festschrift für Joachim Veit zum 60. Geburtstag. München: Allitera Verlag 423–435; online unter: 
	https://nbn-resolving.org/urn:nbn:de:bsz:14-qucosa2-233392
               
               
                   Münzmay, Andreas (2018): „Lesen und Schreiben im digitalen Dickicht, Musikwissenschaft, Digital Humanitites und die hybride Musikbibliothek“ in: BIBLIOTHEK Forschung und Praxis 42; 236–246.
      
               
                   Münzmay, Andreas  (2019): „Kulturtransferforschung und Musikwissenschaft“, in:  Calella, Michele / Leßmann, Benedikt (eds.): 
        Zwischen Transfer und Transformation: Horizonte der Rezeption von Musik (= Wiener Veröffentlichungen zur Musikwissenschaft 51). Wien 175–190. 
      
               
                   Richts, Kristina / Veit, Joachim (2018): „Stand und Perspektiven der Nutzung von MEI in der Musikwissenschaft und in Bibliotheken“ in: BIBLIOTHEK Forschung und Praxis 42: 292–301.
      
               
                   Pernerstorfer, Matthias J.  (2012): 
        Theater – Zettel – Sammlungen. Erschliessung, Digitalisierung, Forschung. Wien (= Don Juan Archiv Wien: Bibliographica, 1)
      
               
                   Pernerstorfer, Matthias J.  (2015): 
        Theater – Zettel – Sammlungen Bd. 2: 
        Bestände, Erschließung, Forschung. Wien 2015 (= Don Juan Archiv Wien: Bibliographica 2)
      
               
                   Veit, Joachim  (2020): „Notistenspezifische Erwartungen der Wissenschaft an die Web-Präsentation digitalisierter Musikhandschriftenbestände“ in
        : Das Instrumentalrepertoire der Dresdner Hofkapelle in den ersten beiden Dritteln des 18. Jahrhunderts – Überlieferung und Notisten.
               
               
                   Wiermann, Barbara  (2018a): „Bibliothekarische Normdaten und digitale Musikwissenschaft“ in: 
        Die Musikforschung, 71: 338–357.
      
               
                   Wiermann, Barbara  (2018b): „musiconn. performance: musikalische Ereignisdaten im Fachinformationsdienst Musikwissenschaft“ in: Bonte, Achim / Rehnolt, Juliane (eds.):
         Kooperative Informationsinfrastrukturen als Chance und Herausforderung. Thomas Bürger zum 65. Geburtstag herausgegeben von. Berlin, Boston 398–415.
      
            
         
      
   



      
         Der Schubert-Forscher Walther Dürr veröffentlichte 2002 einen Beitrag zu Problemen der Artikulation und Dynamik bei Franz Schubert, in dem er betonte, wie stark das „Lesen“ einer Partitur von der Kenntnis der Schreibgewohnheiten eines Komponisten abhängt. Selbst erfahrenen Handschriftenkennern bereiten Phänomene wie jenes von „Schuberts so viel diskutiertem Akzentzeichen“ Schwierigkeiten: Akzente und 
                decrescendo-Winkel „sind bei Schubert oft nicht leicht zu unterscheiden“ und gelegentlich handele es sich um „etwas dazwischen, das sich im Druck unserer Ausgabe nicht wiedergeben läßt“. Dies gelte auch für manche Bogensetzungen, die „offenbar nicht anzeigen, 
                was, sondern nur, 
                daß überhaupt gebunden werden sollte“. Er rät dem Editor daher, zwar Beliebigkeiten der Schubertschen Schreibweise zu kennzeichnen, aber wo „Präzision gemeint“ sei, „diese auch dort anzuzeigen, wo 
                das Manuskript sie 
                nicht hergibt“. Von einer (analogen) Edition erwarte die Aufführungspraxis „genaue Anweisungen“, und „musikalische Plausibilität“ sei dabei zweifellos ein wichtiger Orientierungspunkt (Dürr 2002: 322-326).
            
         In der gedruckten Edition sorgen die Entscheidungen des Editors und die Normierungen des modernen Notensatzes (wie jede Übertragung eines Schriftträgers in einen anderen) zwangsläufig für eine Verengung des in der Vorlage gegebenen (oder laut Dürr bloß vom Lesenden empfundenen) Interpretationsspielraums, der nur durch verbale Erläuterungen im Kritischen Apparat wieder geöffnet werden kann. 
         Bei den ersten digitalen Editionen von Musik der klassisch-romantischen Epoche mit der „Edirom“-Software (Edirom 2005/2010, Reger-Werkausgabe, OPERA) ging es genau um diese Frage der Transparenz editorischer Entscheidungen, die nun durch die fallspezifische Einbindung von Digitalisaten der zur Erstellung des Edierten Textes herangezogenen historischen Quellen erreicht werden sollte. Die Kombination „eindeutiger“ Edierter Texte mit deren „Vorlagen“ sollte Interpretationsspielräume wieder öffnen, was durch eine zusätzliche Kombination mit Annotationen an Ort und Stelle (also nicht im separierten Apparat) erleichtert wurde. Das Konzept ging teilweise auf, auch wenn die Verführung durch Bilder alles andere als unproblematisch ist – die gebotenen Ausschnitte verkürzen Wirklichkeit und können bei entsprechender Auswahl (und ohne Kenntnis einschlägiger Notationsgepflogenheiten) ebenso manipulativ sein wie traditionelle verbale Erläuterungen des Editors (vgl. dazu Sahle 2013, Kap. 3.2).
         Abhilfe versprach die auch für die praktische Nutzung beobachteter Alternativen notwendige Überführung des bildlich Vorgefunden in maschinenles- und verarbeitbare Repräsentationen. Für diese wurde im Bereich wissenschaftlich-kritischer Editionen in den letzten zwanzig Jahren das Format der 
                Music Encoding Initiative (MEI) entwickelt, das im Gegensatz zu anderen, auf spezifische Erfordernisse zugeschnittenen Codierungsformen oder proprietären Notensatzprogrammen von Anfang an (in Anlehnung an TEI) auf die dokumentarischen Bedürfnisse der wissenschaftlichen Community zielte (vgl. Richts/Veit 2018). Mit dieser Codierung können nun Interpretationsspielräume wie die erwähnten bzw. unterschiedliche Deutungen dieser Symbolschrift erfasst und explizit festgehalten werden.
            
         Was bedeutet dies konkret? – MEI ist keine „Auszeichnungssprache“ im engeren Sinne (wie TEI), sondern ein „beschreibendes Markup“, das die auf Konventionen beruhende konkrete graphische Gestalt durch Begriffe bezeichnet und bei deren inhaltlicher Deutung auch den Zeichenkontext berücksichtigt – so kann eine Note aufgrund der äußeren Form als „Viertel“ und durch ihre Position im zweiten Zwischenraum in Verbindung mit einem vorausgehenden Schlüssel als Tonhöhe „c2“ bzw. mit einem vorausgehenden Akzidens als „Viertelnote cis2“ bezeichnet werden. Dabei sagen historische Notensatzregeln, dass ein anschließend wiederholter Ton im gleichen Zwischenraum kein Akzidens benötigt, also graphisch wie ein „c“ aussieht, klingend aber als „cis“ realisiert wird. In die Codierung fließt also – wie im Computernotensatz – Wissen um Notationsregeln mit ein. Wenn der Rechner aber mit dem Ton arbeiten soll, muss ihm explizit mitgeteilt werden, dass die graphische Form hier durch zusätzliche Kontextinformationen in ein anderes klingendes Ergebnis verwandelt wird. 
         Ebenso könnte in dem genannten Schubertschen Beispiel die Ausdehnung des Akzent- bzw. 
                decrescendo-Zeichens im Verhältnis zu den Notenpositionen konkret festgehalten und damit expliziter als in einer bloßen verbalen Beschreibung dokumentiert werden. Zusätzlich wären die Deutungsmöglichkeiten als Alternativen in der Codierung – eventuell in einem Apparateintrag als Lesarten – festzuhalten. Dabei erlaubt MEI Angaben zum Urheber der jeweiligen Interpretation sowie prozentuale Festlegungen der Wahrscheinlichkeit der jeweiligen Lösung. Die Codierung erzwingt also eine möglichst präzise Beschreibung der Alternativen – aber wie hoch ist der Erkenntnisgewinn? Und wo liegen – von dem heilsamen Zwang zur präziseren Erfassung der Phänomene abgesehen – die Vorteile eines solchen Verfahrens gegenüber der traditionellen analogen Arbeit? 
            
         Um die Frage zuzuspitzen: Menschenlesbar wird das, was hier codiert wird, erst bei einer Rücküberführung in die gewohnte Notendarstellung – dort aber sorgt die Normierung des Drucksatzes dafür, dass der Eindruck, den die Handschrift vermittelte, ein völlig anderer ist. Und die präzise Codierung etwa von Bogenlängen, die nicht mit, sondern irgendwo nach Noten beginnen oder enden (und damit ggf. Bedeutungsunterschiede suggerieren), erweist sich letztlich als verlorene Liebesmüh’, denn sie wirkt in der normierten Umgebung völlig anders und bleibt als bloße Positionsbestimmung blind für eine maschinelle Auswertung, die die Werte in Beziehung zu unterschiedlichen Kontextfaktoren setzen müsste. Das Projekt „Beethovens Werkstatt“, das sich mit der komplexen Genese Beethovenscher Kompositionshandschriften beschäftigt, hat daraus die Konsequenz gezogen, solche kodikologischen Aspekte nicht im Neusatz nachzuahmen (und damit zu verfälschen), sondern die Codierung sozusagen fest mit den Einträgen im Manuskript zu verdrahten – das Markup zur Beschreibung des problematischen Bogens wäre dementsprechend direkt mit einer SVG-Erfassung dieses Objekts im Handschriftendigitalisat verknüpft. Das erleichtert das Erkennen von im Apparat erfassten Mehrdeutigkeiten, dennoch bleiben diese ohne Annotation schwer nachvollziehbarund der Urteilsfähigkeit eines Betrachters anvertraut, der zudem schreibereigene Notationsgepflogenheiten zu berücksichtigen hätte. Zwar könnte man vorgefundene Phänomene unter Kategorien subsumieren und damit rascher auffind- und auswertbar machen – aber dennoch bewegen wir uns hier noch in einem Denkraum, für den das Digitale zwar Erleichterungen bringt, der aber traditionellen Herangehensweisen verpflichtet bleibt, weil die Repräsentation des Objekts, die MEI in der bisher beschriebenen Form bietet – wie bei allen derartigen Repräsentationen – nur auf einer sehr spezifischen, von bestimmten Interessen geleiteten Wahrnehmung des Gegenstands beruht (Sahle 2013, Kap. 2).
         Greifen also bisherige digitale Editionsmethoden oder Codierungen zu kurz? Wie aber könnten digitale Methoden helfen, mit den genannten Interpretationsspielräumen sinnvoller umzugehen? Ist hier nicht ein radikal anderes Denken erforderlich? 
         Zunächst muss man sich bewusst machen, welche Fragen überhaupt mit Rechnerunterstützung sinnvoll beantwortbar sind. Bei den erwähnten Bögen mit unklarem Anfang und Ende oder der Akzent/
                decrescendo-Unterscheidung bleibt die Geltungsdauer der Zeichen stark von individuellen Schreibgewohnheiten abhängig und dürfte kaum schreiberunabhängig beurteilbar sein. Aber Dürrs (durch Doppelautographe Carl Maria von Webers bestätigte) Hypothese, andere Bogenformen suggerierten nur, „daß“ und nicht „was“ genau gebunden werden solle – bezeichneten also im Sinne eines 
                sempre legato nur grundsätzlich das Binden (nicht aber z.B. den Strichwechsel) –, wäre auf einem großen Korpus an Digitalisaten (ggf. epochenspezifisch) untersuchbar. Was wäre hier nötig?: Es muss zunächst händisch ein ausreichend großer Bestand (zu Festlegung seiner Größe fehlen noch jegliche Erfahrungswerte!) erfasst werden, um die Frage – auch im Hinblick auf zumindest zeichenspezifisches OMR – präzisieren zu können. Darauf aufbauend könnte eine maschinelle Auswertung umfassender Bibliotheksbestände (z.B. über den Zugriff auf im IIIF-Format zur Verfügung gestellte Digitalisate) erfolgen. Das angewandte Verfahren müsste auch in der Lage sein, aufgefundene Stellen so zu „markieren“ (bzw. ihre Koordinaten zu erfassen), dass sie für spätere Einzelfallstudien rascher auffindbar wären. Je nach Aufbereitung der Trainingsdaten könnte dabei bereits eine automatisierte Sortierung der Fundstellen nach vorgegebenen Kategorien erfolgen, um darauf aufsetzende Arbeiten zu erleichtern. (Bei der Interpretation des Akzentzeichens wäre z. B. ein Einbeziehen verbaler Bezeichnungen wie 
                sf und 
                fz
sowie weiterer orthographischer Varianten denkbar, um die Frage zu klären, inwieweit die Verwendung zeitlich, lokal oder im Hinblick auf die Faktur der Musik variiert, je nach Kontext unterschiedliche Deutungen suggeriert oder lediglich auf Synonymität hindeutet.) Bei dieser Form des Markup hilft die für Common Western Notation im Moment entwickelte automatische Taktmarkierung (Waloschek), da sie ein Festhalten von Phänomenen nicht bloß in abstrakten Koordinaten, sondern auch in Bezug auf ein inhaltliches Modell (hier die in MEI abgebildete Werkstruktur) erlaubt. 
            
         Ein zweites Beispiel, das im Falle Mozarts gar zu einem Preisausschreiben geführt hat (Albrecht): Die in der Aufführungspraxis heiß diskutierte Frage nach dem Unterschied zwischen „Punkt“ und „Strich“ in Artikulationsbezeichnungen bzw. die Grundsatzfrage, ob überhaupt ein Bedeutungsunterschied zu konstatieren sei (Brown: 200ff.). Die subjektive Beobachtung, dass z. B. Striche in 
                forte-Abschnitten eindeutig überwiegen, wäre statistisch und im Hinblick auf bestimmte Zeitabschnitte belegbar. Ebenso die Hypothese, dass besonders deutliche Striche Akzentfunktion haben. Aber für die zahllosen, gerade bei Handschriften kaum unterscheidbaren Zwischenformen wäre zunächst ein auch begrifflich schwer zu differenzierendes Vergleichskorpus anzulegen und zusätzlich auf „Normalwerte“ eines Schreibers zu beziehen (welche zudem von Schreibmittel und beschriebener Oberfläche abhängen), um die Auswertung nicht zu verfälschen. Neben die Auswertung des graphischen Befunds muss außerdem stets eine Auswertung aufführungspraktischer Hinweise in Unterrichts- und Lehrwerken oder erläuternder Texte und Selbstzeugnisse im zeitlichen Kontext treten – trotz des komplizierten Zusammenspiels, das m.E. methodisch die Grenzen unseres Faches überschreitet und auch für die Informatik interessante Modellierungsprobleme bietet, sind hier hilfreiche Erkenntnisse zu erwarten. 
            
         Letztlich wird man sich derartigen Interpretationsproblemen von zwei Seiten nähern können: Wenn etwas für bedeutungstragend gehalten wird, sollte es in der Codierung festgehalten (also „be-zeichnet“) werden, um eine Sammlung von Befunden anzulegen, die maschinell leicht akkumulierbar und strukturierbar ist – auf der anderen Seite werden Materialitäts- und Schriftlichkeitsuntersuchungen im großen Stil durch neuronale Netze oder Künstliche Intelligenz erst jetzt (bzw. künftig) sinnvoll durchführbar. Dann können solche Verfahren wirklich zum Rettungsanker zumindest bei ausgewählten Interpretationsproblemen werden. Bisherige isolierte Untersuchungen bergen stets die Gefahr sehr eingeschränkter Gültigkeit, die heute möglich werdende Korpus-Analyse birgt andererseits die Gefahr, dass die Bedingungen des Einzelfalls nicht genügend berücksichtigt sind – zwischen beiden Extremen kann sich künftig eine sinnvolle Nutzung digitaler Techniken bewegen, die mit neuen Mitteln die Spielräume von Interpretation auszuloten versucht. 
      
      
         
             Links
            
               
            
            
               https://music-endocing.org/
            
         
         
            
               Bibliographie
               
                   Albrecht, Hans (1957): 
    Die Bedeutung der Zeichen Keil, Strich und Punkt bei Mozart. Fünf Lösungen einer Preisfrage, Kassel: Bärenreiter
  
               
                  Brown, Clive (1999): 
    Classical & Romantic Performance Practice 1750–1900, Oxford: OUP
  
               
                  Dürr, Walther (2002): „Notation und Aufführungspraxis: Artikulation und Dynamik bei Schubert“, in: 
    Musikedition. Mittler zwischen Wissenschaft und musikalischer Praxis, hg. von Helga Lühning (Beihefte zu editio 17), Tübingen: Niemeyer, S. 313-327
  
               
                  Edirom  (2005): 
    Carl Maria von Weber. Sämtliche Werke, Serie VI, Bd. 3: 
    Kammermusik mit Klarinette, hg. von Gerhard Allroggen, Knut Holtsträter und Joachim Veit. Mit einer digitalen Edition des Quintetts op. 34 von Johannes Kepper u. Ralf Schnieders, Mainz: Schott Musik International
  
               
                  Edirom (2010): 
    Carl Maria von Weber. Sämtliche Werke, Serie V, Bd. 6: 
    Konzertante Werke für Klarinette, hg. von Frank Heidlberger. Mit einer digitalen Edition der Werke, erarbeitet von Benjamin Wolff Bohl, Daniel Röwenstrunk u. Joachim Veit unter Mitwirkung von Philemon Jacobsen, Mainz: Schott Musik International
  
               
                  Ertel, Wolfgang (2016): 
    Grundkurs Künstliche Intelligenz: Eine praxisorientierte Einführung (Computational Intelligence), 4. Auflage, Wiesbaden: Springer Vieweg
  
               
                  OPERA. Spektrum des europäischen Musiktheaters in Einzeleditionen (2013ff.), hg. von Thomas Betzwieser, Kassel: Bärenreiter (bislang 3 Bde.)
  
               
                  Reger, Max (2008–2015): 
    Orgelwerke. Reger-Werkausgabe, Serie I, Bd. 1–7, hg. von Alexander Becker et al., Kritischer Bericht auf DVD, Stuttgart: Carus
  
               
                  Richts, Kristina /  Veit, Joachim (2018): „Stand und Perspektiven der Nutzung von MEI in der Musikwissenschaft und in Bibliotheken“, in: 
    Bibliothek – Forschung und Praxis 42 (2) ,S. 291-301
  
               
                  Sahle, Patrick (2013): 
    Digitale Editionsformen. Zum Umgang der Überlieferung unter den Bedingungen des Medienwandels (Schriften des Instituts für Dokumentologie und Editorik 9), Teil 1: Das typographische Erbe, Teil 2: Befunde Theorie und Methodik, Teil 3: Texbegriffe und Recodierung, Norderstedt: BoD
  
               
                  Schmid, Manfred Hermann (2012): 
    Notationskunde. Schrift und Komposition 900–1900 (Bärenreiter Studienbücher 18), Kassel
  
               
                  Waloschek, Simon / Hadjakos, Aristotelis / Pacha, Alexander (2019): „Identification and Cross-Document Alignement of Measures in Music Score Images“, in: 
    Proceedings of the 20th International Society for Music Information Retrieval Conference (ISMIR), Delft 2019
  
            
         
      
   



      
         
            Kategorien wie „Textsorte“, „(kommunikative) Gattung“ oder „Genre“ gehören zu einem disziplinenübergreifenden Bestand und werden entsprechend in Sprach-, Literatur-, Kultur- sowie den Sozialwissenschaften verwendet. Allgemein lässt sich die Frage stellen, ob und inwieweit die genannten Kategorien in die Digital Humanities eingehen und inwieweit sie methodologisch reflektiert werden.
         
         Textanalysen in der Text- und Korpuslinguistik
         
            Die Auseinandersetzung mit einer Kategorie wie „Textsorte“ kann auf eine jahrzehntelange Fachgeschichte, besonders in der Sprachwissenschaft, zurückblicken, in der zwar keine Einigkeit hinsichtlich des Verständnisses des Konzepts „Textsorte“ erzielt worden ist, jedoch deutlich geworden ist, dass in diesem Zusammenhang die Unterschiede zwischen Texten unter Einbezug unterschiedlichster Textebenen zu modellieren sind. Textsorten zeigen sich – grosso modo – nicht nur anhand von musterhaften Ausprägungen auf textgrammatischer, -semantischer und -pragmatischer Ebene, sondern berühren auch die Materialität, Kodalität (Nutzung unterschiedlicher Zeichenressourcen) und ggf. eine spezifische Ortsgebundenheit, die Lokalität. Heutige Textsortenmodelle sind Mehrebenen-Modelle, was mit der Annahme verknüpft ist, dass die einzelnen Ebenen in einem wechselseitigen Abhängigkeitsverhältnis stehen (vgl. Adamzik
            2
            2016; Heinemann/Heinemann 2002). Um eine „Textsorte“ oder ein „Textmuster“ zu erfassen, ist eine umfassende Nutzung des linguistischen Beschreibungsinstrumentariums erforderlich. Die Attraktivität von Kategorien wie „Textsorte“ ist v.a. darin gesehen worden, dass sie Einblick in den ‚kommunikativen Haushalt‘, also in spezifische Ordnungsleistungen einer Gesellschaft und Kultur ermöglichen (vgl. Fix 2006). I.d.R. wird die Ausprägung von Textmustern auf rekurrente Aufgaben und deren Lösung zurückgeführt, die wiederum einen Einblick in gesellschaftliche Relevanzen bieten. Gerade der in den letzten zwei Jahrzehnten geführte (text)linguistische Diskurs hat zudem erbracht, dass zunächst als dem Text äußerlich gedachte Faktoren wie Kontext, einschließlich der Beziehung zwischen Textproduzent und -rezipient, nichts dem Text Äußerliches sind, sondern durch den Text hergestellt werden. Zudem ist eine Kategorie wie „Stil“ die etwa auch Dialogizität oder Perspektivität umfasst, verstärkt als Textsortenstil verstanden worden, der sich aus der Sichtung aller Textebenen im Zusammenspiel ergibt (vgl. Sandig 
            2
            2006). Eine wichtige Neuorientierung in der textlinguistischen Betrachtung stellen Modelle dar, die konsequent von der textlichen Oberfläche ausgehend, ohne sich allerdings auf Syntax und Lexik zu beschränken, thematische, situative und funktionale Hinweise und damit zentrale Textdimensionen erschließen (vgl. Hausendorf et al. 2017, historisch: Schuster 2019). 
         
         
            Mehrebenen-Modelle zur Beschreibung von Textsorten sind fast ausnahmslos Produkt von Annahmen, die ebenso aus Sprach- und Kommunikationstheorien wie aus einzelnen Textexemplaren hergeleitet werden. Diese werden zumeist nur an geringen Textmengen überprüft. Da wichtige Untersuchungsebenen ‚vorgegeben‘ sind, verfährt die Methode top down. Wie korpuslinguistische Untersuchungen mit kulturanalytischen Interesse – also nicht im engeren Sinne textlinguistische Studien – deutlich gemacht haben, ließen sich einige auch in der Textlinguistik für wichtig erachtete Ausdrucksmuster durch die Berechnung von Kollokationen, n-Grammen auf Wort und Phrasenebene oder Keywords ermitteln (vgl. Bubenhofer/Scharloth 2016). Dabei handelt es sich um Bottom-Up-Verfahren, die zu neuen Hypothesen und Annahmen führen können. 
         
         
            Innerhalb der Diskussionen um Textsortenklassifikation und Texttypologie ist deutlich geworden, dass „Textsorten“ keine starren Entitäten sind; sie sind nicht vollständig festgelegt und erlauben Veränderungen. Aus dieser Variabilität ergibt sich das generelle Potential zum Wandel von Textsorten, der durch die Nutzung und Grenzen von Spielräumen bestimmt wird. Die entsprechenden Konventionalisierungsprozesse sind jedoch bisher kaum betrachtet worden.
         
         Textanalysen in den Digital Humanities
         
            Den bisher skizzierten Textauffassungen stehen Zugriffe auf die Kategorie „Text“ gegenüber, die in den Digital Humanities bevorzugt werden. Grundsätzlich scheint die Kategorie „Textsorte“ eine Hilfskategorie zu sein, mit der größere Datenmengen (z.B. Referenzkorpora) geordnet werden. Fragen der Textstrukturiertheit werden im Zusammenhang mit dem Text-Encoding z.B. in digitalen Editionen aufgeworfen (vgl. z.B. TEI-P5 Guidelines 2019), wobei die Ergebnisse nur selten Niederschlag in quantitativen Analysen finden. Texte werden zudem für das Training von Methoden ganz unterschiedlicher Anwendungen (z.B. Sentiment-Analysis, Stilometrie oder Topic Modelling) verwendet. Der Text(sorten)begriff bleibt dabei unterspezifiziert, indem „Text“ mit Dokumenten, Sätzen oder Mengen sinntragender Struktureinheiten gleichgesetzt (vgl. z.B. Ravi/Ravi 2015: 16; de Rose et al. 1997: 6) oder nach Alltagsverständnis differenziert wird (vgl. z.B. Medhat et al. 2014: 1096). Einschlägige Kategorien der DH sind daneben die des (Gattungs)Stils, Autorenstils oder Registers. Dabei deckt sich das Stilverständnis nicht mit dem holistischen Verständnis von „Stil“ als einer alle Textebenen durchwirkenden Kategorie, mit der sozialer Sinn erzeugt wird. Das Text- und insbesondere auch das Stil- und Registerverständnis der DH ist wesentlich an Merkmalen orientiert, wie dies etwa in der folgenden Äußerung zum Tragen kommt, die hinsichtlich des Verständnisses hochaggregierter geisteswissenschaftlicher Kategorien in den DH charakteristisch ist: „Style is a property of texts constituted by an ensemble of formal features which can be observed quantitatively or qualitatively.“ (Hermann et al. 2015: 44). 
         
         
            Merkmale bei Untersuchungen zu Textgattungen und Diskursen sind etwa Frequenzen von Inhalts- und Funktionswörtern, der Variantenreichtum des Wortschatzes, Satzlängen, n-Gramme oder mit Parsern ermittelte syntaktische Strukturen; die Auswahl wird in der Regel nicht begründet und scheint durch ihre Operationalisierbarkeit selbst gerechtfertigt. Exemplarisch hierfür steht das Topic-Modeling (Fankhauser et al. 2016, Viehhauser 2017). Dabei wird Text im Sinne des „bag-of-words“-Ansatzes als „Behältnis“ von Wörtern verstanden, wobei die grammatikalische Struktur und selbst die Wortfolge unberücksichtigt bleiben (vgl. Blei et al. 2003). Bibers (1988) und Bibers/Finegans (2014) multidimensionale Analysen (Schöch/Pielström 2014: S. 2f.), die sich am Genre- und Registerbegriff orientieren, fassen eine Vielzahl von Merkmalen zu Merkmalbündeln zusammen, berücksichtigen jedoch kaum die Funktionalität bzw. pragmatische Dimension von Texten. Auffällig ist, dass in diesen und anderen Studien Merkmalen wie der Satzlänge oder Komplexität von Sätzen eine Bedeutsamkeit für Stil, Register oder Genre zugeschrieben wird, die in qualitativen Studien randständig ist. Dass „formal features“ auch durchaus auf interpretierbaren Kategorien basieren, rückt ebenfalls wenig ins Bild. Zusammenfassend darf behauptet werden, dass bei Text- und Stilklassifizierungen in den DH Merkmale der Textoberfläche bevorzugt behandelt werden.
         
         Unvereinbare Traditionen? Ein Fallbeispiel
         
            Man kann mit Blick auf diese unterschiedlichen Forschungstraditionen, die hier bewusst pointiert gegenübergestellt wurden, konstatieren, dass herkömmliche qualitativ-linguistische Studien, obgleich sie stark mit dem Begriff „Muster“ operieren, sich bisher kaum für statistische Signifikanzen u.ä. interessiert haben, während wiederum stilo- und textometrische Studien mit einem „unterkomplexen Textbegriff“ arbeiten und nach Bubenhofer/Scharloth es bisher versäumt haben, „Texte als komplexes Gewebe zu operationalisieren“ (2015: 13). Grundsätzlich gilt: Während merkmalsorientierte Zugänge auf der textlichen bzw. sprachlichen Oberfläche operieren, gehen phänomenorientierte Modelle von textlichen Dimensionen (z.B. der Beziehungsdimension) aus, die in ihrer Relevanz für die textliche Kommunikation erkannt worden sind und auf ihre sprachliche Gestaltung hin befragt werden. Zwar mehren sich in den letzten Jahren die Versuche, im Sinne der „mixed methods“ quantitative und qualitative Methoden miteinander zu verbinden, jedoch ist im Hinblick auf den Text- und Textsortenbegriff bisher nicht deutlich, ob sich diese komplementär zueinander verhalten oder zu möglicherweise sich widersprechenden Befunden führen.
         
         
            In unserem Beitrag möchten wir ein Mehrebenen-Modell vorstellen, das in dem DFG-Projekt: „Die Evolution von komplexen Textmustern: Entwicklung und Anwendung eines korpuslinguistischen Analyseverfahrens zur Erfassung der Mehrdimensionalität des Textmusterwandels“ entstanden ist. Es verbindet unterschiedliche Zugriffe auf die Kategorie „Text“ und bezieht quantitative und qualitative Text(sorten)analyse spiralförmig aufeinander. Am Beispiel der Verwendung personaldeiktischer Ausdrücke (
            ich
             – 
            du 
            – 
            wir
             – 
            ihr
            ) und entsprechender Possessiva sowie Indefinitpronomen wie 
            man
            , die in unterschiedlichen historischen Textgruppen leicht identifizierbar sind, möchten wir auf Basis eines Pilotkorpus von Zeitungstextsorten des Zeitraums 1830 bis 1930 sowie mehrerer Vergleichskorpora aus dem Deutschen Textarchiv (DTA) zeigen: 
         
         
            
               welche Texteigenschaften (allein) durch die automatische, korpusbasierte Textanalyse, insbesondere durch die Nutzung von Part-of-Speech- und Lemma-Informationen, auch in Bezug auf verschiedene Binnentextsorten, zutage treten und hinsichtlich welcher Forschungsfragen dies aufschlussreich ist. So werden durch diachrone Längsschnittuntersuchungen Frequenz, Signifikanz und Typizität entsprechender Ausdrücke, letzteres insbesondere durch Bezugnahme auf Vergleichskorpora, jedoch auch eine hohe Varianz der Ausdrücke sichtbar. Eine derartige Zugriffsweise erlaubt, ergänzt durch POS-sensitive Suchen, einen Einblick in Konstanz und Wandel von Verfasserreferenz und Rezipientenansprache. Sie bieten durch ihre Irritationsmomente einen Ansatzpunkt, um Hypothesen zu Zeiträumen, die für Wandelphänomene interessant sind, zu bilden. Sie dienen ferner zum Abgleich mit auf schmalen Korpora generierten Ergebnissen (vgl. Lefévre 2017: 150), die durch eine solche Zugangsweise relativiert werden. So zeigt sich – gemessen an der vorliegenden Forschungsliteratur und an Vergleichskorpora – ein erstaunlicher Anstieg von 
               ich
               -, 
               du
               -, 
               -wir
                und 
               ihr
               -Verwendungen.
            
            
    was durch eine flankierende manuelle Annotation mit einem vordefinierten Tagset ins Blickfeld rückt. Es wird deutlich, dass die personaldeiktischen Verwendungen sich nicht gleichmäßig über alle Textsorten verteilen, sondern sich besonderen Textsorten wie dem Erfahrungs- und Erlebnisbericht verdanken. Ferner wird deutlich, dass sich relativ von Textkotext und -kontext bestimmte Lesarten (z.B. das Verfasserkollektive oder Rezipienten umschließende, inklusive wir) herausbilden, die weiterführende Analysen zu sprachlicher Inklusion und Exklusion erlauben und damit die Beziehungsdimension von Texten erschließen sowie die Beantwortung von Fragestellungen zu Funktionalität und Sprachhandlungsprofilen der vorliegenden Textsorten ermöglichen. 
  
         
         
            Somit stehen einerseits die Wandelbarkeit der Verteilung von sprachlichen Einheiten vor dem weiten Horizont von Textgruppen, andererseits die Funktionalität von sprachlichen Einheiten für die Konstitution bestimmter Textsorten im Vordergrund. Sowohl die unterschiedliche Verteilung von personaldeiktischen Formen als auch die spezifische Funktionalität von sprachlichen Einheiten, wie wir diskutieren möchten, ist nicht selbsterklärend, sondern gleichermaßen von Forschungshypothesen und -interessen abhängig. Abschließend möchten wir deshalb Überlegungen zu den folgenden Fragen bieten: Ist die „Bricolage“ (Bubenhofer/Dreesen 2018) aus Ansätzen und Methoden sehr unterschiedlicher Forschungstraditionen überhaupt sinnvoll? Lassen sich komplexe, kontextbasierte deiktische Kategorien messen, aber auch: Lassen sich damit verknüpfte Handlungsmuster überhaupt operationalisieren und in einem Tagset darstellen?
         
      
      
         
            
               Bibliographie
               
                  Adamzik, Kerstin
                   (2016): 
                  Textlinguistik. Grundlagen, Kontroversen, Perspektiven.
                   2., völlig neu bearbeitete, aktualisierte und erweiterte Neuauflage. Berlin, Boston: De Gruyter.
               
               
                  Biber, Douglas / Finegan, Edward
                   (1994): 
                  Multi-Dimensional Analyses of Authors’ Styles: 
                  Some Case Studies from the Eigtheenth Century. Oxford: Oxford University Press.
               
               
                  Biber, Douglas
                   (1988): 
                  Variation across Speech and Writing
                  . Cambridge: Cambridge University Press.
               
               
                  Blei, David M. / Ng, Andrew Y. / Jordan, Michael I.
                   (2003): "Latent Dirichlet Allocation", in: 
                  Journal of Machine Learning Research
                  3: 993–1022.
               
               
                  Bubenhofer, Noah / Dreesen, Philipp
                  (2018): "Linguistik als antifragile Disziplin? Optionen in der digitalen Transformation", in: 
                  Digital Classics Online 
                  4: 63–75. 
               
               
                  Bubenhofer, Noah / Scharloth, Joachim
                   (2016): "Kulturwissenschaftliche Orientierung in der Computer- und Korpuslinguistik", in:
                   Sprache – Kultur – Kommunikation / Language – Culture – Communication
                  . Ein internationales Handbuch zu Linguistik als Kulturwissenschaft / An International Handbook of Linguistics as a Cultural Discipline. Bd. 43. Berlin, Boston: De Gruyter: 924–933. 
               
               
                  Bubenhofer, Noah / Scharloth, Joachim
                   (2015): "Maschinelle Textanalyse im Zeichen von Big Data und Data-driven Turn – Überblick und Desiderate", in: 
                  Zeitschrift für Germanistische Linguistik
                   43: 1–26. 
               
               
                  DeRose, Steven J. / Durand David G. / Mylonas, Elli / Renear, Allen H.
                   (1997): "What is text, really?", in: 
                  SIGDOC Asterisk Journal of Computer Documentation
                   21.3: 1–24.
               
               
                  Fankhauser, Peter/ Knappen, Jörg / Teich, Elke
                   (2016): "Topical Diversification Over Time In The Royal Society Corpus" in: Eder, Maciej / Rybick, Jan (eds.): 
                  Digital Humanities
                  , 11.–16. Juli 2016, Krakow: Conference Abstracts.
               
               
                  Fix, Ulla
                   (2006): "Was heißt Texte kulturell verstehen? Ein- und Zuordnungsprozesse beim Verstehen von Texten als kulturellen Entitäten", in: Blühdorn, Hardarik / Breindl, Eva / Waßner, Ulrich Hermann (eds.): 
                  Text – Verstehen. 
                  Grammatik und darüber hinaus. Berlin / Boston: De Gruyter: 254–276.
               
               
                  Hausendorf, Heiko / Kesselheim, Wolfgang / Kato, Hiloko / Breitenholz, Martina
                   (2017). 
                  Textkommunikation
                  : ein textlinguistischer Neuansatz zur Theorie und Empirie der Kommunikation mit und durch Schrift. Berlin / Boston: De Gruyter.
               
               
                  Heinemann, Wolfgang / Heinemann, Margot
                   (2002): 
                  Grundlagen der Textlinguistik
                  . Interaktion – Text – Diskurs. Berlin / Boston: De Gruyter.
               
               
                  Herrmann, Berenike J. / Dalen-Oskam, Karina van / Schöch, Christof
                  (2015): "Revisiting Style, a Key Concept in Literary Studies" in
                  : Journal of Literary Theory
                   9: 25–52. 
               
               
                  Hermanns, Fritz
                   (2009): "Linguistische Hermeneutik. Überlegungen zur überfälligen Einrichtung eines in der Linguistik bislang fehlenden Teilfaches", in: Felder, Ekkehard (eds.): 
                  Sprache
                  . Heidelberg: Springer: 179–214.
               
               
                  Jannidis, Fotis
                   (2019): "Digitale Geisteswissenschaften – Offene Fragen, schöne Aussichten", in: 
                  ZMK
                   10: 63–70. 
               
               
                  Lefévre, Michel
                   (2017): "Von der "Berlinischen Privilegierten Zeitung" zur "Königlich Privilegierten Berlinischen Zeitung". Entwicklungstendenzen in der Äußerungsstruktur, Textgestaltung und Syntax", in: Pfefferkorn, Oliver / Riecke, Jörg / Schuster, Britt-Marie (eds.): 
                  Die Zeitung als Medium
                  . Berlin / Boston: De Gruyter: 149–163.
               
               
                  Medhat, Walaa / Hassan, Ahmed / Korashy, Hoda
                   (2014): "Sentiment analysis algorithms and applications. A survey", in: 
                  Ain Shams Engineering Journal
                   5: 1093–1113.
               
               
                  Ravi, Kumar / Ravi, Vadlamani
                   (2015): "A survey on opinion mining and sentiment analysis: Tasks, approaches and applications", in: 
                  Knowledge-Based Systems
                   89: 14–46.
               
               
                  Sandig, Barbara
                   (2006): 
                  Textstilistik der deutschen Sprache
                  . 2. völlig neu bearbeitete und erweiterte Auflage. Berlin / Boston: De Gruyter.
               
               
                  Schöch, Christof / Steffen Pielström
                   (2014): 
                  Für eine computergestützte literarische Gattungsstilistik, Jahrestagung der Digital Humanities im deutschsprachigen Raum.
                  http://dig-hum.de/sites/dig-hum.de/files/Schoch-Pielstrom_2014_Gattungsstilistik.pdf. [letzter Zugriff 23. September 2019]
               
               
                  Schuster, Britt-Marie
                   (2019): "Sprachgeschichte als Geschichte von Texten", in: Bär Joachim / Lobenstein-Reichmann, Anja /Riecke, Jörg (eds.): 
                  Handbuch Sprache in der Geschichte
                  . Berlin / Boston: De Gruyter: 219–240.
               
               
                  TEI Consortium
                   (2019): TEI P5. Guidelines for Electronic Text Encoding and Interchange. Originally edited by C.M. Sperberg-McQueen and Lou Burnard for the ACH-ALLC-ACL Text Encoding Initiative, now entirely revised and expanded under the supervision of the Technical Council of the TEI Consortium. Version 3.6.0 (16. Juli 2019)
               
               
                  Viehhauser, Gabriel
                   (2017): “Digitale Gattungsgeschichten. Minnesang zwischen generischer Konstanz und Wende.” In: Zeitschrift für digitale Geisteswissenschaften. 2017. PDF Format ohne Paginierung. DOI: 10.17175/2017_003.
               
            
         
      
   



      
         
            Einleitung
            Häufig gibt es unterschiedliche Quellen oder Auflagen zu einem Werk, deren Analyse Rückschlüsse auf die Entstehungsgeschichte oder auf unterschiedliche Akzentuierungen, aber z. B. auch auf Transkriptionsfehler zulässt. Dazu gehören nicht nur Differenzen in den Texten, sondern auch in der Typographie (z. B. kursive Hervorhebungen oder Fontgröße). Wir präsentieren das Open-Source Web-Tool „Variance-Viewer" das, anders als die übliche Diff-Funktion in Texteditoren, nicht nur zwei Texte vergleichen und die Unterschiede markieren und hervorheben, sondern auch die Varianzen mit Regeln in Typen einteilen kann. Die verschiedenen Typen können ein- oder ausgeblendet sowie mit unterschiedlichen Farben markiert werden. Weiterhin können die zu vergleichenden Texte vor dem Vergleich normalisiert werden. Dadurch wird die Übersichtlichkeit bei vielen kleineren Unterschieden erheblich gesteigert, und es kann auf fachlich relevante Differenzen fokussiert werden. Es ist ein TEI-Export verfügbar, in dem für die Varianzen vordefinierte Tags generiert werden. Folgender Workflow soll beim Vergleich zweier Werke unterstützt werden:
                
             1. Übersicht über Differenzen bekommen (dafür eignet sich praktisch jedes Diff-Tool).
             2. Wiederhole:
             a. Definition von Typen der Differenzen mittels Konfigurationsdatei.
             b. Ein- und Ausblenden der Typen und Untersuchung der Restkategorie, ob weitere Typ-Definitionen sinnvoll sind.
             3. Weitere editorische Arbeiten, ggf. TEI-Export der typisierten Differenzen. 
         
         
            Verwandte Arbeiten 
            Die meisten Texteditoren verfügen über eine Diff-Funktion, mit der sich zwei Texte (auch Programmcode oder DNA-Sequenzen) vergleichen und insbesondere Änderungshistorien von Dokumenten nachverfolgen lassen (vgl. z. B. die Darstellungen von Varianten in der Faust-Edition). Dabei gibt es häufig zwei Darstellungen: zum einen die der Änderungen innerhalb eines Dokumentes und zum anderen die Gegenüberstellung der beiden Dokumente mit jeweiliger Hervorhebung der Änderungen. Viele Algorithmen basieren auf der Publikation von Myers (Myers 1986), der gezeigt hat, dass die Suche nach der längsten gemeinsamen Teilfolge und der kürzesten Transformation eines Strings A in einen String B als äquivalent angesehen werden können. Eine Implementierung ist die Suche nach einem kürzesten Weg in einem Edit-Graphen bzw. einer Matrix, der aus den Wörtern oder Buchstaben der beiden Dokumente als Zeilen bzw. Spalten besteht. Für literarische Texte ist im Allgemeinen eine feinere Differenzierung wünschenswert, in der Typen von Änderungen erkannt und ein- oder ausgeblendet werden können. Diese können sich sowohl auf den Text als auch die Typographie beziehen. Da die Typen von den individuellen Interessen der jeweiligen Philologen abhängen, sollten sie nicht fest vorgegeben, sondern leicht anpassbar sein. Weiterhin ist neben einer Visualisierung auch ein Export nach TEI wünschenswert. Im Folgenden präsentieren wir ein solches Tool, da wir kein vergleichbares, einfach bedienbares Werkzeug kennen (so wird z. B. in der Übersicht über Digital-Humanities-Tools und Services in (Bulatovic 2016) diese Kategorie nicht erwähnt). Ein ähnliches, aber anspruchsvolleres Tool ist CollateX (Haentjens Dekker 2014), das in der Lage ist, zwei und mehr Texte zu kollationieren und das Ergebnis als Graph zu visualisieren. Dabei können auch Transpositionen, d. h. verschobene Texte gefunden werden, teilweise einschließlich Erkennung von Varianten der verschobenen Texte. Ein weiteres anspruchsvolles Tool ist Stemmaweb, dessen GitHub-Repository jedoch darauf hindeutet, dass es nicht oder kaum noch aktiv gepflegt wird. Im Beitrag (Andrews 2014), bei dem es um eine kritische Bewertung der Entstehungsgeschichte von drei Werken geht, wird u. a. kritisiert, dass bestimmte Typen von Änderungen vorschnell als „insignifikant“ bewertet werden. Entwurfsregeln zur Visualisierung von Text-Varianz-Graphen werden in (Jänicke 2014) dargestellt. Der Variance-Viewer hat einen anderen Schwerpunkt: er gibt die Typen von Änderungen nicht vor, sondern überlässt deren Definition dem Anwender durch einfache Konfiguration. Die Darstellung enthält keine Graphen, sondern eine farbige Kennzeichnung und bietet das Aus- und Einblenden von Typen von Varianten durch einfachen Klick an, so dass Editoren die Übersicht behalten, wenn Sie sich auf bestimmte Differenztypen konzentrieren wollen.
                
         
         
            Methoden
            Der Variance-Viewer verwendet für die Berechnung von Differenzen zwischen zwei Texten eine Implementierung des Algorithmus von Myers und fügt dann Nachbearbeitungen zur Differenzierung verschiedener Typen von Änderungen hinzu. Die Kategorien sind frei konfigurierbar (s. Abbildung 1 links unten für einen Auszug aus der Konfigurationsdatei). Die Nachbearbeitung prüft für jede gefundene Änderung, ob die Bedingungen für einen der definierten Typen vorliegen und ordnet sie dann dem entsprechenden Typ zu. Die Änderungen werden auf Wortebene berechnet und zusätzlich die für die Änderung verantwortlichen Buchstaben identifiziert, so dass beides hervorgehoben werden kann, wobei zusätzliche Leerzeichen auch wortübergreifend gefunden werden. Alle nicht zugeordneten Typen werden einem Default-Typ (z. B. „Inhalt“ bzw. „Content“) zugeordnet, wobei noch zwischen einfachen und komplexen Änderungen unterschieden werden kann (einfache Änderungen unterscheiden sich nur in einem Buchstaben). Die Ergebnisse können in TEI ausgegeben werden, indem das „app“-Tag mit speziellen Attributen für die Änderungstypen benutzt wird. Weiterhin können sie visuell präsentiert werden, wobei den Typen verschiedene Farben zugeordnet werden und bei Bedarf jeder Typ auch ausgeblendet werden kann, um die Übersicht zu verbessern. Das Programm präsentiert beide Texte in einer synoptischen Darstellung, wobei zur Gewährleistung einer zeilenäquivalenten Darstellung in einem Dokument freier Platz auf Abschnittsebene hinzufügt wird, falls das notwendig ist.
                
            Den Umgang mit den Typen erläutern wir an zwei philologischen Anwendungsprojekten, in denen der Variance-Viewer eingesetzt wurde: Die Analyse der Änderungen in den Schriften von Richard Wagner im Projekt RWS und die Analyse der verschiedenen Auflagen von Drucken im Narragonien digital Projekt.
                
            Im RWS-Projekt liegen die Texte als TEI-Dokumente vor. Bei der Analyse der Varianzen sind nicht nur textuelle Änderungen interessant, sondern auch Änderungen bzgl. der Formatierung, die in TEI im Element „rend“ hinterlegt sind. Daher wird dieses genauer analysiert. Insgesamt sind folgende Typen von Änderungen durch projektspezifische Regeln definiert (vgl. Abbildung 1):
            
               Satzzeichen (Punctuation): Die Änderung bezieht sich nur auf ein Satzzeichen (. , ; - ? ! usw.).
               Grapheme (Graphemics): Die Änderung bezieht sich nur auf bestimmte Schreibweisen (y i; u v; s ſ; ss ß; Groß/Kleinschreibung; th t; usw.).
               Abkürzungen (Abbreviation): Die Änderung bezieht sich nur auf Abkürzungen (z. B. Dr. Doktor; Hr. Herr Herrn; usw.).
               Typographie (Typography): Die Änderung ist keine inhaltliche, sondern bezieht sich auf das Layout oder die Typographie und wird in dem TEI-Attribut „rend“ mit entsprechenden Werten spezifiziert (kursiv; gesperrt; usw.).
               Inhalt (Content): Alle übrigen Änderungen, die keiner der obigen Kategorien zugeordnet werden können einschließlich Hinzufügen oder Löschen sowie Änderungen, bei denen mehr als eine Änderung der obigen Typen gleichzeitig vorkommt. 
            
            Im Narragonien-Projekt liegen die Drucktexte als Plain Text Dateien vor. Hier werden folgende Typen von Änderungen unterschieden (vgl. Abbildung 2):
            
               Grapheme (mit anderer Liste von Buchstabenersetzungen wie im RWS-Projekt).
               Abkürzungen (mit anderer Bedeutung als im RWS-Projekt; hier sind es meist einzelne Buchstaben mit Unter- oder Überstrichen, die expandiert werden). 
               Leerzeichen im Wort, die ein Wort in zwei oder mehrere Wörter auftrennen. (Separation). Diese Option ist technisch aufwändiger, weil nicht einzelne Wörter sondern Wortgruppen miteinander verglichen werden müssen.
                Inhaltsänderungen mit nur einem Zeichen Unterschied (OneDifference), die nicht in der Graphem-Liste enthalten sind und anders bewertet werden als komplexere Änderungen. 
               Inhalt (Content): Alle übrigen Änderungen.
            
         
         
            Erfahrungen
            Das Tool wurde in beiden Projekten erfolgreich eingesetzt, und dabei auch für die Verarbeitung sehr langer Dokumente genutzt. Im Folgenden zeigen wir zwei Screenshots aus dem RWS- und dem Narragonien-Projekt. Dabei ist besonders hervorzuheben, dass der Rest-Typ „Content“, der alle sonst nicht speziell erkannten Typen von Änderungen beinhaltet, nur noch ca. die Hälfte der Änderungen ausmacht, während die andere Hälfte spezielleren Typen zugeordnet werden konnte. Wenn das Ziel die Feinanalyse bestimmter Änderungstypen ist, können auch iterativ weitere Typen definiert und der Analysealgorithmus damit erneut ausgeführt werden.
            
               
                Abbildung 1: Vergleich zweier Texte aus dem Schriften-Verzeichnis von Richard Wagner mit Hervorhebung der Änderungstypen in verschiedenen Farben (Erläuterungen der Typen im Text; Auszug aus Konfigurationsdatei links unten). Die Texte liegen im Format TEI vor, wobei TEI-Attributwerte auf CSS abgebildet wurden, um die Darstellung unterschiedlicher Typen sichtbar zu machen, und die gefundenen Differenzen mit ihren Typen können auch als TEI exportiert werden (Auszug für die erste Zeile s. rechts unten).
            
            
               
                Abbildung 2: Vergleich des edierten Lesetextes der Narrenschiff-Ausgabe GW5041 (links) mit dem Ergebnis der OCR auf dem Originaltext einer anderen Druckausgabe (rechts), wobei die Änderungen sowohl OCR-Fehler als auch Normalisierungen der Schrift im Lesetext umfassen. Dies ist durch Hervorhebung der Änderungstypen in verschiedenen Farben leichter nachvollziehbar (Erläuterung der Typen im Text). Unten eine Statistik, die die 6.523 gefundenen Änderungen nach Änderungstypen aufschlüsselt. Der Gesamttext umfasste 150 Seiten mit 4.200 Zeilen, 26.000 Wörtern und 121.000 Zeichen und die zugehörige Konfigurationsdatei („Settings“) ca. 100 Zeilen. Für diese Analyse brauchte der Variance-Viewer in dem serverseitig ausgeführten Demo-Modus im Web ca. 25 Sekunden (für intensive Nutzung sollte der Open-Source Code lokal installiert werden).
            
            Die bisherigen Erfahrungen zeigen, dass noch eine Reihe von relativ einfachen Erweiterungen wünschenswert sind, wobei zu erwarten ist, dass in weiteren Projekten weitere Aspekte hinzukommen: 
            
               Gelegentlich enthält ein Wort mehrere Änderungen (z. B. mehrere Grapheme und/oder Satzzeichen). Wenn es mehrere Änderungen desselben Typs gibt, werden diese dem Typ zugeordnet (z. B. „Schiff“ in Zeile 5 in Abbildung 2 mit zwei graphemischen Differenzen). Wenn es jedoch Änderungen unterschiedlicher Klassen sind, werden diese als ein nicht näher differenzierter Unterschied („Content“) betrachtet (z. B. „ist /“ und „i
                  ʃt/“ in Zeile 81 mit zwei verschiedenen Typen von Änderungen bezüglich Leerzeichen und Graphem). Hier wäre eine Mischklasse aus den jeweiligen Ursprungsklassen wünschenswert. Das Problem lässt sich teilweise durch vorherige Normalisierung lösen, indem z. B. alle Graphem-Änderungen vorab normalisiert werden und sich dann manche komplexe Fehler zu einfachen Fehlertypen reduzieren.
                Es gibt Ausnahmen zu den Regeln, in denen die Nutzer die Änderungsklasse manuell ändern und ggf. kommentieren können sollten. Bisher zeigt der Viewer nur das automatisch generierte Ergebnis der Regelauswertung an, er sollte um eine Editierfunktion erweitert werden. Dies ist z. B. hilfreich, wenn bei automatischer OCR von verschiedenen Ausgaben eines Werkes zwischen OCR-Fehlern und Textvarianten unterschieden werden soll.
            
         
         
            Zusammenfassung und Ausblick
            Der vorgestellte Variance-Viewer ermöglicht die Feindifferenzierung und Klassifikation von Textvarianten mittels selbstdefinierter Typen. In verschiedenen Ausgaben von literarischen Texten treten oft zahlreiche „technische“ Varianzen auf, die sich auf Satzzeichen, Leerzeichen, Buchstabenvarianten und ggf. auch auf das Layout oder die Typographie beziehen, die von eigentlichen inhaltlichen Änderungen zu trennen sind. Hier werden bei Verwendung eines einfachen Diff-Werkzeugs häufig so viele Änderungen angezeigt, dass der Überblick verloren geht. Ein Filtern bzw. Hervorheben bestimmter Typen von Varianzen erleichtert die philologische Arbeit beträchtlich. Wichtig ist, dass die Typen von Varianzen abhängig von den Fragestellungen und individuellen Interessen des jeweiligen Philologen leicht konfiguriert werden können. Der vorgestellte Varianz-Viewer erfüllt diese Anforderungen und hat sich in zwei größeren philologischen Projekten bewährt. Er ist Open-Source, webbasiert, leicht zu installieren und zu bedienen. Perspektiven der Weiterentwicklung umfassen eine einfachere oder sogar automatische Definition der Varianztypen sowie funktionelle Erweiterungen:
            
               Aus technischer Sicht sollte für die Definition von Varianztypen ein Editor bereitgestellt werden, so dass deren Definition im Vergleich zur bisherigen Konfigurationsdatei noch weiter vereinfacht wird. Dazu kann eine Regelsprache bereitgestellt werden oder ein Lernverfahren, dem einige Beispiele präsentiert werden und der das Muster dann selbständig erkennt.
               Die häufigsten Typen von Varianzen können auch durch Lernverfahren vollautomatisch erkannt werden (ohne vorgegebene Varianztypen), indem alle vom Diff-Algorithmus gefundenen Varianten auf gemeinsame Muster hin analysiert werden.
               Eine umfassende Änderung wäre die Weiterentwicklung des relativ einfachen Tools zur Erkennung komplexerer Änderungen wie Transpositionen und zur Visualisierung der Änderungen, auch von mehreren Werken, in Graphen, ggf. durch Übernahme entsprechender Funktionalitäten z. B. aus CollateX oder Stemmaweb.
            
         
      
      
         
            
      Web-Link vom Variance-Viewer: 
      http://variance-viewer.informatik.uni-wuerzburg.de; Code Open-Source unter: 
      https://github.com/cs6-uniwue/Variance-Viewer
            
            
               http://faustedition.net
            
            
               https://collatex.net
            
            
               https://stemmaweb.net
            
            
               https://github.com/tla/stemmatology
            
            
      Java-diff-utils: 
      https://code.google.com/archive/p/java-diff-utils. Die Software wird von Google gehostet und wurde von 2009-2013 von verschiedenen Autoren entwickelt (s. 
      https://code.google.com/archive/p/java-diff-utils/source/default/commits).
    
            
      Richard Wagner Schriften (RWS): Historisch-Kritische Gesamtausgabe: 
      http://www.musikwissenschaft.uni-wuerzburg.de/forschung/richard-wagner-schriften.
    
            
      „Narragonien digital“: 
      http://kallimachos.de/kallimachos/index.php/Narragonien.
    
         
         
            
               Bibliographie
               
                  Andrews, Tara L (2014): Analysis of variation significance inartificial traditions using Stemmaweb, in 
                        Digital Scholarship in the Humanities, 31(3).
                    
               
                  Bulatovic, Natasa / Gnadt, Timo / Romanello, Matteo / Schmitt, Viola / Stiller, Juliane / Thoden, Klaus (2016): Usability von DH­Tools und Services, in 
                        DARIAH Working Papers: 
                        https://wiki.de.dariah.eu/download/attachments/14651583/AP1.2.3_Usability_von_DH-Tools_und-Services_final.pdf.
                    
               
                  Haentjens Dekker, Ronald / van Hulle, Dirk / Midell, Gregor / Neyt, Vincent / van Zundert, Joris (2014): Computer-supported collation of modern manuscripts: CollateX and the Beckett Digital Manuscript Project, in 
                        Digital Scholarship in the Humanities, 30(3), 452-470.
                    
               
                  Jänicke, Stefan / Geßner, Annette / Büchler, Marco / Scheuermann, Gerik (2014): 5 Design Rules for Visualizing Text Variant Graphs, in 
                        DH: 
                        https://www.informatik.uni-leipzig.de/~stjaenicke/5_Design_Rules_for_Visualizing_Text_Variant_Graphs.pdf.
                    
               
                  Myers, Gene (1986): An O(ND) difference algorithm and its variations, in 
                        Algorithmica, 1, 251-266.
                    
            
         
      
   



      
         Seit März 2006 entsteht in der Hans Kelsen-Forschungsstelle (bis 2011 in Erlangen, seitdem in Freiburg i. Br.) die historisch-kritische Ausgabe der Werke des österreichischen Rechtstheoretikers Hans Kelsen (1881–1973). Mit der Aufnahme in das Programm der Akademie der Wissenschaften und der Literatur | Mainz im Jahr 2018 wurde eine weitere Arbeitsstelle für die digitale Komponente der Edition in Frankfurt a. M. etabliert.
                Die im Mohr Siebeck Verlag erschienenen Druckbände werden nach dem 
                Moving-Wall-Prinzip ebenfalls als digitale Edition aufbereitet und unter einer CC BY 2.0 Lizenz zur Verfügung gestellt. Fünf der geplanten 35 Bände (Jestaedt 2007–2013) sind bereits vor 2018 als Print erschienen und werden nachträglich für das Digitale aufbereitet, während zukünftig die digitale und die analoge Form der Edition 
                single source in einer XML-basierten digitalen Editionsumgebung erarbeitet wird. Somit werden für die (Rechts-)Wissenschaften maschinenlesbare Forschungsdaten zum Werk einer zentralen rechtshistorischen Figur des 20. Jahrhunderts nachnutzbar.
            
         Wie bei vielen Editionsprojekten, die bisher in rein analoger Form zur Verfügung standen und nun eine „digitale Wende“ vollziehen, stellen sich für die HKW vielschichtige Herausforderungen. Diese betreffen insbesondere die Beschaffenheit der Datengrundlage und deren Aufbereitung, die Umstellung bisher etablierter Redaktionsprozesse sowie Lern- und Lehrabläufe der digitalen und der klassischen Geisteswissenschaften. In unserem Poster stellen wir dar, inwiefern etablierte Workflows und Standards der Digital Humanities für die „Hans Kelsen Werke“ eingesetzt werden und geben einen Ausblick auf den Wert für die (digitale) Rechtswissenschaft insgesamt.
         
            Datengrundlage und -modellierung
            Vor Abschluss der Redaktionsumstellung in eine XML-basierte digitale Editionsumgebung bilden die Drucksatzdaten der nach bisherigem Verfahren erstellten Bände (innerhalb des Textverarbeitungsprogramms Microsoft-Word) die Datengrundlage für die digitale Edition der entsprechenden Werke Hans Kelsens. Die Datenmodellierung der Texte und der Register anhand der Guidelines der 
                    Text Encoding Initiative (TEI) in XML orientiert sich am Basisformat des Deutschen Textarchivs und wurde um projektspezifische Anforderungen erweitert. Eine Besonderheit der Texte im Vergleich zur herkömmlichen Quellenedition besteht beispielsweise im doppelten Fußnotenapparat – demjenigen Hans Kelsens und den Anmerkungen der HKW-Editor*innen – sowie der Heterogenität der Texte an sich (Buchbesprechungen, Gesetzestexte, Aufsätze, Monografien). Auch der vielfältige Einsatz des editorischen Fußnotenapparats der HKW birgt für die Übertragung in eine semantische Kodierung der Texte kreative Möglichkeiten.
                
            Die Bände der HKW liefern umfangreiche und für die Kelsen-Forschung unverzichtbare Personen- und Sachregister. Die einzelnen Personenregister wurden zusammengeführt und mit entsprechenden Normdaten der 
                    Gemeinsamen Normdatei (GND) versehen. Sie bilden ein digitales Register, welches bereits vor der endgültigen Umstellung des Redaktionsprozesses in eine digitale Editionsumgebung in die reguläre Editionsarbeit integriert wird. Ebenso wurden die Schriftenverzeichnisse der bisher publizierten Bände homogenisiert in das Literaturverwaltungssystem 
                    Zotero
               
                übertragen, in der zukünftig die Literatur nicht nur verwaltet, sondern auch in die neue Arbeitsumgebung integriert wird. Eine besondere Herausforderung stellt das heterogene und komplexe – und dafür umso bedeutendere – Sachregister dar, welches perspektivisch eine Grundlage für die Erarbeitung einer Ontologie in den digitalen Rechtswissenschaften und damit einen Einstieg der Fachrichtung in das Feld der 
                    Linked Open Data darstellen kann. Die datenbasierte Modellierung der Hans Kelsen Werke bietet somit vielfältige innovative Spielräume für die Digital Humanities hinsichtlich ihrer Wirkung auf die Rechtswissenschaften.
                
         
         
            Digitale Infrastruktur
            In der digitalen Infrastruktur und Editionsumgebung der HKW werden etablierte Standards und Angebote aus den DH zur Anwendung gebracht und weiterentwickelt. Die zu edierenden Quellen und Forschungsdaten werden in einer Instanz der XML-Datenbank eXist-db verwaltet und über eine Integration in den oXygen XML-Editor im Author-Modus editorisch bearbeitet. Zum Einsatz kommt hierbei ein projektspezifisches Erweiterungsframework auf Basis von 
                    ediarum sowie die eXistdb-App ediarum.db
               
               .
                
            Die Präsentation der digitalen Edition findet sich perspektivisch auf 
                    kelsen.online, zunächst werden hier nähere Projektinformationen, die PDF der bisher analog publizierten HKW-Bände und ein kumuliertes Gesamtregister der entsprechenden Bände zur Verfügung gestellt. Die Präsentationsschicht basiert auf dem Content Management System TYPO3, in das die Forschungsdaten aus der eXist-db importiert werden und redaktionelle Arbeiten an der Website stattfinden. Neben einer ansprechenden und benutzerfreundlichen Präsentation der Forschungsdaten und korpusinterner sowie -externer Interaktion werden diese über Schnittstellen beziehbar und für weitere maschinengestützte Forschungen nutzbar sein.
                
            Von der digitalen Redaktionsumstellung profitiert auch die gedruckte Buchausgabe der Edition, welche weiterhin ein gleichwertiger Bestand des Projektes bleibt. So beispielsweise durch die Reduzierung bisheriger Arbeitsschritte und einheitliche Ansetzungen in den Verzeichnissen.
         
         
            Ausblick
            Die digitale Edition der Hans Kelsen Werke wird in der 25-jährigen Laufzeit des Projektes die Entwicklungen und Standards der Digital Humanities verfolgen, gegebenenfalls adaptieren und sich dem Forschungsgegenstand “Hans Kelsen” mit dem Einsatz digitaler Methoden nähern. Auch für die rechtswissenschaftliche Forschung insgesamt hoffen wir durch die Erarbeitung von Standards für die digitale Aufbereitung fachspezifischer Daten einen nachhaltigen Beitrag zu leisten.
         
      
      
         
            
      www.deutschestextarchiv.de/doku/basisformat/.
    
            
      https://www.zotero.org/.
    
            
      http://exist-db.org/exist/apps/homepage/index.html.
    
            
      https://www.oxygenxml.com/.
    
            
      https://github.com/ediarum.
    
            
      https://typo3.org/.
    
         
         
            
               Bibliographie
               
                  Jestaedt, Matthias (eds.) (2007): Hans Kelsen Werke. Band 1. Veröffentlichte Schriften 1905–1910 und Selbstzeugnisse. Tübingen: Mohr Siebeck.
                    
               
                  Jestaedt, Matthias (eds.) (2008): Hans Kelsen Werke. Band 2: Veröffentlichte Schriften 1911. Tübingen: Mohr Siebeck.
                    
               
                  Jestaedt, Matthias (eds.) (2010): Hans Kelsen Werke. Band 3: Veröffentlichte Schriften 1911–1917. Tübingen: Mohr Siebeck.
                    
               
                  Jestaedt, Matthias (eds.) (2013): Hans Kelsen Werke. Band 4: Veröffentlichte Schriften 1918–1920. Tübingen: Mohr Siebeck.
                    
               
                  Jestaedt, Matthias (eds.) (2011): Hans Kelsen Werke. Band 5: Veröffentlichte Schriften 1919–1920. Tübingen: Mohr Siebeck.
                    
               
                  Reinthal, Angela (2014): „InterNationalität und InterDisziplinarität der Hans Kelsen Werke (HKW)“ in: Stolz, Michael / Chen, Yen-Chun (eds.): Internationalität und Interdisziplinarität der Editonswissenschaft (= Beihefte zu editio 38). Berlin: De Gruyter 303-314.
                    
               
                  Nečaský, Martin / Knap, Tomáš / Klímek, Jakub / Holubová, Irena / Vidová-Hladká, Barbora (2013) Linked Open Data for Legislative Domain – Ontology and Experimental Data, in: Abramowicz W. (eds) Business Information Systems Workshops (= Lecture Notes in Business Information Processing 160). Berlin / Heidelberg: Springer Verlag 172–183.
                    
               
                  Sahle, Patrick (2013): Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. Teil 1: Das typografische Erbe. (= Schriften des Instituts für Dokumentologie und Editorik 7). Norderstedt: BoD https://kups.ub.uni-koeln.de/5351/.
                    
               
                  Sahle, Patrick (2013): Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. Teil 2: Befunde, Theorie und Methodik. (= Schriften des Instituts für Dokumentologie und Editorik 8). Norderstedt: BoD https://kups.ub.uni-koeln.de/5352/.
                    
               
                  Sahle, Patrick (2013): Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. Teil 3: Textbegriffe und Recodierung. (= Schriften des Instituts für Dokumentologie und Editorik 9). Norderstedt: BoD https://kups.ub.uni-koeln.de/5353/.
                    
               
                  Staab, Steffen / Studer, Rudi (2009) (eds.): Handbook on Ontologies. Berlin / Heidelberg: Springer Verlag.
                    
               
                  TEI Consortium (eds.) (2019): TEI P5: Guidelines for Electronic Text Encoding and Interchange. Version 3.6.0 vom 17.07.2019. TEI Consortium. https://tei-c.org/guidelines/P5/ [letzter Zugriff 25.09.2019].
                    
            
         
      
   



      
         
            Fachwissenschaftliche Hintergründe
            
Theaterhistorische Wissensdinge sind divers und unmittelbar mit vielfältigen historisch-kulturellen Bezügen aufgeladen. Die Objekte stammen teils unmittelbar aus der Aufführungspraxis (z.B. Masken und Requisiten) und sind damit Akteure und Zeugen des Geschehens. Zum Teil stammen sie aber auch aus Produktionsprozessen (z.B. Regiebücher, Bühnenbildentwürfe und -modelle) und ermöglichen damit einen Blick in die Prozesse, Mechanismen und Materialitäten des Theaters. Andere Objekte hingegen dokumentieren das ephemere Ereignis (z.B. Fotografien oder Kritiken) und erlauben so einen Zugriff auf die performativen Spuren einer Aufführung. Und wieder andere Objekte wie Programmhefte und Theaterzettel beinhalten historisch-kulturelle Kontextinformationen. Ergänzend zu den genannten Objektarten tragen Ego-Dokumente wie Briefe und Kalender eher eine sozio-historische Bedeutung: Sie geben Hinweise auf die beteiligten Individuen sowie deren Lebensläufe und Beziehungen. Die Bedeutungsebenen und Bezüge der Objekte sind nicht statisch: Mit jeder neuen Kontextualisierung weiten sich ihre Bedeutungsräume aus. Oft befinden sich Objekte an den Schnittstellen von diversen Verwendungsmöglichkeiten – bspw. beschreiben Kritiken nicht nur, was auf der Bühne geschehen ist, sie bilden auch Publikumsreaktionen und damit einen wichtigen Teil der Rezeptionsgeschichte ab.
                
            
            
Theateraufführungen lassen sich verstehen als das Ergebnis einer intensiven und oft monatelangen Zusammenarbeit unterschiedlichster Professionen: Akteur_innen
                aus den Tätigkeitsbereichen Regie, Darstellung, Bühnen- und Kostümbild, Bühnentechnik und vielen weiteren bilden ein komplexes Netzwerk, das für einen begrenzten Zeitraum besteht und im organisierten Zusammenspiel eine Inszenierungsidee auf der Theaterbühne verwirklicht. Theaterhistorisch relevante Forschungsobjekte beinhalten implizite und explizite Spuren dieser Verwirklichung. Der Wirkungsraum der Objekte erstreckt sich dabei sowohl auf Ereignisse als auch auf die Akteur_innen, die an diesen Ereignissen beteiligt waren. Die unterschiedlichen Ereignisse, zwischen denen Beziehungen und Interdependenzen bestehen, lassen sich als das Ergebnis von komplexen Interaktionen zwischen Akteur_innen beschreiben und können somit als Modelle für personenbezogene Interaktionsnetzwerke dienen. Um diese Netzwerke erfassen zu können, müssen nicht nur die relevanten Objekte mit 
               inhaltsreichen Daten erschlossen werden, sondern auch die Relationen zwischen diesen Daten modelliert werden. 
            
            
               Diesen vergänglichen Netzwerken von Ereignissen und Akteur_innen im 20. Jahrhundert spürt die Theaterwissenschaftliche Sammlung (TWS) in Zusammenarbeit mit Cologne Center for eHumanities (CCeH) nach. Das Forschungsprojekt 
               (Re-)Collecting Theatre History
                zielt auf die theaterwissenschaftliche Resystematisierung personenbezogener Bestände in den theaterhistorischen Sammlungen der Universität zu Köln und der FU Berlin sowie ergänzend der Theatermuseen Düsseldorf und München ab. Die scheinbare ‚Zufälligkeit‘ von Lebenswegen, die sich nicht den Ordnungsbegriffen der politischen oder der Kunstgeschichte unterordnet, soll zum Ausgangspunkt genommen werden, um die Netzwerke ebenso zu erhellen wie die Frage nach personellen und ästhetischen Kontinuitäten im Theaterbetrieb.
            
            Die im Projekt entwickelte Plattform eröffnet Querverbindungen und Vergleichsmöglichkeiten der Bestände in den wichtigsten universitären Theatersammlungen und öffentlichen Theatermuseen in Deutschland – als solche ist sie offen und auch für zukünftige Projekte erweiterbar –, und schafft dadurch ein umfangreiches Forschungsnetzwerk.
         
         
            Metadaten: Modell und Erfassung
            Aus methodischer Sicht lässt sich das Projekt auf den ersten Blick zwar noch als klassische Nachlassdigitalisierung und Erschließung beschreiben, doch schon auf den zweiten Blick wird deutlich, dass die einfache Erfassung von objektbezogenen Metadaten keine ausreichende Datengrundlage für die intendierte Aufdeckung und Erforschung der beschriebenen komplexen Netzwerk- und Interaktionsstrukturen bieten würde. Die Gesamtheit der Objekte und, in Konsequenz daraus, die Objektdatenbank bildet zwar das Rückgrat des Projektes, sie muss aber den gesamten sozio-historischen Wirkungsraum und Kontext der Objekte erfassen können und darf nicht auf die reine materielle Beschreibung beschränkt sein. 
            Für das Projekt war klar, dass ein Metadaten-Standard wie z.b. Dublin-Core hier viel zu kurz greifen würde und nicht die nötige Spezifität und semantische Tiefe wiedergeben kann. Stattdessen werden die Daten im LIDO (Lightweight Information Describing Objects) Standard erfasst. Dieser ermöglicht eine detaillierte Objektbeschreibung, bietet darüber hinaus aber auch Strukturen für die Beschreibung von “events” und “actors”. Als weit verbreiteter Standard in der Museums- und Sammlungswelt bietet LIDO zum einen eine Basis für konsistente und vergleichbare Datenerhebung, zum anderen genügend Flexibilität und Spielraum innerhalb der Strukturen um die sozio-kulturellen Kontexte der Objekte beschreiben zu können. LIDO ist das Produkt einer CIDOC-Arbeitsgruppe und ist mit den Ansätzen von CIDOC Conceptual Reference Model (CRM) zu großen Teilen konform. Insbesondere bildet die Event-Actor-Struktur der CRM Ontologie einen Kernaspekt des LIDO Standards, der in diesem Projekt von besonderem Interesse und Nutzen war. Durch diese eventbasierte Modellierung können die Daten auch für Graphdatenbank- und Netzwerkansätze verwendet werden, ohne das Objekt, dessen Beschreibung natürlich der Kern eines Nachlass-Projektes bleiben muss, als Informationsträger aus dem Fokus zu verlieren. Ein LIDO Dokument kann auch jederzeit in CRM übersetzt werden, um die Daten für andere Forschungsziele zu verwenden.
            Neben den unmittelbar objektbezogenen Metadaten enthält jeder Datensatz Daten zu drei vordefinierten Ereignissen: “Herstellung”, “Inszenierung” und “Erwerb”, welche die Biographie der Objekte rudimentär nachzeichnen. Zusätzlich können weitere nicht vordefinierte Ereignisse aufgenommen werden. Ihnen können jeweils beliebig viele Akteure zugeordnet werden. Mit diesen Strukturen können dann komplexe Aussagen erfasst werden wie z.B. “Die Maske wurde von einer bekannten Requisitenwerkstatt hergestellt” oder “Schauspieler X war der Inszenierung Y beteiligt, die auf diesem Programmzettel beschrieben wird”. Es werden also teilweise bereits in den objektbezogenen Daten Beziehungen zwischen Akteuren, Ereignissen und Objekten explizit modelliert und beschrieben. 
            Interoperabilität und Nachhaltigkeit haben für solch ein Erschließungsprojekt maßgebliche Bedeutung. Um beides zu gewährleisten, spielen Normdaten und kontrollierte Vokabulare eine wichtige Rolle. Insbesondere personenbezogene Daten profitieren von Verknüpfungen zu externen Datensätzen. Daher werden die Personendatensätze mit GND-Nummern verknüpft (sofern vorhanden). Außerdem werden die Objekttypen unter Zuhilfenahme des Art and Architecture Thesaurus (AAT) erfasst, da sich der heterogene theaterhistorische Objektbestand mit der polyhierarchischen und multilingualen Struktur des AAT adäquat erfassen lässt. Für weitere Datenfelder und -typen, für die keine passende Standards existieren (z.B. Funktionen der Personen oder erweiterte Eventtypen), werden Theaterwissenschafts-spezifische Vokabulare entwickelt, die möglicherweise hilfreich sein können für die zukünftige Weiterentwicklung der LIDO-Terminologie.
            
            
An dieser Stelle ist es wichtig anzumerken, dass LIDO als umfangreiches Datenaustauschformat entwickelt worden ist. Um LIDO für das Projekt effektiv nutzen zu können, wurde ein LIDO-Sub-Schema für die projektspezifischen Bedürfnisse entwickelt, das im Laufe des Projektes erweitert und an die Bedürfnisse des Projekts angepasst wurde. Die Eingabe der Daten findet über den LIDO-Maker statt, der auf dem Metadateneditor CMDI-Maker basiert und entsprechend weiterentwickelt wurde.
            
         
         
            Von Objektdaten zu Akteur- und Inszenierungsdaten
            Verwandte Projekte wie z.B. IbsenStage oder AusStage
stellen die breite Erfassung von Ereignissen, Akteuren und anderen Entitäten in den Vordergrund, und erfassen Objektbezüge nur mit rudimentäre Metadatensets. Auch im institutionsübergreifenden Nachweis- und Rechercheportal 
                    performing-arts.eu
               
(FID Darstellende Kunst) werden nur die Kerndaten zu Objekten präsentiert. Der Ausgangspunkt dieses Projektes ist dagegen die detaillierte Erschließung von Nachlassobjekten. In der Objektdatenbank werden, wie beschrieben, bereits Informationen zu den Beziehungen von Objekten zu Ereignissen und Akteur_innen explizit erfasst. Dagegen sind die Beziehungen zwischen Akteur_innen oder zwischen Ereignissen bisher nur impliziert. Um diese erforschbar und auch referenzierbar zu machen, wurden mithilfe von XSLT zwei zusätzliche Datenbanken aus der im ersten Schritt erstellten LIDO-Objektdatenbank extrapoliert. Um die komplexen XSL-Transformationen kontrollieren zu können, wurden Prinzipien der Konversionspipelines umgesetzt, so dass der Prozess der Datenkuratierung offen gelegt wird. (Barabucci 2018).
                
            Die Personendaten orientieren sich hierbei an der Struktur des TEI:person-Modells, wurden aber mit einigen projektspezifischen Elementen und Attributen ergänzt. Analog dazu wurden auch die Inszenierungsdaten explizit modelliert, indem die Informationen zu einer spezifischen Inszenierung, die aus unterschiedlichen Objekten stammen, kohärent zusammengefügt wurden. In diesem Schritt wurden eindeutige Bezeichner (UUID) zu den Personen- und Inszenierungsdaten zugeordnet, so dass diese nun schon bei der Erfassung von neuen Objektdatensätzen referenziert werden können. Auch die Referenzierungen der Daten untereinander und die entsprechende Abfragen zu diesen Referenzen werden ermöglicht, um nah an der Idee der Netzwerke bleiben zu können. 
                
            Innerhalb eines Akteur_in-Datensatzes sind dann komplexe personenbezogene Informationen wie Wirkungsorte und damit auch Karriereverläufe unmittelbar abrufbar. Wenn eine Person in einem Objektdatensatz als Hersteller_in des Objektes vorkommt – bspw. als Bühnenbildner_in eines Bühnenbildmodells, so wird diese Information einerseits als Objektbezug verarbeitet, andererseits landet sie im Personendatensatz als Berufs- oder Tätigkeitsbeschreibung mit zeitlichen und örtlichen Angaben. Außerdem wird die Verknüpfung mit der entsprechenden Inszenierung gewährleistet, die mit dem jeweiligen Objekt in Beziehung steht. Somit ist die Person nicht nur als Hersteller_in eines Objektes signifikant, sondern auch als Akteur_in der Inszenierungen.
         
         
            Ausblick und Fazit
            Als letzter Schritt von der reinen Objektdatenerfassung hin zu einer Plattform, die Akteur_innen- und Inszenierungdatenbank vernetzt, wurden im Projekt experimentell Möglichkeiten der weiteren Datenanreicherung angedacht und bereits in Teilen umgesetzt. 
            So wurden zum Beispiel, um weitere Informationen über die biographische Hintergründe der Akteur_innen zu erhalten, die biographischen Artikel (ADB und NDB) von der 
Deutschen Biographie
mithilfe der dort bereitgestellten API für die im Projekt erfassten Akteure abgefragt.
                Informationen zu den familiären und beruflichen Hintergründen werden mithilfe eines Skriptes extrahiert und nach einer Überprüfung in die Datensätze der Akteur_innendatenbank eingepflegt. Weiteren biographische Informationen werden aus studentischen Dossier übernommen, die im Rahmen des Projektes von Studierenden der Universität zu Köln erarbeitet wurden. 
            
            
               Diese zwei Ansätze der Datenanreicherung sind als Schritt zu einer Öffnung von Diskussions- und Interaktionsräumen mit den Daten zu verstehen. Gleichzeitig besteht auch der Wunsch die im Projekt erarbeiteten Personendaten anderen Projekten zur Verfügung zu stellen, um so auch über den Projekt und Theaterkontext hinausgehende, prosopographische Studien zu ermöglichen. Hierbei erwies sich der Ansatz von einer prosopographischen Schnittstelle als sinnvoll – Personendaten 
profitieren von der Interoperabilität, die automatisierte Extraktion von Personenrelationen ermöglicht. (cf. Vogeler 2019).
            
            Als ein Ergebnis der Datenanalyse konnte zum Beispiel festgestellt werden, dass ein Akteur immer wieder an zentraler Stelle auftauchte: Carl Hagemann (1871-1945), der vor allem in den 1910er und 1920er Jahren als Intendant in Mannheim, Hamburg, Wiesbaden und beim frisch gegründeten Rundfunk in Berlin sowohl als Regisseur wie auch als Organisator tätig war. Darüber hinaus hat Hagemann in seiner gesamten Laufbahn intensiv als Autor gewirkt. Da sich anhand dieses Akteurs die komplexen Netzwerke der Theaterschaffenden besonders eindrücklich zeigen lassen, wurden die Bezüge zu seiner Theaterarbeit bei den zu digitalisierenden Objekten in den Nachlässen bevorzugt berücksichtigt. Hagemann repräsentiert in diesem Sinne idealtypisch auch jene im Antrag in den Blick genommenen biographischen Verläufe, die unterschiedliche historische Zäsuren überspannen.
            Es lässt sich festhalten, dass es durch die Daten- und Prozessmodellierung realisierbar ist, aus den Objektdaten Datensätze zu extrahieren, die unterschiedlich modelliert und anders definiert sind - wie beispielsweise Akteure oder Ereignisse. Auf diese Weise agieren sie nicht mehr objektgebunden, stehen mit ihnen aber noch immer in engen relationalen Beziehungen. Hiermit können die Datensätze sowohl komplexe Interaktionsnetzwerke zwischen Entitäten abbilden als auch unabhängig vom intendierten Zweck alleinstehend verwendet werden. Mit dieser Aufbereitung wird eine Datenbasis generiert, die als Grundlage für eine Forschungsumgebung dient. Im Laufe des Projektes wurden bisher 1217 Objekte erfasst. Mit Hilfe dieser Objekten sind 3198 Akteur_innen und 290 Ereignisse erschlossen worden. Diese Basis erlaubt zum einen die theaterwissenschaftliche Neuperspektivierung der Bestände, zum anderen kann sie durch die generierten Netzwerke als Grundlage einer fachwissenschaftlichen Neukonfiguration der zentralen Epochen von Theater- und Kulturgeschichte der Moderne genutzt werden.
         
      
      
         
            
    Von dem Arbeitsbuch “100 Jahre Theaterwissenschaftliche Sammlung Köln Dokumente, Pläne, Traumreste” kann ein Überblick über die Forschung an theaterwissenschaftlich relevanten Objekten und deren Kontexte gewonnen werden (Marx, 2019)
            
      Der Begriff “Akteur_in” wird hier weit gefasst und bezieht sich sowohl auf natürliche Personen als auch auf Körperschaften.
    
            siehe Stand von LIDO Terminology unter: 
    .
    
            
               
            
            
               , https://www.ausstage.edu.au/
            
            
               
            
            siehe 
    https://www.tei-c.org/release/doc/tei-p5-doc/de/html/ref-person.html
            
            
               siehe 
               https://www.www.deutsche-biographie.de,
      für API siehe 
            
            
      siehe auch 
            
         
         
            
               Bibliographie
               
                  Barabucci, Gioele  (2018): 
	Funktionale und deklarative Programmierung-basierte Methode für nachhaltige, reproduzierbare und verifizierbare Datenkuration. DHd Konferenz 2018: Kritik der digitalen Vernunft, 214–219, doi: 
	  [letzter Zugriff 10. September 2019].
      
               
                  Bogatzki, Jakob (2019): »(Re-)Collecting Theatre History. Neue Perspektiven biografischer und theaterhistoriografischer Forschung«, in: 
	Die vierte Wand, Heft 9, S. 26–29.
      
               
                  Deutsche Biographie. Hauptseite, 
	https://www.deutsche-biographie.de [letzter Zugriff 10. September 2019]
      
               
                  Deutsche Biographie. Services, 
	  [letzter Zugriff 10. September 2019]
      
               
                  Illmayer, Klaus (2017): „Aufbau einer digitalen Infrastruktur für Theaterwissenschaft“ (Abstract),
	  [letzter Zugriff 7. September 2019].
      
               
                  Leonhardt, Nic (2014): 
	Digital Humanities and the Performing Arts: Building Communities, Creating Knowledge.
	Keynote auf der SIBMAS/TLA Konferenz, New York (NY), 12. Juni 2014,
	 [letzter Zugriff 3. August 2019].
      
               
                  LIDO Working Group. What is LIDO, 
	
	[letzter Zugriff 10. September 2019].
      
               
                  LIDO Working Group. LIDO Terminology, 
	
	[letzter Zugriff 10. September 2019].
      
               
                   Marx, Peter W., Hrsg.  (2019): 100 Jahre Theaterwissenschaftliche Sammlung Köln. Dokumente, Pläne, Traumreste. Berlin: Alexander Verlag.
      
               
                   Probst, Nora / Pinto, Vito: Re-Collecting Theatre History. Theaterhistoriografische Nachlassforschung mit Verfahren der Digital Humanities. In: Wihstutz, Benjamin; Hoesch, Benjamin (Hg.): Neue Methoden der Theaterwissenschaft (Druck in Vorbereitung).
      
               
                  Vogeler, Georg / Vasold, Gunter / Schlögl, Matthias  (2019): 
	Von IIIF zu IPIF? Ein Vorschlag für den Datenaustausch über Personen. In: Sahle, Patrick (Hg.): DHd 2019 Digital Humanities: multimedial & multimodal. Konferenzabstracts. Frankfurt / Mainz. DHd. 2019 DOI: 
	10.5281/zenodo.2600812. pp. 239-241. 
      
            
         
      
   



      
         Im Sommer 2019 konnte die Historische Kommission bei der Bayerischen Akademie der Wissenschaften ihr 2014 begonnenes Pilotprojekt für ein neues Konzept historisch-kritischer Editionsarbeit zu einem erfolgreichen Abschluss bringen. Mit dem neunten Band der Reihe „Protokolle des Bayerischen Ministerrats 1945–1962“, bearbeitet von Dr. Oliver Braun (München), entstand die erste Edition der Historischen Kommission auf XML-Basis. Das neue Konzept ermöglicht es, ohne großen Zusatzaufwand, einen vollwertigen gedruckten Band und eine digitale Version mit zahlreichen Such- und Verlinkungsfeatures herzustellen. Vor allem aber ist es darauf ausgelegt, dass die Bearbeiter und Bearbeiterinnen ihren Editionstext in vollwertigem TEI-XML herstellen können, ohne technische Vorkenntnisse zu benötigen. Dank einer speziell eingerichteten Arbeitsumgebung im Programm                Oxygen XML Editor
             können sie ihre bisher gewohnte Arbeitsweise weitgehend beibehalten. Unser Vortrag möchte dieses neue Editionskonzept und die Arbeitsumgebung vorstellen sowie von den Erfahrungen berichten, die wir bei der Herstellung des Bands 9 der Bayerischen Ministerratsprotokolle gemacht haben.
            
         Die Edition „Protokolle des Bayerischen Ministerrats 1945–1962“, die seit Anfang der 1990er Jahre
                , dank Förderung durch den Freistaat Bayern, bei der Historischen Kommission entsteht, ist eines der wichtigsten Forschungsprojekte zur bayerischen Zeitgeschichte. In bisher acht Bänden, die die Jahre 1945 bis 1951 abdecken, dokumentieren die ausführlichen Gesprächsprotokolle der Ministerratssitzungen das Handeln der Bayerischen Staatsregierung. Sie geben Einblick in die Fragen und Herausforderungen der Nachkriegszeit in Bayern, erst unter amerikanischer Kontrolle, dann in der noch jungen Bundesrepublik. Deutlich lassen sich aus Ihnen die Kontroversen und die teilweise heftig geführten Diskussionen zwischen den Kabinettsmitgliedern herauslesen. Nicht selten gleichen dabei die vor 70 Jahren geführten Debatten zu Themen wie Flüchtlingen, Verkehrspolitik und Bildungswesen verblüffend den aktuellen politischen Auseinandersetzungen in Bayern und Deutschland. Die ersten acht Bände der Edition (bis 1952) stehen im Volltext retrodigitalisiert auch online zur Verfügung.
         
         
            
            Abbildung 1: Ein Protokoll aus der Edition in seiner XML-Grundform, …
         
         Die Arbeiten an dem hier vorgestellten Editionskonzept begannen im Jahr 2014. Ausgangsbasis war damals das Projekt 
                ediarum
             der Berlin-Brandenburgischen Akademie der Wissenschaften (Dumont/Fechner 2015). Während es Inspiration und eine wertvolle technische Grundlage lieferte, ergaben sich rasch Anforderungen, die mit dem damals verfügbaren Werkzeugen von 
                ediarum nicht vollständig zu erfüllen waren. Und so begannen Matthias Reinert und Maximilian Schrott (beide München), Mitarbeiter in der Abteilung „Digitale Publikationen“ mit der Entwicklung eines eigenen Ansatzes. Dessen Kerngedanke ist es, dass der Bearbeiter möglichst gar keine Berührungspunkte mit der XML-Syntax der Dokumente hat. Stattdessen soll er sich voll auf die Arbeit am Editionstext konzentrieren können. Dennoch legt Herr Dr. Braun teils komplexe Strukturen auf Basis des TEI Lite Schemas in den Protokolltext an und baut während der gesamten Editionsarbeit interne und externe Verknüpfungen in den Text ein. So wird das Projekt mit Metainformationen angereicht, die vor allem bei der digitalen Veröffentlichung im Internet einen Mehrwert bringen. Entscheidend unterstützt wird er dabei durch den 
                Oxygen XML Editor. Dieses leistungsstarke XML-Bearbeitungsprogramm, bietet die Möglichkeit eine stark individualisierte Arbeitsumgebung einzurichten. 
            
         Die wichtigsten Bestandteile dieser Arbeitsumgebung für unser Editionsprojekt sind Operationen und Stile. Die Operationen sind eine Reihe von vorbereiten Funktionen, die eine Eingabe des Bearbeiters entgegen nehmen, daraus XML-Fragmente generieren und in den Text einfügen. Sie werden entweder aus Standardoperationen, die der 
                Oxygen XML Editor bietet, konfiguriert oder können selbst über eine Java-Schnittstelle programmiert werden. Herr Dr. Braun löst sie nach Bedarf über Buttons in der Menüleiste des Editors aus. Die Operationen ermöglichen es ihm mit wenigen Mausklicks und Eingaben in Textmasken komplexe Strukturelemente wie den Protokollkopf einzufügen, den Protokolltext in Tagesordnungspunkte und Unterpunkte zu gliedern oder Verweise auf andere Abschnitte und Fußnoten innerhalb des Editionskorpus zu setzen. Auf diese Weise kann Herr Dr. Braun das XML seiner Editionsdokumente bequem und ohne Angst vor lästigen Flüchtigkeitsfehlern bearbeiten und das mit minimalem Einarbeitungsaufwand.
            
         Die Stile werden über Cascading Style Sheets (CSS) eingerichtet. Mit diesen kann die Darstellung der XML-Dokumente in 
                Oxygen XML Editor weitreichend angepasst werden. Für die Bayerischen Ministerratsprotokolle fiel der Entschluss, das XML-Markup, das oft als störend empfunden wird, größtenteils auszublenden und nur den reinen Editionstext darzustellen. Die semantischen Strukturen werden stattdessen über Formatierung, Textsetzung und farbliche Markierungen kenntlich gemacht. Herr Dr. Braun kann zwischen mehreren hinterlegten Stilen wechseln, die jeweils einen bestimmten Schritt des Workflows besonders unterstützen.
            
         
            
            Abbildung 2: … in der Darstellung der Arbeitsumgebung …
         
         Zusammengenommen ermöglichen Operationen und Stile ein bequemes und produktives Arbeiten in XML. Durch das vollständige Verlagern der Syntax in den Hintergrund, sinkt die Einstiegshürde für XML-unkundige Bearbeiter. Gleichzeitig ist die Arbeit schneller, bequemer und weniger fehleranfällig, als wenn das Markup von Hand eingegeben werden müsste. Und der Blick des Bearbeiters auf den Text wird nicht durch unübersichtliche XML-Element-Konstruktionen versperrt.
         Eine neu hinzugekommene Aufgabe für Herrn Dr. Braun während der Editionsarbeit ist die Markierung der registerrelevanten Entitäten (Personen, Orte, Schlagwörter, Gesetze, Quellen und Literaturtitel) im Text. Die systematische Referenzierung dieser Textstellen erhöht den Grad der thematischen Verknüpfung innerhalb der Edition enorm und ermöglicht ein hohes Maß an Erschließ- und Durchsuchbarkeit für die digitale Präsentation. Kernstück dieser Verknüpfungsarbeit ist die Registerdatenbank (RDB). In diese trägt der Bearbeiter sämtliche registerrelevanten Entitäten im TEI-Format ein, ebenfalls unterstützt durch die Arbeitsumgebung. Da Herr Dr. Braun zwar an verschiedenen Orten aber alleine an seinen Editionstexten arbeitet, schien es zunächst unnötig die RDB als 
                eXist-Datenbank einzurichten. Stattdessen handelt es sich um eine Reihe von einfachen XML-Dateien, die jeweils einen einzelnen Entitätstypen beinhalten. Diese Lösung spart Wartungsaufwand und vermeidet einen Onlinezwang während der Arbeit, ohne dass sich dadurch ein merkbarer Nachteil ergab. Die Sicherung der RDB-Dateien, wie auch der Editionsdokumente, erfolgt stattdessen über 
                Sync+Share
            , das Cloud-Speicherangebot des Leibniz-Rechenzentrums. Dieses sorgt gleichzeitig für den Austausch der Arbeitsdateien mit den technischen Betreuern.
            
         Vom Text aus kann Herr Dr. Braun über entsprechende Operationen den jeweils benötigten RDB-Eintrag heraussuchen und eine Referenz auf diesen hinterlegen. Zusätzlich zu den Registerinformationen wie Personen- und Ortsname kann die RDB auch weiterführende Daten zu ihren Einträgen speichern. Zum Beispiel die geographischen Koordinaten eines Ortes, Normdaten-Identifier (GND) oder biographische Informationen zu den Personen. Diese können während der Bearbeitung und später in der digitalen Präsentation genutzt werden, um zusätzlichen Funktionen zu realisieren.
            
         Während sich Herr Dr. Braun auf seine Editionsarbeit konzentrieren kann, kümmert sich die Abteilung Digitale Publikationen der Historischen Kommission um die technische Umsetzung. Zu Projektbeginn prüfte sie, in Absprachen mit dem Bearbeiter, die Anforderungen des Editionsprojekts. Auf dieser Basis wurde dann entschieden, welche Phänomene des Editionstextes in XML kodiert werden mussten und wie. Aus diesen Überlegungen entstanden schließlich die digitalen Editionsrichtlinien. Anhand dieser richtete Maximilian Schrott dann die Editionsumgebung in 
                Oxygen XML Editor ein, designte die Stile und programmierte die Operationen. Nach dieser Erstkonfiguration leistete er Herrn Dr. Braun während des gesamten Bearbeitungszeitraums fortlaufend Support. Er verbesserte Fehler in der Arbeitsumgebung, entwickelte diese auf Basis seines Feedbacks weiter und überwachte die korrekte Form der Editionsdokumente. Zum Ende der Editionsarbeit hin übernahm die Abteilung Digitale Publikation schließlich die Umwandlung des Editionstextes in die gedruckte Form.
            
         
            
            Abbildung 3: … und in der finalen PDF-Druckfassung.
         
         Denn auch wenn die Veröffentlichung einer hochwertigen, digitalen Version im Fokus des Editionskonzepts steht, sollte auch der neunte Band der Bayerischen Ministerratsprotokolle noch gedruckt erscheinen. Die PDF-Vorlage für den Druck wird dabei direkt aus den erstellten XML-Dateien erzeugt. Als Satzprogramm dient das in den 
                Oxygen XML Editor integrierte 
                Apache-FOP
            . In dieses werden die Editionsdateien mittels einer XSL-Transformation überführt. So lässt sich in weniger als einer Minute eine vollständige Druckversion des kompletten 1000seitigen Druckbandes im gewünschten Layout erstellen. Der finale Satz erfolgt somit komplett intern, ohne Zuarbeiten durch den Verlag. Der automatisierte Satz funktioniert insgesamt sehr gut, es bleiben aber vereinzelte Mängel. Vor allem Zeilen-, Spalten- und Seitenumbruch müssen an einigen Stellen manuell nachgearbeitet werden. Auch bei der Erstellung der gedruckten Version ergeben sich durch die Anreicherungsarbeiten in XML Vorteile. So kann durch die wortgenaue Kennzeichnung der registerrelevanten Entitäten und die Verknüpfung mit der RDB auch das Druckregister halbautomatisch erstellt und dem Bearbeiter so eine besonders mühsame Arbeit erspart werden.
            
         Das abgeschlossen Pilotprojekt wird von Seiten der Historischen Kommission als sehr erfolgreich eingestuft. Die Arbeit im 
                Oxygen XML Editor wurde von Herrn Dr. Braun als angenehm empfunden, die Drucklegung wurde pünktlich abgeschlossen und es konnten wertvolle Erfahrungen zur Verbesserung der Arbeitsumgebung und des Workflows gesammelt werden. Der Komfort für den Bearbeiter und die Qualität der Ergebnisse wird freilich durch eine vergleichsweise hohen Einrichtungs- und Supportaufwand erkauft. Diese Mehrarbeiten werden aber von der technischen Betreuung geleistet. Eine zusätzliche Belastung für den wissenschaftlichen Editor wird weitestgehend vermieden. Die Veränderungen in seinem Workflow sind zwar merkbar, aber nicht einschneidend. In der jetzigen Form funktioniert die Zusammenarbeit zwischen Bearbeiter und Technikern für beide Seiten sehr fruchtbringend. Außerdem sind die gewonnen Erfahrungen und Programmierarbeiten zum größten Teil für andere Projekte wiederverwendbar. Die Historische Kommission sieht sich deshalb in ihrem Entschluss bestätigt, zukünftig verstärkt auf das digitale, XML-basierte Editionskonzept zu setzen. Neben den noch ausstehenden Bänden der Bayerischen Ministerratsprotokolle sind bereits drei Briefeditionsprojekte in Arbeit, die auf diesem Ansatz beruhen.  Weitere Projekte befinden sich in der Vorbereitungs- und Planungsphase. 
            
         Der neunte Band der Bayerischen Ministerratsprotokolle ist im Oktober 2019 gedruckt erschienen. Die Veröffentlichung im Internet wird sich noch bis voraussichtlich 2022 verzögern. Dann endet mit dem Erscheinen des 10. Bandes, die mit dem Verlag vereinbarte Exklusivitätsperiode im Druck. Der Inhalt von Band 9 und der RDB können somit zum bestehenden Webangebot der Protokolle des Bayerischen Ministerrats hinzugefügt werden. Ein größeres Funktionsupdate für die Website, das hiermit einhergehen soll, befindet sich derzeit noch in der Konzeptionsphase. Es soll das Webangebot um zahlreiche neue Features erweitern, die die Anreicherungen und Vernetzungen im Editionstext für die User zugänglich macht. Noch geprüft wird, welche Verlinkungsmöglichkeiten zu externen Webangeboten und Normdatenbanken umgesetzt werden können. Außerdem gibt es Überlegungen für spezialisierte Recherche- und Visualisierungsmöglichkeiten, zum Bespiel mittels der in der RDB erfassten Orte auf Karten.
            
      
      
         
            
      Das Kabinett Ehard III 18.Dezember 1950 bis 14. Dezember 1954, Band 3 8.1.1953–29.12.1953. bearb. von Oliver Braun, München [2019] (= Die Protokolle des Bayerischen Ministerrats. 1945-1962, Hg. von der Historischen Kommission bei der Bayerischen Akademie der Wissenschaften durch Andreas Wirsching und von der Generaldirektion der Staatlichen Archive durch Margit Ksoll-Marcon, Band 9).
    
            
               http://www.tei-c.org ; alle URLs in diesem Artikel wurden zuletzt am 25.09.2019 aufgerufen.
    
            
               http://oxygenxml.com
            
            
      Die Protokolle des Bayerischen Ministerrats 1945-1962 [ehemals 1945-1954], Hg. von der Historischen Kommission bei der Bayerischen Akademie der Wissenschaften und der Generaldirektion der Staatlichen Archive Bayerns, 9 Bände, bearbeitet von Karl-Ulrich Gelberg [Bände 1-5] und Oliver Braun [Bände 6-9], 1995-2019.
    
            
               www.bayerischer-ministerrat.de
            
            
               http://www.bbaw.de/telota/software/ediarum
            
            
               https://exist-db.org
            
            
               https://syncandshare.lrz.de
            
            
               https://www.dnb.de/DE/Professionell/Standardisierung/GND/gnd_node.html
            
            
               https://xmlgraphics.apache.org/fop/
            
            
      Die wissenschaftliche Korrespondenz des Historikers Karl Hegel (1813-1901), bearbeitet von Marion Kreis; Der Briefwechsel zwischen Adolf Harnack und Friedrich Althoff (1886-1908), bearbeitet von Claudia Kampmann; Zwischen Wissenschaft und Politik. Hans Delbrück – Ausgewählte Korrespondenz (1868-1929), bearbeitet von Andreas Rose und Jonas Klein.
    
            
      Siehe Anm. 5.
      
         
         
            
               Bibliographie
               
                  Dumont, Stefan / Fechner, Martin (2012): „Digitale Arbeitsumgebung für das Editionvorhaben ‚Schleiermacher in Berlin 1808–1834‘“ http://digiversity.net/2012/digitale-arbeitsumgebung-fur-das-editionsvorhaben-schleiermacher-in-berlin-1808-1834 [letzter Zugriff 25.09.2019].
                    
               
                  Dumont Stefan / Fechner Martin (2015): „Bridging the Gap: Greater Usability for TEI encoding, in: Journal of the Text Encoding Initiative 8“ http://jtei.revues.org/1242 [letzter Zugriff 25.09.2019].
                    
            
                  
         
      
   



      
         Die Entwicklungen im Bereich der digitalen Musikedition haben seit ihrer Entstehung eine Vielzahl von Projekten initiiert. Durch die Möglichkeit der Codierung und der mehrdimensionalen Darstellung musikbezogener Inhalte (vgl. Wiering 2009) konnte wesentlich zur Überwindung der Vorstellung von musikalischen Werken 
                einer festen Gestalt verholfen,und das intersektionale Arbeiten der Digital Humanities in den Musikwissenschaften verankert werden.
            
         Das Potenzial digitaler Musikedition – so zeigt es das hier vorgestellte, im 
                Zentrum Musik – Edition – Medien angesiedelte und Ende 2019 abzuschließende Dissertationsprojekt – erschöpft sich jedoch nicht an der Bearbeitung des Werk-Faktors und des notenschriftbasierten Quellenmaterials. Digitale Edition eröffnet durch ihr Potenzial der tiefen und mehrdimensionalen Erschließung eines Gegenstandes auch das Potenzial, den zu edierenden musikbezogenen Gegenstand auszuweiten. Sie suggeriert somit die Möglichkeit, Musik nicht alleine mit Bezug auf ihren logischen Inhalt zu erschließen und dessen editorische Darstellung durch die optische und akustische Domäne musikbezogener Quellen zu flankieren, sondern Musik im Sinne 
                gelebter Wirklichkeiten zu repräsentieren, in Musik also auch im editorischen Sinne mehr zu sehen als Notentext. 
                Digitale Musikedition eröffnet im Sinne der Digital Humanities somit ein Erkenntnispotenzial, das es ermöglicht, aus editorischer Sicht die grundlegende Frage zu stellen, was Musik ist.
            
         Die Arbeit zeigt dabei, dass der Versuch, Musikedition mit digitalen Mitteln über den Notentext hinaus auszuweiten, Erkenntnis über den Gegenstand „Musik“ offenbart und geht von der kulturwissenschaftlich inspirierten Prämisse aus, dass Musik ein vom Handeln geprägtes Ereignis ist. Am Beispiel einer dichten Beschreibung eines Ausschnitts einer Konzert-Aufzeichnung des Sängers Marius Müller-Westernhagen, wird die Vielfalt des Komplexes „Musik“ verdeutlicht und der Frage nachgegangen, auf welcher entitätenbezogenen Basis dieses musikbezogene Handeln in editorische Kontexte integrierbar ist, um nicht nur digitale Notenedition, sondern digitale Musikedition im umfassendsten Sinne zu betreiben – als dichte Beschreibung mit digitalen Mitteln. Neben der Beleuchtung bisheriger musikwissenschaftlicher Editionspraxis und damit verbundener Prinzipien, gilt es, das Wesen digitaler Notenedition vorzustellen, um zunächst zu verdeutlichen, dass diese unter der Nutzung der xml-basierten MEI- und TEI-Standards weitgehend die Prinzipien traditioneller Notenedition in das Digitale transferiert hat und qua der Struktur des Codes an der Edition von Meisterwerken festhält. Kulturwissenschaftliche Erkenntnisse (wie die Bedeutung musikbezogener Handlungen) sind hier kaum in editorischen Kontexten wiederzufinden oder in diese integrierbar. Diese Arbeit verdeutlicht durch experimentelle Anreicherung einer MEI-Codierung die Notwendigkeit der grundsätzlichen ontologischen Erschließung des (handlungsbezogenen) Gegenstands „Musik“ sowie die Notwendigkeit des grundsätzlichen Lösens vom bisherigen werkbezogenen Blickwinkel.
         Bestehende Projekte entwickeln bereits vielfältige, durch digitale Techniken ermöglichte Insellösungen, die damit beginnen, die Betrachtung des Komplexes „Musik“ auszuweiten. Doch der Faktor des Werkes scheint hier ein schwer zu überwindendes Hindernis. Um in diesem Kontext die (auch editorische) Betrachtung von Musik in einen größeren Zusammenhang zu stellen, frage ich, was Musik ist und stelle im Zusammenhang mit Christopher Smalls Konzept des 
                Musicking einen handlungsbezogenen Musikbegriff vor. (Small
                 1998) Als Verifizierung seiner These und zur Überbrückung von in seiner Arbeit vorzufindenden Defiziten, wird der Begriff des Musicking zunächst ontologisch differenziert. Das Musicking kann somit auf der Basis von fünf grundlegenden Musicking-Entitäten – Akteur, Ding, Ereignis, Text, Raum – präzisiert werden. Diese werden als ontologische Basis einer Musikedition vorgeschlagen, die den Status von Musik als Handeln anerkennt und widerspiegelt. Der Begriff der Musikedition wird dabei präzisiert und vom Komplex der Noten- oder Werkedition unterschieden. Das Projekt verdeutlicht so die Notwendigkeit, diesen Ansatz als Ontologie des Musicking weiter auszubauen, um Musik mit digitalen Mitteln einer „wirklichen“ Musikedition zuzuführen und – im Sinne der Digital Humanities als „intersection“ (vgl. Nyhan/Flinn 2016:1.) – Edition als digitale kulturwissenschaftliche Edition zu betreiben. Bestehende, in Insellösungen manifestierte Bestrebungen zu Edition, Codierung und Erforschung musikbezogener Kontexte können durch das vorgeschlagene Prinzip aufgegriffen werden, welches mittels einer „Partitur des Musicking“ u.a. mit Techniken der Graphenvisualisierung einen editorischen Rahmen für alle bisher durchgeführten Konzepte vorschlägt.
            
      
      
         
            
               Bibliographie
               
                  Babbitt, Milton (1965): „The Use of Computers in Musicological Research“, in: 
  Perspectives of New Music 3/2 74–83.

               
                  Grotjahn, Rebecca / Iffland, 
                  Joachim (2018): „Digitale Musikedition und die Wissenschaft der Populären Musik“, in: 
  Die Musikforschung 71/IV 379–393.

               
                  Iffland, Joachim (2018): „Materialität und Schriftlichkeit“, in: 
  Zentrum Musik – Edition – Medien  https://zenmem.de/confluence/pages/viewpage.action?pageId=33718295 [letzter Abruf 19. September 2019).

               
                  Kaden, Christian (2004): 
  Das Unerhörte und das Unhörbare. Was Musik ist, was Musik sein kann, Kassel/Stuttgart.

               
                  Kepper, Johannes (2011): 
  Musikedition im Zeichen neuer Medien. Historische Entwicklung und gegenwärtige Perspektiven musikalischer Gesamtausgaben, Diss., Paderborn 2009, Norderstedt.

               
                  Kepper, Johannes / 
                  Pugin, 
                  Laurent (2017): „Was ist eine Digitale Edition? Versuch einer Positionsbestimmung zum Stand der Musikphilologie im Jahr 2017“, in: 
  Musiktheorie 32/4 347–363.

               
                  McCarty, Willard (2003): „Humanities Computing“, in: 
  Encyclopedia of Library and Information Science, DOI: 10.1081/E-ELIS 120008491, New York 1224– 1235.

               
                  McCarty, Willard (2016): „Becoming Interdisciplinary“, in: 
  Schreibman, Susan / Siemens,   Ray / Unsworth, John (eds.): 
  A New Companion to Digita l Humanities, Chichester 69–83.

               
                  Nyhan, Julianne / 
                  Flinn,
                   Andrew (2016): 
  Computation and the Humanities. Towards an Oral History of Digital Humanities, Cham. 

               
                  Small, Christopher (1998): 
  Musicking. The Meanings of Performing and Listening (Music/Culture), Middletown.

               
                  Veit, Joachim (2015): „Musikedition 2.0. Das ‚Aus‘ für den edierten Notentext?“, in: 
  editio 
                  29/1 187–197.

               
                  Veit, Joachim / 
                  Richts,
                   Kristina (2018): „Stand und Perspektiven der Nutzung von MEI in der Musikwissenschaft und in Bibliotheken“, in: 
Bibliothek – Forschung und Praxis 42/2 292–301.

               
                  Walser, Robert (2016): 
  The Christopher Small Reader (Music/Culture), Middletown.

               
                  Wiering, Frans (2009): „Digital Critical Editions of Music: A Multidimensional Model“, in: 
  Crawford, 
                  Tim / Gibson, 
                  Lorna: 
                        Modern Methods for Musicology. Prospects, Proposals, and Realities, Farnham 23–45.

            
         
      
   



      
         Kochtraditionen, ob regional oder international, sind eine der herausragendsten Elemente der europäischen Kultur und ein wichtiger Bestandteil der europäischen Identität. Aber die Fragen nach ihrem Ursprung, den Einflüssen und ihrer Entwicklung sind nach wie vor unklar. In den letzten Jahrzehnten kam die Forschung zu zwei wichtigen Schlussfolgerungen, welche mittlerweile die Forschungsbestrebungen prägen: Erstens gibt es keine quantitativen Studien über den Ursprung und die Entstehung der regionalen Küche in Europa; zweitens sind erst ab dem Mittelalter Handschriften mit Tausenden von Kochrezepten überliefert, was wohl als die Geburt der modernen europäischen Küche angesehen werden kann (vgl. Flandrin & Hyman 1988, Laurioux 2005). Auf dem europäischen Kontinent stellen frühneuhochdeutsche, mittelfranzösische und mittellateinische Rezepte den größten Teil der kulinarischen Überlieferung dar, die mehr als 80 Manuskripte und etwa 8000 Rezepte umfasst. Das Projekt “Cooking Recipes of the Middle Ages” bereitet die Kochrezeptüberlieferung von Frankreich und dem deutschsprachigen Raum auf, um ihre Herkunft, ihre Beziehung und ihre Migration innerhalb Europas zu analysieren (vgl. thematisch ähnliche Studien mit unterschiedlicher Fokussetzung: Hieatt 1995 mit linguistischem Fokus, Flandrin 1984, Adamson 1995, Carlin 1989, Adamson 2002, Karg 2007 allgemein und van Winter 1989, Hyman 2005, Laurioux 2002 mit Fokus auf spezielle Gerichte). Die Partner, das Zentrum für Informationsmodellierung der Universität Graz und das Laboratoire CESR (Centre d’Etudes Supérieures de la Renaissance) der Universität Tours werden diese mehrsprachigen Texte mit Hilfe von digitalen Standards aufarbeiten und sie mit aktuellen quantitativen und qualitativen Forschungsmethoden untersuchen. Der Vergleich der französischen und deutschen Ernährungsgeschichte eignet sich besonders gut für diese Aufgabe, da Frankreich einen kulturell prägenden Einfluss auf die deutschsprachigen Völker hatte. 
         Kochrezepte sind kulturell aufgeladene, flüchtige Texte; die erhaltenen Niederschriften stellen daher nur ein punktuelles Zeugnis, eine individuelle Zubereitungsweise eines Gerichts (Hieatt 1985, 26) in Raum und Zeit dar. Das inhaltliche Verständnis dieser Rezepte, ihre möglichen Entstehungs- und Anwendungskontexte und ihre Überlieferung ist zudem kein einfacher Prozess, denn die Fachbegriffe, Zutaten, Utensilien, Verfahren und Bräuche der damaligen Zeit, die in den Rezepten eher öfter implizit als direkt genannt werden, sind auch für Sprach- und Geschichtswissenschaftler, die sich auf das Thema spezialisiert haben, immer wieder eine Herausforderung. Ihre Entwicklung sollte daher am besten diachron und räumlich analysiert werden, was mittlerweile mit digitalen geisteswissenschaftlichen Methoden verhältnismäßig leicht möglich ist – vorausgesetzt, die entsprechenden Daten liegen vor. Im aktuellen Projekt werden die historischen Texte auf mehreren Ebenen erschlossen: So werden die Texte nicht nur neu transkribiert und philologisch-editorisch bearbeitet (vgl. Klug, Kranich 2015), sondern auch in unterschiedlichen Wissensgebieten semantisch angereichert. Das schafft jene Spielräume, die nötig sind, um Analysen wie maschinengestützte Abgleiche von Zutaten, Kochprozessen oder Kochutensilien, die Suche nach Rezepttradition und -migration oder standardisierte philologische Vergleiche, wie z.B. Kollationierungen, durchzuführen. Die Basis unserer Daten sind customized TEI/XML-Dokumente mit einem zusätzlichen adaptierten Schema, das die semantische Annotation von Kochrezepten im Allgemeinen erleichtern soll. 
         Die Rezeptüberlieferung – in Form einzelner Rezeptsammlungen – wird mithilfe einer , die sich an den Handschriftenbeschreibungen renommierter Bibliotheken orientiert und die in Kooperation mit der Abteilung für Sondersammlungen der Grazer Universitätsbibliothek entstanden ist, räumlich und zeitlich fixiert. Besonderes Augenmerk wird dabei auf Informationen zur Handschriftenentstehung (materiell wie auch inhaltlich) und auf die Schriftbeschreibung bzw. den Schreibhandbefund der Rezeptsammlungen gelegt, wobei erstere Informationen meist den Katalogen entstammen, letztere aus der Arbeit mit den Texten kommen. Die Grundlage des Projekts ist die einheitliche Erfassung der überlieferten Texte durch eine hyperdiplomatische Neutranskription der historischen Quellen: Als Arbeitsumgebung fungiert Transkribus, wo das Textlayout automatisch erkannt und die Texte manuell mittels proprietären Codierungen erfasst werden. Mithilfe mehrerer Transformationsschritte wird aus den Rohdaten die Basis für die elektronische Quellenabbildung erstellt, die sich an germanistisch-editorischen Richtlinien orientiert. Die Quellentexttranskription verzeichnet dabei nicht nur das unterschiedliche Schriftzeicheninventar, sondern auch alle textstrukturierenden Elemente. Das gesamte Zeicheninventar ist in einer nach den Richtlinien der TEI erstellten Zeichenbeschreibung erfasst. Die Beschreibung stützt sich dabei auf die theoretischen Ergebnisse zur Beschreibung von Zeichen aus dem DigiPal-Projekt und verwendet außerdem die Zeichenidentifikatoren der Medieval Unicode Font Initiative (vgl. Böhm, Klug 2020). Die so produzierten Daten sind nicht nur der Ausgangspunkt für die wissenschaftlichen Fragestellungen im Projekt, sondern bieten eine solide Grundlage für viele weitere Forschungsfragen aus Germanistik/Linguistik, Paläographie usw. Diese Erarbeitungsstufe wird nach editorischen Richtlinien normalisiert und gibt im Rahmen des Webauftritts in Form einer Text-Bild-Synopse detaillierten Einblick in die historische Quelle.
            
         Aus den Transkriptionsdaten wird außerdem eine auf Zeichenebene normalisierte, in Sinneinheiten untergliederte Textfassung geschaffen, in der die semantischen Informationen annotiert werden. Diese Sinneinheiten umfassen neben dem eigentlichen Rezept und Rezepttitel Eingangs- und Schlussformeln, Handlungsanweisungen, Küchen- und Serviertipps, Hinweise auf medizinische und religiöse Aspekte und selbstverständlich Zutaten, Gerichte und Küchenutensilien. 
         Den Kern der digitalen Forschungsstrategie bildet das Semantic Web beziehungsweise die Anbindung und Integration unserer Daten an Linked Open Data. Wir sind innerhalb der Geisteswissenschaften in der vorteilhaften Position, dass sich unser Projekt zu einem großen Teil mit Lebensmittelzutaten befasst, d.h. mit Tieren, Pflanzen und Pilzen. Das sind Forschungsgebiete, in denen sich bereits eine signifikante Menge an relevanten Ontologien etabliert haben und die gut an die Linked Open Data Cloud, einschließlich der allgemeinen Wissensdatenbanken Wikidata und DBPedia angeschlossen sind. Ontologien werden, wenn auch mit unterschiedlichen Schwerpunkten und Granularität der Daten, außerdem bereits erfolgreich für die Repräsentation von Kochrezepten (Hoehndorf & Lange 2018, Sam et al. 2014, Ribeiro et al. 2006) und in deren Analyse eingesetzt (Chow & Grüniger 2019, Jovanovic et al. 2015, Vadivu & Waheeta Hopper 2010). In unserem digitalen Forschungsansatz setzen wir zwar teilweise auch auf Textähnlichkeiten, der größte Teil unserer Analyse basiert jedoch auf dem Vorkommen von Zutaten, Kochprozessen bzw. Zubereitungshinweisen und Kochutensilien. Weitere Entitäten, die wir für die Analyse der Rezepte heranziehen sind Serviervorschläge sowie medizinische, kulturelle und religiöse Aspekte in den Texten. Die Annotation dieser Entitäten gestaltet sich schon aufgrund ihrer schieren Menge in historischen Kochrezepten als sehr komplex. Neben den Möglichkeiten zuvor unbekannte Beziehungen zwischen den Quellen und deren Entitäten zu finden, war das Arbeiten außerhalb von Sprachbarrieren ein Hauptargument für die Entscheidung, Semantic Web-Technologien in den Mittelpunkt des Projekts zu stellen. 
            
         Durch die Verwendung von 
                Konzepten, im Sinne einer Idee oder eines mentalen Bildes und nicht eines 
                Begriffes, versuchen wir, historische und sprachliche Grenzen zu überwinden. Ein konkretes Beispiel für diese Diskrepanz zwischen Begriff und mentaler Vorstellung liefert uns die österreichische / süddeutsche Variante für Kartoffel: "Erdapfel" ("erdaphel" im Frühneuhochdeutschen) wird etwa in einem Manuskript aus der Zeit um 1488 erwähnt, lange bevor die Kartoffel von Südamerika nach Europa importiert wurde, was uns zeigt, dass das Konzept von "Erdapfel" ein anderes gewesen sein muss (wahrscheinlich jede Art von Rübe) als das heutige Konzept des Erdapfels. Wie oben bereits erwähnt, war es also nötig einen Workflow zu finden, der nicht nur die philologische, sondern auch die semantische Komplexität der Rezepte widerspiegelt. Während die phrasenartigen Informationseinheiten manuell annotiert werden, erfolgt die Annotation auf Wortebene semiautomatisch, indem die Texte mithilfe von XSLT- und Python-Skripten und individuellen Vokabularien, vorgehalten als CSV Dateien, angereichert werden, die alle darauf ausgerichtet sind, die Varianz der historischen Sprachstufen auszugleichen. Für die frühneuhochdeutschen Texte stand uns aus einem früheren Projekt eine Liste mittelalterlicher Pflanzennamen und ihrer Übersetzungen in modernes Englisch und Deutsch sowie ihrer mittelalterlichen Variantendiktionen zur Verfügung. Dies gab uns die Möglichkeit, mit Hilfe der von OpenRefine bereitgestellten Reconciliation Service API, einen teilautomatisierten Prozess zur Annotation von Wikidata-Konzepten zu starten. Die daraus resultierenden Daten bildeten den Grundstock für die zuvor genannten Vokabularien. Ähnliche Listen wurden von den Projektpartnern in Frankreich erstellt, die mithilfe des von den französischen Kollegen entwickelten Tools “Heterotoki” in einem kollaborativen Arbeitsschritt konsolidiert werden können. Sobald jeder Begriff mit einem Konzept verbunden ist, werden diese Konzepte verwendet, um die Zutaten innerhalb der eigentlichen Rezepttexte in den TEI-Dokumenten anzureichern. Ein entscheidender Faktor dieses semiautomatischen Prozesses bleibt jedoch die menschliche Interpretation der angereicherten Einheiten und die Entscheidung für ein konkretes bereits bestehendes Konzept bzw. die Erstellung eines neuen Konzepts in Wikidata.
            
         Wir befinden uns derzeit mitten in dieser semantischen Annotationsphase. Ist diese abgeschlossen, bieten sich mannigfaltige Analysemethoden an. Sobald die Einheiten der einzelnen Rezepte mit Konzepten ausgestattet sind, kann die Analyse des Projekts übereinstimmende oder abweichende Essgewohnheiten, Textmigration sowie den Einfluss der Nachbarländer auf die jeweilige Küche aufzeigen. Die Implementierung von Ontologien aus den Naturwissenschaften wie FoodOn oder SNOMED ermöglicht es uns, Verbindungen von historischen Essgewohnheiten zu modernen Konzepten von Lebensmitteln herzustellen und neues Wissen für den Bereich der Ernährungsgeschichte zu generieren. Die Ontologiedaten werden zusammen mit den Entitäten in einem Triplestore gespeichert und können mit Hilfe von SPARQL Queries befragt werden. Die Ergebnisse dienen als Grundlage für eine räumliche und zeitliche Visualisierung der Daten.
            
          Die Speicherung, Analyse und Dissemination der Projektdaten erfolgt über das vom Zentrum für Informationsmodellierung in Graz entwickelte Repository GAMS (Geisteswissenschaftliches Asset Management). Innerhalb dieser auf Langzeitarchivierung ausgerichteten Infrastruktur wird auf den Triplestore “Blazegraph” über einen Webservice zur Speicherung und Abfrage von RDF-Triples zugegriffen.
            
      
      
         
            
               https://transkribus.eu/Transkribus/
            
            
      Describing Handwriting I-VII; http://www.digipal.eu/blog
            
            
               https://folk.uib.no/hnooh/mufi/
            
            
      Für eine Übersicht an Ontologien in diesen Bereichen siehe: 
      http://www.ontobee.org/, 
      http://aims.fao.org/, 
      https://ndb.nal.usda.gov/ndb/, 
      https://agclass.nal.usda.gov/about.shtml, 
      http://zbw.eu/stw/version/latest/thsys/70498/about.de.html.
      
	Alle sind an die Linked Open Data Cloud angeschlossen indem eine oder mehrere Serialisierungen in OWL und/oder RDF(S) vorliegen. 
      
            
            
               http://medieval-plants.org
            
            
               https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation-Service-API
            
            
               https://github.com/ponchio/heterotoki
            
            
               http://foodon.org/
            
            
               https://browser.ihtsdotools.org/
            
            
               https://gams.uni-graz.at
            
            
               https://www.blazegraph.com/
            
         
         
            
               Bibliographie
               
                  Adamson, M. W. (Ed.)  (1995): Food in the
      Middle Ages. A Book of Essays. New York, London:
      Garland. Adamson, M. W. (Ed.). (2002). Regional Cuisines of
      Medieval Europe: A Book of Essays. New York, London: Routledge.
      
               
                  Amoia, M. / Martínez, J.M.M.  (2019): SaCoCo Diachronic Corpus [WWW Document]. URL 
	http://fedora.clarin-d.uni-saarland.de/sacoco/ (accessed 1.7.20).
      
               
                  Böhm, A.  /  Klug, H.: Quellenorientierte
      Aufbereitung historischer Texte im Rahmen digitaler Editionen:
      Das Problem der Transkription in mediävistischen
      Editionsprojekten. In: [Titel steht noch nicht fest] Hrsg. von
      Ingrid Bennewitz und Martin Fischer (= Bamberger
      interdisziplinäre Mittelalterstudien.) [in Vorbereitung]
      
               
                  Carlin, M. /  Rosenthal, J. T. (Eds.)
      (1998): Food and Eating in Medieval Europe. London: Hambledon
      Press.
      
               
                  Chow, A. E. / Ninger, M. G.  (o. J.): 
	Multimodal Event Recognition with an Ontology For Cooking Recipes. 12.
      
               
                  Dooley, D. M. / Griffiths, E. J. / Gosal, G. S. / Buttigieg, P. L. / Hoehndorf, R. / Lange, M. C.,  / …  / Hsiao,
	W. W. L.  (2018): FoodOn: A harmonized food ontology to increase global food traceability, quality control and data integration. 
	Npj Science of Food, 2(1), 23. 
	https://doi.org/10.1038/s41538-018-0032-6
               
               
                  Flandrin, J.-L. (1984): «Internationalisme,
      nationalisme et régionalisme dans la cuisine des XIVe et XVe
      siècles: le témoignage des livres de cuisine». In Manger et
      boire au Moyen âge. Actes du Colloque de Nice (15-17 octobre
      1982). (pt. 2, p. 75-91). Paris.
      
               
                  Flandrin, J.-L.  /  Hyman, P. (1988):
      “Regional tastes and cuisines: Problems, documents, and
      discourses on food in Southern France in the 16th and 17th
      centuries”. Food and Foodways 1-3, p. 221-251.
      
               
                  Gloning, T.,  (2000): Monumenta Culinaria et Diaetetica Historica. Corpus of culinary & dietetic texts of Europe from the Middle Ages to 1800. Corpus älterer deutscher Kochbücher und Ernährungslehren [WWW Document]. URL 
	http://www.staff.uni-giessen.de/gloning/kobu.htm (accessed 1.7.20).
      
               
                  Hammad, R. /  Hassouna, M.  (2011): 
	Multi-Language Semantic Search Engine. 6.
      
               
                  Hieatt, C.  (1995): Sorting through the
	Titles of Medieval Dishes: What Is, or Is Not, a “Blanc
	manger”. In M. W. Adamson (Ed.), Food in the Middle Ages. A
	Book of Essays. (pp. 25-43). New York, London: Garland.
      
               
                  Hyman, P.  /  M.  (2005): «Les associations de saveurs dans les livres de cuisine français du XVIe siècle». In 
	Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles). Actes du colloque international à la mémoire de Jean-Louis Flandrin (Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 135-150). Saint-Denis: Presses Universitaires de Vincennes.
      
               
                  Karg, S. (Ed.) (2007): Medieval Food
      Traditions in Northern Europe. Copenhagen: National Museum of
      Denmark.
      
               
                  Klug, H. W. /  Kranich, K.  (2015): “Das Edieren von handschriftlichen Kochrezepttexten am Weg ins digitale Zeitalter. Zur Neuedition des Tegernseer Wirtschaftsbuches.” In T. Bein (Ed.), Vom Nutzen der Editionen. Zur Bedeutung moderner Editorik für die Erforschung von Literatur- und Kulturgeschichte. (pp. 121-137). Berlin, Boston: De Gruyter.
               
                  Lamé, M. /  Pittet, P. (2018):
      Heterotoki: Non-st / ructured and heterogeneous terminology alignment for Digital Humanities data producers. 12.
      
               
                  Laurioux, B.  (2005): «Les voyageurs et la gastronomie en Europe à la fin du Moyen âge ». In Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles), Actes du colloque international à la mémoire de Jean-Louis Flandrin (Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 99-117). Saint-Denis, Presses Universitaires de Vincennes.
               
                  Ribeiro, R. / Batista, F. / Pardal, J. P.  / Mamede, N. J. /  Pinto, H. S.  (2006): Cooking an Ontology. In J. Euzenat & J. Domingue (Hrsg.), 
	Artificial Intelligence: Methodology, Systems, and Applications (S. 213–221). Springer Berlin Heidelberg.
      
               
                  Sam, M. / Krisnadhi, A. A. / Wang, C. / Gallagher, J. /  Hitzler, P. (2014): 
	An Ontology Design Pattern for Cooking Recipes – Classroom Created.
      
               
                  Vadivu, G. / Hopper, S. W.  (2010): Semantic Linking and Querying of Natural Food, Chemicals and Diseases. 
	International Journal of Computer Applications, 
	11(4), 35–38. 
	https://doi.org/10.5120/1567-2093
               
               
                  van Winter, J. M. (1989):
	“Kochen und Essen im Mittelalter.” In B. Herrmann
	(Ed.). Mensch und Umwelt im
	Mittelalter. (pp. 88-100). Frankfurt am Main: Fischer
	Taschenbuch Verl.
      
            
         
      
   



      
         Der folgende Beitrag stellt das Forschungsvorhaben und erste Ergebnisse des DFG geförderten Projekts "Synoptische Edition des kabbalistischen Traktats 
Keter Shem Ṭov mit englischer Übersetzung, Stellenkommentar und rezeptionsgeschichtlichen Studien" vor.

         Der im 13. Jahrhundert auf Hebräisch verfasste Traktat 
Keter Shem Ṭov (“Krone des
guten Namens”) ist einer der wichtigsten
Einführungstexte in die esoterischen Lehren der
jüdischen Kabbala. Er wird häufig Abraham ben Axelrad
von Köln zugeschrieben, der möglicherweise ein Schüler
von El
            ‘azar von Worms (ca. 1176–1238) und Ezra ben Salomo von Gerona (um 1240) war. Der Traktat verbindet die in der spanischen Kabbala klassisch ausgeprägte Symbolik der zehn Sefirot bzw. Manifestationen der Gottheit mit solchen Deutungen des Tetragrammatons, d.h. des vierbuchstabigen („guten“) Gottesnamens, wie sie aus der Literatur der „Deutschen Frommen“, den 
Ḥaside Ashkenaz, bekannt sind. 
Keter Shem Ṭov ist der älteste bekannte Text, der diese beiden mystischen Traditionen vereint. Er wird in verschiedenen Versionen in etwa 100 Handschriften bezeugt, die sich voneinander deutlich in Umfang und Struktur unterscheiden. Die Herkunft dieser Handschriften umfasst ashkenazische, sefardische, italienische, byzantinische und orientalische Provenienzen. Die ältesten handschriftlichen Textzeugen stammen bereits aus der zweiten Hälfte des 13. Jahrhunderts und sind somit nur wenige Jahre nach der Komposition dieses Traktats entstanden. Der Traktat wurde nicht nur in Kreisen der jüdischen Kabbala, sondern auch von christlichen Kabbalisten rezipiert.
            
         
            Das Ziel des Projekts, das für drei Jahre von der Deutschen Forschungsgemeinschaft (DFG) bewilligt wurde, ist eine kritische Edition der verschiedenen Versionen dieses Traktats in Form einer Spaltensynopse, die sowohl in einer Druckausgabe als auch in einer digitalen und interaktiven Edition online verfügbar gemacht werden soll. Eine englische Übersetzung des Textes, ein detaillierter Stellenkommentar und Studien zur Geschichte seiner Rezeption werden die Edition ergänzen.
            
         Für die Kollationierung und Analyse der Textvarianten kommt das
webbasierte Werkzeug LERA zum Einsatz, das
im Rahmen des Projekts weiterentwickelt wird. Die ersten
transkribierten Textfassungen – in UTF-8 codierte Textdateien – konnten bereits mit dem Werkzeug verglichen werden. Abbildung 1 vermittelt einen ersten Eindruck der Oberfläche von LERA und zeigt beispielhaft den Vergleich von vier verschiedenen Textfassungen.
            
         
            
               
                Abbildung 1: Auszug einer mit LERA erstellten Synopse für vier Textfassungen von Keter Shem Ṭov. Im Ausschnitt zu sehen sind je zwei alignierte Absätze der vier Fassungen mit den farblich hervorgehobenen Textvarianten. Unter dem zweiten Absatz ist zudem der Variantenapparat eingeblendet.
            
         
         Ein nächster Arbeitsschritt besteht darin, an das Werk und das Hebräische angepasste Vergleichsfilter zu entwickeln, um spezifische Textvarianten auf Wunsch ausblenden zu können. Beispielsweise enthält 
                Keter Shem Ṭov verschiedenartige Bezeichnungen für "Gott", die in ausgeschriebenen oder abgekürzten Formen auftreten und oftmals mehrdeutig sind, sodass ein einfacher Wörterbuchansatz fehlschlägt. Wesentlicher Bestandteil des Textes sind zudem zitierte Bibelverse, die neben ihrer orthografischen Varianz auch in unterschiedlicher Länge wiedergegeben werden. Viele Textfassungen enthalten dabei nur die ersten Wörter; das Wissen um den vollständigen Vers setzt der Verfasser voraus. So entstehen Textvarianten, die vom System gesondert hervorgehoben oder verborgen werden sollen, was auf Basis entsprechender Auszeichnungen realisiert wird. Eine teilautomatisierte Erkennung, die den manuellen Aufwand dafür reduziert, ist derzeit in Entwicklung.
            
         Ein wesentlicher Aspekt des Projektes ist der Umgang mit sehr vielen Textfassungen, der mit voranschreitender Transkribierung weiterer Manuskripte stetig an Bedeutung gewinnt. LERA wurde ursprünglich für die Verarbeitung und Darstellung umfangreicher, aber weniger Textfassungen konzipiert (Bremer et al. 2015), obgleich seitdem für andere Editionsvorhaben angepasst, siehe bspw. (Gründler und Pöckelmann 2018, Roeder 2019). Um den Anforderungen von 
                Keter Shem Ṭov
             gerecht zu werden, sind verschiedene Erweiterungen in Arbeit. So wird die bisherige synoptische Darstellungsform mit je einer Spalte pro Textfassung um eine zeilenweise bzw. partitursynoptische Darstellung ergänzt. Über die Nutzeroberfläche soll es möglich sein, einzelne Textfassungen dynamisch ein- und auszublenden, was mit Hilfe der bereits integrierten Übersichtsleiste realisiert werden kann. Es ist eine dynamische Auswahl der Leithandschrift geplant, was neben der Anpassung der Oberfläche auch die Modifizierung des Vergleichsalgorithmus notwendig macht, da dieser die Textfassungen derzeit stets gleichrangig betrachtet. Es wird angestrebt diese interaktive Eingriffsmöglichkeit durch geschickte Vorberechnung möglichst effizient zu gestalten, damit für den Nutzer des Systems keine Wartezeiten entstehen. Ferner ist das Zusammenfassen mehrerer ähnlicher Handschriften in Gruppen angedacht. Der synoptische Vergleich findet dann auf Basis dieser Gruppen statt und wird größere Textunterschiede aufzeigen, während ein Apparat auch die kleinsten Änderungen innerhalb einer Gruppe aufschlüsselt.
            
         
            Anmerkungen
            Diese Arbeiten werden durch die Deutsche Forschungsgemeinschaft (DFG) [Projektnummer 414786977] im Rahmen des Projekts „Synoptische Edition des kabbalistischen Traktats 
                    Keter Shem Ṭov mit englischer Übersetzung, Stellenkommentar und rezeptionsgeschichtlichen Studien“ unter Leitung von apl. Prof. Dr. Gerold Neckerund Prof. Dr. Paul Molitor gefördert.
                
         
      
      
         
             Homepage des Projekts:https://kabbalaheditions.org
             LERA – Locate, Explore, Retrace and Apprehend complex text variants, siehe https://lera.uzi.uni-halle.de
         
         
            
               Bibliographie
               
                  Abrams, Daniel (2013): 
"Kabbalistic Manuscripts and Textual Theory:
Methodologies of Textual Scholarship and Editorial Practice in the
Study of Jewish Mysticism", Jerusalem / Los Angeles: Cherub Press.
                    
               
                  Bremer, 
                  Thomas /
                   Molitor, 
                  Paul / 
                  Pöckelmann, 
                  Marcus / 
                  Ritter, 
                  Jörg
                   / Schütz, 
                  Susanne (2015):
"Zum Einsatz digitaler Methoden bei der Erstellung und Nutzung genetischer Editionen gedruckter Texte mit verschiedenen Fassungen – Das Fallbeispiel der Histoire philosphique des deux Indes von Guillaume Thomas Raynal" in: 
Editio, Internationales Jahrbuch für Editionswissenschaften, Hrsg. R. v. Nutt-Kofoth, B. Plachta und W. Woesler, Band 29, Heft 1, S. 29–51, de Gruyter.

               
                  Gründler, Beatrice / Pöckelmann, Marcus (2018): 
"Adjusting LERA For The Comparison Of Arabic Manuscripts Of Kalīla
wa-Dimna", in: Book of Abstracts of the
29. International Annual Conference of Digital Humanities, DH2018,
Mexico City, pp. 467–468.

               
                  Idel, Moshe (2007): 
"Ashkenazi Esotericism and Kabbalah in Barcelona", in:
Hispania Judaica Bulletin 5, S. 69–113.

               
                  Jellinek, Adolph (1988):
  "Kleine Schriften zur Geschichte der Kabbala",  Hildesheim: Olms, S. 29–48.

               
                  Oron, Michal (2013/14): 
"Ha-Sifrut ha-parshanit le-
                  ʿeser Sefirot", in: Yoram Ben-Na‘eh et. al. (Hg.), 
                  ʾAsupa le-Yosef, FS Joseph Hacker, Jerusalem: Shazar, S. 212–229 (Hebr.).

               
                  Roeder
                  , Torsten (2019): 
"Genesis and Variance: From Letter to Literature", in:
Book of Abstracts of the 19th Annual Conference and Members’ Meeting of the Text Encoding Initiative Consortium(TEI), Graz, pp. 92–93.

               
                  Scholem, Gershom (1933): 
"Index to the Commentaries on the Ten Sefirot", Qiryat Sefer 10, S. 498–515 (Hebr.).

            
         
      
   



      
         
            Einführung: Handschriftenforschung und Digitale Transformation
            Wenn sich auch in den Geisteswissenschaften so etwas wie ein „Digital Turn“ oder eine „Digitale Transformation“ (Pousttchi 2017) beobachten lässt, wirft das unweigerlich die Frage auf, inwiefern die Einführung digitaler Prozesse in geisteswissenschaftliche Forschung Konsequenzen für den Aufbau bzw. das Design von konkreten Forschungsprojekten hat.
            In folgendem Vortrag soll diese Frage an einem Fallbeispiel aus dem Bereich der Jüdischen Studien entwickelt werden. Hier wurde unlängst von Gerben Zaagsma herauspräpariert, dass durch die nunmehr in großem Umfang verfügbaren digitalen Ressourcen zu jüdischer Geschichte und Kultur sich "die Sicherung, Bereitstellung und Analyse des in alle Welt verstreuten vielsprachigen und mehrschriftlichen Quellenmaterials“ als eine der zukünftigen Schlüsselaufgaben stellt (Zaagsma 2019:3ff.). 
            Dies lässt sich in besonderem Maße an dem Teilbereich der Handschriftenforschung exemplifizieren: Gerade durch die großen Digitalisierungsinitiativen der letzten Jahre an hebräischen Manuskripten, v.a. National Library of Israel, der Polonsky Foundation in Zusammenarbeit mit der British Library und der Bodleian Library stehen der wissenschaftlichen Community umfangreiche Quellenbestände zur Verfügung, die erst die Grundlage für weitere inhaltliche Tiefenerschließung, Edition und Corpusanalyse bilden.
            Betrachtet man weiter das Teilgebiet der wissenschaftlichen Erforschung des hebräischen Bibeltextes, so lässt sich zeigen, dass forschungsgeschichtlich gesehen die Editionspraxis hebräischer Bibelhandschriften bereits teilweise auf digitale Prozesse zurückgreifen kann, etwa in computerlinguistischer Hinsicht durch die langjährigen Projekte des Eep Talstra Centre for Bible and Computer (ETCBC; van Lit 2019) oder durch die elektronische Edition des Westminster Leningrad Codex (WLC). Ob man dies bereits als Symptom eines digitalen Transformationsprozesses betrachten sollte, der jenseits der Einführung einzelner Tools und Verfahren übergreifende Veränderungen in Fragestellung, Methodologie und Methodenkritik sichtbar macht, ist damit noch nicht ausgemacht. Immerhin gilt nach wie vor die Print-Edition der "Biblia Hebraica Stuttgartensia" auf Grundlage der Handschrift Ms. Fircovitch B19a als eine der massgeblichen kritischen Textausgaben für Theologie und hebräische Bibeltextforschung und bildet damit noch den Stand und die Möglichkeiten der analogen Bibeltextkritik ab.
                
            Hinsichtlich der Perspektive von primär digitalen wissenschaftlichen Editionen der Hebräischen Bibel stellt sich infolgedessen die Frage nach der Anwendbarkeit bereits etablierter Verfahren; während sowohl im WLC als auch im "Digital Mishnah Project" XML-Textauszeichungen zum Einsatz kommen (entweder teilweise oder vollständig entlang der TEI P5 Spezifikationen implementiert), finden XML-basierte "best practices“ im Bereich linksläufiger Schriftsysteme wie dem Hebräischen bislang nur zögerlich Akzeptanz, zumal unter Einsatz von XML-Quelltext-Autorensystemen wie oXygen ganz basale handwerkliche Probleme das Schreiben von rechtsläufigen Tagsets und linksläufigen Schriften zur Herausforderung macht. So konstatiert auch noch das DARIAH Wiki: „Solange dieses Problem nicht grundsätzlich gelöst ist, wird die Akzeptanz von TEI und/oder XML in Hebraistik und Arabistik gering sein.“. Gleichwohl berührt dies eher die Frage, inwiefern sich solche technischen Hürden durch geeignete grafische Benutzerschnittstellen nehmen lassen, die die systemischen Anforderungen bidirektionaler Unicode-Texte von anwendungsseitigen Annotationsebene wegabstrahieren. 
                
         
         
            Textcodierung: Modelle
            Inhaltlich bedeutsamer scheint aber die mittlerweile ausführlich beschriebene Problematik zu sein, dass sich XML als semi-strukturierte, hierarchisch organisierte Markup-Sprache mit seinen strikten Regeln zur Wohlgeformtheit und Validität von Auszeichnungen nur bedingt dazu eignet, Phänomene zu annontieren, die nicht linear/hierarchisch, sondern mit Überlappungen oder sich überschneidenden Sequenzen strukturiert sind. Ebenso zwingt es die Bearbeitenden, die dokumentenzentrierte und die textzentrierte Perspektive einer zu edierenden Quelle durch zwei unterschiedliche Kodierungsstrategien zu lösen (Brüning/Henzel/Pravida 2013; Pierazzo 2017); gleichzeitig belastet die Verarbeitung von internen wie externen Verweisstrukturen im Dokument (Lesartvarianten, Zitate, Querbezüge) die Kodierung damit, die referentielle Integrität von Links zuverlässig verwalten zu können. Innovative Lösungsansätze werden für dieses Problem unter anderem entlang des Modells von Textvarianten-Graphen beschrieben (Schmidt 2008; Schmidt 2009; Schmidt/Colomb 2008:498) oder unter Verwendung von "Labelled Property Graph“-Systemen wie der Graphendatenbank Neo4J diskutiert (Kuczera 2016a, Kuczera 2016b). 
            An dieser beispielhaften Gegenüberstellung verschiedener Datenmodellierungsansätze einen Unterschied zwischen Standardverfahren/Best Practice und Innovation auszuloten, der bereits eine implizite Wertung von Innovation als „fortschrittlich“ mitmeint, griffe sicherlich zu kurz - gleichwohl spannt sich durch die in der Literatur diskutierten Anwendungsfälle durchaus ein Spannungsfeld auf: Einerseits belastet der Einsatz spezialisierter Datenbankmanagementsysteme die Anforderungen an Offenheit und Langzeitverfügbarkeit von zu speichernden Forschungsdaten; auch die Neumodellierung von zu erhebenden Forschungsdaten schneidet im Sinne der FAIR-Prinzipien (Wilkinson / Dumontier / Aalbersberg, 
                    et al. 2016) zunächst von Anschlussmöglichkeiten ab, sind doch neuartige Datenmodelle, möglicherweise eben noch nicht „interoperable“ und „reusable“. 
                
            Andererseits bedeutet auch das Anwenden bestehender 
                    best practices eine interpretative Einschränkung: Mit der Umsetzung standardisierter Auszeichnungsschemata, sei es TEI-XML, eine bestimmte RDF-Ontologie oder Datenbankstruktur lässt sich am Quellenmaterial nur beobachten, was sich innerhalb der Unterscheidungsmöglichkeiten des jeweiligen Schemas bezeichnen lässt. Die Praxis der XML-basierten Quellenannotation zeigt hier, dass gerade bei steigenden Komplexitätsgraden am Material sich der Focus stärker in Richtung auf Einhaltung der Schema-Compliance und weg von der Beschreibung neuer Merkmalskategorien bewegt. Gute Indikatoren für dieses Phänomen sind beispielsweise vermehrter Einsatz von Standoff-Markup, individuelle, d.h. projektbezogene Schema-Erweiterungen, aber auch steigende Mehrdeutigkeiten im Markup bestimmter Phänomene wie etwa Marginalien in Handschriften (Estill 2016).
                
            Dieses hier am Beispiel zweier Modellierungsstrategien angedeutete Spannungsverhältnis zwischen Standardisierung und Innovation lässt sich gerade im Rahmen des Projektdesigns produktiv nutzen, zwingt doch zum einen das Einführen digitaler Methoden in die Quellenerschliessung zu einer strengen Formalisierung des eigenen Forschungsprozesses, zum anderen gewinnt die Perspektive der Datenmodellierung (Owens 2011) eine größere Bedeutung. Beides hat nicht zuletzt auch entscheidenden Einfluss auf die Auswahl der zum Einsatz kommenden Technologie-Stacks.
         
         
            Fallbeispiel: Corpus Masoreticum
            Am folgenden Fallbeispiel soll entwickelt werden, wie die skizzierten Überlegungen in einem konkreten Projekt umgesetzt werden können: Das von der Deutschen Forschungsgemeinschaft geförderte Langzeit-Editionsvorhaben „Corpus Masoreticum“, das an der Hochschule für Jüdische Studien Heidelberg angesiedelt ist, befasst sich mit dem sogenannten masoretischen Text in mittelalterlichen Bibelcodices. In der heutigen Forschung meint der Begriff der Masora alle meta-textuellen Elemente zum Konsonantentext der Hebräischen Bibel. Dazu gehören Grapheme, grammatische, syntaktische und statistische Notizen, Referenzen und Verweise. Ab dem 12. Jh. entstehen im Kulturraum Aschkenas (Nord-Frankreich und Deutschland) hebräische Bibel-Kodizes, in denen die Masora mit mikrographischer Schrift in ornamentalen Formen auf der Seite platziert wurde und als Fabelwesen, vor allem aber als zoomorphe Gestalten (Hunde, Pferde, Hasen, Gazellen, Vögel, Fische) und sogar als anthropomorphe Darstellungen gestaltet werden - hierfür wurde der Begriff der Masora figurata zur Unterscheidung von linearer Masora magna geprägt. Sie kann darüber hinaus Zitate aus Kommentarliteratur enthalten, die weit über die üblichen quantitativen und referentiellen Annotationen zum hebräischen Konsonantentext hinausgingen (vgl. Ms Vat. ebr. 14). Als Beispiel für in diesen masoretischen Metatexten häufig enthaltenes Listenmaterial lassen sich die sogenannten „Okhla-Listen“ herausgreifen, in denen als bewahrenswert gedachte Textphänomene und Schreibungen in unterschiedlichen Strukturen und Layouts dem Bibeltext mitgegeben werden und ihrerseits auf verschiedene extern überlieferte Rezensionen dieser Listen referieren (als Überblick: Liss/Petzold 2016). 
            Bereits oberflächliche Untersuchungen an diesem sehr speziellen Quellenmaterial zeigen, dass hier besonders komplexe Anforderungen an das zu definierende Editionsdatenmodell gestellt werden: Zu dokumentieren ist nicht nur linearer Text, sondern hochgradig vernetzte interne und externe Verweisstrukturen nicht nur mit Bezug auf Lesartvarianten, sondern auch auf Kommentarliteratur und Listenmaterial mit spezifischen Listenmustern, die auf extern tradiertes Listenwissen verweisen. Darüber hinaus bedarf die doppelte Lesbarkeit von Masora figurata als Text und Bild gleichermaßen in ihrem Bezug zum Bibeltext eines besonderen Dokumentationsverfahrens. 
            
               
                  
                  Abbildung 1: Überblick über das beispielhafte mise-en-page von Bibeltext, Masora parva, Masora Magna und Masora figurata in Ms Vat ebr. 14, Fol. 85v. Prototyp einer Visualisierungssoftware für digitale Erschliessung hebräischer Bibelcodices. Quelle: 
                  
               
            
         
         
            Implementierung von Modellen und Workflows
            Durch die netzwerkartige Struktur der in die untersuchten Handschriften eingebetteten Metatexte lag es nahe, die Beschreibung von Text als Daten von vornherein als Graph zu modellieren; der „labelled-property“-Ansatz von Graphdatenbankensystemen wie Neo4J macht es durch seine sogenannte „whiteboard-friendliness“ möglich, einfache Modellskizzen rasch in lauffähige digitale Speichermodelle zu implementieren. Im Editionsworkflow wird zunächst der Import von Handschriftendigitalisaten samt Metadaten über IIIF-kompatible Archive realisiert und die Handschriftendaten im Importprozess in Graphendaten als Knoten und Kanten umgewandelt. Ab hier werden über eine grafische Benutzeroberfläche erzeugte Texttranskriptionen kontextbezogen als Datenknoten verlinkt, wobei der Bezug zum Digitalisat über die Kodierung von Text als SVG-Textpfaden erhalten bleibt. Transkriptionen werden bei Bedarf weiter tokenisiert, um weitere Metadaten oder Kontextrelationen in den Graphen integrieren zu können. Aus dem so generierten Text- bzw. Knowledge Graph lassen sich Subsets (Datenaggregate) generieren, die im späteren Prozess sowohl als TEI-XML, RDF-Graph oder auch als angereicherte IIIF-Manifeste (Text, Übersetzung, Kommentar) ausgeliefert werden können. 

            
               
                  
                  Abbildung 2: Beispielgraph anhand Cod. Vat. Ebr. 468, folio 1v, Darstellung in Neo4J. Quelle: Liedtke 2019 (in Vorbereitung) 
               
            
            Die technischen Komponenten sind stark modularisiert und, wo möglich, als Microservices implementiert, so dass die einzelnen Ressourcen der Edition mit REST-APIs ausgeliefert und abgefragt werden können. Die Softwarearchitektur ist als Docker-Container-Umgebung in einer skalierbaren Cloud-Computing Landschaft realisiert, die vom heiCloud-Service des Rechenzentrums der Universität Heidelberg bereitgestellt wird, wobei die Rahmenbedingungen von Langzeitarchivierung und Nachnutzbarkeit in Zusammenarbeit mit Fachdiensten der Universitätsbibliothek Heidelberg gewährleistet werden.
                
         
         
            Ausblick
            Bezieht man das Spannungsverhältnis von Standardisierung und Innovation digitaler Prozesse in den architektonischen Aufbau eines geisteswissenschaftlichen Forschungsprojektes ein, lässt sich diese Dynamik produktiv für die Entwicklung eigener Workflows nutzen und öffnet Spielräume für das Modellieren der eigenen Forschungsdaten. Die formale Beschreibung eines digitalen Datenmodells kann dann methodenkritisch dazu verwendet werden, die im Prozess anstehenden Ergebnisse wieder an die Ausgangsfragestellung rückzubinden und Modelle auf ihre Plausibilität zu prüfen. Die Umsetzung in Technologie-Stacks oder digitale Frameworks führt im gezeigten Fallbeispiel zu der Konstruktion einer quasi „hybriden“ Editionsumgebung und beschränkt den digitalen Anteil eines Projektes nicht nur auf das Ausliefern von „Tools“, sondern betrachtet den Aspekt der digitalen Transformation als integralen Bestandteil des gesamten Forschungsprozesses. 
            
               
               Abbildung 3: Corpus Masoreticum als DH-Projekt (Schema). Quelle: Liedtke 2019 (in Vorbereitung)
            
         
      
      
         
            
               http://etcbc.nl
            
            
               http://tanach.us/
            
            
               http://dev.digitalmishnah.umd.edu/
            
            
               https://wiki.de.dariah.eu/display/publicde/3.5+Judaistik+und+Hebraistik
            
            
      Die auch im nachfolgenden Fallbeispiel verwendete Community-Version von Neo4J ist unter einer GPLv.3 Lizenz als Open Source verfügbar, ist also im strengen Sinne keine proprietäre Software – die Entwicklung wird aber massgeblich von Neo4J Inc. als kommerziellem Unternehmen vorangetrieben.
    
            
               https://neo4j.com/developer/guide-data-modeling/#whiteboard-friendly
            
            
               https://heidata.uni-heidelberg.de/
            
         
         
            
               Bibliographie
               
                  Estill, Laura (2016): “Encoding the Edge: Manuscript Marginalia and the TEI.” 
                        Digital Literary Studies 1, no. 1. https://journals.psu.edu/dls/article/view/59715/59912.
                    
               
                  Kuczera, Andreas (2016a): “Graphbasierte Digitale Editionen.” Blog. 
                        Mittelalter. Interdisziplinäre Forschung Und Rezeptionsgeschichte (blog), April 19, 2016. https://mittelalter.hypotheses.org/7994.
                    
               
                  Kuczera, Andreas (2016b): “Digital Editions beyond XML – Graph-Based Digital Editions.”, in: 
                        Proceedings of the 3rd HistoInformatics Workshop on Computational History (HistoInformatics 2016), edited by Marten Düring, Adam Jatowt, Johannes Preiser-Kappeller, and Antal van Den Bosch. Krakow, 2016. http://ceur-ws.org/Vol-1632/paper_5.pdf.
                    
               
                  Liedtke, Clemens (2019): 'How am I supposed to read this?' Challenges and Opportunities of Medieval Western Masorah as a Digital Scholarly Edition", in: J. Leipziger/H. Liss/K. J. Petzold (eds.), 
                        Philology and Aesthetics: Figurative Masorah in Western European Manuscripts (Judentum und Umwelt), Frankfurt am Main et al.: Peter Lang (in Vorbereitung)
                    
               
                  Liss, Hanna / Petzold, Kay Joe (2016): “Die Erforschung der westeuropäischen Bibeltexttradition als Aufgabe der Jüdischen Studien. Ein halbes Jahrhundert Forschung und Lehre über das Judentum in Deutschland.“, in: 
                        Orchidee oder Mimose. Versuch einer Standortbestimmung der Jüdischen Studien, edited by Andreas Lehnardt and Guiseppe Veltri. Berlin et al, 2016.
                    
               
                  Owens, Trevor (2011): “Defining Data for Humanists: Text, Artifact, Information or Evidence?” 
                        Journal of Digital Humanities 1, no. 1. http://journalofdigitalhumanities.org/1-1/defining-data-forhumanists-by-trevor-owens/.
                    
               
                  Pierazzo, Elena (2017): “Facsimile and Document-Centric Editing.”, in: 
                        Creating a Digital Scholarly Edition with the Text Encoding Initiative, edited by Marjorie Burghart. https://www.digitalmanuscripts.eu/wp-content/uploads/sites/6/2017/09/05-Digital-Facsimiles-EP.pdf.
                    
               
                  Pousttchi, Key (2017): “Digitale Transformation.”, in: 
                        Enzyklopädie der Wirtschaftsinformatik. http://www.enzyklopaedie-der-wirtschaftsinformatik.de/lexikon/technologien-methoden/Informatik--Grundlagen/digitalisierung/digitale-transformation/digitale-transformation/?searchterm=digitale%20transformation.
                    
               
                  Schmidt, Desmond (2008): “What’s a Multi-Version Document?” 
                        Multi-Version Documents (blog), May 3. http://multiversiondocs.blogspot.com/2008/03/whats-multi-version-document.html.
                    
               
                  Schmidt, Desmond / Colomb, Robert (2009): “A Data Structure for Representing Multi-Version Texts Online.” 
                        International Journal of Human-Computer Studies 67, no. 6 (June 2009): 497–514. https://doi.org/10.1016/j.ijhcs.2009.02.001.
                    
               
                  Schmidt, Desmond (2009): “‘Merging Multi-Version Texts: A Generic Solution to the Overlap Problem.’ Presented at Balisage: The Markup Conference 2009, Montréal, Canada, August 11 - 14, 2009,” Proceedings of Balisage: The Markup Conference 2009, 2 (2009). 
                        https://doi.org/10.4242/BalisageVol3.Schmidt01.
                    
               
                  Wilkinson, M. / Dumontier, M. / Aalbersberg, I. 
                  et al.
                   (2016): “The FAIR Guiding Principles for scientific data management and stewardship.” 
                        Sci Data 3
                        , 160018. doi:10.1038/sdata.2016.18
                    
               
                  Zaagsma, Gerben (2018): “#DHJewish – Jewish Studies in the Digital Age.” 
                        Medaon. Magazin für Jüdisches Leben in Forschung und Bildung, no. 12: 1–11.
                    
               
                  Zundert, Joris J. van / Andrews, Tara L. (2016): “Apparatus vs. Graph: New Models and Interfaces for Text.”, in: 
                        Interface Critique, edited by Florian Hadler and Joachim Haupt, 139:183–206. Berlin: Kulturverlag Kadmos.
                    
            
         
      
   



      
         
            Die Komponenten: DraCor und forTEXT
            
               DraCor
               Mit ELTeC (European Literary Text Collection; 
https://github.com/COST-ELTeC) und DraCor gibt es mittlerweile zwei europäische Initiativen, die eine korpusbasierte Infrastruktur für die digitalen Literaturwissenschaften aufbauen, wobei sich DraCor (Drama Corpora Project; 
https://dracor.org) der Sammlung TEI-kodierter Dramen in verschiedenen Sprachen widmet (vgl. Fischer u.a. 2019). DraCor liefert über seine API etwa Netzwerkdaten zu Dramen aus, die auf der Kookkurrenz von Sprecherïnnen basieren und es ermöglichen, die Kommunikationsstrukturen mithilfe von Network-Analysis-Metriken zu erforschen. Darüber hinaus bietet die Plattform mit ezlinavis (Easy Literary Network Analysis Visualisation) ein didaktisches Tool an, das den Einstieg in die systematische Erhebung von Netzwerkdaten erleichtert. Außerdem wurde aus DraCor heraus mit dem 
Dramenquartett (vgl. Fig. 1) ein Kartenspiel entwickelt, mit dem das Verständnis von Netzwerkmetriken ebenso wie die typologische und historische Vielfalt von Dramennetzwerken spielerisch entdeckt und erlernt werden kann (vgl. Fischer u.a. 2018 und Fischer/Schultz 2019).

               
                  
                  Figure 1: Beispielkarte aus dem Dramenquartett (Rück- und Vorderseite)
               
            
            
               forTEXT
               
                  
                  Figure 2: Die Startseite des Disseminationsportals forTEXT.net
               
               Das DFG-Projekt 
  forTEXT (https://fortext.net; vgl. Fig. 2) bietet in diesem Zusammenhang einen Methodeneintrag (vgl. Schumacher 2018b), eine (Gephi-)Lerneinheit (vgl. Schumacher 2019b), ein Fallstudien-Video, vier Tutorialvideos sowie praktische Anwendungen in der Lehre. Hierbei sind jeweils unterschiedliche Abstraktionsgrade und verschiedene Arten der Vermittlung abgedeckt: Der Methodeneintrag ist eine abstrakte, sprachlich-theoretische Beschreibung der Methode mit dem Schwerpunkt der Anschlussfähigkeit an die traditionelle Literaturwissenschaft. Die Lerneinheit ist eine konkrete Klick-für-Klick-Einführung für Autodidakten aus der Zielgruppe junger Geisteswissenschaftlerïnnen in Form einer Text-Bild-Kombination.
  
               Die Videos vermitteln die Methode über eine Text-Bild-Audio-Kombination: Das Methodenvideo bietet eine Fallstudie zum Figurennetzwerk von 
  Emilia Galotti. Es vermittelt die Methode an eine autodidaktische Zielgruppe geisteswissenschaftlicher Studentïnnen und wählt einen Einstieg über das ,traditionelle’ Thema, taucht in technisch-theoretische Hintergründe ein und schließt dann wieder an das literarische Thema an. Die textbasierte Thematik wird so auf eine Bildebene überführt, die DH-Tool-Grafiken mit strichmännchenartigen figurativen Darstellungen koppelt (vgl. Fig. 3).
  
               
                  
                  Figure 3: Vorschaubild eines forTEXT-Fallstudien-Videos
               
               Angesprochen werden hier theoretische, strukturelle und emotionale autodidaktische Vermittlungsmuster (zur Bedeutung von Emotionen für autodidaktisches Lernen vgl. Mega u.a. 2014). Auf der Tonebene ist ein erklärender Duktus vorherrschend. Die ,selfmade’-Anmutung der Videos vermittelt, dass die autodidaktische Erarbeitung der Inhalte Betrachterïnnen und Erstellerïnnen des Videos miteinander verbindet (vgl. Horstmann & Schumacher 2019).
               Die Tutorial-Reihe schließlich funktioniert ähnlich wie die Lerneinheit als Schritt-für-Schritt-Anleitung und bietet die Möglichkeit, die Arbeit mit Gephi als Screencast zu erlernen. Das Tutorial-Video zur Nutzung des DraCor-Tools ezlinavis verknüpft die praktische Erstellung von Netzwerken mit der Nutzung der Ressource TextGrid Repository (vgl. Horstmann 2018) und den Methoden Named Entity Recognition (vgl. Schumacher 2018a) und Annotation in CATMA (vgl. Jacke 2018 und Schumacher 2019a).
            
            
               Das Dramenquartett als Erweiterung des Disseminationsmodells in forTEXT
               Die Dissemination einer digitalen Methode wie der Netzwerkanalyse durch ein nicht-digitales Kartenspiel bietet Möglichkeiten, die die bisher genannten digitalen Medien nicht abdecken konnten. Die Spielerïnnen werden in einer nicht-digitalen Umgebung mit den funktional reduzierten Ergebnissen einer digitalen Analyse konfrontiert, können diese visuell und haptisch erfahren und spielerisch explorieren. Der empfohlene Spielmodus ist ,Supertrumpf’, bei dem Werte der Netzwerke verglichen werden. Die Spielregeln sind online veröffentlicht
  (https://dramenquartett.github.io/). Neben einem neuen und vor allem kompetitiven Blick auf Dramen – der die relationale Perspektive auf figürliche Kopräsenzen hervorhebt – wird zusätzlich die Neugier auf die den Netzwerken zugrunde liegende Methode geweckt, sodass in didaktisch-produktiver Hinsicht der Prozess einer Art 
  Reverse Engineering im Sinne einer Mustererkennung auf unterschiedlichen Komplexitätsstufen angestoßen werden kann. Der Weg hin zu einem Umgang mit digitalen Ressourcen und Tools wie DraCor, ezlinavis und sogar die Anwendung eines komplexeren Tools wie Gephi ist damit geebnet, die kritische Methodenreflexion kann folgen. Dieser niedrigschwellige Zugang fügt sich in das auf Zugänglichkeit und Benutzerfreundlichkeit konzentrierte Disseminationsmodell von forTEXT ein und erweitert dieses durch den zusätzlichen Abbau von Schwellenängsten oder Vorbehalten gegen digitale Methoden.
  
               Die im Folgenden vorgestellte, reflektierte und erprobte Pipeline geht von einer ersten theoretischen Annäherung durch forTEXT-Tutorials aus, auf die eine spielerische Vertiefung der spezifischen Objektkonstitution qua Netzwerkanalyse und der entsprechenden Metriken mittels des Dramenquartetts folgt. Anschließende Arbeitsphasen könnten, wie in 3. skizziert, z. B. die formalisierte Erstellung, Gestaltung und Analyse von Dramennetzwerken mittels ezlinavis und Gephi oder die konkrete Bearbeitung von literarhistorischen Forschungsfragen mittels DraCor umfassen.
            
         
         
            Epistemische und didaktische Implikationen
            
               Epistemische Dimensionen des Medienwechsels
               Der quantifizierende Zugriff auf Dramentexte kann als „radikale ,Anästhetisierung’ der Objekte” (Trilcke, im Erscheinen) beschrieben werden. Auf die qua Formalisierung erfolgende Anästhetisierung, bei der die ursprüngliche ästhetische Dimension des literarischen Kunstwerks zunächst ausgesetzt wird, folgt jedoch eine reästhetisierende Transformation im Zuge der Diagrammatisierung (vgl. ebd.). Die Dramen werden somit zunächst zwar nicht mehr primär als textuelle Artefakte wahrgenommen, dennoch aber als ästhetische Artefakte in Form ihrer netzwerkartigen Repräsentation, wodurch andere epistemische Dimensionen angesprochen und andere epistemische Praktiken vollzogen werden können (vgl. Trilcke und Fischer 2018). Dabei ist der Weg zurück zum Dramentext vom Kartenspiel über die digitale Darstellung der entsprechenden Netzwerke dadurch geebnet, dass Medien in Form „transkriptiver Bezugnahmen” (Jäger 2010, 301) generell intermedial aufeinander Bezug nehmen und Übersetzungsprozesse somit keine einseitig vorgegebene Richtung haben.
               
               Ein entscheidender Vorteil digitaler Diagramme ist die Möglichkeit der Interaktion (vgl. Horstmann, im Erscheinen): Netzwerke lassen sich je nach Wahl des Layoutalgorithmus unterschiedlich darstellen, ein semantischer Zoom ermöglicht überdies, zusätzliche Informationen des Ausgangsmaterials zu visualisieren. Dramennetzwerke in einer festgelegten (und damit nicht mehr veränderbaren) Form als Spielkarte zu drucken, bedeutet daher in erster Linie eine funktionale Reduktion. Gerade diese funktionale Reduktion eröffnet jedoch didaktische Spielräume: Das Wissen, dass die abgedruckten Netzwerke ebenfalls in digitaler Form vorhanden und dort sogar manipulierbar sind, wird im Laufe des Spielprozesses die Neugier auf diese Funktionsvielfalt steigern, sodass der Übergang in die ,digitale Arbeit’ fließend stattfinden kann und nicht mehr als etwas kategorial anderes empfunden wird. Die Interaktion zwischen Benutzerïnnen und Netzwerken als konzeptioneller Bestandteil digitaler Netzwerkdarstellungen wird übertragen auf die Interaktion zwischen den Spielerïnnen, wodurch nicht zuletzt die von Jenkins (2006, 2) sog. 
  participatory culture im nicht-digitalen Bereich eine Entsprechung erfährt.
  
            
            
               Ansprechen unterschiedlicher Lerntypen 
               Das Kartenspiel entfaltet seinen didaktischen Mehrwert auch, weil es situational gerahmt ist: Es wird in kollektiven Unterrichtsphasen eingesetzt, die darauf abzielen, sich einem abstrakten Unterrichtsgegenstand auf spielerische Weise anzunähern. Da Menschen in ihrer Rolle als 
  visual beings vor allem ihren Sehsinn als einen wichtigen Wahrnehmungskanal nutzen, um Informationen zu verstehen (vgl. Ward et al. 2010), stellen Visualisierungen bei der Präsentation von wissenschaftlichen Erkenntnissen ein wichtiges, den Verstehens- und Erinnerungsprozess begünstigendes Element dar. Der Einsatz des Kartenspiels greift darauf zurück und spricht unter den vier Lerntypen (auditiv, haptisch, kommunikativ und visuell) v. a. visuelle, aber auch kommunikative Lerntypen an, indem das Spiel die Kommunikation über Fachinhalte fokussiert und Sprache als Medium des Lernens einsetzt (vgl. Anselm und Werani 2017).
  
               Im Fokus steht der Versuch, nicht nur kumulatives bzw. assimilatives Lernen zu initiieren, wodurch v. a. begrenztes, anwendungsorientiertes Wissen oder thematisch, anwendungsorientiertes Wissen produziert werden würde (vgl. Illeris 2010). Die – von der konkreten Kenntnis des Spielprinzips ,Supertrumpf‘ unabhängige – spielerische Aktivierung unterschiedlicher Sinneskanäle und die damit einhergehende Diskussion über Fachinhalte zielt auf die Einleitung akkommodativer und transformativer Lernprozesse und darauf, über Fachwissen in relevanten Kontexten frei verfügen zu können.
            
            
               Anwendung in der universitären Lehre und Lehrerïnnenbildung
               Das im Wintersemester 2019/2020 an der Universität Hamburg durchgeführte Seminar „Digitale Literaturwissenschaft und pädagogische Praxis” hat unterschiedliche Standardverfahren und Werkzeuge erprobt, die gegenwärtig in der digitalen Literaturwissenschaft eingesetzt werden. Dieses Feld wird zunehmend auch für Lehrerïnnen insbesondere im gymnasialen Bereich relevant: Bereits die heutige Schülerïnnengeneration zählt zu den 
  digital natives, für die der Umgang mit digitalen Medien und Werkzeugen selbstverständlich ist, die aber zugleich in Schule und/oder Studium in eine vertiefte 
  data literacy eingeführt werden müssen. Der Transfer von Digital-Humanities-Methoden in den schulischen Bereich kann deshalb als wichtige Herausforderung identifiziert werden. Gleichzeitig geht es darum, das vernetzte Denken zu fördern, mithin literaturwissenschaftliche und fachdidaktische Zugänge zu 
  einem Gegenstand stark zu machen. Um in Seminaren kein starres Wissen zu produzieren, auf das die angehende Lehrkräfte in der nächsten Phase ihrer Ausbildung – dem schulischen Alltag – nicht zugreifen können, muss die Kooperation zwischen Fachdidaktik und Fachwissenschaft gefördert werden. Neben der Einarbeitung in die Methoden steht deshalb die Frage der Komplexitätsreduktion und des schulischen Anwendungsbezuges im Zentrum des Seminars, wofür das DraCor-Kartenspiel exemplarisch herangezogen und getestet wird. Die konzeptionelle Einbettung des Kartenspiels in eine didaktische Heranführung an digitale Methoden ergänzend, wurde damit sowohl in diesem als auch im Seminar „Gender modellieren – Genderrollen und -stereotype in der Literatur des 19. Jahrhunderts”, das ebenfalls im Wintersemester 19/20 an der Universität Hamburg angeboten wurde, eine praktische Anwendung durchgeführt, deren Erfolg qualitativ evaluiert wurde. Damit soll auch ein Beitrag zur Evaluation konkreter DH-Lehrformen geleistet werden.
  
            
            
               Erste Ergebnisse
               Um den Effekt des Dramenquartetts auf den Lernerfolg der Studierenden zu untersuchen, wurde eigens ein Testverfahren entwickelt, das die Wissensstände vor und nach dem Einsatz des Quartetts mess- und v. a. vergleichbar macht. Das Verfahren setzt sich aus fünf aufeinander aufbauenden Phasen zusammen:
               (1) 
                        Vorab: Gruppeneinteilung und eigenständige Vorbereitung (Gruppe 1: Methodenbeitrag/Lerneinheit, Gruppe 2: Video-Tutorials)
                    
               Vorbereitend befasst sich ein Teil der Lerngruppe mit schriftlichen forTEXT-Lernmaterialien zur digitalen Netzwerkanalyse, während der andere Teil die Video-Fallstudien und -Tutorials konsultiert.
               
               (2)
                         Praxisphase 1: Erste Umfrage
               
               Ausgangspunkt der Erhebung stellt folglich ein gruppenspezifisch relativ homogener Wissensstand dar, der grundlegende Kenntnisse über die Methode der digitalen Netzwerkanalyse beinhaltet. Um die Wissensstände beider Gruppen vor dem Einsatz des Quartetts zu erfassen, wurde eine Umfrage entworfen und zu Beginn des Seminars in Einzelarbeit mit dem Audience Response System ARSnova durchgeführt. Die Umfragen adressieren mit jeweils neun Fragen drei Anforderungsbereiche (I: Reproduktionsleistung, II: Reorganisation- und Transferleistung, III: Reflexion und Problemlösung). Den Anforderungsbereichen entsprechend beinhalten sie Single-Choice-, Multiple-Choice- sowie Freitextfragen.
               (3)
                         Praxisphase 2: Einsatz des Dramenquartetts
               
               Nach der ersten Quizphase wurde die gesamte Testgruppe in Kleingruppen eingeteilt, die im Supertrumpf-Modus das Dramenquartett spielen.
               (4)
                         Praxisphase 3: Zweite Umfrage
               
               Eine zweite Umfrage erfasst den Wissensstand beider Gruppen, nachdem sie das Dramenquartett gespielt haben.
               (5)
                         Auswertung der Umfrage: Erste Ergebnisse und Ausblick
               
               Die Auswertung des ersten Testdurchlaufs, der mit 11 Teilnehmenden durchgeführt wurde, verweist auf einen lernförderlichen Effekt des Dramenquartetts. Im Rahmen der ersten Quizrunde wurden 43% der Fragen, nach der zweiten Umfrage 52% der Fragen richtig beantwortet. Darüber hinaus verweist ein erster Blick auf die Freitextantworten darauf, dass der spielerische Zugang die intrinsische Motivation, sich über den Seminarkontext hinaus mit digitaler Netzwerkanalyse auseinanderzusetzen, steigert. Das erarbeitete Verfahren zur vergleichenden Lernstandserhebung hat sich bewährt und wird in einem weiteren Seminar eingesetzt, um den Einfluss einer spielerischen Wissensvermittlung auf Kompetenz- und Wissensstand zu untersuchen.
            
         
         
            Ausblick: zukünftig mögliche Arbeitsfelder
            Das Projekt lotet das didaktische Potenzial von Gamification-Ansätzen in den DH konzeptionell und praktisch aus, indem es das DraCor-Kartenspiel mit Tools und Tutorials in einer didaktischen ,Pipeline‘ verbindet und damit in die Disseminationsstrategie von forTEXT integriert. Der damit entwickelte Prototyp eines Konzepts, das auch fachdidaktisch Weiterentwicklungspotenzial birgt, ermöglicht diverse Adaptionen und Transformationen: in Hinblick auf die Netzwerkanalyse literarischer Texte, in Hinblick auf andere Methoden der Digital Humanities sowie in Hinblick auf das didaktische Szenario einer Verzahnung von analogen und digitalen Ansätzen.
            So ließen sich auf der Grundlage der Netzwerkdaten aus anderen DH-Projekten, etwa zu Romanen, andere generische Karten-Sets entwerfen, wobei auch die – durch ezlinavis in Kombination mit Gephi ermöglichte – kollaborative Erstellung eigener Sets denkbar ist. Diese selbstständige Erstellung von Karten-Sets würde nicht zuletzt auch den haptischen Lerntyp ansprechen. Eine Weiterentwicklung der didaktischen Engführung von Analogem und Digitalem ließe sich über eine Verzahnung des Kartenspiels mit der digital-interaktiven Repräsentation der einzelnen Dramen auf DraCor vornehmen (z. B. über QR-Codes). Unter didaktischen Gesichtspunkten bietet sich des Weiteren die Möglichkeit, kreativ-produktionsorientierte Elemente in die skizzierte Pipeline einzubauen, etwa indem die Lernenden Netzwerke ,erfinden‘, die sie zunächst händisch zeichnen und dann – den Schritt in den digitalen Raum machend – mittels ezlinavis formal erfassen müssen.
            Der im Projekt durchgeführte Testlauf soll in diesem Sinne zu einer weiteren Diskussion über didaktische Potenziale sowohl von Gamification-Ansätzen als auch der Verzahnung von analogen und digitalen Lehrmitteln anregen und damit grundsätzlich der Reflexion über didaktische Szenarien dienen, die den spielerischen, kreativen Übergang zwischen lebensweltlich vertrauten Situationen und der Abstraktion digitaler Forschungsprozesse gestalten.
         
      
      
         
            
                Vgl. 
               https://fortext.net/ressourcen/videos/fallstudien/analyse-der-figurennetzwerke-in-lessings-emilia-galotti.
    
            
                Vgl. 
               https://fortext.net/ressourcen/videos/tutorials/netzwerkanalyse-und-literaturanalyse.
    
            
                Vgl. 
               https://de.wikipedia.org/wiki/Supertrumpf.
    
            
      Zum Diagrammatikbegriff vgl. etwa Krämer 2016.
    
            
                Zu intermedialen Übersetzungsprozessen vgl. Schmid, Veits und Vorrath 2018.
            
            
                Beide Seminare richten sich ausdrücklich an Studierende ohne technische Vorkenntnisse. Es ist also davon auszugehen, dass die Personen beider Testgruppen über keinerlei Vorbildung bezüglich Methoden der digitalen Netzwerkanalyse verfügen.
            
         
         
            
               Bibliographie
               
                  Anselm, Sabine / Werani, Anke (2017): 
  Kommunikation in Lehr-Lernkontexten. Bad Heilbrunn: Klinkhardt.

               
                  Fischer, Frank / Kittel, Christopher / Milling, Carsten / Trilcke, Peer / Wolf, Jana (2018): „Dramenquartett – Eine didaktische Intervention“, in: 
  DHd 2018. Kritik der digitalen Vernunft. Konferenzabstracts, 397–398. DOI: 
  https://doi.org/10.6084/m9.figshare.5926363.v1.

               
                  Fischer, Frank / Schultz, Anika (2019): „Dramenquartett – Eine didaktische Intervention“. Unter Mitarbeit von Christopher Kittel, Carsten Milling, Peer Trilcke und Jana Wolf. 32 Blatt in Kartonbox, Farbdruck. Bern: edition taberna kritika 2019. (Spielanleitung: 
  https://dramenquartett.github.io/)

               
                  Fischer, Frank / Börner, Ingo / Göbel, Mathias / Hechtl, Angelika / Kittel, Christopher / Milling, Carsten / Trilcke, Peer (2019): „Programmable Corpora. Die digitale Literaturwissenschaft zwischen Forschung und Infrastruktur am Beispiel von DraCor“, in: Sahle, Patrick (ed.): 
  DHd 2019. Digital Humanities: multimedial & multimodal. Konferenzabstracts, 194–197.

               
                  Horstmann, Jan (2018): „TextGrid Repository“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/ressourcen/textsammlungen/textgrid-repository [letzter Zugriff 12. September 2019].

               
                  Horstmann, Jan (im Erscheinen): „Textvisualisierung: Epistemik des Bildlichen im Digitalen”, in: Huber, Martin / Krämer, Sybille / Pias, Claus (eds.): 
  Wovon sprechen wir, wenn wir von Digitalisierung sprechen? Gehalte und Revisionen zentraler Begriffe des Digitalen, CompaRe: Fachinformationsdienst Allgemeine und Vergleichende Literaturwissenschaft.

               
                  Horstmann, Jan / Schumacher, Mareike (2019): „Social Media, YouTube und Co: Multimediale, multimodale und multicodierte Dissemination von Forschungsmethoden in forTEXT“, in: Sahle, Patrick (ed.): 
  DHd 2019. Digital Humanities: multimedial & multimodal. Konferenzabstracts, 207–211. DOI: 
  10.5281/zenodo.2596095.

               
                  Illeris, Knud (2010): 
  Lernen verstehen. Bedingungen erfolgreichen Lernens. Bad Heilbrunn: Klinkhardt.

               
                  Jacke, Janina (2018): „Manuelle Annotation“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/methoden/manuelle-annotation [letzter Zugriff 12. September 2019].

               
                  Jäger, Ludwig (2010): „Intermedialität – Intramedialität – Transkriptivität: Überlegungen zu einigen Prinzipien der kulturellen Semiosis”, in: Deppermann, Arnulf / Linke, Angelika (eds.): 
  Sprache intermedial: Stimme und Schrift, Bild und Ton. Berlin, New York: de Gruyter 299–324. DOI: 
  10.1515/9783110223613.299.

               
                  Jenkins, Henry (2006): 
  Convergence Culture: Where Old and New Media Collide. New York, London: New York University Press.

               
                  Krämer, Sybille (2016): 
  Figuration, Anschauung, Erkenntnis. Grundlinien einer Diagrammatologie. Berlin: Suhrkamp.

               
                  Mega, Carolina / Ronconi, Lucia / De Beni, Rossana  (2014):
  „What makes a good student? How emotions, self-regulated learning, and motivation contribute to academic achievement”, in:
   Journal of Educational Psychology, 
                  Vol 106(1), 121–131.
               
               
                  Odebrecht, Carolin / Burnard, Lou / Navarro Colorado, Borja / Eder, Maciej / Schöch, Christof (2019): „The European Literary Text Collection (ELTeC)”, in: 
  DH 2019. Complexities. Utrecht University. [Poster.]

               
                  Schmid, Johannes C. P. / Veits, Andreas / Vorrath, Wiebke (eds. 2018): 
  Praktiken medialer Transformationen. Übersetzungen in und aus dem digitalen Raum. Bielefeld: transcript. DOI: 
  10.14361/9783839441145.

               
                  Schumacher, Mareike (2018a): „Named Entity Recognition (NER)“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/methoden/named-entity-recognition-ner [letzter Zugriff 12. September 2019].

               
                  Schumacher, Mareike (2018b): „Netzwerkanalyse“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/methoden/netzwerkanalyse [letzter Zugriff 12. September 2019].

               
                  Schumacher, Mareike (2019a): „CATMA“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/tools/tools/catma [letzter Zugriff 12. September 2019].

               
                  Schumacher, Mareike (2019b): „Netzwerkanalyse mit Gephi“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/lerneinheiten/netzwerkanalyse-mit-gephi [letzter Zugriff 12. September 2019].

               
                  Trilcke, Peer / Fischer, Frank (2018): „Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen”, in: Huber, Martin / Krämer, Sybille (eds.): 
  Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden (Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3). DOI: 
  10.17175/sb003_003.

               
                  Trilcke, Peer (im Erscheinen): „Small Worlds, Change Rates und die Netzwerkanalyse dramatischer Texte. Reflexionen aus dem Rabbit Hole”, in: Jannidis, Fotis / Winko, Simone / Rapp, Andrea / Meister, Jan Christoph / Stäcker, Thomas (eds.): 
  Digitale Literaturwissenschaft. DFG-Symposium Villa Vigoni, 2017. Berlin, New York: de Gruyter.

               
                  Ward, Matthew / Grinstein, Georges / Keim, Daniel (2010): 
  Interactive Data Visualization. Foundations, Techniques, and Applications. Wellesley: Peters.

            
         
      
   



      
         Die kulinarische Tradition ist eine der prägendsten Elemente der europäischen Kultur und sie stellt einen großen Teil der nationalen Identitäten dar. In den letzten Jahrzehnten kam die Forschung zu zwei wichtigen Schlussfolgerungen in Bezug auf dieses Thema: Erstens, es gibt keine quantitativen Studien über die Herkunft und die Bildung von regionalen Küchen in Europa. Zweitens, im Mittelalter entstehen wesentliche Quellen: Manuskripte mit tausenden von Kochrezepten. Damit kann das Mittelalter als die Wiege der modernen europäischen Küche angesehen werden. Auf dem europäischen Kontinent bilden lateinische, mittelfranzösische und frühneuhochdeutsche Rezepte den Großteil der kulinarischen Überlieferung.
         Das vorliegende internationale Projekt (ANR-17-CE27-0019-01, fwf I 3614) zielt darauf ab, die interkulturelle Forschung der mittelalterlichen Kochrezepte und deren Wechselbeziehung mithilfe eines interdisziplinären Ansatzes zu verwirklichen. Das Projekt nimmt die Kochrezeptüberlieferung von Frankreich und den deutschsprachigen Ländern als Korpus – dieses umfasst mehr als 80 Manuskripte und an die 8000 Rezepte – und untersucht sie in Hinblick auf ihre Entstehung, ihre Beziehung untereinander und ihre Migration durch Europa. Der Vergleich der französischen und deutschen Kulinargeschichte eignet sich besonders für diese Aufgabe, da Frankreich seit jeher einen kulturell prägenden Einfluss auf deutschsprachigen Völker hatte!
         Die Partner, das Zentrum für Informationsmodellierung der Universität Graz und das Laboratoire CESR (Centre d’Etudes Supérieures de la Renaissance) der Universität Tours werden diese mehrsprachigen Texte nach modernen Standards aufarbeiten und sie mit aktuellen quantitativen und qualitativen Forschungsmethoden untersuchen. Für eine computergestützte Analyse werden die Rezeptsammlungen und die darin enthaltenen Texte und deren Metadaten in TEI/XML (Digitale Transkription und Edition) modelliert und mit Semantic Web Technologien analysiert (Digitale Annotation und Datenvisualisierung). Die Daten werden einer Langzeitarchivierungsinfrastruktur (GAMS, Zentrum für Informationsmodellierung Graz) zugeführt, in der sie weiter erforscht werden können. Alle Rezepte werden mithilfe von Vokabularien für Zutaten, Kochprozesse und Kochutensilien sowie kulturhistorisch relevanten Metadaten (z. B. in Bezug auf religiöse, kulturelle oder medizinische Aspekte) angereichert. Aufgrund dieser Informationen wird das Projekt über die Sprachgrenzen hinweg konkurrierende oder abweichende Essgewohnheiten, Textmigration sowie den gegenseitigen Einfluss der Nachbarländer auf ihre jeweilige Küche zu Tage fördern. Für die Analyse der deutschsprachigen Texte werden außerdem NLP-Methoden für historische Sprachstufen herangezogen, um Textverwandtschaften innerhalb dieser Überlieferung untersuchen zu können. Die Forschungsdaten und die Auswertungsergebnisse werden die Grundlage für eine räumliche und zeitliche Visualisierung und statistische Auswertung bilden, die neue Ansätze zur Interpretation des historischen und kulturellen Vermögens fördern wird.
         Die im Projekt erarbeiteten Workflows und Daten werden ganz im Sinne des Open Science Gedanken und den FAIR-Prinzipien für die Nachnutzung zur Verfügung gestellt:
         Der Transkriptionsworkflow und die Transkriptionsprinzipien (Theorie und Praxis, in Kooperation mit KONDE) können zur Gänze nachgenutzt werden. Da die Manuskripte mit Transkribus
                 transkribiert wurden, steht ein trainiertes HTR-Modell zur Verfügung mit dem eine automatische Handschriftenerkennung von ähnlichen Texten denkbar ist. Das Annotationsvokabular (Zutaten, Speisen, Küchengeräte, Zubereitungsweisen) wird samt der zugewiesenen semantischen Wikidata-Konzepte zur Verfügung gestellt und stellt somit eine essentielle Basis für die Forschung im Bereich Kulinarhistorik dar. Die Konzepte in Wikidata werden falls vorhanden kontrolliert und gegebenenfalls mit weiteren Daten (wie etwa Links zu relevanten Ontologien wie FoodO oder SNOMED) von unserer Seite angereichert. Noch nicht vorhandene Konzepte werden von uns neu erstellt und mit allen nötigen Daten (Statements) versehen. Die Nutzung von Wikidata verfolgt neben praktischen Überlegungen hauptsächlich das Ziel, die im Projekt gewonnenen Daten auf einfache Art und Weise für die Community bereitzustellen und eine weitere Bearbeitung dieser Daten zu ermöglichen. Überdies hinaus werden von uns auch die Annotationsskripte (Python und XSLT) für die Übertragung der Annotationsvokabularien nach TEI/XML zum Download angeboten. 
            
         Die überlieferten Texte werden durch eine hyperdiplomatische Neutranskription der historischen Quellen einheitlich erfasst und stehen als TEI/XML ebenfalls zur weiteren Nutzung zur Verfügung. Die Quellentexttranskription verzeichnet dabei nicht nur das unterschiedliche Schriftzeicheninventar, sondern auch alle textstrukturierenden Elemente. Das gesamte Zeicheninventar ist in einer nach den Richtlinien der TEI erstellten Zeichenbeschreibung erfasst. Die Beschreibung stützt sich dabei auf die theoretischen Ergebnisse zur Beschreibung von Zeichen aus dem DigiPal-Projekt und verwendet außerdem die Zeichenidentifikatoren der Medieval Unicode Font Initiative (vgl. Böhm, Klug 2020). Die so produzierten Daten sind nicht nur der Ausgangspunkt für die wissenschaftlichen Fragestellungen im Projekt, sondern bieten eine solide Grundlage für viele weitere Forschungsfragen aus Germanistik/Linguistik, Paläographie usw. Die Textdaten werden für eine Nutzung durch NLP Tools auch als Plaintext angeboten und die Handschriftenabbildungen sind je nach Nutzungsbedingungen der Bibliotheken frei verfügbar. 
            
         Darüber hinaus wird aus dem CoReMA-Projekt heraus ein Modell für die Integration weiterer Texte in die Forschungsumgebung bereitgestellt. Das Projekt soll fachliche Impulse für alle betroffenen Disziplinen der mittelalterlichen und frühneuzeitlichen Geschichte, Kulinargeschichte, Digitale Edition und Digital Humanities liefern.
      
      
         
            
               http://www.digitale-edition.at/
            
            
               https://transkribus.eu/Transkribus/
            
            
               https://www.wikidata.org
            
            
               http://foodon.org/
            
            
               https://browser.ihtsdotools.org/
            
            
      Describing Handwriting I-VII; http://www.digipal.eu/blog
            
            
               https://folk.uib.no/hnooh/mufi/
            
         
         
            
               Bibliographie
               
                  Adamson, M. W. (Ed.)  (1995): Food in the Middle Ages. A Book of Essays. New York, London: Garland. Adamson, M. W. (Ed.). (2002). Regional Cuisines of Medieval Europe: A Book of Essays. New York, London: Routledge.
      
               
                  Amoia, M., Martínez, J.M.M.  (2019): SaCoCo Diachronic Corpus [WWW Document]. URL 
	http://fedora.clarin-d.uni-saarland.de/sacoco/
	(accessed 1.7.20).
      
               
                  Böhm, A. & Klug, H.: Quellenorientierte Aufbereitung historischer Texte im Rahmen digitaler Editionen: Das Problem der Transkription in mediävistischen Editionsprojekten. In: [Titel steht noch nicht fest] Hrsg. von Ingrid Bennewitz und Martin Fischer (= Bamberger interdisziplinäre Mittelalterstudien.) [in Vorbereitung]
      
               
                  Carlin, M., & Rosenthal,
	J. T. (Eds.).  (1998): Food and Eating in Medieval Europe. London: Hambledon Press.
      
               
                  Flandrin, J.-L.  (1984): «Internationalisme, nationalisme et régionalisme dans la cuisine des XIVe et XVe siècles: le témoignage des livres de cuisine». In Manger et boire au Moyen âge. Actes du Colloque de Nice (15-17 octobre 1982). (pt. 2, p. 75-91). Paris.
      
               
                  Flandrin, J.-L. & Hyman, P.  (1988): “Regional tastes and cuisines: Problems, documents, and discourses on food in Southern France in the 16th and 17th centuries”. Food and Foodways 1-3, p. 221-251.
      
               
                  Gloning, T.,  (2000): Monumenta Culinaria et Diaetetica Historica. Corpus of culinary & dietetic texts of Europe from the Middle Ages to 1800. Corpus älterer deutscher Kochbücher und Ernährungslehren [WWW Document]. URL http://www.staff.uni-giessen.de/gloning/kobu.htm
	(accessed 1.7.20).
      
               
                  Hieatt, C.  (1995): Sorting through the Titles of Medieval Dishes: What Is, or Is Not, a “Blanc manger”. In M. W. Adamson (Ed.), Food in the Middle Ages. A Book of Essays. (pp. 25-43). New York, London: Garland.
      
               
                  Hyman, P. & M.  (2005). «Les associations de saveurs dans les livres de cuisine français du XVIe siècle». In 
	Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles). Actes du colloque international à la mémoire de Jean-Louis Flandrin
	(Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 135-150). Saint-Denis: Presses Universitaires de Vincennes.
      
               
                  Karg, S. (Ed.).  (2007): Medieval Food Traditions in Northern Europe. Copenhagen: National Museum of Denmark.
      
               
                  Klug, H. W., & Kranich, K. (2015): “Das Edieren von handschriftlichen Kochrezepttexten am Weg ins digitale Zeitalter. Zur Neuedition des Tegernseer Wirtschaftsbuches.” In T. Bein (Ed.), Vom Nutzen der Editionen. Zur Bedeutung moderner Editorik für die Erforschung von Literatur- und Kulturgeschichte. (pp. 121-137). Berlin, Boston: De Gruyter.
      
               
                  Laurioux, B. (2005): «Les voyageurs et la gastronomie en Europe à la fin du Moyen âge ». In Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles), Actes du colloque international à la mémoire de Jean-Louis Flandrin (Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 99-117). Saint-Denis, Presses Universitaires de Vincennes.
      
               
                  van Winter, J. M.  (1989). “Kochen und Essen im Mittelalter.” In
      B. Herrmann (Ed.). Mensch und Umwelt im
      Mittelalter. (pp. 88-100). Frankfurt am Main: Fischer
      Taschenbuch Verl.
      
            
         
      
   



      
         Die journalistische Gattung der „Spectators“ des 18. Jahrhunderts stellt ein wichtiges Kulturerbe aus der Zeit der Aufklärung dar. Die Zeitschriften entsprachen dem demokratischen Ideal, kulturelle und moralische Fragen in nicht-akademischen Kreisen zu verbreiten und Werte der Aufklärung wie Weltoffenheit, Toleranz, intellektuelle Kritik und soziale Verantwortung zu popularisieren. Ausgehend von den englischen Modellzeitschriften 
                The Tatler (1709-1711), 
                The Spectator (1711-1712 bzw. 1714) und 
                The Guardian (1713) hat sich dieses journalistische Genre über ganz Europa mittels Übersetzungen, Adaptionen und Imitationen verbreitet. Auf Basis des mehrsprachigen Korpus (derzeit Italienisch, Spanisch, Französisch, Englisch, Deutsch, Portugiesisch) der Digitalen Edition 
                The Spectators in the International Context (Ertler et al.) zeigt das Poster die Ausbreitung zentraler Themen des Aufklärungsdiskurses, wie etwa Theater, Sitten und Bräuche, Frauen- und Männerbild. 
            
         Anhand der Untersuchung von populären Themen mit maschinellen Methoden präsentiert der Beitrag zentrale Linien des Transfers von Diskursen innerhalb des Genres der Spectators und reflektiert damit den Zeitgeist des 18. Jahrhunderts. Mit Hilfe einer Kombination aus 
                close reading, 
                distant reading und Visualisierungsmethoden wird aufgezeigt, welche Themen lokale Relevanz hatten und welche länderübergreifend in den Diskurs aufgenommen wurden. Innerhalb letzterer soll zwischen Themen kontrastiert werden, die in unterschiedlichen Sprachgemeinschaften unabhängig voneinander Relevanz erlangt hatten, und jenen, die durch Imitation und Übersetzung verbreitet wurden.
            
         Als Fallbeispiel kann das Thema Theater angeführt werden, das in zahlreichen Zeitschriften unterschiedlicher Länder unabhängig voneinander diskutiert wurde. Während in Frankreich das neoklassizistische Theater im 18. Jahrhundert bereits vollends etabliert und somit als Thema in den Spectators weniger relevant war, erlebten Italien und Spanien eine bewegte nationale Theaterdiskussion. Seit dem 16. Jahrhundert hatte sich das italienische Theater stetig weiterentwickelt. Im 18. Jahrhundert wurde es jedoch als Repräsentationsmedium für ein modernes Italien auserkoren und erlebte eine massive Veränderung. Unabhängig davon wurde auch in Spanien der politische Streit zwischen Progressisten und Traditionalisten über das Thema des Theaters ausgetragen: Spanische Intellektuelle versuchten zunehmend durch kulturelle Reformen den intellektuellen Anschluss an Europa zu schaffen und lieferten sich mit Traditionalisten einen regelrechten Streit über eine radikale Reform des Theaters nach neoklassizistisch französischem Vorbild. (Vgl. z.B. Guinard 1973, 133-138; Ertler 2003, 120-124)
         Die Basis der Untersuchungen bildet das Korpus von etwa 4000 Texten, das im 
                close reading-Verfahren hinsichtlich der narrativen, thematischen und sachorientierten Inhalte annotiert und mittels des XML/TEI Standards (TEI Consortium 2019) ausgezeichnet wurde. Der Vorteil der TEI Kodierung für die Analyse ist neben der quantitativen Auswertung der Metadaten, Schlagworte und Entitäten die Möglichkeit von Untersuchungen auf Basis der narrativen Textstruktur. So können nicht nur der gesamte Text, sondern in den Zeitschriftentexten ausgezeichnete narrative Erzählformen (wie etwa Traumsequenzen oder Leserbriefe) separiert voneinander analysiert und kontrastiert werden. Neben der quantitativen Auswertung der manuell zugewiesenen Themen wird Topic Modelling (z.B. Blei et al. 2003, 
                Jelodar et al. 2019, Kuang et al. 2015) eingesetzt, um diese Annotationen zu bestätigen und zu ergänzen und neue Perspektiven auf die Rezeption des Materials zu eröffnen. Abb. 1 und 2 zeigen einen Vergleich der manuellen und der maschinellen Auswertung von Themen: Es zeigt sich eine Übereinstimmung beider Herangehensweisen dahingehend, dass die maschinelle Auswertung das verstärkte Auftreten des Themas Theater in den Ausgaben 18-27, 37-46 und 68-78 bestätigt.
            
         
            
             Abbildung 1: Statistische Auswertung der manuell zugewiesenen Themen (in rosa z.B. “Theatre Literature Arts”) auf die Ausgaben des El Pensador.
         
         
            
            Abbildung 2: Topic Modelling: Verteilung des Topics “Comedia & Theatro” (mit den 10 häufigsten Wörtern comedia, theatro, poetas, pueblo, pieza, amor, accion, nacion, pasion, tragedia) auf die Ausgaben des El Pensador. 
         
         Darüber hinaus werden die (manuellen und maschinellen) Annotationen verwendet, um das zeitliche Auftreten ausgewählter Themen innerhalb einer Sprachgemeinschaft und sprachenübergreifend zu analysieren. Als Referenz für diese Untersuchungen dient das Zeitschriftennetzwerk der Spectators: Dieses stellt die Abhängigkeit von Zeitschriften unterschiedlicher Sprachgemeinschaften hinsichtlich Übersetzung, Adaption und Imitation dar.
         
            Der wissenschaftliche Beitrag des Posters und der dem Beitrag zugrunde liegenden Arbeit am Projekt 
                Distant Spectators: Distant Reading for Periodicals of the Enlightenment – einer Kooperation zwischen dem Zentrum für Informationsmodellierung und dem Institut für Romanistik der Universität Graz, sowie dem Institute of Interactive Systems and Data Science der Technischen Universität Graz und dem Know-Center Graz – kann in zwei Aspekte unterteilt werden. Einerseits kann durch die Anwendung von 
                distant reading-Methoden die Liste der manuell selektierten Themen erweitert und feiner granuliert werden. Dieser Zugang relativiert die vorab generierte Leseerwartung und ermöglicht somit einen unvoreingenommenen Blick auf den Text sowie die Analyse größerer, ggf. noch nicht annotierter Korpora. Andererseits wird durch die vorgestellte Arbeit die öffentlich zugängliche Digitale Edition der Spectators (Ertler et al.) zusätzlich (semi-automatisiert) angereichert und durch auf den TEI-Annotationen basierende Visualisierungen augmentiert. Dies ist ein erster explorativer Schritt in Richtung intelligentes maschinelles Lernen im Kontext der Spectators und eröffnet neue Wege der Erschließung, Analyse und Präsentation dieser multilingualen literaturwissenschaftlichen Ressource.
            
      
      
         
            
               Bibliographie
               
                   Ertler, K.; Fuchs, A.; Fischer, M.; Hobisch, E.;
  Scholger, M.; Völkl, Y. (Hg.)  (2011-2019): 
  The Spectators in the International Context, https://gams.uni-graz.at/spectators.

               
                   Ertler, K. (2003): 
  Moralische Wochenschriften in Spanien. José Clavijo y Fajardo: ‚El Pensador’. Tübingen: Gunter Narr.

               
                   Guinard, P.  (1973): 
  La Presse espagnole de 1737 à 1791. Formation et signification d’un genre. Paris: Centre de recherches hispaniques.

               
                  Blei, D. M., Ng, A. Y., & Jordan, M. I.  (2003): Latent dirichlet allocation. 
  Journal of machine Learning research, 3 (Jan), 993-1022.

               
                  Jelodar, H., Wang, Y., Yuan, C., Feng, X., Jiang, X., Li, Y., & Zhao, L. (2019): Latent Dirichlet Allocation (LDA) and Topic modeling: models, applications, a survey. 
  Multimedia Tools and Applications, 78 (11), 15169-15211.

               
                  Kuang, D., Choo, J., & Park, H.  (2015): Nonnegative matrix factorization for interactive topic modeling and document clustering. In 
  Partitional Clustering Algorithms (pp. 215-243). Springer, Cham.

               
                  TEI Consortium  (2019): eds. 
  Guidelines for Electronic Text Encoding and Interchange. July 2019. http://www.tei-c.org/P5/.

            
         
      
   



      
         Digitale Editionen stellen als digitalisierte und tiefenerschlossene Textressourcen eine wertvolle Quelle zur Nachnutzung innerhalb großflächiger linguistischer und literaturwissenschaftlicher Analysen dar (Rybicki, 2019). Zusätzlich werden innerhalb von digitalen Editionsprojekten selbst immer öfter textanalytische Verfahren eingesetzt.
         Das am Zentrum für Informationsmodellierung der Universität Graz entwickelte und betriebene Repositorium GAMS (Geisteswissenschaftliches Asset Management System) umfasst als Forschungsdateninfrastruktur Daten von mehr als hundert Forschungsprojekten aus verschiedenen Wissenschaftsbereichen. Digitale Editionen und Textsammlungen machen dabei, neben neben digitalen Sammlungen aus dem Kulturerbebereich, den Großteil der im Repositorium vorhandenen Bestände aus.
            
         Um die bereits im Repositorium vorhandenen Textressourcen in geeigneten Formaten nachnutzbar bereitzustellen, beziehungsweise diesen Aspekt im Zuge laufender und zukünftiger Projekte berücksichtigen zu können, wurden während der letzten Monate Adaptierungen an der GAMS-Infrastruktur vorgenommen, die mit diesem Poster erläutert und dargestellt werden sollen.
         
            Technischer Hintergrund
            GAMS ist eine registrierte, trusted Repositoriumsinfrastruktur, die auf der Free and
Open Source Software Fedora Commons
basiert. Sie setzt auf eine OAIS-konforme Architektur und verfolgt
eine weitgehend XML-basierte Content-Strategie. GAMS ermöglicht seinen
Benutzer*innen die Verwaltung und Veröffentlichung von Ressourcen aus
Projekten mit permanenter Identifizierung und Anreicherung mit
Metadaten. Ein speziell entwickelter Client (Cirilo) stellt Funktionalitäten für Massenoperationen an den gespeicherten Objekten zur Verfügung. (Stigler/Steiner 2018)
                
         
         
            Objekt Modell
            
               Content Models definieren komplexe digitale Objekte, die dem Fedora-Objektmodell entsprechen. Sie sind speziell auf die Anforderungen, die Forschungsdaten aus unterschiedlichen geisteswissenschaftlichen Bereichen an Langzeitarchivierung und Datendissemination stellen, ausgelegt. Für wissenschaftliche Editionen wird beispielsweise ein speziell entworfenes 
                    TEI Content Model eingesetzt.
                
            Jedes Modell enthält einen primären Datenstrom, der die Inhaltsdaten des Objekts enthält, zum Beispiel ein TEI-Dokument. Zusätzliche Datenströme können Metadaten (z.B. Dublin Core), weitere Inhaltsdaten oder aus dem primären Datenstrom derivierte Daten enthalten (z. B. aus dem TEI-Dokument extrahierte RDF Daten).
            Für die jeweiligen Modelle definierte Services kombinieren und transformieren Datenströme zu Präsentationsinhalten, auf die in verschiedenen Ausgabeformaten über im Content Model definierte Schnittstellen zugegriffen werden kann. Ein häufig verwendetes Format zur Dissemination ist HTML, was die Präsentation der Daten über eine dynamisch erzeugte Webseite ermöglicht.
            
               Contexte, als spezielle Containerobjekte, ermöglichen es, einzelne Inhaltsobjekte in größere Einheiten zusammenzufassen und zu organisieren. Sie enthalten wiederum eigene Datenströme und Disseminationsmethoden.
                
         
         
            Anpassungen für Textressourcen
            Zur Verwaltung und Bereitstellung von im GAMS vorliegenden Textressourcen wie auch dezidiert linguistischen Forschungsdaten wurde das bestehende TEI Content Model angepasst und erweitert. Über den Cirilo Client können Objekte als Text- bzw. Sprachressourcen gekennzeichnet werden. So gekennzeichnete Objekte werden dann automatisch mit für das CMDI Framework (Goosen et al., 2015) aufbereiteten, komponentenbasierten Metadaten und einem eigenen Handle Identifier versehen. Solche Daten können dann geharvestet werden und über das Virtual Language Observatory (Van Uytvanck et al., 2012) der CLARIN Infrastruktur als Sprachressource gefunden werden.
                
            Der OAI-Endpoint des Repositoriums wurde dementsprechend angepasst. Auf inhaltlicher Ebene wurde ein XML-basiertes Konfigurationsformat eingeführt, das es erlaubt, auf den Ausgangsdaten operierende Pipelines bzw. Toolchains zu definieren und als Massenoperation zu triggern. Ein Anwendungsfall hierfür ist beispielsweise Preprocessing zur Aufbereitung der Daten für darauf aufbauende Analyseschritte. Per Default wird eine, auf dem an der Österreichischen Akademie der Wissenschaften entwickelten XSL-Tokenizer (Schopper, 2019) basierende Pipeline ausgeführt, was einerseits ein tokenisiertes TEI-Dokument als separaten Datenstrom im Objekt erzeugt, und andererseits die Daten als Plain Text, im von den im Rahmen von CLARIN entwickelten Weblicht Tools verwendeten 
                    Text Corpus Format (TCF), sowie im von gängigen Corpus Tools verwendeten 
                    Vertical-Format bereit stellt. Diese Daten können daraufhin direkt mit den genannten Tools verarbeitet und analysiert werden. Wie der Tokenizer ist auch die Pipeline selbst projektbezogen anpassbar und kann aus mehreren Transformationsschritten bestehen, darunter beispielsweise auch die Möglichkeit, die jeweiligen Texte via TreeTagger (Schmid, 1995) zu annotieren.
                
            Die über diese Pipelines erzeugten Datenformate können benutzerdefiniert gekapselt und mit dem primären TEI-Datenstrom als Objekt im Repositorium langzeitarchiviert werden. Durch die Speicherung der Verarbeitungspipeline gemeinsam mit den zu verarbeitenden Daten wird jeder Prozessierungsschritt dokumentiert und nachvollziehbar gemacht, was wesentlich für die Nachnutzung ist.
            Für die Aggregation mehrerer TEI Objekte zu einem verarbeitbaren Corpus wurde ein sogenanntes 
                    Corpus Context Model geschaffen. Diesem Modell entsprechende Objekte können vom Benutzer selbst über den 
                    Cirilo Client angelegt und mit entsprechenden Textobjekten befüllt werden.
                
            Dieser spezielle Context stellt über die entsprechenden Datenströme Dublin Core wie auch CMDI Metadaten bereit. Die VERTICAL Datenströme der zugeordneten TEI-Objekte werden zu einem Datenstrom aggregiert, welcher bei Bedarf in einem Corpus Management System (Vorzugsweise NoSketch Engine) indiziert und über dieses abgefragt werden kann. Das aggregierte Corpus kann außerdem als ZIP-Datei heruntergeladen werden. 
            Die beschriebenen Features stehen für sämtliche im Repositorium vorhandenen Textressourcen, also nicht nur für genuin linguistische Daten zur Verfügung. Das bedeutet, dass etwa bestehende, im Repository vorhandene Digitale Editionen mit geringem Aufwand auch für linguistische Analysen verfügbar gemacht werden können.
         
      
      
         
            
               http://gams.uni-graz.at/
            
            
               https://www.re3data.org/
            
            
               https://www.coretrustseal.org
            
            
      Flexible Extensible Digital Object Repository Architecture,
      https://duraspace.org/fedora
            
            
      European Research Infrastructure for Language Resources and Technology, 
      https://www.clarin.eu/
            
            
               http://weblicht.sfs.uni-tuebingen.de
            
         
         
            
               Bibliographie
               
                   Goosen T., et al.  2015. CMDI 1. 2: Improvements in the CLARIN Component Metadata Infrastructure. Selected papers from the CLARIN 2014 Conference, pp. 36-53. 
        https://hdl.handle.net/20.500.11755/91536b93-31cb-4f4a-8125-56f4fe0a1881.
      
               
                   Rybicki, J.  (2019). Keynote at the 2019
	TEI Conference and members meeting “What is text, really? TEI
	and beyond”.
      
               
                   Schmid, H.  (1995):
	Improvements in Part-of-Speech Tagging with an Application to German. Proceedings of the ACL SIGDAT-Workshop. Dublin, Ireland.
      
               
                   Schopper, D.  (2019). XSLT-Tokenizer (Software), 
        https://github.com/acdh-oeaw/xsl-tokenizer.
      
               
                   Van Uytvanck, D., et al. (2012). Semantic metadata mapping in practice: the Virtual Language Observatory. Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC 2012), pp. 1029-1034. 
        http://www.lrec-conf.org/proceedings/lrec2012/pdf/437_Paper.pdf.
      
            
         
      
   



      
         ODD (One Document does it all) ist eine Metasprache, entwickelt im Kontext der Text Encoding Initiative (TEI), zur (formalen) Beschreibung und Dokumentation von XML Schemata. ODD bildet die Grundlage (d.h. den Quellcode) der Richtlinien und Schemata sowohl der Text Encoding Initiative (TEI) als auch der Music Encoding Initiative (MEI). Aber ODD ist nicht auf diese bestehenden Codierungsrichtlinien beschränkt; so lässt es sich auch zur Beschreibung anderer bestehender XML-Dialekte benutzen (beispielsweise HTML [4]), oder zur Entwicklung eigenständiger Codierungsrichtlinien in ganz anderen Kontexten, wie etwa im Falle von Music Performance Markup (MPM) [1].
         Das grundlegende Designprinzip von ODD folgt einem Literate Programming Ansatz, d.h. in einem ODD-Dokument sind sowohl die Code-Bestandteile zur Beschreibung der Grammatik eines Schemas als auch die menschenlesbare Beschreibung – und Exemplifizierung – dieser Regeln miteinander verwoben [6]. Im Kontext der TEI und MEI Communities wird dies insbesondere von digitalen Editionen genutzt. Der besondere Anreiz liegt für diese Unternehmungen hierbei darin, dass die Dokumentation (im ODD-Format) der jeweils spezifischen Nutzung der TEI bzw. der MEI-Richtlinien gewissermaßen das digitale Pendant zu herkömmlichen Editionsrichtlinien darstellt.
         Aus „Endnutzersicht“ wird ODD meist zur Maßschneiderung von TEI oder MEI Schemata genutzt. Die aktive Weiterentwicklung von ODD hat jedoch interessante neue Möglichkeiten eröffnet; so ist es neuerdings mittels so genanntem ODD-Chaining [2] auch möglich geworden, eigene Editionsrichtlinien nicht unmittelbar von den TEI- oder MEI-Richtlinien abzuleiten, sondern vermittelt von anderen ODD-Anpassungen. Dies wird beispielsweise im Deutschen Textarchiv genutzt, um die Richtlinien zur Auszeichnung von Drucken bzw. von Manuskripten von einem gemeinsamen DTA Basisformat abzuleiten [3].
         Solchen Anwendungsfällen von ODD werden jedoch meist nur wenige „Eingeweihte“ gewahr, da die häufig eine Verständnisbarriere für die Mechanismen von ODD vorliegt. Diesem soll der vorliegende Workshop entgegenwirken. Deshalb ist der ganztägige Workshop speziell auf die Bedürfnisse von Einsteigern ausgerichtet. Es werden sowohl die notwendigen theoretischen Hintergründe vermittelt, als auch praktische Hilfestellungen zum Erstellen eines ersten eigenen Schemas bzw. dessen Dokumentation vermittelt. Hierfür wird im ersten Teil des Workshops zunächst als niederschwelliger Einstieg der von Raffaele Viglianti neu entwickelte webbasierte ODD-Editor „Roma“ [5] vorgestellt und damit die Erstellung einfacher Anpassungen des TEI-Schemas verwendet; dazu gehören etwa Operationen wie das Hinzufügen oder Entfernen von ganzen teilbereichen des Schemas (Modulen), von einzelnen Elementen oder Attributen, sowie das Einschränken von Attributwerten auf geschlossene Listen.
         Im zweiten Teil des Workshops sollen dann durch direktes Bearbeiten des ODD-Quelldokuments erweiterte Funktionen wie Modularisierung, ODD-Chaining oder das Generieren von Schemata mit mehreren Namespaces erläutert werden.
         Der Workshop ist als Hands-On-Session konzipiert, in der die Teilnehmer an Ihrem eigenen Laptop direkt Erfahrungen sammeln sollen. Das Tutoren-Team steht ihnen dabei stets mit Rat und Tat zur Seite.
         
            Formalia
            
               Maximale Zahl der möglichen Teilnehmerinnen und Teilnehmer: 25
               benötigte technische Ausstattung: Beamer; die Teilnehmer*innen sollen eigene Laptops mit vorinstalliertem oXygen-XML-Editor mitbringen. Eine oXygen-Lizenz zur Nutzung im Workshop wird gestellt
            
         
         
            Beiträger
            
               Peter Stadler
                Wissenschaftlicher Mitarbeiter an der Carl-Maria-von-Weber-Gesamtausgabe an der Universität Paderborn und Mitglied des TEI Councils. Zu seinen Forschungsgebieten zählen digitale Musik- und Briefeditionen.
            
            
               Raffaele Viglianti
                Research Programmer am Maryland Institute for Technology in the Humanities (MITH) at the University of Maryland und Mitglied des TEI Councils. Seine Forschung kreist um Textwissenschaft und digitale Editionen mit einem Fokus auf Musik.
            
            
               Benjamin W. Bohl
                Research Software Engineer der Bernd Alois Zimmermann-Gesamtausgabe und in seiner Mitgliedschaft im MEI Board als Co-chair des MEI Technical Teams eingesetzt. In seiner Forschung befasst er sich mit Datenmodellierung und -haltung in digitalen Editionsprojekten mit plurimedialen Gegenständen.
            
         
      
      
         
            
               Bibliographie
               
                  [1] Berndt, Axel / Bohl, Benjamin W. (2018): Music Performance Markup: Formale Beschreibung musikalischer Interpretationen. In: Editio Bd. 32 (2018), Nr. 1, S. 185–204.
               
                  [2] Burnard, Lou: ODD chaining for beginners, 
               
               
                  [3] Deutsches Textarchiv: Schema und Dokumentation der DTABf Schema, http://www.deutschestextarchiv.de/doku/basisformat/schema.html
               
                  [4] Holmes, Martin 2018: Using ODD for HTML, in Proceedings of of the Text Encoding Initiative Conference and Members Meeting The Markup Conference, Tokyo, Japan, September 9–13 2018. Pages 240–241, 
               
               
                  [5] TEI Consortium: Roma ODD Editor, https://romabeta.tei-c.org
               
                  [6] Viglianti, Raffaele: One Document Does-it-all (ODD): a language for documentation, schema generation, and customization from the Text Encoding Initiative, 
               
            
         
      
   



      
         
            Einleitung
            
               Aktuelle Diskussionen über ständig wachsende Möglichkeiten der Erfassung, Speicherung und Analyse großer Datenmengen lassen uns vergessen, dass sowohl Wissenschaft als auch Behörden bereits seit Jahrhunderten Praktiken zur Datenerhebung und -verarbeitung entwickelt haben (Borck 2017, Oertzen 2017). So wurden bereits im 17. Jahrhundert astronomische und meteorologische Beobachtungsdaten in Formularen und Tabellen erfasst, in Zahlen und Symbolen kodiert und in Karten und Diagrammen visualisiert (Daston 2011, Mendelsohn 2011, Hess 2011). Auch die empirische Erfassung von personenbezogenen Körperdaten nahm ihren Anfang in den Praxisjournalen frühneuzeitlicher Ärzte, die alle Symptome, Zustandsmerkmale und Reaktionen sowie den Krankheitsverlauf, die Medikamentierung und den Heilungsprozess ihrer Patienten genau dokumentierten (Geyer-Kordesch 1990, Stolberg 2007). Nicht einmal das von den Anhängern der 
               Quantified Self
               -Bewegung praktizierte 
               Self-Tracking
                (Lupton 2016) gehört gänzlich dem 21. Jahrhundert an, wie die Tagebücher zahlreicher religiöser Selbstoptimierer aus dem 17. und 18. Jahrhundert zeigen.
            
            
               Eines der ambitioniertesten historischen 
               Self-Tracking
               -Projekte waren zweifellos die 
               Observationes in me ipso factae
                des pietistischen Arztes Johann Christian Senckenberg (1707–1772). In ihnen protokollierte er tagtäglich sein gesamtes Körper- und Seelenleben, um seinen Lebenswandel zu vervollkommnen. Neben Ernährung, Stoffwechsel, Körperaktivität sowie Schlaf- und Ruhephasen notierte er auch alle Reizempfindungen, Körperreflexe und Gemütszustände sowie alle spürbaren Umwelt- und Witterungseinflüsse (Faßhauer 2017). Zeitweise brachte er auf diese Weise täglich bis zu 5.000 Wörter zu Papier, so dass auf den Gesamtzeitraum von dreizehn Jahren gerechnet ca. 14.000 Seiten mit 12.600.000 Wörtern zusammenkamen. Der Beitrag beleuchtet zunächst den epistemischen Zweck dieses riesigen frühneuzeitlichen Datenpools, aus dem derzeit in Frankfurt ausgewählte Bände digital ediert werden (Faßhauer 2018), und setzt sie mit zeitgenössischen Positionen zum Verhältnis von Daten und Theorie in Beziehung. Anschließend wird die Möglichkeit einer Analyse dieser Daten mit modernen 
               Distant-reading
               -Methoden und deren Vereinbarkeit mit dem epistemischen Ziel des religiösen Autors diskutiert.
            
         
         
            Selbstbeobachtung und Anti-Rationalismus
            
               Wenn ein Selbstoptimierer des digitalen Zeitalters beschließt, seine Lebenszeit effizient zu nutzen, seinen Körper gesund zu erhalten oder seine Finanzen zu organisieren, bezweckt er damit meist größtmöglichen persönlichen Erfolg und Selbstzufriedenheit im Diesseits. Ein religiöser Self-Tracker des 18. Jahrhunderts hatte hingegen zu allererst sein Seelenheil und seine Erlösung nach dem Tod im Sinn. Diese konnte jedoch nur erlangen, wer die ihm anvertrauten Gottesgeschenke auf Erden treulich verwaltete, pflegte und mehrte. Genau wie für materielle Güter galt dies auch für Gesundheit und Wissenskapital. Nach Auffassung religiöser Gelehrter wie Senckenberg war der Mensch seit dem Sündenfall jedoch geistig so zerrüttet, dass er durch seine Verstandeskräfte zu keinen verlässlichen Erkenntnissen gelangen konnte. Insbesondere theoretische Modelle, die durch „künstliches syllogisiren der Vernunfft“ dem Verstand der Gelehrten („ex mente Doctorum“) entstiegen sind, repräsentierten nur fragmentarisches oder abstraktes Wissen. Zudem müssten ohnehin „alle Regulae universaliores erstlich ab experientia in particularibus“ abgeleitet werden, deren Vielfalt jedoch so viele Ausnahmen aufzeige, „daß die Regulae selbst wieder darüber zernichtet werden“ (Senckenberg 1735: 1r/v, Faßhauer 2017). Sichere medizinische Erkenntnisse waren deshalb nur „ohne Praeoccupation von einer vorher gefassten Hypothesi“ durch mehrfach wiederholte unmittelbare Selbsterfahrung zu erlangen, deren Resultate möglichst vollständig aufgezeichnet und induktiv ausgewertet werden mussten. Auf diese Weise ließen sich Vergleiche mit Aufzeichnungen aus ähnlichen Situationen herstellen, wobei einzelne Faktoren miteinander korreliert, auf ihre Relevanz und Rolle im Gesamtkontext befragt und als mögliche Ursachen oder Auswirkungen anderer Faktoren in Betracht gezogen werden konnten.
            
            
               Ganz ähnliche Überzeugungen wie Senckenberg äußerte der amerikanische Journalist Chris Anderson, als er im Jahre 2008 das Ende der Theorie und den Beginn des Datenzeitalters verkündete. Auch er ging dabei von der Prämisse aus, dass Theorien die Realität nur verzerrt wiedergäben und letztlich allein in den Hirnen der Wissenschaftler existierten: „The scientific method is built around testable hypotheses. These models, for the most part, are systems visualized in the minds of scientists”. Stattdessen verwies er wie Senckenberg auf die Möglichkeit, durch Erhebung und Speicherung einer möglichst großen Datenfülle ungleich genauere und verlässlichere Aussagen über die Welt in ihrer ganzen Komplexität zu treffen. War der Verzicht auf die Suche nach letztgültigen Kausalitäten bei Senckenberg noch religiös motiviert, entspringt er bei Anderson aus der pragmatischen Erkenntnis, dass die Verfügbarkeit nie gekannter Datenmengen die Notwendigkeit zur Hypothesenbildung schlichtweg erübrige, da sie unter den verschiedensten Gesichtspunkten miteinander korreliert werden könnten. Durch ihre maschinelle Auswertbarkeit gerieten zudem auch Einzelheiten und Muster ins Blickfeld, die der theoriegeleiteten Forschung entgingen: „Correlation supersedes causation, and science can advance even without coherent models, unified theories, or really any mechanistic explanation at all“ (Anderson 2008). Seither wurde gegen Andersons These wiederholt eingewandt, dass bereits die Erstellung von Datensätzen auf theoretischen Prämissen und Selektionskriterien beruhe (boyd/Crawford 2012; Boellstorff 2014). Zudem stelle jede Datenanalyse eine subjektive Interpretation innerhalb bestimmter kultureller, dogmatischer oder ideologischer Kontexte dar, so dass eine hypothesenfreie Datenauswertung unmöglich sei. Die gleiche Problematik lässt sich auch für die Aufzeichnungen Senckenberg aufzeigen: Außer Thermometer und Barometer stand ihm nur sein eigenes Bewusstsein als Messinstanz zur Verfügung, das alle Empfindungen und Wahrnehmungen zwangsläufig subjektiv registrierte, filterte und interpretierte und dabei den protokollierten physischen und seelischen Befindlichkeiten selbst unterworfen war. Auch erfolgte die Aufzeichnung der Daten allein durch seine eigene schreibende Hand, die auf körperliche Irregularitäten ebenso empfindlich reagierte wie auf seelische Erschütterungen und Stimmungsschwankungen. Waren Körper und Bewusstsein anderweitig okkupiert, konnte die Datenerfassung entweder gar nicht oder nur rückwirkend und durch das Gedächtnis vermittelt erfolgen.
            
            
               
               Abbildungen 1a und 1b: Selbstbeobachtung in Senckenbergs Tagebuch, Dezember 1732 (Manuskript und TEI/XML-Transkription) 
            
         
         
            Datengetriebenes Distant Reading?  
            
               Senckenbergs riesiger Datenpool konfrontiert seine modernen Leser mit einem so mikroskopisch detaillierten Bewusstseinsstrom, dass ein Verständnis seiner Erkenntnisse im 
               close reading
               -Modus nahezu unmöglich ist. Die digitale Erschließung einzelner Bände im Rahmen der 
               Frankfurter Auswahledition
                ermöglicht nun eine Annäherung an das umfangreiche Textmaterial aus der Makroperspektive (Abb. 1a–b). Der Literaturwissenschaftler Franco Moretti hat dieses Vorgehen bekanntlich als „distant reading“ bezeichnet, da Distanz hier statt eines Hindernisses eine Bedingung der Erkenntnis darstelle: „it allows you to focus on units that are much smaller or much larger than the text“. Die Reduktion von Senckenbergs umfangreichen Beobachtungsdaten auf abstrakte Schemata scheint jedoch zunächst im Widerspruch zu seiner Absicht zu stehen, die ganze Vielfalt der natürlichen Erscheinungsformen unverkürzt zu erfassen. Auch Moretti hat auf dieses Problem hingewiesen: „If we want to understand the system in its entirety, we must accept losing something. We always pay a price for theoretical knowledge: reality is infinitely rich; concepts are abstract, are poor” (Moretti 2013: 48–49). Die irreversible Reduktion von Texten auf abstrakte Schemen, die in der Forschung sogar als gewaltsame Zerstörung des eigentlichen Untersuchungsgegenstandes beschrieben worden ist (Bradley 2012), kann nur in solchen Forschungsumgebungen vermieden werden, die – wie etwa die Voyant Tools (Sinclair und Rockwell 2003) – einen flexiblen Wechsel zwischen der Text- und der Grafikebene und damit zwischen dem 
               close
               - und 
               distant reading
               -Modus ermöglichen (Jänicke 2016: 20–23). Ein möglicher Ausgangspunkt für eine Fernlektüre ist die Identifizierung von Schlüsselwörtern, die hier anhand von Häufigkeitskriterien erfolgt. Werden in der Liste einzelne Keywords ausgewählt, kann deren Verteilung im Korpus angezeigt werden. Eine Visualisierung der markantesten körperlichen Empfindungen Senckenbergs zwischen August und Dezember 1732 zeigt zum Beispiel, dass im November und Dezember Spannungs- und Druckgefühle vorherrschten, während er im Oktober hauptsächlich Stiche und Zuckungen verspürte, im August und September aber frei von derlei Empfindungen war (Abb. 2). Ein Blick auf die Kollokationen dieser Begriffe zeigt, in welchen Körperteilen sie am häufigsten bemerkbar waren (Abb. 3). Allerdings stellen derlei Festlegungen auf bestimmte Untersuchungszeiträume oder Körperempfindungen bereits Arbeitshypothesen dar, die mit einer bestimmten Erwartungshaltung einhergehen und das Ergebnis dadurch nicht unmaßgeblich präformieren. Die oben aufgezeigte Unmöglichkeit des von Senckenberg projektierten theoriefreien Wissenserwerbs spiegelt sich deshalb unmittelbar in der digitalen Korpusanalyse wider, die gleichfalls nicht rein datengetrieben bzw. ohne hypothetische Vorüberlegungen erfolgen kann. 
            
            
               
               Abbildung 2: Häufigkeit von Körperempfindungen zwischen August und Dezember 1732 
            
            
               
               Abbildung 3: Verteilung von Druckempfindungen auf verschiedene Körperteile 
            
            
               Auch Senckenbergs Essgewohnheiten lassen sich nur mit begriffsbasierten Suchabfragen analysieren. Die Suche nach dem Schlüsselwort „bibi” (ich trank) im Kontext der fünf angrenzenden Wörter zeigt beispielsweise, dass Senckenberg zwar überwiegend Wasser und Tee trank, aber bereits an dritter Stelle der Wein folgte (Abb. 4). Berücksichtigt man jedoch auch andere Getränke wie Kaffee, Bier, Alantwein und Milch sowie die entsprechenden lateinischen Begriffe, so ergibt sich aus dem Verhältnis zwischen alkoholfreien und alkoholischen Getränken das eher moderate Verhältnis von 253 zu 105. Übermäßiger Alkoholkonsum konnte jedoch Gottes Unwillen hervorrufen und durch körperliche und mentale Beschwerden bestraft werden, die sich gleichfalls in den Aufzeichnungen niederschlagen. Die Akribie der Senckenbergischen Selbstbeobachtung ermöglicht es, Vergleiche zwischen den in mehreren solcher Situationen auftretenden Symptomen anzustellen, die als Muster visualisiert und auf Ähnlichkeiten und Abweichungen durch Begleitfaktoren untersucht werden können. Abb. 5 zeigt etwa, dass sich die körperlichen Auswirkungen des Weinkonsums im warmen Monat September (a), den der Diarist für zahlreiche Freiluftaktivitäten nutzte, deutlich von denen im kälteren November (b) unterscheiden, welchen der Autor größtenteils daheim verbrachte. Noch aussagekräftiger sind die Ergebnisse einer Symptomanalyse auf Wochen- oder Tagesbasis. Dabei ist weniger bedeutsam, ob und wie die Symptome kausal zusammenhängen: Wichtiger ist es zu zeigen, dass und auf welche Weise sie gemeinsam auftreten, und wie sich verschiedene Situationen voneinander unterscheiden.
            
            
               
               Abbildung 4: Meistkonsumierte Getränke zwischen August und Dezember 1732 
            
            
               
               Abbildung 5a: Korrelationen zwischen Weinkonsum und Körperempfindungen in zwei verschiedenen Situationen 
            
            
               
               Abbildung 5b: Korrelationen zwischen Weinkonsum und Körperempfindungen in zwei verschiedenen Situationen 
            
         
      
      
         
            
               Bibliographie
               
                  Anderson, Chris (2008): “The End of Theory. Will the Data Deluge Make the Scientific Method Obsolete?” in:
  Edge, http://www.edge.org/3rd_culture/anderson08/anderson08_index.html
  [letzter Zugriff 20. Dezember 2019].

               
                  Borck, Cornelius  (2017): „Big Data. Praktiken und Theorien der Datenverarbeitung im historischen Querschnitt“, in:
  Zeitschrift für Geschichte der Wissenschaften, Technik und Medizin (NTM)  25.4, 399–405.

               
                  Boyd, Danah und Crawford, Kate  (2012): “Critical Questions for Big Data”. In:
  Information, Communication & Society 15:5, 662–679, DOI: 10.1080/1369118X.2012.678878 [letzter Zugriff 20. Dezember 2019].

               
                  Boellstorff, Tom  (2014): „Die Konstruktion von Big Data in der Theorie“. In: Reichert, Ramón (Ed.):
  Big Data. Analysen zum digitalen Wandel von Wissen, Macht und Ökonomie, Bielefeld, 105–131.

               
                  Bradley, Adam James  (2012): 
  Violence and the Digital Humanities Text as Pharmakon, in:
  Proceedings of the Digital Humanities 2012. 
                  http://www.dh2012.uni-hamburg.de/wp-content/uploads/2012/07/HamburgUP_dh2012_BoA.pdf
  [letzter Zugriff 20. Dezember 2019].

               
                  Daston, Lorraine  (2011): “The Empire of Observation, 1600–1800”, in: Daston, Lorraine / Lunbeck, Elizabeth (eds.): 
  Histories of Scientific Observation, Chicago/London: University of Chicago Press, 81–113.

               
                  Faßhauer, Vera  (2017): 
  Sacra à Deo in corde discenda, natura ex natura. Die Observationes Johann Christian Senckenbergs als medico-theologische Aufzeichnungspraktik“, in:
  Berichte zur Wissenschaftsgeschichte  40, 225–246.

               
                  Faßhauer, Vera  (2018): “Accessing, Editing and Indexing Large Manuscript Collections: The Selected Edition of J. Chr. Senckenberg’s Journals.” In:
  Knowledge Organization for Digital Humanities. Proceedings of the 15th Conference on Knowledge Organization WissOrg’17 of the German Chapter of
  the International Society for Knowledge Organization (ISKO), 30th November – 1st December 2017, Freie Universität Berlin, ed. Christian Wartena, Michael Franke-Maier and Ernesto de Luca, Berlin 2018, 31–36.
  https://refubium.fu-berlin.de/bitstream/handle/fub188/20535/ProcWissOrg2017.pdf
  [letzter Zugriff 20. Dezember 2019].

               
                  Geyer-Kordesch, Johanna  (1990): „Medizinische Fallbeschreibungen und ihre Bedeutung in der Wissensreform des 17. und 18. Jahrhunderts“, in:
  Medizin, Gesellschaft und Geschichte  9, 7–19.

               
                  Hess, Volker (2011): „Das Material einer guten Geschichte. Register, Reglements und Formulare“, in: Dickson, Sheila / Goldmann, Stefan / Wingertszahn, Christof (eds.):
  Fakta, und kein moralisches Geschwätz. Zu den Fallgeschichten im Magazin zur Erfahrungsseelenkunde (1783–1793), Göttingen: Wallstein, 115–139.

               
                  Jänicke, Stefan  (2016): 
  Close and Distant Reading Visualizations for the Comparative Analysis of Digital Humanities Data, Diss. Leipzig.
   [letzter Zugriff 20. Dezember 2019].

               
                  Lupton, Deborah  (2016): 
  The Quantified Self. A Sociology of Self-Tracking, Cambridge: Polity Press.

               
                  Mendelsohn, J. Andrew  (2011): “The World on a Page: Making a General Observation in the Eighteenth Century”, in: Daston, Lorraine / Lunbeck, Elizabeth (eds.): 
Histories of Scientific Observation, Chicago/London: University of Chicago Press, 396–420.

               
                  Moretti, Franco  (2013): Distant Reading, London/New York: Verso.

               
                  Oertzen, Christine von  (2017): „Die Historizität der Verdatung: Konzepte, Werkzeuge und Praktiken im 19. Jahrhundert“, in: 
  Zeitschrift für Geschichte der Wissenschaften, Technik und Medizin (NTM)  25.4, 407–434.

               
                  Senckenberg, Johann Christian  (1732):
  Tagebücher, Bd. 2: Observationes in me ipso factae, August–Dezember 1732, Senckenbergisches Archiv, Na 31, 2, UB Frankfurt am Main, Digitalisat unter 
  
  [letzter Zugriff 20. Dezember 2019].

               
                  Senckenberg, Johann Christian (1735): 
Briefentwurf an einen unbekannten Empfänger, 24. Januar 1735, Senckenbergisches Archiv, Mp. 57, UB Frankfurt am Main.

               
                  Sinclair, Stéfan / Rockwell, Geoffrey (2003): Voyant Tools. 
  http://‌voyant-tools.org.

               
                  Stolberg, Michael (2007): „Formen und Funktionen medizinischer Fallberichte in der Frühen Neuzeit (1500– 1800)“ in: Süßmann, Johannes / Scholz, Susanne / Engel, Gisela (eds.): 
Fallstudien: Theorie – Geschichte – Methoden. Berlin: Trafo, 81–89. 

            
         
      
   



      
         
            Einleitung
            Digitale Analyseverfahren verändern immer intensiver die Forschungsweise der GeisteswissenschaftlerInnen und mit dem wachsenden Spielraum der Methoden wächst auch die Anzahl an Fragen, die sich vor allem an den Grad der Genauigkeit und wissenschaftliche Relevanz dieser Methoden richtet. Das Topic Modeling gewinnt als eine Methode für automatische Erkennung von versteckten thematischen Strukturen in großen Textmengen (Blei 2012: 8) immer mehr an Beliebtheit, erweckt aber auch Unsicherheiten. Daher beschäftigt sich diese Arbeit mit den Möglichkeiten und Problemen des Topic Modeling am Beispiel von Briefen und stellt unter anderem die Fragen, 1) wie Topic Modeling in der Analyse von Briefkorpora eingesetzt werden kann und 2) wie die Qualität der Ergebnisse dieses Prozesses beeinflusst werden kann. 
         
         
            Forschungsmaterial
            Das Forschungsmaterial besteht aus Briefen des Grazer Sprachwissenschaftlers Hugo Schuchardt (1842-1927). Die umfangreiche und mehrsprachige Korrespondenz dieses schon seinerzeit sehr geschätzten Wissenschaftlers ist seit 2007 Teil des Digitalisierung-Projektes 
                    Hugo Schuchardt Archiv (Hurch 2019). Für die Topic-Modeling-Analyse werden 2261 Briefdateien im TEI-Format in Betracht gezogen, da die restlichen zurzeit noch in keinem entsprechenden Format vorhanden sind. Der Vorteil einer solchen Methode ist es aber, dass das gleiche Modell jederzeit auf eine erweiterte Menge an Daten anwendbar ist. Eine Besonderheit dieses Korpus ist, dass Schuchardt in mehreren Sprachen korrespondiert hat, von denen hier elf repräsentiert sind (Abbildung 1). Daher wird das Modell für einzelne Sprachen separat angewendet. Dies ist insofern eine Herausforderung, weil 1) Vorgänge den jeweiligen Sprachen angepasst werden müssen (wie etwa die Lemmatisierung), 2) der Textumfang bei vielen Sprachen nicht ausreichend ist und daher nicht auf alle Sprachen effektiv angewendet werden kann und 3) die verschiedenen Ergebnisse pro Sprache verglichen werden sollten. Ein weiteres Problem für das Topic Modeling ist die große Diskrepanz in den Textlängen der einzelnen Dateien (Abbildung 2), da die Korrespondenz auch kürzere Formen wie Postkarten und Telegramme beinhaltet. So enthalten etwa die kürzesten deutschsprachigen Dateien etwa drei Tokens, die längste jedoch 3947. Dies ist aber ein Zustand, den viele Briefkorpora in der Realität begegnen, da wir als ForscherInnen selten einem ‚idealen‘ Korpus gegenüberstehen. Die Auseinandersetzung mit solchen Problemen ist ein fester Bestandteil unserer Arbeit.
                
            
               
                Abbildung 1: Anteil der einzelnen Sprachen im Briefkorpus
            
            
               
                Abbildung 2: Menge der deutschsprachigen Briefdateien nach ihrer Anzahl der Tokens
            
         
         
            Methode
            Für die Beantwortung der Forschungsfragen war zuerst die Literaturrecherche nötig, und zwar erstens zum Topic Modeling, zweitens zur Textsorte Brief und drittens zu dieser Korrespondenz. Um eine genauere Vorstellung zum Forschungsstand des Topic Modeling zu bekommen, wurden wissenschaftliche Aufsätze und Anwendungsbeispiele in Betracht gezogen, wie etwa Blei 2010, Jagarlamudi/Daumé 2010, Boyd-Graber/Blei 2012, Riddell 2015, Vulić et al. 2015, Bock et al. 2016, Andorfer 2017, Fechner/Weiß 2017, Schöch 2017, Murakami et al. 2017 und Arora et al. 2018. Zudem wird am genannten Korpus Topic Modeling mit Hilfe der Programmiersprache 
                    Python (Python Software Foundation 2001-2019), der Software MALLET (McCallum 2002-2019) und der Anweisungen der Jupyter-Notebooks von DARIAH-DE (DARIAH-DE 2019) vollzogen. Darüber hinaus werden verschiedene Tools zur Vorverarbeitung evaluiert – z. B. 
                    spaCy (Explosion AI 2019) und DTA::CAB (Berlin-Brandenburgische Akademie der Wissenschaften 2011-2018) für die Lemmatisierung – sowie verschiedene Tools und Parameter für die Topic-Modellierung – z. B. 
                    Topics Explorer (DARIAH-DE 2018) – und die daraus resultierenden Ergebnisse und Erfahrungen verglichen. 
                
         
         
            Ergebnisse
            Obwohl es sich um ein laufendes Projekt handelt, gibt es bereits einige relevante Ergebnisse und Schlussfolgerungen. 
            1) Die Vorverarbeitung stellt einen wichtigen Schritt in der Topic-Modellierung dar und beeinflusst die Ergebnisse. Dabei spielen nicht nur die eingesetzten Tools eine Rolle, sondern auch die gewählte Vorgehensweise.
            2) Die Lemmatisierung, auf die beim Topic Modeling oft verzichtet wird, ermöglicht mehr semantische Differenz in den Topics. 
            3) Der unterschiedliche Textumfang von einzelnen VerfasserInnen kann zu falschen Ergebnissen führen, wenn die Topics pro VerfasserIn analysiert werden.
            4) Entscheidungen über Parameter wie Optimierungsintervall, Topic- und Iterations-Anzahl können die Ergebnisse beeinträchtigen und müssen immer projektspezifisch getestet werden, bis ein sinnvolles Resultat vorliegt. Das ‚Sinnvolle‘ zu erkennen ist eine Herausforderung, die fachwissenschaftliches Verständnis verlangt. 
            Die Inkonsistenz der Topics und manchmal verwirrende Ergebnisse zeigen, dass die naive Anwendung eines Topic-Modeling-Tools nicht immer befriedigend sein kann. Intensivere Beschäftigung mit den einzelnen Schritten und Ergebnissen kann sich jedoch positiv auf den Erfolg der Analyse auswirken. Die weitere Arbeit wird zeigen, ob und welchen Mehrwert Topic Modeling bei der Analyse der Schuchardt-Korrespondenz leisten kann, die durch 
                    close reading nicht erreicht werden können. 
                
         
      
      
         
            
               Bibliographie
               
                  Andorfer, Peter (2017): 
"Turing Test für das Topic Modeling. Von Menschen und Maschinen
erstellte inhaltliche Analysen der Korrespondenz von Leo von
Thun-Hohenstein im Vergleich", in: 
Zeitschrift für digitale Geisteswissenschaften 2. http://zfdg.de/2017_002 [letzter Zugriff 27. September 2019].

               
                  Arora, Sanjeev / Ge, Rong; Halpern, Yoni / Mimno, David / Moitra, Ankur / Sontag, David / Wu, Yichen / Zhu, Michael (2018): 
  "Learning topic models - provably and efficiently",  in: 
  Communications of the ACM 61 / 4: 85–93. 10.1145/3186262.

               
                  Berlin-Brandenburgische Akademie der Wissenschaften (ed.) (2011-2018): 
  Das DTA-Basisformat. http://www.deutschestextarchiv.de/doku/basisformat/ [letzter Zugriff 27. September 2019].

               
                  Blei, David M. (2010): 
 "Introduction to Probabilistic Topic Models", in: 
 Semantic Scholar.
 https://pdfs.semanticscholar.org/5f10/38ad42ed8a4428e395c96d57f83d201ef3b3.pdf
 [letzter Zugriff 27. September 2019].

               
                  Blei, David M. (2012): 
  "Topic Modeling and Digital Humanities", in: 
  Journal of Digital Humanities 2 / 1: 8–11.
  http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/ [letzter Zugriff 27. September 2019].

               
                  Bock, Sina / Du, Keli / Huber, Michael / Pernes, Stefan / Pielström, Steffen (2016): 
  Der Einsatz quantitativer Textanalyse in den Geisteswissenschaften. Bericht über den Stand der Forschung. (= DARIAH-DE working papers 18). Göttingen: GOEDOC – Dokumenten- und Publikationsserver der Georg-August-Universität Göttingen. http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-18.pdf [letzter Zugriff 27. September 2019].

               
                  Boyd-Graber, Jordan / Blei, David (2012): 
  Multilingual Topic Models for Unaligned Text. http://arxiv.org/pdf/1205.2657v1 [letzter Zugriff 27. September 2019].

               
                  DARIAH-DE (2018): 
  Topics Explorer. V. 2.0.1. https://github.com/DARIAH-DE/TopicsExplorer [letzter Zugriff 27. September 2019].

               
                  DARIAH-DE (2019): 
  DARIAH Topics. Easy Topic Modeling in Python. V. 2.0.1. https://github.com/DARIAH-DE/Topics [letzter Zugriff 27. September 2019].

               
                  Explosion AI (2019): 
  spaCy. V. 2.1.6. https://github.com/explosion/spaCy [letzter Zugriff 27. September 2019].

               
                  Fechner, Martin / Weiß, Andreas (2017): 
  "Einsatz von Topic Modeling in den Geschichtswissenschaften: Wissensbestände des 19. Jahrhunderts", in: 
  Zeitschrift für digitale Geisteswissenschaften 2. http://zfdg.de/2017_005 [letzter Zugriff 27. September 2019].

               
                  Hurch, Bernhard (2019): 
  "Hugo Schuchardt Archiv". Institut für Romanistik, Karl-Franzens-Universität Graz (ed.). https://schuchardt.uni-graz.at [letzter Zugriff 27. September 2019].

               
                  Jagarlamudi, Jagadeesh / Daumé, Hal (2010): 
  "Extracting Multilingual Topics from Unaligned Comparable Corpora",  in: Gurrin, Cathal (ed.): 
  Advances in information
  retrieval. Proceedings 444–456. (= Lecture notes in computer science 5993). Berlin / Heidelberg / New York: Springer.

               
                  McCallum, Andrew Kachites (2002-2019): 
  MALLET. A Machine Learning for Language Toolkit. V. 2.0.8. http://mallet.cs.umass.edu [letzter Zugriff 27. September 2019].

               
                  Murakami, Akira / Thompson, Paul / Hunston, Susan / Vajn, Dominik (2017): 
  "‘What is this corpus about?’: using topic modelling to explore a
  specialised corpus", in: 
  Corpora 12 / 2: 243–277. https://www.euppublishing.com/doi/10.3366/cor.2017.0118 [letzter Zugriff 27. September 2019].

               
                  Python Software Foundation (2001-2019): 
  Python. V. 3.7.4. https://github.com/python [letzter Zugriff 27. September 2019].

               
                  Riddell, Allen (2015): 
  Text Analysis with Topic Models for the Humanities and Social Sciences — Text Analysis with Topic Models for the Humanities and Social Sciences. DARIAH-DE Initiative (ed.). https://liferay.de.dariah.eu/tatom/ [letzter Zugriff 27. September 2019].

               
                  Schöch, Christof (2017): 
  "Topic Modeling Genre. An Exploration of French Classical and
  Enlightenment Drama",
  in:  Digital Humanities Quarterly 11 / 2. http://www.digitalhumanities.org/dhq/vol/11/2/000291/000291.html [letzter Zugriff 27. September 2019].

               
                  Vulić, Ivan / Smet, Wim de / Tang, Jie / Moens, Marie-Francine (2015): 
  "Probabilistic topic modeling in multilingual settings. An overview
  of its methodology and applications",  in: 
  Information Processing & Management 51 / 1: 111–147. https://www.sciencedirect.com/science/article/pii/S0306457314000739 [letzter Zugriff 27. September 2019].

            
         
      
   



      
         
            Einleitung 
            Vorliegender Posterbeitrag geht davon aus, dass digitale Editionen Produkte teils formalisierender, teils interpretierender Prozesse sind und damit in ein herausforderndes „Spiel- und Spannungsfeld“ geraten. Sie sind einerseits Standards verpflichtet, vermitteln jedoch andererseits – bedingt durch Editionsentscheidungen wie Auswahl der Quellen, Modellierung und Präsentation – eine bestimmte Sicht auf das edierte Material, welche durch Wissen und Erkenntnisinteressen der jeweiligen Herausgeber*innen geprägt ist. 
            Somit stellt sich aber die Frage, welche Spielräume den Herausgeber*innen und welche den Benutzer*innen bei der digitalen Erschließung von Quellen zugestanden werden und wie letztere damit umgehen, dass die ihnen zur Verfügung gestellten Ressourcen bereits durch andere vorgeformt sind. Um hierauf Antworten zu finden, haben die Autorinnen das etablierte Rezensionsorgan „RIDE – A review journal for digital editions and resources“ (https://ride.i-d-e.de/) herangezogen und die Äußerungen der Rezensent*innen als stellvertretend für die Perspektiven von Nutzer*innen untersucht. 
         
         
            RIDE als Untersuchungskorpus 
            RIDE wird vom Institut für Dokumentologie und Editorik verantwortet und möchte gemäß der Eigendefinition, „ExpertInnen ein Forum zur kritischen Auseinandersetzung“ mit Editionen bieten und damit dazu beitragen, „die gängige Praxis zu verbessern und die zukünftige Entwicklung voranzutreiben“ (RIDE 2019). Insofern gehen aus den bereits erschienenen Rezensionen auch Überlegungen hervor, wie Editionen in Zukunft konzipiert werden könnten. Zudem ist der Kriterienkatalog von Patrick Sahle (in Zusammenarbeit mit Georg Vogeler und anderen Mitgliedern des IDE erstellt, vgl. http://www.i-d-e.de/publikationen/weitereschriften/kriterien-version-1-1/), an dem sich Gutachter*innen orientieren, einerseits „Bewertungsgrundlage“ und andererseits „Checkliste für Wissenschaftler [d.h. für die Ersteller*innen digitaler Editionen]“ (vgl. Henny 2017). Nicht zuletzt sind dadurch auch Minimalanforderungen für das zeitgemäße Edieren formuliert (vgl. Schnöpf 2013: 75), welche dazu beitragen, die Qualität digitaler Editionen zu sichern. 
            Zum Zeitpunkt der Untersuchung waren auf der Website „ride.i-d-e.de“ im Zeitraum von 2014 bis 2019 bereits sieben Bände zu „wissenschaftlichen Editionen“ mit insgesamt 35 Rezensionen in deutscher und englischer Sprache veröffentlicht worden. Die Daten dieser Rezensionen stehen auf GitHub (https://github.com/i-d-e/ride) zur Verfügung und wurden für die Untersuchung heruntergeladen und als Textkorpus mit insgesamt 161.553 Token aufbereitet. Die Auswertung erfolgte korpusbasiert mithilfe der Suche nach ausgewählten Keywords (etwa: „leider“, „Vorteil“ oder „wünschenswert“), um relevante Textstellen schnell auffinden zu können, sowie über ein „close reading“-Verfahren, um das Verständnis der einzelnen Kontexte zu sichern. Dabei wurden die Belegstellen in Anlehnung an Sahles Kriterienkatalog inhaltlich sortiert und ausgehend von mehreren Fragen ausgewertet: 
            
               In welchen Bereichen digitaler Editionen sehen sich Rezensent*innen durch Vorannahmen und Interpretationen eingeschränkt?
               In welchen Bereichen digitaler Editionen wäre aus Sicht der Rezensent*innen mehr Formalisierung/Standardisierung wünschenswert?
               Welche Maßnahmen schlagen Rezensent*innen vor, um neue Spielräume zu eröffnen und verschiedene Interpretationsmöglichkeiten offen zu halten?
            
         
         
            Ergebnisse und Ausblick
            In der Auswertung der 35 Rezensionen zeigt sich unter anderem, dass Gutachter*innen es zunehmend schätzen, wenn den Nutzer*innen digitaler Editionen möglichst viele unterschiedliche Perspektiven auf die jeweiligen Daten ermöglicht werden. So wird etwa die „Möglichkeit verschiedener Präsentationsmodi“ als positiv hervorgehoben, wohingegen das Fehlen von Faksimiles als Defizit gewertet wird. Der als ideal angenommene Zugang zu den Daten beinhaltet zudem in fast allen Rezensionen die Downloadbarkeit der XML/TEI-Dateien.
            Diese Beobachtungen sprechen dafür, dass großes Interesse daran besteht, Daten nachzunutzen und damit zu eigenen Einschätzungen und Interpretationen zu gelangen – ein potentieller Mehrwert, welcher teils auch deutlich formuliert wird: „Die Zurverfügungstellung der Transkription in XML oder einem anderen für die Nachnutzung der Daten geeigneten Format wäre wünschenswert und wertvoll.“ Gegeben ist diese Option im überwiegenden Teil bislang rezensierter Editionsprojekte jedoch nicht, wie die von RIDE selbst generierten Auswertungen offenbaren (vgl. Chart Nr. 20 und Nr. 23 unter https://ride.i-d-e.de/data/charts/). Somit ergibt sich hier ein erster möglicher Ansatzpunkt für die Optimierung zukünftiger digitale Editionen. 
            Genauso zeigt sich aber auch im Bereich der Dokumentation digitaler Editionsprojekte noch Verbesserungsbedarf. Schließlich machen Rezensent*innen mehrfach auf das Fehlen editorischer Richtlinien aufmerksam und thematisieren – wie in folgendem Fall – die fehlende Transparenz und „Selbstverortung“ (Schnöpf 2013: 72) der ihnen gegebenen Ressourcen: „I do not doubt that the transcribers and editors had a clear idea of what they were doing, but they have not documented it in the edition and so the user (and the reviewer) can only retrospectively deduce what that idea might have been.” 
            Zu diesen (und weiteren gefundenen) Kritikpunkten kommt freilich erschwerend hinzu, dass Nutzer*innengruppen mit ihren Forschungspraktiken, Forschungsprozessen und Motivationen sehr heterogen sein können (vgl. Kramer 2016, Hewing/Mandl/Womser-Hacker 2016) und „[k]omplexe digitale Ressourcen [...] auch an die Benutzer höhere Anforderungen [stellen]“ (Sahle 2013: 262). Es gilt hier also, neue Interaktionsmuster zu entwickeln, um die Kluft zwischen Editor*innen und Nutzer*innen zu überbrücken. In diesem Sinne soll der Posterbeitrag durch abschließende Empfehlungen abgerundet werden, wie Nutzer*innen in Zukunft (noch) erfolgreicher an digitale Editionen herangeführt werden könnten.
         
      
      
         
            
               Bibliographie
               
                  Henny, Ulrike (2018): „Reviewing von digitalen Editionen im Kontext der Evaluation digitaler Forschungsergebnisse“, in: Kamzelak, Roland S. / Steyer, Timo (eds.): 
                        Digitale Metamorphose: Digital Humanities und Editionswissenschaft (= Sonderband der Zeitschrift für digitale Geisteswissenschaften 2) 10.17175/sb002_006.
                    
               
                  Hewing, Ben / Mandl, Thomas / Womser-Hacker, Christa (2016): “Methods for User-Centered Design and Evaluation of Text Analysis Tools in a Digital History Project”, in: 
                        ASIST Annual Meeting Proceedings. 
                        Joining Research and Practice 53: 1–10 https://onlinelibrary.wiley.com/doi/full/10.1002/pra2.2016.14505301078 [letzter Zugriff 30. Dezember 2019]. 
                    
               
                  Kramer, Michael J. (2016): 
                        The Digital Humanities Reader. http://www.michaeljkramer.net/the-digital-humanities-reader/ [letzter Zugriff 30. Dezember 2019]. 
                    
               
                  Sahle, Patrick (2013): 
                        Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels. Teil 1–3: Befunde, Theorie und Methodik (= Schriften des Instituts für Dokumentologie und Editorik 7–9). Norderstedt: BoD. 
                    
               
                  Schnöpf, Markus (2013): „Evaluationskriterien für digitale Editionen und die reale Welt“, in: 
                        HiN - Humboldt im Netz. Internationale Zeitschrift für Humboldt-Studien 27: 69–76.
                    
            
         
      
   



      
         Schulbücher transportieren gesellschaftliche und staatlich sanktionierte Werte und Normen. Als Quellengattung stellen sie einen vielversprechenden Gegenstand für zahlreiche wissenschaftliche Fragestellungen dar. Schulbücher werden in zahlreichen Bibliotheken gesammelt, aber in den seltensten Fällen werden sie systematisch erschlossen und für die Digitalisierung genießen sie in der Regel keine hohe Priorität.
         Im Rahmen der digitalen Schulbuchbibliothek GEI-Digital wurden in den letzten 10 Jahren historische deutsche Schulbücher der Fächer Geschichte, Geographie und Politik, sowie Realien- und (Erst-)Lesebücher von den Anfängen der Schulbuchproduktion im 17. Jahrhundert bis zum Ende des Ersten Weltkriegs digital zugänglich gemacht (Hertling/Klaes 2018). Digitalisiert und integriert wurden dabei sowohl Schulbücher aus den Beständen der Forschungsbibliothek des Georg-Eckert-Instituts – Leibniz-Institut für internationale Schulbuchforschung (GEI) als auch Schulbücher aus zahlreichen Partner-Bibliotheken im deutschsprachigen Raum. Die externen Schulbuchbestände wurde dem GEI zum Zwecke der Digitalisierung im Rahmen von Kooperationen leihweise überlassen oder als Fremddigitalisate virtuell in die GEI-Digital-Sammlung integriert.
            
         Die bibliothekarische Erschließung des historischen Schulbuch-Korpus folgt den spezifischen Bedürfnissen der Schulbuchforschung. Typisches Kennzeichen von Schulbüchern sind dabei viele Ausgaben und die unterschiedlichen Bände eines Schulbuchs. Neben Angaben zum Verlag und Erscheinungsjahr werden zusätzlich auch Schulfächer und Schulstufen als deskriptive Metadaten erfasst. Sie stehen auf GEI-Digital als MARCXML, Metadata Object Description Schema (MODS) oder Dublin Core (DC) zur Nachnutzung zur Verfügung. Die Erschließung umfasst auch die intellektuelle Verknüpfung der Schulbuchautoren mit Normdateneinträgen in der Gemeinsamen Normdatei (GND), um das Korpus für biographische Forschungsansätze zu öffnen. Darüber hinaus werden die digitalisierten Schulbücher in Form einer Tiefenerschließung in ihrer Struktur erschlossen und die Elemente, wie Titelblätter, Inhaltsverzeichnisse, Abbildungen etc. im Metadata Encoding & Transmission Standard (METS) ausgewiesen.
         Die über 1,5 Millionen in GEI-Digital gescannten Seiten wurden zudem einer Optical Character Recognition (OCR) unterzogen und stehen als durchsuchbare Volltexte über eine OAI-PMH-Schnittstelle zur Verfügung. Die Resultate der Volltexterkennung werden im Zuge des Digitalisierungsworkflows im XML-Schema Analyzed Layout and Text Object (ALTO) ausgegeben.
         Mit GEI-Digital ist für die Digital Humanities ein einzigartiges
Korpus mit über 6.100 digitalisierten Schulbüchern entstanden, dass
die gesamte Epoche der deutschen Schulbücher von deren Entstehung bis
1918 mit hoher Vollständigkeit virtuell zusammenführt. Die
Digitalisate und Daten werden in zahlreichen
Digital-Humanities-Projekten bereits nachgenutzt, z.B. im Projekt
„Welt der Kinder“ (Heuwing/Weiß 2018 und
Nieländer/Weiß 2018), in dem das Korpus mit Topic Modeling-Verfahren
untersucht wurde . Das Portal GeoPortOst nutzt u.a. das in GEI-Digital vorhandene Kartenmaterial für Georeferenzierungen. 

         In einem nächsten Schritt ist geplant, das Korpus in dem von CLARIN
betriebenen Virtual Language Observatory (VLO)nachzuweisen, um es für weiterführende und
v.a. linguistische Analysen zugänglich zu machen. Voraussetzung für
einen Nachweis ist die Repräsentation der digitalisierten Schulbücher
in derComponent MetaData Infrastructure
(CMDI). CMDI stellt ein Framework zur Verfügung, um Profile für Metadaten für die Beschreibung und Benutzung bereitzustellen. 
            
         Ausgehend von Metadatenformaten, die sich v.a. an bibliothekarischen Standards orientieren, werden auf dem Poster Anforderungen und Strategien für Mapping-Prozesse als Grundlage für Digital-Humanities-Projekte präsentiert. Im Mittelpunkt stehen dabei die in GEI-Digital gemachten Mapping-Erfahrungen mit den Formaten METS/MODS, TEI, CMDI und Dublin Core (DC) und die Herausforderung ihrer jeweiligen Interoperabilität.
         In einem ersten Schritt In einer Machbarkeitsstudie stellte sich im Projekt GEI-Digital ein Mapping von METS zu CMDI als undurchführbar heraus (Fallucchi/De Luca 2019). Ein Mapping von Dublin Core (DC) zu CMDI als CLARIN-Empfehlung ist mit Blick auf die Besonderheiten der Erschließung von Schulbüchern insbesondere mit Blick auf die für die Forschung wichtigen Ausgabebezeichnungen und Bandangaben stark verlustbehaftet. Vor dem Hintergrund werden derzeit alternative Optionen diskutiert, die auf dem Poster aufgezeigt und erörtert werden sollen. Eine Möglichkeit stellt die Anreicherung von Dublin Core-Metadaten und ihre Konvertierung in CMDI dar. Eine weitere Option besteht in der Umwandlung von textbasierten ALTO-Dateien in CMDI.
      
      
         
            
      Projektwebseite GEI-Digital – die digitale Schulbuch-Bibliothek:: 
      
            
            
      Projektwebseite Welt der Kinder: 
            
            
      Projektwebseite GeoPortOst: Portal für
      thematische und versteckte Karten zu Ost- und Südosteuropa:
      
            
            
      CLARIN Virtual Language Observatory:
    
            
         
         
            
               Bibliographie
               
                   Fallucchi, Francesca / De Luca, Ernesto William
(2019): “Connecting and Mapping LOD and CMDI Through Knowledge Organization” Springer, Cham, pp. 291-301.
               
                   Hertling, Anke / Klaes, Sebastian (2018): Historische Schulbücher als digitales Korpus für die Forschung: Auswahl und Aufbau einer digitalen Schulbuchbibliothek, in: Maret Nieländer / Ernesto William De Luca (eds): Digital Humanities in der internationalen Schulbuchforschung - Forschungsinfrastrukturen und Projekte. Göttingen: V&R unipress 21–44
               
                   Hertling, Anke / Klaes, Sebastian  (2018): »GEI-Digital« als Grundlage für Digital-Humanities-Projekte: Erschließung und Datenaufbereitung, in: Maret Nieländer / Ernesto William De Luca (eds): Digital Humanities in der internationalen Schulbuchforschung - Forschungsinfrastrukturen und Projekte. Göttingen: V&R unipress 45–68
               
                   Heuwing, Ben / Weiß, Andreas  (2018): Suche und Analyse in großen Textsammlungen: Neue Werkzeuge für die Schulbuchforschung in: Maret Nieländer / Ernesto William De Luca (eds): Digital Humanities in der internationalen Schulbuchforschung - Forschungsinfrastrukturen und Projekte. Göttingen: V&R unipress 145-170
               
                   Nieländer, Maret / Weiß, Andreas  (2018): »Schönere Daten« – Nachnutzung und Aufbereitung für die Verwendung in Digital-Humanities-Projekten, in: Maret Nieländer / Ernesto William De Luca (eds): Digital Humanities in der internationalen Schulbuchforschung - Forschungsinfrastrukturen und Projekte. Göttingen: V&R unipress 91-116
            
         
      
   

