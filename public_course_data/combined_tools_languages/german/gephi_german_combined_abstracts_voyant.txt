

      
         
            Netzwerke und Historische Epistemologie
            Methoden der Netzwerkanalyse kommen immer stärker als heuristisches Tool zum
          Einsatz, wenn es darum geht historische Prozesse zu beschreiben und zu
          analysieren. Netzwerktheorie stellt hierbei eine Möglichkeit dar, um
          systematisch Wissensstrukturen und die Formation und Transformation von
          Wissenssystemen zu beschreiben. Methodische Ansätze können hierbei von den
          Sozialwissenschaften, der Innovationsforschung sowie aus der Informatik und der
          Graphentheorie übernommen werden. Zunächst machten die meisten Ansätze in der
          historischen Forschung hauptsächlich von den qualitativen Konzepten gebrauch und
          weniger von den quantitativen Methoden, die die Netzwerktheorie liefert. Ein
          eindrucksvolles Beispiel zeigt sich im Werk von Irad Malkin in seinen Arbeiten
          über die griechische Antike (Malkin 2011). Mit der verbesserten Verfügbarkeit
          von Tools für die qualitative Analyse verändert sich diese Situation nun
          deutlich. Die Anzahl der Fallstudien hat mittlerweile die Größe erreicht, dass
          erste Theoretisierungen dieser Methode in der historischen Forschung unternommen
          wurden (van den Heuvel 2015). Große Impulse gehen von der sich um http://historicalnetworkresearch.org organisierenden Gruppe aus.
          Netzwerktheorie hat insbesondere ein großes Potential in der
          Wissenschaftsgeschichte insbesondere der historischen Epistemologie, die
          Entwicklung von Wissen als eine Verknüpfung von sozialen, materiellen und
          kognitiven Wissenssystemen sieht. Diese Systeme lassen sich als Netzwerke im
          Sinne der Netzwerktheorie verstehen. Dazu schlagen wir drei miteinander
          interagierende Netzwerke vor: ein soziales Netzwerk, ein semiotisches Netzwerk,
          sowie das darüber liegende epistemische Netzwerk: grob gesprochen ein Netzwerk
          von Akteuren, ein Netzwerk der Repräsentation von Wissen, das sich in
          materiellen Objekten oder auch kodifizierten Verfahren darstellt und schließlich
          das eigentliche kognitive Netzwerk. Wir werden in unserem Beitrag die ersten
          Ergebnisse zweier Fallstudien vorstellen. Dabei liegt unser Schwerpunkt weniger
          auf den konkreten Ergebnissen, sondern darauf zu zeigen, wie sich der Prozess
          der Übersetzung von historischen Fakten und Annahmen in netzwerktheoretisch
          auswertbare Daten darstellt und damit die Digital Humanities dazu beitragen,
          interdisziplinäre geisteswissenschaftliche Forschung zu ermöglichen. 
         
         
            Fallstudien
            In der ersten Fallstudie gehen wir der Frage nach, wie sich eine bestimmte
            Wissenstradition in der frühen Neuzeit mittels gedruckter Traktate in der Zeit
            von 1472 bis in das Jahr 1650 in ganz Europa verbreiten konnte. Es geht hierbei
            um die mit der Sphaera des Sacrobosco (Thorndike 1949),
            einem ursprünglich handgeschriebenen grundlegenden Text, verbundenen
            Wissenstradition. Ursprünglich ein Text über Astronomie und Kosmologie wurde
            dieser im Laufe seiner Editionsgeschichte immer wieder durch zusätzliche Texte
            erweitert und umfänglich kommentiert. Es gehörte zum verbindlichen Wissenskanon
            der Universitäten dieser Periode. In der Zeit von 1472 bis 1650 haben wir bisher
            363 Editionen identifiziert, die in ganz Europa veröffentlicht wurden. Welches
            sind die Voraussetzungen, die eine solche Verbreitung ermöglichten und welche
            Wissensinhalte wurden verbreitet und wie veränderten sich diese über die Zeit?
            Wir sehen in der Netzwerktheorie einen vielversprechenden Ansatz diese Fragen zu
            klären. Dazu haben wir ein erstes Netzwerk von wesentlichen Akteuren, den
            Verlegern, identifiziert, die maßgeblich für die Verbreitung des Traktats waren.
            Von welcher Form die Interaktionen zwischen den Verlegern waren, können wir
            bisher nur in sehr begrenztem Rahmen sicher bestimmen. Gleiches gilt für Ihre
            Rolle in den lokalen Wissensnetzwerken, in die sie eingebunden sind. Es ist
            jedoch möglich, Hypothesen über die Einflussbereiche aufzustellen, die die
            Kanten in dem Netzwerk rechtfertigen. Diese Hyptohesen lassen sich mit
            Netzwerktools in unserem Falle durch den Einsatz von Skripten in iPython (cf.
            Pérez / Granger 2007) unter Benutzung der Pakete networkx (cf. NetworkX
            developer team 2014) und graph-tool (cf. Peixoto) testen und modifizieren, in
            unserem Beitrag werden wir die von uns angewandten Verfahren darstellen. Die
            Skripte werden in Zukunft auf der Webseite des Projektes veröffentlicht werden.
            Neben den Verlegern gibt es ein weiteres Netzwerk, das eigentliche Netzwerk der
            Publikationen besser der Publikationsereignisse, diese stellen im Rahmen der
            angerissenen Theorie der Wissenssysteme eine andere Kategorie dar, sie sind
            materieller Ausdruck von Wissen, gehören damit zum semantischen Wissenssystem.
            Auch hier stellt sich die Frage, wie diese Editionen miteinander in Beziehung
            stehen. Offensichtlich stehen diese Beziehungen in enger Verbindung mit dem
            Akteursnetz. Jedoch gibt es auch andere Beziehungen, wie zum Beispiel die in den
            Editionen enthaltenen zusätzlichen Traktate und Kommentare, oder Abweichungen
            von Vorgängern. Auch hier sind wir bisher nur in der Lage Hypothesen
            anzustellen, aber auch hier zeigen uns Methoden der Netzwerktheorie bereits
            jetzt Wege auf, wie diese sich überprüfen lassen. Schließlich ist die
            entscheidende Frage: Welches Wissen wird durch die Traktate in welcher Form
            vermittelt und wie trägt dieses zur Wissensorganisation in der frühen Neuzeit
            bei? Wie oben beschrieben war das Traktat über die Sphaera ursprünglich ein
            Traktat über Astronomie und Kosmologie, die Ergänzungen umfassen jedoch einen
            wesentlich erweiterten Wissensbereich. Die Themenfelder, um die das Traktat
            erweitert wurden, umfassen Felder der mathematischen Astronomie,
            Kalenderberechnungen, die Benutzung und in einigen Bereichen konkrete Anleitung
            für die Konstruktion von astronomischen Instrumenten, nautische Astronomie und
            Geographie, Kartographie, Meteorologie, Arithmetik, Geometrie, der Konstruktion
            und des Gebrauchs von mathematischen Instrumenten zur Ausführung arithmetischer
            Berechnungen, Astrologie, Literatur, angewandte Optik und Mechanik. Die Antwort
            auf die Frage, welche Inhalte an welchen Stellen in die einzelnen Editionen
            eingeflossen sind, gibt Aufschlüsse über die Veränderung der frühneuzeitlichen
            Wissensstruktur. Auch hier kann Netzwerktheorie nach unserer Überzeugung einen
            wesentlichen Beitrag dazu leisten, schlüssige Antworten aufzufinden. Wir
            untersuchen dazu welche Ko-Autoren in welchen Traktaten genannt werden,
            insbesondere Clavius und Pedro Nuñes, und wie sich diese Subnetze ausprägen.
            Diese Koautoren stehen jeweils für bestimme Wissensgebiete geben also ein Indiz
            dafür welche Wissensbereich sich neu etablierten. Auch für diese Analysen haben
            wir Skripte entwickelt, die helfen diese Netzwerke zu visualisieren und deren
            Charakteristika zu untersuchen. Erste Ergebnisse der Netzwerkanalyse lassen
            Rückschlüsse auf die Stabilität des Wissensnetzwerkes zu. Wir können im
            wesentlichen drei Phasen deutlich identifizieren: eine noch instabile radiale
            Verbreitung, eine relative lange Phase der Stabilität und schließlich eine Phase
            der Reduktion. 
            
               
               
                  Abb. 1: Minimaler verbundener Graph der Editionen der
            Sphaera. Kanten spiegeln hier die chronologische Ordnung wieder. Dieser Graph
            legt die Randbedingungen für die weiter Analyse der Einflussbereiche der
            Editionen fest. Blaue Punkte stellen die lateinischen Editionen, rote die
            Editionen in lokalen Sprachen dar.
            
            
               
               
                  Abb. 2: The graph shows the role of editions where Clavius
            is given as one co-author (squares). The color indicates the publishers which
            are clustered by city. Only the places where Clavius is a co-author have labels.
            The big unnamed cluster in the middle is Paris.
            
            Die zweite Fallstudie widmet sich einem anderen Bereich. Dem epistemischen
              Netzwerk der Allgemeinen Relativitätstheorie. Kurz umrissen geht es hierbei um
              die Analyse des Prozesses, der dazu führte, dass die ART in der Nachkriegsphase
              von einem Randproblem zu einem zentralen Ankerpunkt der theoretischen Physik
              wurde. Dieser Prozess wurde auch als "Renaissance der allgemeinen Relativität"
              (Will 1983) bezeichnet. Aus einem zersplitterten lose gebundenen Netzwerk von
              Einzelpersonen wird in dieser Zeit ein stabiles Netzwerk, dass von Institutionen
              getragen wird. Darauf aufbauend entwickelt sich ein Netzwerk materieller
              Kommunikation in Form von neuen Journalen und Konferenzreihen, sowie eine neue
              disziplinäre Sprache, die das Wissen über die ART kodifizierte. Die ART
              entwickelte sich so zur unangefochtenen Theorie von Raum und Zeit (Blum 2015).
              Zum jetzigen Zeitpunkt haben wir ein Netzwerk aus über 500 Akteuren
              identifiziert und ihre Beziehungen klassifiziert. Mit Methoden der
              Netzwerkanalyse können wir zeigen, welche Akteure über die Zeit an Bedeutung
              gewonnen und verloren und die Formation von Subclustern zeigen. Auch hier werden
              wir in unserem Beitrag im wesentlichen darauf eingehen, wie der konkrete Prozess
              der Umwandlung von historischen Fakten und Hypothesen in netzwerktheoretische
              Konzepte als Anwendung der DH in der historischen Forschung vollzogen wird.
              Insbesondere werden wir auf die Überlegungen und Schwierigkeiten eingehen,
              sinnvolle Verknüpfungen zwischen den Akteuren zu definieren und insbesondere zu
              Gewichten, d. h. in der Sprache der Netzwerktheorie, weak and strong ties zu
              idenfizieren. Von besonderer Bedeutung ist auch hier die Frage des Umgangs mit
              der dynamischen Entwicklung des Netzwerkes. Wie können diese so visualisiert
              werden, dass der historisch Forschende daraus Rückschlüsse ziehen kann, wie
              können diese Tools so zur Verfügung gestellt werden, dass sie auch direkt durch
              den Forscher nutzbar sind? In unserem Fall haben wir dazu unterschiedliche
              Ansätze verfolgt, erneut die Umsetzung mittels iPython, und die Visualisierung
              dann in Gephi und Cytoscape.
            
               
               
                  Abb. 3: Visualization of the network of collaborations of
              scientists who worked on general relativity in the post-war period. Node size is
              proportional to the degree centrality. The color of the nodes is a
              representation of betweenness centrality (from lighter to darker). 
            
         
      
      
         
            
               Bibliographie
               
                  Drucker, Johanna (2013): "Performative Materiality
                and Theoretical Approaches to Interface", in: Digital
                Humanities Quartely 7,1 http://www.Digitalhumanities.org/dhq/vol/7/1/000143/000143.html
                [letzter Zugriff 03. September 2015]. 
               
                  Edwards, Charlie (2012): "The Digital Humanities and
                Its Users", in: Gold, Matthew K. (ed.): Debates in the
                Humanities
                  http://dhdebates.gc.cuny.edu/debates/text/31 [letzter Zugriff 03.
                  September 2015]. 
               
                  Kirschenbaum, Matthew (2008): Mechanisms. New Media and the Forensic Imagination. Cambridge: MIT
                  University Press.
               
                  Lauer, Gerhard (2013): "Die digitale Vermessung der
                  Kultur. Geisteswissenschaften als Digital Humanities", in: Geiselberger,
                  Heinrich / Moorstedt, Tobias (eds.): Big Data. Das
                  neue Versprechen der Allwissenheit. Berlin: Suhrkamp. 
               
                  Manovich, Lev (2001): Language of
                  New Media. Cambridge: MIT Press.
               
                  Warwick, Claire (2012): "Studying users in digital
                  humanities, in: Warwick, Claire / Terras, Melissa / Nyhan, Julianne (eds.):
                  Digital Humanities in Practice. London: Facet
                  Publishing. 
            
         
      
   



      
         Die zunehmende Mengen an Volltexten in den Geschichtswissenschaften und vor allem
        auch in der Mediävistik bietet neue Chancen für die Forschung, erfordern aber auch
        neue Methoden und Sichtweisen. Der Beitrag möchte die Verwendung von
        Graphdatenbanken für die Speicherung von Erschließungsinformationen vorstellen. 
            Momentan werden digitale Quellen und die mit ihnen verbundenen
        Erschließungsinformationen meist in XML oder in SQL-Datenbanken abgelegt. XML hat
        sich als Standard bewährt und findet in vielen Editionsprojekten als Datenformat
        Verwendung während Datenbanken auf Websites meist auf SQL-Datenbanken als
        Daten-Repositories zurückgreifen. XML-Dateien sind in der Regel bis zu einem
        gewissen Grade noch verständlich lesbar, bei SQL-Datenbanken ist die Lesbarkeit
        ohne Kenntnis der zu Grunde liegenden Datenstrukturen in der Regel nicht mehr
        gegeben. Dies liegt nicht zuletzt auch an den Architekturen der Datenbanken: um
        optimale Performance zu erhalten werden die Datenstrukturen normalisiert. Hier kommt
        es für die optimalen Nutzungsmöglichkeiten entscheidend auf die Gestaltung des
        Frontends der Datenbank an. Oft sind die User-Interfaces jedoch vor allem auf die
        Bedürfnisse jener Personen ausgerichtet, die die Datenbank selbst erstellt haben. Da
        diese Personen in der Regel die Datenstrukturen tief durchdrungen haben, kann es bei
        der Gestaltung des Frontends leicht zu einseitigen Ausrichtung auf Experten-Nutzer
        kommen. Solche Nutzer wissen bereits vor der Suchanfrage wie ihr Ergebnis aussieht.
        In den Fachwissenschaften wird eine solche Anfrage als CIN-Anfrage bezeichnet
        (concrete information need). Davon zu unterscheiden sind POIN-Anfragen
        (problem-oriented information need), bei denen der Nutzer ohne tiefere Kenntnisse
        des Datenmaterials und den zu Grunde liegenden Strukturen eine Anfrage startet (Vgl.
        hierzu Frants / Shapiro / Voiskunskii: 1997). Die Ausrichtung auf CIN-Anfragen zeigt
        sich auch in den größeren Quellenportalen zur Mediävistik (Vgl. Kuczera 2014). Hier
        ist die Verwendung von Graphdatenbanken ein alternativer Ansatz für die Speicherung
        von erschließendem Wissen. 
            In SQL-Datenbanken sind die Informationen in Tabellen abgelegt, die
        untereinander verknüpft sind. Graphdatenbanken folgen hier einem völlig anderen
        Ansatz. In einem Graph gibt es Knoten und Kanten. Vergleicht man die Knoten mit
        einem Eintrag in einer Tabelle einer SQL-Datenbank, wäre eine Kante eine
        Verknüfung zwischen zwei Tabelleneinträgen. Im Unterschied zu SQL- Datenbanken
        können Knoten und Kanten jeweils Eigenschaften haben. 
         
            
            
               Abb. 1: Direkte Verwandschaftsverhältnisse Karls des
          Großen als Graph visualisiert
         
         Daneben lassen sich die in Graphdatenbanken abgelegten Informationen sehr gut
          visualisieren. Gerade komplexere Datenbestände können hier sinnvoll für den
          Wissenschaftler erschlossen werden. Explorative Erschließungsmöglichkeiten
          erleichtern hierbei den Zugriff auf weitergehende Wissensdomänen des Repositoriums
          (Vgl. Kuczera 2015).
         Das Datenmodell einer Graphdatenbank bildet quasi die semantische Repräsentation des in der Datenbank abgelegten Wissens. Ergänzt man die Eigenschaften der Knoten mit Identifikatoren wie den Angaben aus der GND oder legt man den Verknüpfungsstrukturen fachspezifische Ontologien zu Grunde können die Informationen der Graphdatenbank auch für automatisierte Abfragen über das Internet erschlossen werden.
         In der Posterpräsentation werden in einem ersten Beispiel die Strukturen einer Graphdatenbank erläutert und anschließend mit der Graphenrepräsentation der Register der Regesten Kaiser Friedrichs III. und der genealogischen Datenbank Nomen-et-Gens Anwendungsbeispiele vorgestellt.
         
            
            
               Abb. 2: Beispielgraph zu Karl dem Großen, Einhard und
            der Vita Karoli Magni
         
      
      
         
            
               Bibliographie
               
                  Frants, Valery I. / Shapiro, Jacob / Voiskunskii, Vladimir
                G. (1997): Automated information retrieval.
                Theory and methods (= Library and information science). San Diego: Academic
                Press.
               
                  Kuczera, Andreas (2014): "Digitale Perspektiven
                mediävistischer Quellenrecherche", in: mittelalter.hypotheses.org
                  http://mittelalter.hypotheses.org/3492 [letzter Zugriff 28.
                  September 2015].
               
                  Kuczera, Andreas (2015): "Graphdatenbanken für
                  Historiker. Netzwerke in den Registern der Regesten Kaiser Friedrichs III.
                  mit neo4j und Gephi", in: mittelalter.hypotheses.org
                  http://mittelalter.hypotheses.org/5995 [letzter Zugriff 28.
                    September 2015].
            
         
      
   



      
         
            Kurzbeschreibung
            Im Verlauf der letzten 10 Jahre hat die Menge an digital verfügbaren,
          fachwissenschaftlich annotierten Volltexten für die historische Forschung stark
          zugenommen. Damit einher geht auch eine Veränderung sowohl der Nutzungsformen
          digitaler Quellen als auch der Möglichkeiten der historischen Arbeitsweise.
          Bestand um die Jahrtausendwende noch enger Kontakt zwischen Historiker_innen und
          Quellen, nimmt dies mit zunehmender Digitalisierung perspektivisch ab. Hat der /
          die Forscher_in früher die für seine Forschungsfragen relevanten Quellen in der
          Regel alle mindestens einmal gelesen, scheint dies bei den heute
          recherchierbaren Mengen an digitalen Quellen kaum noch möglich. Ein Hauptproblem
          ergibt sich hier aus der Schnittstelle zwischen Forscher_innen und den im Netz
          erreichbaren Quellendatenbanken. Die Suchinterfaces der Datenbanken sind oft für
          die Nutzung durch Expert_innen des jeweiligen Materials optimiert. Dies ist auf
          der einen Seite zu begrüßen, da sie den Fachwissenschaftler_innen damit besten
          Zugriff auf das Material gewähren. Daneben sollten aber weitere
          Zugriffsmöglichkeiten für übergreifende Text-Mining- oder Big-Data-Recherchen
          bereitgestellt werden, mit denen verschiedene Quellenkorpora parallel im
          Hinblick auf übergreifende Fragestellungen untersucht werden können.
            Neben Such- bzw. sammlungsorientierten Zugriffen auf die Daten wird die Fähigkeit, mittels bestimmter Visualisierungsmethoden und -instrumente neue Zusammenhänge in den Daten zu erkennen und diese dann für die historische Analyse zu nutzen immer wichtiger. Insbesondere im Bereich der graphorientierten Visualisierungsmethoden ist im Moment ein regelrechter Boom an Softwarebibliotheken und Online-Tools zu beobachten. Auch in den einschlägigen Forschungsumgebungen bzw. Forschungsinfrastrukturen für die Geisteswissenschaften wie TextGrid und DARIAH werden zunehmend Visualisierungsinstrumente für annotierte Fachdaten eingebunden.
            Im Workshop zweier Partnerinstitutionen aus dem DARIAH-DE Cluster
            "Fachwissenschaftliche Annotationen" (Salomon Ludwig Steinheim-Institut für
            deutsch-jüdische Geschichte / Akademie der Wissenschaften und der Literatur
            Mainz, Digitale Akademie) soll es mit konkretem Praxisbezug um die Potentiale,
            Methoden und Instrumente zur Visualisierung von historischen Fachdaten gehen.
            Als Anwendungsbeispiel dienen die historischen Fachdatenrepositorien, die beide
            Partner in den Workshop mit einbringen (bspw. Epidat –
            epigraphische Datenbank; die Deutschen
            Inschriften Online; die Regesta Imperii Online). 
            Visualisiert werden zunächst klassische Fragestellungen, wie beispielsweise die
              nach bestimmten Familienbeziehungen, nach Orts- oder Zeitbezügen in den Daten.
              Genutzt werden Instrumente wie beispielsweise die Graphdatenbank neo4j, das
              Netzwerkvisualisierungsinstrument Gephi, aber auch JavaScript-Tools auf Basis
              von sigma.js,
              dracula oder auch  d3 . Der Workshop wird
              schrittweise vorgehen. Ein grundlegendes Verständnis für TEI/XML, JSON, RDF
              sowie JavaScript-Webtechnologien ist für die Teilnahme am Workshop hilfreich,
              aber nicht zwingend. Nach einer allgemeinen Einführung in den Bereich des
              fachwissenschaftlichen Annotierens wird es um praktische Beispiele gehen, die in
              den Daten vorhandenen Annotationen für verschiedene Visualisierungsinstrumente
              aufzubereiten und dann die jeweiligen Visualisierungen zu erzeugen. In kurzen
              Impulsreferaten werden sich die Workshop-Teilnehmer_innen die gemeinsam
              erarbeiteten Visualisierungen und die Instrumente, mit denen diese
              Visualisierungen erzeugt worden sind, gegenseitig vorstellen. 
            Im Fazit soll der Workshop ein Bewusstsein und auch schon erste Fähigkeiten
                entwickeln, sich mit Hilfe von Visualisierungsinstrumenten neue Sichten auf das
                Quellenmaterial und somit neue Forschungsperspektiven zu eröffnen. Deutlich
                werden wird aber auch, dass dieser Ansatz nicht automatisch zu einer
                „Antwort-Maschine“ führt, die dem / der Wissenschaftler_in die interpretative
                Arbeit abnimmt. Vielmehr können sich durch Visualisierungen neue
                Interpretationsmöglichkeiten des Quellenmaterials bieten, die vorher einfach auf
                Grund der Datenmasse nicht sichtbar gemacht werden konnten.
            Weiterführende Literatur:
            Kuczera, Andreas (2015): Graphdatenbanken für Historiker.
                Netzwerke in den Registern der Regesten Kaiser Friedrichs III. mit neo4j und
                Gephi http://mittelalter.hypotheses.org/5995.
              
            Schrade, Torsten (2013): "Datenstrukturierung", in: Frietsch, Ute / Rogge, Jörg
                (eds.): Über die Praxis des kulturwissenschaftlichen
                Arbeitens. Ein Handwörterbuch. Bielefeld: transcript 91–97.
         
         
            Teilnehmerzahl
            10 - 15 Personen
         
         
            Benötigte Ausstattung
            
               WLAN-Zugang
               Beamer
               Workshop-Teilnehmer sollten ihre eigenen Laptops mitbringen
            
         
      
   



      
         
            
         
         
            Beispiele Graphbasierter Erschließung der Datenbank Nomen et Gens
            Die Datenbank Nomen et Gens ist aus einem DFG-Projekt hervorgegangen und verzeichnet Quellen und die in Ihnen belegten Personen für die vier Jahrhunderte vor der Zeit Karls des Großen. Das Abfragefrontend der Mysql-Datenbank ist im Internet unter www.nomen-et-gens.de zu erreichen.
            Ein Teil der Datenbank wurde im Vorfeld eines DFG-Antrages in eine Graphdatenbank konvertiert um neue Auswertungsmöglichkeiten zu testen, die im Rahmen dieses Posters dargestellt werden sollen.
            Frage an die Datenbank:
            Zeige mir die Quelle „Annales Petaviani“ und alle Personen, die in dieser Quelle belegt sind und alle Quellen, in denen diese Personen wiederum gemeinsam belegt sind. 
            
               
                  
                  Abbildung 1: Visualisierung der Graphdatenbankabfrage.
               
            
            Bewertung des Abfrageergebnisses:
            
               Der interessanteste Information der Visualisierung ist die traversale Abfrage zu Personen in einer Quelle, die wiederum in einer Quelle gemeinsam vorkommen. Die Abfrage „Zeige mir alle Personen in einer Quelle" funktioniert ja auch mit einer relationalen Datenbank. Aber es gibt keine Möglichkeit, auf die Weise auch noch gleich zu sehen, in welcher Quelle eine Person außerdem noch steht. Insofern fügt die Graphdatenbank hier eine "Dimension" hinzu. 
               Verwandtschaftliche Beziehungen zwischen Personen stehen beim jeweiligen Personeneintrag und sind damit immer nur bis zum nächsten Glied zu sehen.
               Für Frühmittelalterhistoriker besonders interessant sind gemeinsam urkundlich belegte Personen. Eine solche Abfrage ist über die relationale Datenbank nur schwer umfassend durchzuführen.
               Die Visualisierung aus der Graphdatenbank veranschaulicht die Überlieferungssituation von einzelnen Personen. Der umkringelte Radbod ist, anders als z.B. Bonifatius zu dem es sehr viele Belege gibt, nur schwer historisch fassbar.
            
         
      
      
         
            
               Bibliographie
               
                  Kuczera, Andreas (2016c): 
                        „Digital Editions beyond XML – Graph-based Digital Editions“, 
                        in: 
                        Proceedings of the 3rd HistoInformatics Workshop on Computational History (HistoInformatics 2016) 37–46
                        http://ceur-ws.org/Vol-1632/paper_5.pdf.
                    
               
                  Kuczera, Andreas (2016b): 
                        „Graphbasierte digitale Editionen“, 
                        in: 
                        Mittelalter: Interdisziplinäre Forschung und Rezeptionsgeschichte 19. April 2016 
                        mittelalter.hypotheses.org/7994
               
               
                  Kuczera, Andreas (2016a): 
                        „Endcoding and Presenting Historical Biographical Data with Graph Data Bases“,
                        in: 
                        CO:OP. The Creative Archives' and Users' Network
                  https://coop.hypotheses.org/297.
                    
               
                  Kuczera, Andreas (2015): 
                        „Graphdatenbanken für Historiker. Netzwerke in den Registern der Regesten Kaiser Friedrichs III. mit neo4j und Gephi“,
                        in: 
                        Mittelalter: Interdisziplinäre Forschung und Rezeptionsgeschichte 5. Mai 2015
                        mittelalter.hypotheses.org/5995
               
               
                  Kuczera, Andreas (2014b): 
                        „Big Data History“, 
                        in: 
                        Mittelalter: Interdisziplinäre Forschung und Rezeptionsgeschichte 10. Oktober 2014
                        mittelalter.hypotheses.org/3962
               
               
                  Kuczera, Andreas (2014a): 
                        „Digitale Perspektiven mediävistischer Quellenrecherche“, 
                        in: 
                        Mittelalter: Interdisziplinäre Forschung und Rezeptionsgeschichte 18. April 2014
                        mittelalter.hypotheses.org/3492
               
            
         
      
   



      
         
         
            Ausgangslage
            Der Einfluss der unter Digital Humanities (DH) zusammengefassten digitalen Theorien und Methoden auf die geisteswissenschaftlichen Disziplinen wächst stetig. Digitale Projekte erleben in den Geisteswissenschaften einen rasanten Aufschwung (Koller 2016: 43). Damit einher geht der Bedarf an Absolventen geisteswissenschaftlicher Fächer, die bereits während ihres Studiums Kompetenzen im Bereich der Digital Humanities erwerben konnten. Bereits 2013 forderten Vertreter/innen im „Manifest für die DH“ eine Etablierung „digitale[r] Trainingsprogramme in den Geisteswissenschaften“ (DH-Manifest: 2013), angepasst an die unterschiedlichen Bedürfnisse der Fachbereiche und die jeweiligen Karrierestufen. Auch der DHd misst der Ausgestaltung der IT-Ausbildung von Studierenden eine gesteigerte Bedeutung zu. Die Arbeitsgruppe zur Erarbeitung eines „Referenzcurriculums Digital Humanities“
                    2 beschäftigt sich mit der Suche nach einer 
                    bestpraxis, von der Anwender und Institutionen gleichermaßen profitieren (Sahle 2013; Thaller 2015: 3).
                
            Zahlreiche Universitätsstandorte haben auf die neuen Anforderungen mit der Einrichtung unterschiedlich ausgestalteter DH-Studiengänge reagiert (Bartsch/Borek/Rapp 2016: 173; DH Course Registry). Trotz dieser neugeschaffenen Angebote besteht ein zusätzlicher Bedarf an informationstechnologischer Ausbildung in der Breite (Ehrlicher 2016: 625). Zunehmend wird auch in „klassischen“ geisteswissenschaftlichen Berufsfeldern Sicherheit im Umgang mit Software und digitalen Technologien vorausgesetzt. Dieses Grundverständnis digitaler Methoden kann nicht mehr ausschließlich im Selbststudium angeeignet werden (Spiro 2013: 332; Sahle 2016: 79).
         
         
            Projektziele und Rahmenbedingungen
            Hier setzt das Projekt „Digitaler Campus Bayern – Digitale Datenanalyse in den Geisteswissenschaften“ an, welches von der IT-Gruppe Geisteswissenschaften (ITG) der Ludwig-Maximilians-Universität München (LMU) seit Beginn dieses Jahres durchgeführt wird. Grundgedanke ist eine IT-Grundausbildung („
                    IT for all“), welche die Studierenden problemorientiert in die Anwendung digitaler Methoden einführt. Ausgehend von fachwissenschaftlichen Fragestellungen werden Lehrveranstaltungen mit IT-Inhalten in Kooperation mit verschiedenen geschichtswissenschaftlichen Disziplinen und der Kunstgeschichte konzipiert. Dabei soll eine möglichst umfassend angelegte Grundlagenvermittlung in Erfassung, Modellierung, Analyse und anschließender Visualisierung von Daten erfolgen (Lücke/Riepl 2016: 77). Das Verständnis digitaler Methoden steht ebenso im Vordergrund wie eine fachliche Reflexion ihrer Potentiale (Rehbein 2016: 17).
                
            Die Situation der DH an der LMU gestaltete sich bis Projektbeginn (Januar 2016
                    3) ambivalent. In den vorgenannten Studiengängen wurden regelmäßig Überblicksveranstaltungen zur Einführung in die Informatik für Historiker bzw. Kunsthistoriker angeboten. Eine praktische Umsetzung des theoretischen Wissens konnte im Rahmen dieser Veranstaltungen jedoch nicht geleistet werden. Demgegenüber werden durch die ITG, die auf langjährige und umfangreiche Erfahrungen im Bereich des digitalen Projektmanagements
                    4 verweisen kann, optimale Voraussetzungen für eine fortan praxisnahe IT-Ausbildung geschaffen.
                
            Als Mitglied im Münchner Arbeitskreis für digitale Geisteswissenschaften (dhmuc)
                    5 kooperiert die IT-Gruppe zudem fach- und institutionsübergreifend mit zahlreichen kulturellen Einrichtungen. An der Schnittstelle zur universitären Lehre ist es möglich, die „
                    IT for all“-Ausbildung geisteswissenschaftlicher Studierender auf die Anforderungen und Wünsche der potentiellen Arbeitgeberseite im (digitalen) Kultur-, Wissenschafts- und Informationssektor auszurichten.
                
         
         
            Interaktive Lehr- und Lernumgebung 
                    DHVLab
            
            Für die praktische Umsetzung kommt eine interaktive Lehr- und Lernumgebung, das 
                    Digital Humanities Virtual Laboratory – kurz 
                    DHVLab – zum Einsatz
                    6. Die im Entstehen begriffene Plattform umfasst mehrere Komponenten, die im Folgenden vorgestellt werden sollen:
                
            
               Virtuelle Rechenumgebung
               Die virtuelle Rechenumgebung ist das „Herzstück“ der Ausbildungsplattform. Auf dem virtuellen Desktop werden in Abstimmung mit dem/der Kursleiter/in Software und Tools installiert. Dadurch wird die sukzessive Installation durch die Teilnehmer/innen obsolet, wodurch Probleme aufgrund unterschiedlicher Betriebssysteme und Versionierungen vermieden werden. Bei Anmeldung im 
                        DHVLab erhält jede/r Teilnehmer/in eine eigene SQL-Datenbank. Gleichzeitig werden strukturierteDatensammlungen vorgehalten. Diese sind für die Kursteilnehmer/innen zugänglich und für eigene oder im Kurs behandelte Fragestellungen verwendbar. Im Laufe der Lehrveranstaltung können neue Forschungsfragen ausgearbeitet und ein grundsätzliches Verständnis für den sinnvollen Einsatz von Tools und Software
                        7 in den Geisteswissenschaften entwickelt werden.
                    
            
            
               Ausbildungsmaterialien
               Im vergangenen Semester wurde das System testweise in vorgenannten Einführungsveranstaltungen eingesetzt. Die bei der Evaluation gesammelten Erfahrungen fließen unmittelbar in die Erstellung bzw. Erweiterung der Ausbildungsmaterialien. Anhand praxisnaher Manuale wird IT-Grundlagenwissen, in einzelne Lehreinheiten gegliedert, anschaulich dargestellt und erklärt. Die Erstellung von Lehrvideos und Übungsaufgaben ist vorgesehen. Aus diesem Portfolio können Dozentinnen und Dozenten Module entsprechend ihrer fachwissenschaftlichen Schwerpunktsetzung und der Voraussetzungen der Teilnehmer/innen auswählen. Die Seminarplanung und -durchführung erfolgt stets in enger Abstimmung mit den Projektmitarbeitern.
            
            
               Publikationsumgebung
               Für die Vor- und Nachbereitung der einzelnen Sitzungen steht ein WordPress-Blog zur Verfügung. Dort können die Kursleiter/innen Materialen einstellen, die Studierenden ihren Erkenntnisfortschritt und Analyseergebnisse dokumentieren. Dabei erlernen sie gleichzeitig 
                        in praxi das wissenschaftliche Bloggen als innovative Form des Publizierens. Eine abschließende Publikation der studentischen Seminararbeiten ist auf dieser Plattform möglich.
                    
            
            
               Datenrepositorium
               In einem gesonderten Bereich der Datenbankumgebung werden die von den Studierenden im Rahmen einer Lehrveranstaltung erarbeiteten Datenbestände modelliert und nachhaltig abgelegt. Langfristiges Ziel ist der Aufbau eines Forschungsdatenrepositoriums. Nachfolgende Kurse mit ähnlichen Seminarthemen können auf diese Datensammlungen zugreifen, für die eigene Forschungsarbeit verwenden und dadurch sukzessive erweitern. Unterstützung erfährt die ITG durch die Universitätsbibliothek der LMU als Kooperationspartnerin auf dem Gebiet der nachhaltigen und nachnutzbaren elektronischen Publikation von Forschungsdaten.
            
            
               Entwicklung eigener Analyse- und Softwarekomponenten
               Mit dem 
                        DHVLab Analytics Center wurde eine Webanwendung entwickelt, die dazu dient, konkrete geisteswissenschaftliche Fragestellungen mithilfe quantitativer statistischer Methoden zu beantworten, sowie im Stile eines explorativen Werkzeuges neue Forschungsansätze zu eröffnen. Das 
                        Analytics Center kombiniert einführende deskriptive Analysen mit komplexeren Methoden der multivariaten Statistik. Neben diesem Analysetool entsteht derzeit eine Editionsumgebung, die speziell auf die Anforderungen von Studierenden und Promovierenden ausgerichtet wird. Diese wird erstmals im Sommersemester 2017 in einer Übung zur Edition mittelalterlicher Urkunden zum Einsatz kommen. Die Entwicklung weiterer Instrumente ist geplant.
                    
            
         
         
            Der Einsatz der Plattform in der Lehre
            Nach der technischen Realisierung der Plattform und dem Aufbau grundlegender Ausbildungsmaterialien im ersten Projekthalbjahr kommt das System im Wintersemester 2016/2017 erstmals in eigens konzipierten Lehrveranstaltungen zur Anwendung. In der Kunstgeschichte soll das 
                    Analytics Center in einem Seminar zur Beschäftigung mit informatischen und mathematischen Verfahrensweisen anregen. Parallel dazu erfolgt eine Einführung in die Statistiksoftware 
                    RStudio. Ein geschichtswissenschaftliches Hauptseminar beschreitet den Weg von der Originalquelle über die strukturierte Aufnahme und Modellierung von Forschungsdaten sowie die Einführung in die Arbeit mit relationalen Datenbanken hin zur Georeferenzierung. Die in den Seminaren gewonnenen Erfahrungen und Erkenntnisse fließen unmittelbar in die Verbesserung und Ausweitung des bestehenden Lehrmaterials ein (u.a. Erstellung von Anwendungsszenarien). Neben den genannten Kursen wird die Plattform bereits in zahlreichen Lehrveranstaltungen als technische Grundlage verwendet
                    8.
                
         
         
            Konzeption eines fachspezifischen DH-Curriculums
            Die sukzessive wachsende Plattform und die aktuell angebotenen Kurse dienen als Grundlage für eine Institutionalisierung der IT-Grundausbildung in Form eines fachspezifischen DH-Curriculums. Das Konzept für das geplante freiwillige Zusatz-Zertifikat wird derzeit in der Projektgruppe erarbeitet und baut auf Erfahrungen vergleichbarer Angebote im deutschsprachigen Raum auf
                    9. Angedacht ist eine Kombination aus Veranstaltungen, die explizit IT-Grundlagenwissen vermitteln, und praxisorientierten Kursen, in denen die erlernten IT-Inhalte auf fachwissenschaftliche Gegenstände angewendet werden. Wichtig erscheint eine ausgewogene Verschränkung von 
                    eLearning-Angeboten und Präsenzveranstaltungen, da insbesondere letzteren durch den intensiven Austausch der Studierenden mit DH-Spezialisten ein großer Beitrag zum Lernerfolg beigemessen wird
                    10.
                
         
         
            Grundlage einer nachhaltigen IT-Didaktik
            Neben der Langzeitarchivierung der Forschungsdaten wird auch die Nachhaltigkeit der informationstechnologischen Infrastruktur (Serveranlage mit redundant ausgelegten File-, Datenbank- und Web-Servern sowie ausreichenden Storages) durch die IT-Gruppe Geisteswissenschaften dauerhaft gewährleistet. Die Architektur des 
                    DHVLab ist flexibel und skalierbar gestaltet, sodass sie weiter ausgebaut werden kann (bei Bedarf ist ein Hosting der Server am Leibniz-Rechenzentrum in Garching bei München möglich). Für eine nachhaltige IT-Didaktik spielt neben der langfristig gesicherten technischen Infrastruktur insbesondere die inhaltliche Kontinuität eine entscheidende Rolle. Die im Rahmen des Projektes erarbeiteten Lehreinheiten werden dauerhaft zur Verfügung gestellt. Thematisch sind sie so zu gliedern und fachlich anzupassen, dass eine spezifische Auswahl für eine Lehrveranstaltung und damit eine Integration in ein geisteswissenschaftliches Einzelfach möglich ist. Die IT-Gruppe stellt auch nach Ende der Projektlaufzeit die unterstützende Begleitung der Lehrveranstaltungen sicher. Der Vortrag möchte zur Diskussion anregen, inwiefern sich die Anpassung der Materialien an die sich rasch wandelnden Anforderungen im Bereich der Digital Humanities möglichst effizient gestalten lässt. IT-Didaktik scheint nur dann einen Anspruch auf Nachhaltigkeit zu besitzen, wenn sie sich in einem steten Anpassungsprozess befindet.
                
            Ganz im Sinne des „Digitalen Campus Bayern“ ist das Münchener Pilotprojekt auf eine Ausweitung auf andere Studienstandorte ausgerichtet. Die Plattform wird beispielsweise ab 2017 in einem im Aufbau befindlichen Kooperationsprogramm zur DH-Ausbildung der Universitäten Erlangen, München und Regensburg zum Einsatz kommen. Alle Module des 
                    DHVLab können kollaborativ von anderen Hochschulen genutzt werden, um umfassende Sammlungen von Tutorials, Aufgaben, Softwarebeschreibungen, Anwendungsszenarien sowie Sammlungen fachwissenschaftlicher Objekt- und Metadaten aufzubauen und gemeinsam zu pflegen.
                
         
      
      
         
            Vgl. 
                            http://www.dh-curricula.org/index.php?id=1 [letzter Zugriff: 30. November 2016].
                        
             Die Projektlaufzeit beträgt zwei Jahre. Das Vorhaben ist Teil eines Förderprogramms, welches das Bayerische Wissenschaftsministerium aufgelegt hat. Vgl. 
                            https://www.km.bayern.de/pressemitteilung/9340/.html [letzter Zugriff: 30. November 2016].
                        
             Vgl. die Übersicht unter 
                            www.itg.lmu.de/projekte [letzter Zugriff: 30. November 2016].
                        
             Vgl. 
                            http://dhmuc.hypotheses.org/uber [letzter Zugriff: 30. November 2016].
                        
             Für die Dokumentation der technischen Infrastruktur vgl. 
                            http://dhvlab.gwi.uni-muenchen.de/index.php/Category:
               Architektur [letzter Zugriff: 30. November 2016].
                        
             Derzeit stehen in der virtuellen Umgebung u.a. folgende Software und Programme zur Verfügung: LibreOffice-Paket, OCRFeeder und Ocrad (Texterkennung), Python (PyCharm), RStudio (Statistik), Gephi (Visualisierung), epcEdit (XML-Editor), AntConc und TreeTagger (Korpuslinguistik).
             Vgl. die Zusammenstellung auf der Projektseite: 
                            http://dhvlab.gwi.uni-muenchen.de/index.php/Das_DHVLab_im_Einsatz [letzter Zugriff: 30. November 2016].
                        
             Vgl. insbesondere die Angebote in Köln (
                            http://www.itzertifikat.uni-koeln.de/), Passau (
                            http://www.phil.uni-passau.de/zertifikat-dh/) und Stuttgart („Das digitale Archiv“, 
                            http://www.uni-stuttgart.de/dda), letztgenanntes als Vorläufer eines DH-Masterstudienganges [letzter Zugriff: 30. November 2016].
                        
             Vor diesem Hintergrund erscheinen grundständige 
                            eLearning-Angebote wie „The Programming Historian“ (
                            http://programminghistorian.org/) für einen autodidaktischen Einstieg begrüßenswert. Die Initiatoren des DHVLab sind jedoch der Auffassung, dass eine umfassende Präsenzausbildung nicht ersetzt werden kann.
                        
         
         
            
               Bibliographie
               
                  Bartsch, Sabine / Borek, Luise / Rapp, Andrea (2016): 
                        „Aus der Mitte der Fächer, in die Mitte der Fächer: Studiengänge und Curricula – Digital Humanities in der universitären Lehre“,
                        in: 
                        Bibliothek – Forschung und Praxis 40 (2): 172–178 10.1515/bfp-2016-0030.
                    
               
                  DARIAH-EU: 
                        Digital Humanities Registry – Courses
                  https://dh-registry.de.dariah.eu/ [letzter Zugriff 30. November 2016].
                    
               
                  DHI Paris (Teamaccount) (2013): 
                        „Wissenschaftlicher Nachwuchs in den Digital Humanities: Ein Manifest“,
                        in: 
                        Digital Humanities am DHIP, 23. August 2013 
                        http://dhdhi.hypotheses.org/1995 [letzter Zugriff 30. November 2016].
                    
               
                  Ehrlicher, Hanno (2016): 
                        „Fingerübungen in Digitalien. Erfahrungsbericht eines teilnehmenden Beobachters der Digital Humanities aus Anlass eines Lehrexperiments“, 
                        in: 
                        Romanische Studien 4: 623–636 
                        http://www.romanischestudien.de/index.php/rst/article/view/88 [letzter Zugriff 30. November 2016].
                    
               
                  Koller, Guido (2016): 
                        Geschichte digital: Historische Welten neu vermessen. 
                        Stuttgart: Kohlhammer.
                    
               
                  Lücke, Stephan / Riepl, Christian (2016): 
                        „Auf dem Weg zu einem Curriculum in den Digital Humanities“,
                        in: 
                        Akademie Aktuell 57 (1): 74–77 
                        http://badw.de/fileadmin/pub/akademieAktuell/2016/56/0116_17_Riepl_V04.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Rehbein, Malte (2016): 
                        Geschichtsforschung im digitalen Raum. Über die Notwendigkeit der Digital Humanities als historische Grundwissenschaft. (Preprint) 
                        http://www.phil.uni-passau.de/fileadmin/dokumente/lehrstuehle/rehbein/Dokumente/GeschichtsforschungImDigitalenRaum_preprint.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Sahle, Patrick (2013): 
                        DH studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities (= DARIAH-DE Working Papers 1). 
                        Göttingen: GOEDOC 
                        http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Sahle, Patrick (2016): 
                        „Digital Humanities als Beruf. Wie wird man ein „Digital Humanist“, und was macht man dann eigentlich?“, 
                        in: 
                        Akademie Aktuell 57 (1): 78–83 
                        http://badw.de/fileadmin/pub/akademieAktuell/2016/56/0116_18_Sahle_V04.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Spiro, Lisa (2012): 
                        „Openingup Digital Humanities Education“, 
                        in: Hirsch, Brett D. (ed.):
                        Digital Humanities Pedagogy: Practices, Principlesand Politics 331–363 
                        http://www.openbookpublishers.com/product/161/ [letzter Zugriff 30. November 2016].
                    
               
                  Thaller, Manfred (2015): 
                        „Panel: Digital Humanities als Beruf – Fortschritte auf dem Weg zu einem Curriculum“, 
                        in: 
                        Digital Humanities als Beruf: Fortschritte auf dem Weg zu einem Curriculum, vorgelegt auf der Jahrestagung 2015 3–5 
                        https://www.digitalhumanities.tu-darmstadt.de/fileadmin/dhdarmstadt/materials/Digital_Humanities_als_Beruf_-_Stand_2015.pdf [letzter Zugriff 30. November 2016].
                    
            
         
      
   



      
         Problemstellung
         Im Anschluss an den 1990 durch den Humangeographen Edward Soja ausgerufenen ‚Spatial Turn’ (Soja 1990) haben sich zahlreiche kulturwissenschaftliche Forschungsarbeiten mit einer Beschreibung des Raums beschäftigt. In der Literaturwissenschaft fanden dabei u.a. kartografische Darstellungen große Resonanz: Franco Moretti etwa untersuchte in seinem „Atlas of the European Novel“ Orte der literarischen Produktion und Rezeption (Moretti 1998), Barbara Piattis Studie „Die Geographie der Literatur“ richtete den Fokus auf die Illustration einer konkreten literarisch thematisierten Gegend (die Zentralschweiz, vgl. Piatti 2008). Besondere Aufmerksamkeit wurde literarischen Karten auch im Kontext der Digital Humanities zu Teil, in denen geografische Informationssysteme (GIS) zum Einsatz kommen (typische Workflows beschreiben Gregory et. al. 2015)
         Den meisten dieser Ansätze ist dabei gemein, dass sie für ihre Datengrundlage in erster Linie auf konkrete Nennungen von Ortsnamen (Toponymen) rekurrieren und weitere Ortsmarker weniger stark berücksichtigen. An der Konstitution literarischer Räume sind jedoch in der Regel auch komplexere Faktoren beteiligt, zu deren Beschreibung bereits erste narratologische Ansätze vorliegen (etwa von Kathrin Dennerlein [2009 und 2011] oder Gabriel Zoran [1984], vgl. auch die Überlegungen bei Piatti [2008]), die jedoch im Kontext der Digital Humanities bislang noch zu wenig Beachtung gefunden haben.
         In unserem Beitrag möchten wir diese Ansätze aufgreifen, um das Instrumentarium der digitalen Textanalyse hinsichtlich der Kategorie des Raums zu schärfen und zu erweitern. Dazu scheinen uns insbesondere zwei Aspekte von Bedeutung: Zum einem die Unterscheidung von Raummarkierungen hinsichtlich ihrer Handlungsrelevanz (I), zum anderen die Ausweitung der Analyse auf räumliche Begriffe, die über bloße Namensnennungen hinausgehen (II). Für beide Problemfelder präsentieren wir erste Verfahren zur automatischen Auswertung und geben Ausblicke auf die Möglichkeiten einer vergleichenden Analyse.
         
            I. Differenzierung von Räumen nach Handlungsrelevanz
         
         Im Anschluss an Dennerleins Narratologie des Raumes hat sich insbesondere der Terminus der 
                räumlichen Gegebenheit als Grundeinheit zur Bezeichnung von Ort und Raum durchgesetzt. Sind räumliche Gegebenheiten der Schauplatz eines konkreten Ereignisses, werden sie als 
                Ereignisregionen spezifiziert. Diese haben als die zentralen handlungsrelevanten Räume besondere Bedeutung gegenüber 
                erwähnten räumlichen Gegebenheiten, die bei nicht-situationsbezogener Thematisierung von Raum entstehen. In ähnlicher Weise unterscheiden Piatti (2008) sowie Piatti et. al. (2011) zwischen 
                Schauplatz und 
                projizierten Orten.
            
         Am Beispiel von Jules Vernes „Reise um die Erde in 80 Tagen“ lässt sich die Wichtigkeit dieser Unterscheidung aufzeigen: So bietet z.B. Kapitel 14 eine Zugfahrt durch das Gangestal mit Aufenthalten in Allahabad und Benares sowie der Ankunft in Calcutta. Genannt werden im Text jedoch auch weitere Städte Indiens und das nächste Ziel Hongkong; eine Vielzahl der extrahierten Ortsnamen bezieht sich somit nicht auf den Handlungsort des Kapitels.
         Erst die kategoriale Trennung der Raummarkierungen in Ereignisregionen und erwähnte räumlichen Gegebenheiten ermöglicht die valide Rekonstruktion einer Reiseroute, die dann in einer GIS-Darstellung visualisiert werden kann (Abbildung 1). 
         
            
         
         Eine automatische Unterscheidung dieser Kategorien muss daher das Fernziel computerunterstützter Raumuntersuchungen sein. Ansätze zu einer solchen Differenzierung suchen wir in der Anwendung von computerlinguistischen Methoden der Relationsextraktion, bei denen sich aus strukturellen Auffälligkeiten Regeln zur Klassifikation von Ereignisregionen und erwähnten räumlichen Gegebenheiten ableiten lassen. Hierzu zwei Beispiele: 
         1. Das 14. Kapitel der „Reise um die Erde in 80 Tagen“ beginnt mit dem in Abbildung 2 dargestellten Satz:
         
            
         
         Vor allem der Hauptsatz mit dem Pfad [Figur – SUBJ – Bewegungsverb - OBJ – Raumnomen] zwischen "Phileas Fogg" und "Gangesthal" kennzeichnet letzteres als Ereignisregion. Dabei stellt insbesondere die Verbkategorie ein Indiz für die Klassifikationsentscheidung dar: Statische Verben ("stehen", "sitzen") oder Verben der Bewegung ("gehen", "fahren") zeugen in spezifischen Satzstrukturen häufig von einer Ereignishaftigkeit im Gegensatz zu Verben der Kognition ("denken"). 
         Zur Einteilung der Verben nutzen wir das von der Universität Tübingen entwickelte lexikalisch-semantische Netz 
                GermaNet (Hamp/Feldweg 1997, Henrich/Hinrichs 2010), in dem eine Systematisierung vorliegt, die auf den Verbkategorien von Levin und dem Valenzwörterbuch von Schumacher basiert (Levin 1993, Schumacher 1986).
            
         2. Findet ein erzähltes Ereignis innerhalb einer Bewegung im Raum statt, können mehrere Räume zu einem 
                Bewegungsbereich zusammengefasst werden. Diese können bei Strukturen wie der folgenden leicht maschinell erkannt werden:
            
         "Ich fuhr [...] 
                von Königstadt, wo unser Hauptbüro war, 
                nach Gründerheim, wo wir eine Nebenstelle hatten. 
                Dort holte ich dringende Korrespondenz, Gelder und schwebende Fälle." (Böll, Heinrich: Über die Brücke)
            
         Im zweiten Satz findet sich zudem eine Referenz auf das Antezedens "Gründerheim". Eigentlich werden derartige deiktische Adverbialausdrücke ("hier", "da" und "dort") bei der Koreferenz-Resolution nicht berücksichtigt. Deshalb planen wir eine Erweiterung der Trainingssets bestehender Koreferenz-Systeme hinsichtlich dieser Termini. 
         Die hier an zwei Beispielfällen dargestellten Regeln zur Unterscheidung narratologischer Raumeinheiten sollen in Zukunft kontinuierlich erweitert und zunächst so gestaltet werden, dass sie eine hohe Präzision erzielen. Anschließend können sie als Features für spätere maschinelle Lernverfahren verwendet werden.
         
            II. Anreicherung der Raumbeschreibung
         
         Netzwerkvisualisierung von Raumnomen
         Eine kartografische Darstellung, wie sie in Abbildung 1 ersichtlich ist, bleibt auf realweltliche Ortsnamen beschränkt und lässt wesentliche Aspekte der Raumdarstellung außer Acht. Einen ersten Ansatz, die Vielfältigkeit der tatsächlichen Handlungsräume und ihrer Zusammenhänge abzubilden, bietet das Netzwerk in Abbildung 3. Hier werden für das angesprochene Kapitel 14 neben den Toponymen auch unspezifische Raumnomen als Knoten berücksichtigt und Verbindungen immer dann etabliert, wenn zwei Raumbegriffe gemeinsam in einem Satz auftreten.
         Insbesondere landschaftliche (grün) und architektonische Raumnomen (grau) stellen relevante Klassen von Raummarkern dar, die neben den konkreten Ortsnamen zentrale Komponenten der literarischen Raumbeschreibung bilden. Als räumliche Gegebenheiten kommen aber auch bewegliche Objekte, in denen sich Figuren aufhalten können, in Frage, wie die in der Grafik blau markierten Fahrzeuge. 
         
            
         
         Abbildung 3: Netzwerk
         Lexikon und Taxonomien für Raumbegriffe
         Zur lexikalischen Erfassung von realweltlichen Toponymen greifen wir auf die Named-Entitiy-Recognition von 
                Weblicht (2012) zurück, deren Ergebnisse wir mit dem Rückgriff auf die frei zugänglichen Datenbanken 
                GeoNames (www.geonames.org) und 
                OpenStreetMap (www.openstreetmap.org, Datendownload über www.geofabrik.de) zu verfeinern trachten. Aufgrund des Problems der möglichen Ambiguität von Ortsnamen (Leidner / Liebermann 2011, Gregory / Hardie 2011) ist jedoch eine manuelle Nachbearbeitung nötig. Listen von unspezifischen Raumnomen („Berg“, „Bach“, etc.) erstellen wir (ebenfalls semi-automatisch) auf der Basis von 
                GermaNet.
            
         Innerhalb dieses Lexikons planen wir zudem eine Einordung der Raumbegriffe in spezifische Taxonomien:
         
            i) Vertikale Raum-Ort-Hierarchie
         
         Narratologisch wird unter dem Begriff 
                Raum ein umfassendes Gebiet in der erzählten Welt verstanden, welches ein Innen und Außen besitzt und wiederum lokalisierbare, punktuelle 
                Orte beinhaltet (Dennerlein 2009). Diese Zuordnung erfolgt jedoch meist relational zur Erzählsituation: In Alfred Döblins Roman 
                Berlin Alexanderplatz bildet die Stadt den Raum mit einzelnen Plätzen und Straßen als Orten. Unter einer geringfügigen Erweiterung des Erzählspektrums wäre Berlin aber potentiell nur ein Ort unter vielen im übergeordneten Raum Deutschland.
            
         Statt einer festen Zuschreibung nähern wir uns dem Verhältnis von Ort und Raum über eine vertikale Taxonomie von räumlichen Gegebenheiten an, die von der Planetenebene bis zu jenen Objekten reicht, in denen sich unter Annahme faktualer Gesetzmäßigkeiten keine Figur mehr aufhalten kann. Im Sinne des 
                principles of minimum departure (Ryan 1980) kann dabei so lange von einer nach realweltlichen Gesetzen eingerichteten Erzählwelt ausgegangen werden, bis deren bewusste Aufhebung innerhalb fiktionaler Texte in spezifischen Fällen eine gezielte Anpassung der Taxonomie erfordert (z.B. bei Aladins Flaschengeist in der Wunderlampe).
            
         Abbildung 4 zeigt basierend auf den kapitelweise extrahierten Ereignisregionen in „Reise um die Erde in 80 Tagen“ die obersten taxonomischen Stufen: 1. Kontinent, 2. Land, 3. Stadt bzw. landschaftliche Region (inklusive Transportmittel, markiert in blau). 
         
            
         
         Während in den ersten beiden Ebenen dieses Beispiels ausschließlich Toponyme vorkommen, beinhaltet zumindest die dritte Stufe in der Nominalphrase „Latenenwald bei Allahabad“ ein unspezifisches Raumnomen. Weitere allgemeine Begriffe wären vor allem auf einer hier nicht dargestellten vierten Ebene zu finden (z.B. „Sumpf“, „Bach“, „Weizenfeld“ innerhalb der Landschaft „Behar“, vgl. Abb. 3).
         Zur automatischen Erstellung einer solchen Hierarchie bieten sich bei Toponymen die in 
                GeoNames vorhandenen Metadaten an, in denen bei jedem Stadt-Eintrag Informationen zu Land und Kontinent vorhanden sind. Bei unspezifischen Raumnomen eignet sich hingegen die hierarchische Struktur von 
                GermaNet. So stellt etwa der Begriff „Bach“ ein Hyponym zum übergeordneten Synset „Wasser/Gewässer“ dar, letzteres besitzt wiederum die Hyperonyme „Land/Gegend/Gefilde“.
            
         
            ii) Wortfelder
         
         Wie in Abb. 3 ersichtlich, speisen sich Raumnomen zu großen Teilen aus den Wortfeldern Architektur und Landschaft. Die folgende Analyse basiert auf semiautomatisch erstellten Wortlisten, die auf der Basis von 
                GermaNet durch die Auswertung der entsprechenden Synsets und Implikationen (Hyperonymie und Hyponymie) von zentralen Begriffen aus beiden Wortfeldern gewonnen wurden.
            
         Makroperspektive
         Das Potential digitaler korpusgestützter Raum-Analysen soll anhand des Vergleichs dreier ‚Berlin-Romane’ exemplarisch aufgezeigt werden. Dazu werden die Texte in jeweils zehn Segmente aufgeteilt und hinsichtlich der Frequenz spezifischer Raumbegriffe aus den Wortfeldern Architektur und Landschaft untersucht:
         Dabei lassen sich deutlich höhere Anteile des architektonischen Vokabulars gegenüber dem Segmentmittelwerten eines Vergleichskorpus erkennen, das aus 451 im Textgrid-Repository enthaltenen Romanen besteht (Abbildung 5, oben). 
         
            
         
         Während die Verteilung des architektonischen Wortschatzes in Hesses „Heimliches Berlin“ nur temporäre Spitzen zeigt, sind die Segmentverteilungen von „Berlin Alexanderplatz“ und Wilhelm Raabes „Die Chronik der Sperlingsgasse“ gegenüber der mittleren Verteilung des Korpus signifikant verschieden. Dies wurde sowohl mit dem Wilcoxon-Rangsummentest (Annahme der Varianzhomogenität und Gleichverteilung zwischen den Sampleverteilungen) sowie dem Mood's Median-Test (keine Verteilungsannahme) überprüft (Abbildung 6).
         Das landschaftliche Vokabular hingegen liegt bei den Berlin-Romanen tendenziell etwas unter dem Mittel des Korpus, allerdings sind die Abweichungen nur im Fall von „Berlin Alexanderplatz“ eindeutig signifikant (Abb. 5 unten, Abb. 6)
         
            
         
         Ungeachtet dieser Unterschiede sind die Zusammenhänge zwischen beiden Wortfeldern auffällig (Abbildung 7). Die Spearman-Korrelation zwischen architektonischen und landschaftlichen Begriffen bei „Berlin Alexanderplatz“ beträgt 0.5030488 und bei „Die Chronik der Sperlingsgasse“ sogar 0.7454545. So kann trotz abweichender Anteile der Wortfelder hinsichtlich ihrer Frequenz eine starke Verflechtung spezifischer Klassen von Räumen angenommen werden.
         
            
         
         Ausblick
         Die vorgestellten Ansätze verstehen sich als Anregung für die Entwicklung eines differenzierten Instrumentariums der digitalen Raumanalyse, das in Zukunft weiter ausgebaut werden soll und die Grundlage für die Behandlung weiterführender literaturwissenschaftlicher Fragestellungen bildet, die etwa Aspekte der Semantisierung von Räumen (Lotman 1972), des raumzeitlichen Entwurfs von Erzählwelten (Bachtin 1989) und der Bedeutung von Raumkonstellationen für die Gattungspoetik beinhalten (vgl. zusammenfassend Nünning 2009). 
      
      
         
            
               Bibliographie
               
                  Bachtin, Michail Michailowitsch (1989): 
                        Formen der Zeit im Roman. Untersuchungen zur historischen Poetik. Ed. von Kowalski, Edward / Wegner, Michael.
                        Frankfurt am Main: Fischer.
                    
               
                  Bastian Mathieu / Heymann Sebastian / Jacomy Mathieu (2009): 
                        „Gephi. An open source software for exploring and manipulating networks“, 
                        in: 
                        International AAAI Conference on Weblogs and Social Media.
                    
               
                  Dennerlein, Katrin (2009): 
                        Narratologie des Raumes. 
                        Berlin: de Gruyter.
                    
               
                  Dennerlein, Katrin (2011): 
                        „Raum“,
                        in: Matías Martínez (ed.): 
                        Handbuch Erzählliteratur: Theorie, Analyse, Geschichte. 
                        Stuttgart / Weimar: Metzler 158–165.
                    
               
                  Gregory, Ian / Hardie, Andrew (2011): 
                        „Visual GISting: bringing together corpus linguistics and Geographical Information Systems“, 
                        in; 
                        LLC 26: 297–314. 
                    
               
                  Gregory, Ian / Cooper, David / Hardie, Andrew / Rayson, Paul (2015): 
                        „Spatializing and Analyzing Digital Texts. Corpora, GIS, and Places“, 
                        in: David Bodenhamer / John Corrigan / Trevor Harris: 
                        Deep Maps and Spatial Narratives. 
                        Bloomington: Indiana University Press 150–178. 
                    
               
                  Hamp, Birgit / Feldweg, Helmut (1997): 
                        „GermaNet - a Lexical-Semantic Net for German“, 
                        in: 
                        Proceedings of the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications.
                        Madrid.
                    
               
                  Henrich, Verena / Hinrichs Erhard (2010): 
                        „GernEdiT - The GermaNet Editing Tool“, 
                        in: 
                        Proceedings of LREC 2010 2228–2235.
                    
               
                  Leidner, Jochen / Lieberman, Michael (2011): 
                        „Detecting Geographical References in the Form of Place Names and Associated Spatial Natural Language“, 
                        in: 
                        SIGSPATIAL Special 3: 5–11.
                    
               
                  Levin, Beth (1993): 
                        English Verb Classes and Alternations. 
                        University of Chicago Press.
                    
               
                  Lotman, Juri (1972): 
                        Die Struktur literarischer Texte. 
                        München: Fink.
                    
               
                  Moretti, Franco (1998): 
                        Atlas of the European novel. 1800-1900. 
                        London / New York: Verso.
                    
               
                  Nünning, Ansgar (2009): 
                        „Formen und Funktionen literarischer Raumdarstellung: Grundlagen, Ansätze, narratologische Kategorien und neue Perspektiven“, 
                        in: Wolfgang Hallet / Birgit Neumann (eds.): 
                        Raum und Bewegung in der Literatur: Die Literaturwissenschaften und der Spatial Turn. 
                        Bielefeld: Transcript 33–52.
                    
               
                  Piatti, Barbara (2008): 
                        Die Geographie der Literatur. Schauplätze, Handlungsräume, Raumphantasien. 
                        Göttingen: Wallstein.
                    
               
                  Piatti, Barbara / Reuschel, Anne-Kathrin / Hurni, Lorenz (2011): 
                        „A Literary Atlas of Europe – Analysing the Geography of Fiction with an Interactive Mapping and Visualisation System“, 
                        in: 
                        Proceedings of the 25th International Cartographic Conference. Paris.
                    
               
                  Ryan, Marie Laure (1980): 
                        „Fiction, Non-Factuals, and Minimal Departure“, 
                        in:
                        Poetics 8: 403–422.
                    
               
                  Schumacher, Helmut (1986): 
                        Verben in Feldern: Valenzwörterbuch zur Syntax und Semantik deutscher Verben. 
                        Berlin / New York: de Gruyter Verlag,
                    
               
                  Soja, Edward (1990): 
                        Postmodern Geographies: The Reassertion of Space in Critical Social Theory.
                        London / New York: Verso.
                    
               
                  WebLicht (2012): 
                        CLARIN-D/SfS-Uni. Tübingen 2012. WebLicht: Web-Based Linguistic Chaining Tool. 
                        https://weblicht.sfs.uni-tuebingen.de/ [letzter Zugriff 1. Dezember 2016]
                    
               
                  Zoran, Gabriel (1984): 
                        „Towards a theory of space in narrative“, 
                        in: 
                        Poetics Today 5: 309–335.
                    
            
         
      
   



      
         Mit der Thematik der digitalen Nachhaltigkeit sind Gedächtnisinstitutionen, die sich die umfassende digitale Sicherung und Erschließung des Kulturerbes zum Ziel setzen, gleichermaßen konfrontiert wie Forschungsprojekte, bei denen digitale Daten generiert, computergestützt ausgewertet und in Forschungsdatenbanken zugänglich gemacht werden. Dabei liegt das Hauptaugenmerk nicht nur auf der Langzeitsicherung und Archivierung; die größeren Herausforderungen stellen sich vor allem bei den Aktualisierungen der Datenmodelle, Analysemethoden und Präsentationsformen, um die neuen Werkzeuge der Digital Humanities bestmöglich einsetzen zu können. Die Überlegungen im Vorfeld derartiger Relauncharbeiten bewegen sich erfahrungsgemäß zwischen vorsichtiger Adaptierung und radikalem Umbau der vorliegenden Datenarchitektur. Dass dabei alle vorhandenen Informationen verlustfrei übertragen werden sollen, versteht sich von selbst. Der folgende Beitrag möchte die Transformation von Daten eines Langzeitprojekts in eine neue Datenarchitektur für eine Bilddatenbank vorstellen, bei der eine Graphdatenbank zum Einsatz kommt:
         Am Institut für Realienkunde des Mittelalters und der frühen Neuzeit (IMAREAL), einem interdisziplinär ausgerichteten Forschungsinstitut, das Teil der Universität Salzburg ist, wird die materielle Kultur des Mittelalters und der frühen Neuzeit untersucht. Bildquellen bilden dabei neben Schriftquellen und überlieferten Objekten die Grundlagen der Analysen. Mit dem Aufbau der Bilddatenbank REALonline wurde am IMAREAL in den 1970ern auf der Grundlage der von Manfred Thaller speziell für die Anforderungen der historischen Grundwissenschaften entwickelten Datenbanksysteme begonnen – zunächst 
                Descriptor und in weiterer Folge 
                
               Κλειώ
             (Thaller 1980 u. 1989). Der Datenbestand von REALonline wurde seither und wird weiterhin kontinuierlich erweitert, damit dargestellte Dinge und ihre Kontexte erforscht werden können. Die Datenbank ist seit 2002 unter 
                http://tethys.imareal.sbg.ac.at/realonline online verfügbar (Matschinegg 2004). Anhand der Datenbank ist es möglich, die Bedeutung und Funktion der materiellen Kultur im Bilddiskurs zu untersuchen: Welche Objekte waren zu welchen Zeiten in welchen Gesellschaften und Kontexten als visuelle „Requisiten“ gegenwärtig oder vor- und damit auch darstellbar? Wie wurden Dinge im Bild verhandelt und welche Rolle nehmen sie innerhalb von ins Bild überführten Narrativen ein? 
            
         Um diese Fragen beantworten zu können, wurde am IMAREAL entschieden, neben den Metadaten zum Werk bzw. Bildträger systematisch 
                alle im Bild dargestellten Elemente auszuzeichnen (Abb. 1). Im Gegensatz zu anderen Bilddatenbanken wird das Dargestellte nicht nur mit einigen wenigen Schlagwörtern erfasst. Diesem Umstand ist es zu verdanken, dass die in REALonline erhobenen Daten sowohl im Rahmen von interdisziplinären Forschungen zur materiellen Kultur ausgewertet werden können, als auch in unterschiedlichen geisteswissenschaftlichen Untersuchungskontexten und für Kulturerbedokumentationen eine wertvolle Ressource darstellen.
            
         
            
         
         Abb. 1: Erfassungsschema der Metadaten in REALonline
         Im Modell für die Erfassung der dargestellten Entitäten im Bild werden folgende Informationen erhoben: Für Subjekte werden neben dem Subjektnamen die Kategorien Geschlecht, Beruf bzw. Stand und Gestik erfasst. Bei Objekten werden der Objektname und die Informationen zu Farbe, Material und Form erhoben. Weiters wird die Struktur dieser Metadaten zu den Bildinhalten festgehalten und kann damit im Rahmen von Analysen abfragbar gemacht werden: Direkte Subjekt-Objekt- bzw. Objekt-Objekt-Relationen werden erfasst, um am Körper getragene bzw. von Figuren gehaltene Objekte zu dokumentieren oder einen Bezug zwischen einzelnen dargestellten Dingen (etwa ein auf einem Tisch stehender Krug) in den Daten abbilden zu können. Darüber hinaus können sowohl Körperteile als auch Teile von Objekten als Metadaten zum Dargestellten gespeichert werden (Jaritz 1993: 23-43).
         Graphdatenbanken eignen sich u.a. besonders dafür, die vielfältigen Beziehungen zwischen Personen oder Personen und Gegenständen bzw. Ereignissen sowie auch zwischen den Dingen untereinander möglichst flexibel abzubilden und sind nun auch in der historischen Forschung im Kommen; als Software wird oft Neo4j eingesetzt (Raspe 2014, Kaufmann & Andrews 2015, Kuczera 2015). Die verzweigte Struktur der erfassten Metadaten in REALonline ist einer der Hauptgründe, warum für die neue Datenarchitektur ein property-graph-Modell gewählt wurde. Ein weiterer Leitgedanke war, dass das Beziehungsnetz von Subjekten, Objekten und Handlungen in mittelalterlichen und frühneuzeitlichen Bildern anhand des Modells eines verzweigten Graphen besser veranschaulicht werden kann als in einer langen Liste mit Metadaten. Die Graphdatenbank bietet im Fall von REALonline sowohl bei der Präsentation für die Nutzer_innen im Frontend, für die Abfrage und Darstellung der Abfrageergebnisse als auch für die Eingabe der Daten im Backend (siehe Abb. 2) eine Verbesserung der Usability gegenüber dem zuvor verwendeten hierarchischen Datenbankmodell.
         
            
         
         Abb. 2: Screenshot des Graphen zum Dargestellten im Bild (Backend)
         In unserem Projekt hat sich die Kombination von Neo4j für die Modellierung und Abfrage der „beziehungsrelevanten“ Daten mit einer NoSQL-Mongo-Dokumentendatenbank angeboten (Abb. 3). Diese Lösung baut einerseits auf dem in der Praxis bereits bewährten softwareseitigen Ineinandergreifen bei der semantischen Transformation der Informationen auf und bietet gleichzeitig die Möglichkeit zur Speicherung und Abfrage von werkgeschichtlich wie auch projektgeschichtlich relevanten Informationen zu den einzelnen Bilddokumenten, die im Verlauf dieses Langzeitprojektes erhoben wurden und laufend erweiterbar bleiben sollen.
         
            
         
         Abb. 3: Datenarchitektur von REALonline
         Im Vortrag möchten wir aber auch auf die Herausforderungen hinweisen, denen wir uns im Zuge des Entwurfs der Datenarchitektur von REALonline stellen mussten: So war etwa in der Struktur des hierarchischen Datenmodells die Information zur dargestellten Handlung auf derselben Ebene angesiedelt wie die Entitäten Subjekt und Objekt. Beim Datenexport aus der bis dato verwendeten 
                
               Κλειώ
            -Datenbank und dem Import in Neo4j konnte – nachdem in diesem Fall keine automatisierten Zuweisungen der Handlung zu Personen bzw. Objekten möglich waren – dieser Umstand nur in das neue Datenmodell mitübernommen werden. Mit der Entscheidung für eine Graphdatenbank ist dennoch gewährleistet, dass in einem weiteren Schritt Informationen, wie jene zur dargestellten Handlung, statt in Knoten in die Kanten des Graphen gelegt werden können und damit die Struktur von RDF-Triples (Subjekt-Prädikat-Objekt) bekommen. 
            
         Aufgrund der zeitintensiven Datenerhebung war ein wichtiger Aspekt des Relaunchs, die Dateneingabe so effizient wie möglich zu gestalten. Der Beitrag wird die gefundene Lösung präsentieren. Langfristig gesehen sollte versucht werden, den Zeitaufwand für die Erhebung von Metadaten zu den auf historischen Bildern dargestellten Elementen zu minimieren. Daher möchten wir die in REALonline während mehr als 40 Jahren erhobenen Informationen als Trainingsdaten in transdisziplinäre Projekte zwischen den Geisteswissenschaften und der Computer Vision – insbesondere zur (semi-)automatisierten Bilderkennung – einbringen, so dafür Fördermittel eingeworben werden können.
         Mit dem Relaunch von REALonline kann die Menge der erhobenen Metadaten zum im Bild Dargestellten (aktuell sind innerhalb von 23316 Datensätzen 1.165562 Begriffe dazu erfasst) besser zugänglich gemacht werden: Abfragen der Graphdatenbank und Visualisierungen (z.B. mit Software wie 
                gephi–The Open Graph Viz Platform oder 
                yEd graph editor) dieser Ergebnisse können komplexe Zusammenhänge innerhalb der Bilddetails aufdecken oder Aufschlüsse zu Mustern sowie „Ausreißern“ in unterschiedlichen Samples liefern, die nicht nur als Resultate statistischer Auswertungen verstanden werden sollen, sondern vor allem dazu dienen können, neue Fragen in der (interdisziplinären) Forschung anzustoßen. Beispielsweise wurde in 154 Datensätzen das Bildthema „Geißelung Christi“ erfasst. Bei der Erschließung dieser Datensätze wurden wiederum 516 Objekte verzeichnet, die von Figuren im Bild in der Hand gehalten werden. Die Visualisierung (Abb. 4) beschränkt sich auf jene Objekte, die nur einmal vorkommen (gelb) und die ihnen übergeordneten Thesauruskategorien (grün, dunkelrot). Während die meisten Objekte dem gängigen Narrativ „Geißelung“ zugeordnet werden können, sind die Objekte 
                Münze und 
                Geldbeutel nur über einen Konnex zum mittelalterlichen Drama erklärbar (Nicka 2014, 280–282): Die Darstellung einer Bezahlung der Geißler Christi, die nur auf einem Flügelaltar im niederösterreichischen Pöggstall im Bild festgehalten wurde, kennen wir ansonsten nur aus Passionsspielen, wo jüdische Protagonisten negativ gekennzeichnet werden, indem sie den Gerichtsknechten Geld geben, um besonders fest mit den Ruten zuzuschlagen (siehe auch Abb. 2). 
            
         
            
         
         Abb. 4: Visualisierung der dargestellten Objektbegriffe und Körperbezeichnungen aus den Bildbeschreibungen in ihrer Zuordnung zur jeweiligen Thesauruskategorie
         Abschließend bleibt zu erwähnen, dass sich mit der Notwendigkeit, eine gut eingeführte, aber in ihren technischen Funktionalitäten nicht mehr zeitgemäße Bilddatenbank zu modernisieren, auch die Chance zur besseren Nutzung der umfangreichen Datenbestände verbinden lässt. Im Zuge der Relaunchvorarbeiten haben wir die Konzepte und Lösungsansätze aufgegriffen, die gegenwärtig in den Digital Humanities diskutiert und getestet werden. Wir haben die Umsetzung in enger Zusammenarbeit mit den Grazer Entwicklerfirmen complement.at und zedlacher.net realisiert. Der Beitrag gibt einen knappen Überblick über die wichtigsten Entscheidungsfindungsprozesse sowie die Schwierigkeiten und Potentiale, die bei der Überführung in die neue Datenarchitektur und die gewählte Frontend-Lösung entstanden sind. Der Aspekt der Nachhaltigkeit hat dabei von Anfang an eine große Rolle gespielt; sowohl bei der Erhaltung aller vorhandenen Informationen als auch bei der nachhaltigen Nutzbarkeit der erhobenen Daten. So ist die Zitierbarkeit der Daten über einen PID (persistent identifier) mit einem handle gewährleistet und die Metadaten werden mit einer 
                Creative Commons by-nc-sa 4.0-Lizenz zur Verfügung gestellt. Die neue Online-Version von REALonline wird gegenwärtig getestet und optimiert und 2017 freigeschaltet.
            
      
      
         
            
               Bibliographie
               
                  Jaritz, Gerhard (1993): 
                        Images: A Primer of Computer-Supported Research with Κλειώ IAS. Halbgraue Reihe zur historischen Fachinformatik A 22. 
                        St. Katharinen: Scripta Mercaturae Verlag. 
                    
               
                  Kaufmann, Sascha / Andrews, Tara Lee (2016): 
                        „Bearbeitung und Annotation historischer Texte mittels Graph-Datenbanken am Beispiel der Chronik des Matthias von Edessa“,
                        in:
                        DHd 2016: Modellierung - Vernetzung - Visualisierung 176–178
                        http://dhd2016.de/boa.pdf [letzter Zugriff 20. August 2016].
                    
               
                  Kuczera, Andreas (2015): 
                        „Graphdatenbanken für Historiker. Netzwerke in den Registern der Regesten Kaiser Friedrichs III. mit neo4j und Gephi“, 
                        in: 
                        Mittelalter. Interdisziplinäre Forschung und Rezeptionsgeschichte, 5. Mai 2015, 
                        http://mittelalter.hypotheses.org/5995 (ISSN 2197-6120) [letzter Zugriff 20. August 2016].
                    
               
                  Matschinegg, Ingrid (2004): 
                        „REALonline – IMAREAL's Digital Image-Server“, 
                        in: 
                        [Enter the Past]. The E-way into the Four Dimensions of Cultural Heritage. CAA 2003 | Computer Applications and Quantitative Methods in Archaeology | Proceedings of the 31st Conference, Vienna, Austria, April 2003 (BAR International Series 1227). 
                        Oxford: archaeopress, 214-216.
                    
               
                  Nicka, Isabella (2014): 
                        „Interfaces. Berührungszonen von Transzendenz und Immanenz im spätmittelalterlichen Sakralraum“, 
                        in: Meyer, Marion / Klimburg-Salter, Deborah (eds.):
                        Visualisierungen von Kult. 
                        Wien / Köln / Weimar: Böhlau, 260-293, Abb. auf 438-444.
                    
               
                  Nicka, Isabella (im Erscheinen): 
                        „REALonline–Explore and Find Out. Wohin führt das Digitale die Kunstgeschichte?“,
                        Beitrag zum Tagungsband der vom 6.-8. Nov. 2015 in Wien abgehaltenen Konferenz „Newest Art History“. Wohin geht die jüngste Kunstgeschichte?
               
               
                  Raspe, Martin (2014): 
                        Zuccaro. Ein modernes, konfigurierbares Informationssystem für die Geisteswissenschaften.
                        http://zuccaro.biblhertz.it/dokumentation/zuccaro [letzter Zugriff 20. August 2016].
                    
               
                  Thaller, Manfred (1980): 
                        „Descriptor: Probleme der Entwicklung eines Programmsystems zur computerunterstützten Auswertung mittelalterlicher Bildquellen“, 
                        in: 
                        Europäische Sachkultur des Mittelalters: Gedenkschrift aus Anlaß des 10jährigen Bestehens des Instituts für mittelalterliche Realienkunde Österreichs (Veröffentlichungen des Instituts für mittelalterliche Realienkunde Österreichs 4 / Sitzungsberichte der Akademie der Wissenschaften, Phil.-Hist. Klasse 374). 
                        Wien: Verlag der Österreichischen Akademie der Wissenschaften, 167–194.
                    
               
                  Thaller, Manfred (1989): 
                        Κλειώ.
                        Ein Datenbanksysten. Halbgraue Reihe zur historischen Fachinformatik B1.
                        St. Katharinen: Scripta Mercaturae Verlag.
                    
            
         
      
   



      
         
            Einleitung
            Im vorliegenden Abstract stellen wir eine Methode sowie erste Ergebnisse der Analyse von Entitäten-Assoziationen realer Leserinnen und Leser vor.
            Literaturwissenschaftliche Rezeptions-, Lese- und Lesertheorien gehen seit ihren hermeneutischen und wirkungsästhetischen Anfängen (Schleiermacher 1838, insb. 309f.; Iser 1976) von professionellen (Dijkstra 1994), informierten (Fish 1970, 86), Modell- (Eco 1979) oder sogar idealen (Schmid 2005) Lesern aus (vgl. Willand, 2014). Diesen wird die Kompetenz zugeschrieben, idealerweise sämtliche Textmerkmale referentialisieren zu können, wobei je nach literaturtheoretischer Provenienz unterschiedliche Kontexte die Grundlage der Zuschreibungen an den Text bilden. Dazu gehören u.a. Informationen über den Autor oder über die sozialhistorischen Bedingungen der Textproduktion, über die Rezeptionsbedingungen, über Vorgänger- oder zeitgenössische Texte oder über Wissen aus dem Bereich der Literaturwissenschaftlerin bzw. des Lesers selbst. 
            An bestimmte Wissensbestände dieser 
                    realen Leserinnen und Leser literarischer Texte können wir uns durch eine computergestützte empirische Analyse von Rezeptionszeugnissen aus sozialen Medien annähern. Konkret ist unser Ziel die Rekonstruktion und Analyse der von literarischen Texten ausgelösten Assoziationen. Dabei beschränken wir uns auf die Assoziationen, die reale oder fiktive Entitäten betreffen, also etwa Personen des öffentlichen Lebens oder Figuren aus fiktionalen Werken.
                
            Die Plattform Goodreads bietet Leserinnen und Lesern die Möglichkeit des freien schriftlichen Austauschs über literarische Texte in einer großen Community. 55 Mio. Mitglieder haben bis 2017 über 50 Mio. Reviews geschrieben, wobei die Besprechungen die Inhalte der Bücher selbst und nicht - wie etwa bei Verkaufsplattformen wie Amazon - die Distribution, den Preis o.ä. fokussieren (Piper et al. 2015). 
         
         
            Verarbeitung
            Als Grundlage unserer Analysen  wurden die Reviews in einer lokalen Datenbank gespeichert. 
            Die Datenbank enthält 1,3 Millionen englischsprachige Reviews zu 5.481 besprochenen Texten. Die Reviews umfassen insgesamt etwa 150 Mio. Tokens, d.h. uns steht eine große Datenmenge zur Extraktion der Entitäten zur Verfügung. In einem ersten Schritten wurden die Reviews bereinigt: HTML-Tags wurden entfernt und Wiederholungen von mehr als dreimal dem gleichen Zeichen oder Wort auf drei reduziert. 
            Zur Extraktion der Entitäten aus den Reviews haben wir den Stanford Named Entity Recognizer (Finkel et al., 2005) verwendet. Der Tagger klassifiziert die gefundenen Entitäten in mehrere Klassen. Für uns ist die Klasse „PERSON“ relevant, da diese alle gefundenen Entitäten von Personen enthält.
            Im nächsten Schritt disambiguieren wir die extrahierten Entitäten, da z.B. ein Name wie “Harry” auf viele mögliche Träger des Namens verweisen kann. Mit Hilfe von UKB (Agirre et al., 2009) und UKB-wiki (Agirre et al., 2015) können den Entitäten Wikipedia-Seiten zugeordnet werden, welche die möglichen Entitäten repräsentieren. Für diese Disambiguierung verwendet UKB den PageRank-Algorithmus (Page et al. 1999), der Dokumente nach ihrem Verlinkungsgrad bewertet. Sobald Namen wie „Ron“ und „Dumbledore“ im selben Kontext erwähnt werden, wird die Wahrscheinlichkeit größer, dass mit “Harry” 
                    Harry Potter, mit “Ron” 
                    Ron Weasly und mit “Dumbledore” 
                    Albus Dumbledore aus der Romanreihe 
                    Harry Potter gemeint sind, weil diese Entitäten aus dem selben Kontext kommen und dies in der Wissensbasis Wikipedia durch Verlinkungen explizit ablesbar und quantifizierbar ist.
                
            UKB-wiki stellt einen herunterladbaren Graphen zur Verfügung, der Wikipedia-Seiten und Links auf andere Wikipedia-Seiten repräsentiert. In einem mitgelieferten Wörterbuch sind Entitäten mit allen möglichen Entitäten (Verweise auf Wikipedia Seiten) aufgeführt. 
            Die auf diese Weise gewonnen Wikipedia-Einträge wurden anschließend hinsichtlich des ontologischen Status der referenzialisierten Entität kategorisiert, also ob es sich um eine reale Person oder fiktionale Figur handelt. Dazu wurde die Wissensbasis DBpedia
                     verwendet, die die Daten aus Wikipedia strukturiert und maschinenlesbar kodiert. Da die Disambiguierung Wikipedia-Einträge liefert, können wir anhand dieser die auf den zugehörigen DBpedia-Eintrag zugreifen. Über DBpedia lassen sich neben ontologischen Kategorien  auch andere Eigenschaften extrahieren, die für eine Analyse ggf. interessant sind, etwa Geschlecht oder Relationen zu anderen Figuren.
                
            Die extrahierten Daten werden zunächst als Tabelle gespeichert und erlauben somit eine flexible weitergehende Verarbeitung, etwa in einem Netzwerk. Eine Zeile der Tabelle besteht aus dem Werktitel, der disambiguierten Entität (Verweis auf Wikipedia Seite), einer Liste der extrahierten Entitäten aus den Reviews, einer Liste von Review-IDs, um nachvollziehen zu können in welchen Reviews der Name erwähnt wird, der Anzahl der Erwähnungen und der Angabe ob es sich um eine Figur handelt oder nicht.
            
               
                  Titel
                  Disambiguierte Entität (Verweis auf Wikipedia Seite)
                  Extrahierte Entität
                  Review IDs
                  Anzahl der Erwähnungen
                  Handelt es sich um eine fiktionale Figur?
               
               
                  The Hound of the Baskervilles
                  Agatha_
                            Christie
                  christie, agatha_
                            christie, agatha_
                            christy
                  4707841
                            20, …, 1886
                            08568
                  20
                  False
               
               
                  The Hound of the Baskervilles
                  Spock
                  spock
                  429714
                            73
                  1
                  True
               
               
                  The Hound of the Baskervilles
                  Robert_Downey,_Jr.
                  robert_downey_jr, robert_downey
                  107754
                            3609, …, 125
                            0976986
                  18
                  False
               
               
                  The Hound of the Baskervilles
                  Ann_Radcliffe
                  ann_radcliffe
                  435380
                            655
                  1
                  False
               
            
            
               Tabelle 1: Auszug aus den extrahierten Daten. Die extrahierten Entitäten stammen aus den Reviews zu
                     The Hound of the Baskervilles.
                
            
               Zwischenergebnisse
               Um ein exemplarisches Resultat zu präsentieren, haben wir Reviews zu “The Hound of the Baskervilles” (deutsch: “Der Hund von Baskerville”) analysiert. Unter den häufig erwähnten Entitäten finden sich erwartungsgemäß Sherlock Holmes, Dr. Watson, sowie der Autor Arthur Conan Doyle. Weitere häufig erwähnte Figuren aus der fiktionalen Welt des Sherlock Holmes' sind James Mortimer und Charles Baskerville. Aber auch Professor Moriarty wird häufig erwähnt, obwohl er in diesem Buch der Sherlock-Reihe gar nicht auftaucht. Das System erzeugt jedoch auch Fehler. Beispielsweise wird der Antagonist Stapleton zwar sehr oft erwähnt, da zu ihm aber kein eigener Wikipedia-Eintrag existiert, wird er fälschlicherweise mit dem Footballspieler Frank Stapleton verknüpft. Henry Baskerville, der Sohn von Charles und Erbe des Anwesens, wird im Buch fast durchgehend als Sir Henry bezeichnet, und kommt mit diesem Namen ebenfalls häufig in den Reviews vor. Da auch für ihn kein eigener Wikipedia-Eintrag existiert und der Name Henry extrem mehrdeutig ist, werden eine Reihe klar falscher Entitäten verknüpft: Henry II. von Frankreich; Henry County (Alabama); oder Henry I. von England.
               Bemerkenswert sind insbesondere jedoch die referenzialisierten extra-textuellen Entitäten, also diejenigen, die nicht aus der fiktionalen Welt Sherlocks stammen. Es finden sich etwa 
                        Hercule Poirot und 
                        Agatha Christie unter den erwähnten Entitäten, was als klares Zeichen dafür gesehen werden kann, dass die Leserinnen und Leser den Text vor dem Hintergrund eines starken Gattungsbewusstseins rezipieren. Dafür spricht auch, dass mit 
                        Benedict Cumberbatch, 
                        Robert Downey, Jr., 
                        John Barrymore und 
                        Jeremy Brett gerade die Schauspieler unter den assoziierten Referenzen vertreten sind, die in einer der vielen Verfilmungen die Rolle des Sherlock Holmes verkörpert haben.
                    
            
            
               Fehleranalyse
               Das am häufigsten auftretende Problem ist das Fehlen eines Wikipedia-Eintrages für eine Figur. In der englischsprachigen Wikipedia sind fiktionale Figuren zwar nicht per se davon ausgeschlossen -- Richtschnur hier ist deren “Notability”. Viele Figuren sind jedoch nur auf den Einträgen des entsprechenden Werks erwähnt. Da der Algorithmus nicht in der Lage ist, 
                        keinen Eintrag zu liefern, wird in solchen Fällen eben ein anderer Eintrag verwendet, auch wenn dieser relativ weit entfernt sein mag. Eine technische Lösung wäre sicher, nur ab einem gewissen Schwellwert eine Disambiguierung vorzunehmen, und die nicht-disambiguierten Einträge zumindest als solche erkennen zu lassen. Eine andere Möglichkeit läge in der (zusätzlichen) Verwendung von Literaturlexika, die (womöglich) eine größere Abdeckung zu fiktionalen Figuren aufweisen. Beide Optionen werden wir in zukünftigen Arbeiten genauer untersuchen. 
                    
               Da es sich bei den Reviews letztlich um Inhalte aus einem sozialen Medium handelt, kommt es auch vor, dass Namen falsch geschrieben werden oder gar der gesamte Text schriftsprachliche Konventionen übergeht. Prima vista sind diese Fälle im Vergleich zu Buchrezensionen zwar häufig anzutreffen, wir können das Problem aber umgehen, indem wir nur diejenigen Erwähnungen berücksichtigen, die mehr als einmal vorkommen. Festzuhalten bleibt aber ebenfalls, dass die Texte im Vergleich zu z.B. Twitter-Daten deutlich sauberer sind.
               Eine weitere mögliche (jedoch noch nicht tatsächlich beobachtete) Fehlerquelle liegt in der Natur des PageRank-Algorithmus: Wenn eine Figur in einem Werk existiert, ein Leser oder eine Leserin jedoch explizit z.B. eine Person des öffentlichen Lebens mit dem gleichen Namen erwähnt, wird der Algorithmus diese Erwähnung eher der Figur zuschlagen, da diese dichter mit anderen Figuren verknüpft ist. 
            
         
         
            Auswertung als Netzwerk
            Die oben extrahierten Daten erlauben Auswertungen auf vielfältige Weise. Exemplarisch konzentrieren wir uns hier auf eine Form, in der von Lesern zugeschriebene Gemeinsamkeiten zwischen literarischen Texten untersucht werden. Die Texte und die ihnen zugeschriebenen Assoziationen werden dabei als Knoten in einem Netzwerk repräsentiert. Ein Text ist also mit allen ihm zugeschriebenen Assoziationen verbunden, wobei das Gewicht der Kante die Anzahl der Reviews angibt, in denen eine bestimmte Assoziation auftaucht. 
            Durch diesen Aufbau ergeben sich Kerneigenschaften des Netzwerkes, die bei der Analyse zu beachten sind: Ein Teil der erwähnten Entitäten sind 
                    intratextuelle Referenzen, d.h. Figuren aus dem jeweiligen Text selbst (Veldhues, 1995). Auch wenn diese keine 
                    intertextuellen Assoziationen und damit nur sekundäres Extraktionsziel sind, behandeln wir sie als gleichwertige Assoziationen
                    . 
                
            Figuren, die in mehr als einem Werk auftauchen (z.B. 
                    Sherlock Holmes oder 
                    Harry Potter) bilden eine hoch gewichtete Verbindung zwischen den Texten einer literarischen Reihe, wobei Reihen durch die von ihnen geteilte fiktionale Welt markiert sind. Als gemeinsamer Assoziationsraum sind sie aufgrund der hohen Gewichtung auch angemessen im Netzwerk repräsentiert.
                
            Durch die gemeinsame Darstellung der Werke und assoziierten Entitäten ergeben sich -- bei Auswahl eines geeigneten Layout-Algorithmus z.B. in Gephi
                     -- eng zusammenhängende Gruppen von Werken. Das hier exemplarisch angeführte Resultat eines engen Zusammenhangs repräsentiert jedoch nicht bestimmte Texteigenschaften selbst, sondern lediglich von Leserinnen und Lesern gemeinsam gemachte Zuschreibungen an diese Texte.
                
            Das hier beschriebene Netzwerk wird im Zuge der Konferenz frei zugänglich gemacht.
            
               
                  
                  Abbildung 1: Assoziationen zu Conan Doyles The Hound of the Baskervilles, extrahiert aus den Reviews von Benutzern. Die Abbildung zeigt zur Illustration sämtliche assoziierte Entitäten, unabhängig von der Häufigkeit.
               
            
         
         
            Nächste Schritte
            Durch den Zugriff auf bisher undenkbar große Rezeptionsdatenmengen erhält die empirische Leseforschung einen sie fundamental erweiternden Impetus, war sie methodisch betrachtet bisher überwiegend auf Fragebögen
                     und peripheriephysiologische Messungen angewiesen, jüngst gestützt durch bildgebende Verfahren. Computerlinguistische Methoden der Sprach- und Korpusverarbeitung versprechen nicht nur die Analyse unlesbarer Mengen an Rezeptionszeugnissen, sondern auch eine Modellierung leserattribuierter Kontexte literarischer Texte und somit einen ersten Einblick in die bisher unbeantwortete Frage, mit welchem Vorwissen echte Leser eigentlich lesen.
                
            In diesem Sinne präsentiert das eingereichte Paper erste, jedoch bereits substanzielle Ergebnisse. 
            Die nächsten Schritte leiten sich direkt aus der oben diskutierten Fehleranalyse ab. Zum einen soll die Wissensbasis um fiktionale Figuren aus den Werken erweitert werden (was z.B. über 
                    named entity recognition über den Volltexten machbar wäre). Zum anderen soll der Algorithmus in die Lage versetzt werden bestimmte (fehlerhafte) Zuweisungen zurückzuweisen, etwa mit einem zu definierenden 
                    threshold.
                
         
      
      
         
            http://wiki.dbpedia.org/
            Das Filtern von innertextuellen Figuren ist technisch möglich (Beck, 2017), aber zeitaufwändig und für die hier vorgestellte Nutzung als Explorationswerkzeug letztlich unnötig.
            https://gephi.org
            Groeben 1979; Baurmann 1981; Funke 2003; Christmann u. Schreier 2003; Wübben 2009 u.v.m.
         
         
            
               Bibliographie
               
                  Agirre, Eneko / Soroa, Aitor (2009): “Personalizing PageRank for Word Sense Disambiguation”, in: Proceedings of the 12th conference of the European chapter of the Association for Computational Linguistics (EACL-2009). Athens, Greece.
                    
               
                  Agirre, Eneko / Barrena, Ander / Soroa, Aitor (2015): Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation. http://arxiv.org/abs/1503.01655
                    
               
                  Beck, Jens (2017): How do People Read Literature? - Detection and Identification of Names in Book Reviews. Bachelor’s thesis, Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart.
                    
               
                  Baurmann, Jürgen (1981). „Textrezeption empirisch. Wege zu einem ziel, behelfsbrücken oder holzwege?". Rezeptionspragmatik. Beiträge zur Praxis des Lesens. Uni-Taschenbücher. Band 1026. Hrsg. v. Gerhard Köpf, 201–218. München.
                    
               
                  Christmann, Ursula / Margrit Schreier (2003). „Kognitionspsychologie der Textverarbeitung und Konsequenzen für die Bedeutungskonstitution literarischer Texte". Regeln der Bedeutung. Zur Theorie der Bedeutung literarischer Texte. Revisionen. Hrsg. v. Fotis Jannidis, Gerhard Lauer, Matías Martínez & Simone Winko, 246–284. Berlin.
                    
               
                  Dijkstra, Katinka (1994): Leseentscheidung und Lektürewahl. Empirische Untersuchungen über Einflussfaktoren auf das Leseverhalten. Berlin.
                    
               
                  Dimitrov, Stefan / Zamal, Faiyaz / Piper, Andrew / Ruths, Derek (2015): “Goodreads vs Amazon: The Effect Of Decoupling Book Reviewing And Book Selling", in: International Conference on Web and Social Media (ICWSM-14).
                    
               
                  Eco, Umberto (1979): The Role of the Reader. Explorations in the Semiotics of Texts. Bloomington, IN.
                    
               
                  Finkel, Jenny Rose / Grenager, Trond / Manning, Christopher (2005): “Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling”, in: 
                        Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370.
                    
               
                  Fish, Stanley E. (1970): „Literature in the Reader: Affective Stylistics“, in: 
                        New Literary History 1(2): 123–162.
                    
               
                  Funke, Mandy (2003). „Das Abenteuer der Fragebögen. Aspekte zur empirischen Wirkungsforschung in der DDR". Wissenschaft und Systemveränderung. Rezeptionsforschung in Ost und West – Eine konvergente Entwicklung? Euphorion. Band 44. Hrsg. v. Wolfgang Adam, Holger Dainat & Gunther Schandera, 119–126. Heidelberg.
                    
               
                  Groeben, Norbert (1979). „Zur Relevanz empirischer Konkretisationserhebungen für die Literaturwissenschaft". Empirie in Literatur- und Kunstwissenschaft. Grundfragen der Literaturwissenschaft. Hrsg. v. Siegfried J. Schmidt, 43–82. München.
                    
               
                  Iser, Wolfgang (1976). Der Akt des Lesens. Theorie ästhetischer Wirkung. Band 636. München.
                    
               
                  Page, Lawrence / Brin, Sergey / Motwani, Rajeev / Winograd, Terry (1999): “The PageRank Citation Ranking: Bringing Order to the Web”
                        , technical Report. Stanford InfoLab.
                    
               
                  Schleiermacher, Friedrich (1838): Hermeneutik und Kritik mit besonderer Beziehung auf das Neue Testament. Aus Schleiermachers handschriftlichem Nachlasse und nachgeschriebenen Vorlesungen herausgegeben von Friedrich Lücke. In: Friedrich Schleiermacher’s sämmtliche Werke. Berlin: Reimer.
                    
               
                  Schmid, Wolf (2005): Elemente der Narratologie. Narratologia. Band 8. Berlin.
                    
               
                  Veldhues, Christoph (1995): "Gleich- und Gegenüberstellung".Intratextuelle und intertextuelle Bedeutung in der Literatur. Zeitschrift für französische Sprache und Literatur 40/3 (1995), 243-267.
                    
               
                  Willand, Marcus (2014): Lesermodelle und Lesertheorien. Historische und systematische Perspektiven. Narratologia. Band 41. Berlin.
                    
               
                  Wübben, Yvonne (2009). „Lesen als Mentalisieren? Neuere kognitionswissenschaftliche Ansätze in der Leseforschung". Literatur und Kognition. Bestandsaufnahmen und Perspektiven eines Arbeitsfeldes. Poetogenesis. Band 6. Hrsg. v. Martin Huber & Simone Winko, 29–44. Paderborn.
                    
            
         
      
   



      
         Dieser Posterbeitrag veranschaulicht die Interaktion zwischen computerlinguistischen Methoden und Regestenforschung. Es wird eine Anwendung vorgestellt, die bereits in einem graphbasierten Format vorliegendene Regesten webbasiert anzeigt und es erlaubt, Registereinträge im Text zu verorten. Die daraus entstandene Datenbasis hilft dabei neues Wissen zu generieren, so können z.B. Verwandtschaftsbeziehungen automatisch erkannt und in den Regesten-Graph integriert werden.
         Das im Rahmen des Bund-Länder-geförderten Akademienprogramms angesiedelte Grundlagenforschungsprojekt Regesta Imperii erstellt deutschsprachige Inhaltsangaben (sog. Regesten) von Kaiser-, Königs und Papsturkunden, begonnen von Karl dem Großen bis hin zu Kaiser Maximilian. Seit Projektbeginn 2001 wurden 1829 Regesten erstellt und digitalisiert und stehen inzwischen als Volltext im Internet zur Verfügung. Die Publikation der digitalisierten Register befindet sich gerade in Vorbereitung.
         Neben den Regesta Imperii sind immer mehr Editionen und Regestenwerke als Volltext im Internet zugänglich und können über Suchmasken abgefragt und genutzt werden. Die Nutzungsart unterscheidet sich zumeist aber nicht grundlegend von einer analogen Nutzung des Buches: Das Register wird aufgeschlagen und man kann anschließend die einem Registereintrag zugeordneten Urkunden oder Regesten aufrufen und lesen.
            
         
                Mit der Nutzung von Graphentechnologien in den digitalen Geisteswissenschaften werden neue Nutzungs- und Analyseformen der bereits vorhanden digitalen Editions- und Regestenwerke möglich. Die Digitale Akademie, Mainz (
                
               www.digitale-akademie.de
            ) hat auf ihrer Seite
                
               www.graphentechnologien.de
            einige beispielhafte Anwendungsszenarien für die Nutzung von Graphdatenbanken zur Erschließung von Onlineregesten vorgestellt. Für dieses Beispielprojekt wurden die Regesten Kaiser Friedrichs III. in eine Graphdatenbank konvertiert, anschließend das zugehörige Register digitalisiert und in die Graphdatenbank integriert. Im Graphenmodell ist es über die Abfrage nun möglich herauszufinden, in welchem Regest eine Person genannt wird und eine Analyse der gemeinsam mit ihr im Regest genannten Personen zu veranlassen (Abbildung 1). Das Graphenmodell erlaubt zudem die weitere Ergänzung von Kanten zwischen den Registerknoten. So ist es beispielsweise möglich, dass zwei Personenknoten, die Vater und Sohn darstellen, durch eine KIND-Kante ergänzt werden, um so deren Verwandtschaftsbeziehung explizit im Graphen zu repräsentieren. Mit solchen Zusatzinformationen kann das Register als Erschließungswerkzeug immer weiter wachsen.
            
         
            
               
               Abbildung 1 Graphbasierte Repräsentation von Regesten (gelb) mit zugehörigen Registereinträgen (rot,blau,grau).
                    
            
         
         Methoden aus der Computerlinguistik helfen diesen manuell sehr aufwendigen Grapherweiterungsschritt semi-automatisch durchzuführen. Dazu werden im ersten Schritt
                alle Regesten mit einer auf der Clarin-D Infrastruktur basierenden Sprachverarbeitungs-Pipeline (Malow 2012) maschinell verarbeitet: Auflösung von Abkürzungen, Tokenisierung, Part-of-Speech Tagging, Parsing und Entitätenerkennung. Im zweiten Schritt werden die Texte der einzelnen Regesten in einer interaktiven Webanwendung ausgewertet. Für jedes Regest werden alle Registereinträge (Personen, Orte, usw.), die während der Digitalisierung manuell mit Regesten verbunden wurden, angezeigt. Abbildung 2 stellt rechts den Regestentext und links alle verbundenen Registereinträge dar. In den Ausgangsdaten ist nicht gespeichert wie die Regeistereinträge im Text erwähnt werden, z.B. dass
                sich „der Stadt“ im Text auf den Eintrag „Aachen“ bezieht. Unsere Anwendung ermöglicht es hingegen jede im Text erkannte Entität mit den Registereinträgen per einfachem Mausklick zu verbinden. Mittels dieses Verfahrens konnten bereits ca. 4000 Registereinträge mit passenden Textstellen verbunden werden.
            
         
            
               
               Abbildung 2: links: Regesttext, erkannte Entitäten sind hervorgehoben (bereits neu verortete Textstellen zum Eintrag sind grün); rechts: verknüpfte Registereinträge zum Regest
                    
            
         
         
                Die neu geschaffene annotierte Datenmenge bildet somit einerseits eine wichtige Grundlage, für die Regestenforschung, da nun sehr einfach Wissen aus Texten mit den verknüpften Registereinträgen automatisch abgeleitet werden kann: z.B. die Vater-Sohn-Relation zwischen Colyn Beisse und Johann Beissel (Abbildung 3).
            
         
                Andererseits bietet dieser Datensatz für die computerlinguistische Forschung weitere Anknüpfungspunkte. Mittels Distant Supervision (Blessing 2012) können aus den verknüpften und im Text verankerten Relationen zu den Registereinträgen neue Modelle trainiert werden, die wiederum auf nicht manuell annotierten Textstellen der Regesten Anwendung finden. Durch diesen iterativen Ansatz können sukzessive große
                Regestensammlungen mit immer neuem Wissen angereichert werden und somit eine Grundlage für neue Analyseformen bieten.
            
         
            
               
               Abbildung 3: Depedenzanlayse, die zeigt wie Verwandtschaftsrelationen direkt aus der Analyse extrahiert werden können. In diesem Beispiel ist Colyn Beissel der Sohn von Johann Beissel.
                    
            
         
      
      
         
            
               Bibliographie
               
                  Blessing, Andre / Schütze, Hinrich 
                        (2012) Crosslingual Distant Supervision for Extracting Relations of Different Complexity.
                        In Proceedings of the 21st ACM International Conference on Information and Knowledge Management
               
               
                  Kuczera, Andreas
                        (2017) Herrscherhandeln in den Regesta Imperii. Beispielprojekt an den Regesten Kaiser Friedrichs III. URL:
                        
                        (abgerufen am 14.09.2017)
                    
               
                  Kuczera, Andreas 
                        (2016): Graphdatenbanken für Historiker. Netzwerke in den Registern der Regesten Kaiser Friedrichs III. mit neo4j und Gephi, in: Mittelalter. Interdisziplinäre Forschung und Rezeptionsgeschichte, 05.05.2015. URL:
                        
                        
                  
                     http://mittelalter.hypotheses.org/5995
                  .
                    
               
                  Mahlow, Cerstin / Eckart, Kerstin / Stegmann, Jens / Blessing, Andre / Thiele, Gregor / Gärtner, Markus / Kuhn, Jonas
                        (2014) Resources, Tools, and Applications at the CLARIN Center Stuttgart in Proceedings of the 12th Konferenz zur Verarbeitung natürlicher Sprache  11-21
                    
            
         
      
   



      
         
            Introduction and Motivation
            The emergence of computational methods of text processing has created new paradigms of research in literary studies in recent years (Jockers & Underwood, 2016), for instance 
                    distant reading to find patterns and regularities (Moretti, 2005). Network analysis and extraction of information about relations between characters from literary texts is an example for distant reading methods. Such information can not only be helpful for better understanding of character interactions but can also facilitate the comparison of thereof in different texts.
                
            Existing tools of text analysis and network visualization such as Voyant
                     or Gephi
                     are either missing modules for character network analysis or require preliminary steps on data preprocessing from the user and therefore are not easy-to-use for some humanities scholars who lack programming skills. Interactive tools in addition often lack features to ensure reproducibility of results.
                
            We present our ongoing effort on closing this gap by developing a literary analysis reporting tool 
                    rCAT
               
                  
               , whose primary purpose is to provide an easy-to-use, stable, and reusable solution for automatic extraction of relational information from text and to characterize these relationships automatically to provide the user with deeper qualitative insight. We opt for implementation as a web-based reporting tool instead of an interactive tool for two reasons: (1) automatically generated reports in PDF format can serve as a stable foundation for discussion and can be reused in publications and visualizations easily, and (2) the results are clearly connected to the chosen input parameters such that reproducibility of results is ensured.
                
            As a use-case study, we apply 
                    rCAT to Johann Wolfgang von Goethe's epistolary novel 
                    Die Leiden des jungen Werthers. On the basis of this epistolary novel, we show that not only the network can be generated, but also the characteristic triangular relationship of the protagonists is easily identified. The goal is to automatically determine this triad in the original text and in the adaptations that have been published since the publication of 
                    Werther in 1774.
                
         
         
            
               Previous Work
                
            Previous research on social networks in literary fiction generally fall into one of the two categories: (1) works that explore methods for extracting and formalizing character networks (
                    cf., Elson et al. (2010), Agarwal et al. (2012, 2013), Park et al. (2012)), and (2) works that primarily focus on qualitative implications of network analysis (
                    cf., Rydberg-Cox (2011), Moretti (2011), Nalisnick & Baird (2013), Jayannavar et al. (2015)). It is common to address both tasks at the same time, as in Beveridge & Shan (2016), who introduce a number of formal measures for analyzing the centrality of the characters in 
                    Game of Thrones books, which results in both expected and surprising findings. 
                
            Building on graph theory extensively elaborated in the past fifty years (e.g., Bondy and Murty, 1976 or West, 2001), our work is similar to Beveridge & Shan (2016), in particular, in terms of the weighted degree measure, and to Park et al. (2012), in terms of distance measure for detecting closely related characters in a text.
         
         
            
               Methods
                
            In the following, we explain the different components in 
                    rCAT, which are available for text analysis. After that, we discuss the results based on a use-case study.
                    
            
            
               Character lists and character identification
               To detect character mentions in the text we use a fundamental named-entity recognition approach based on dictionaries. This approach is suitable for scholars who analyze texts they already know. Consequently, we opt for a transparent and simple character recognition procedure: The user provides a list of character names to be included in the analysis specifying a canonical name form and all variations thereof she would like to take into account (
                         e.g., “Lotte” is the canonical name and “Lotten”, “Lottens”, “Lottgen”, “Lottchen”, “Charlotten S.”. are its variants).
                     
            
            
               Relation detection and context words
               We define the closeness of relationship between two characters using a 
                    distance measure
                  dist
                  X
                  (p,q), where 
                    p and 
                    q are the strings corresponding to these characters and 
                    X is the number of tokens between them (Park et al., 2012). In addition, we introduce the 
                    context measure
                  cont
                  Y
                  (p,q), where 
                    p and 
                    q are the strings corresponding to these characters and 
                    Y is the number of tokens before the character 
                    p and after the character 
                    q. While the former measure allows for detecting those characters that are closely related to each other, the latter one enables a contextual analysis of their relationship.
                
            
            
               Network analysis
               We visualize the network of characters with an undirected graph 
                    G=(V,E), where 
                    V are the vertices, each vertex corresponding to one character, and each edge 
                    E=(V
                  i,
                  ,V
                  j
                  ) corresponding to relations between pairs of characters. We output the following measures for each character node: 
                    degree, 
                    edge weight, 
                    weighted degree and 
                    density. The degree is the number of edges occurring with a given vertex. The edge weight, 
                    w
                  i,j
                   ≥ 0, is defined as the number of interactions between the vertices V
                    i and V
                    j. The weighted degree is the sum of weights of the edges occurring with a vertex 
                    i. Density is the ratio of occurring edges between two vertices and all possible vertex pairs.
                    
            
            
               Word clouds
               Word clouds are an approach to visualize the vocabulary of a text. The size of one word corresponds to its frequency. We use two different kinds of word clouds: For each character in the character list, we show word clouds based on the context of a window size 
                        n. For each pair of characters occurring in the network, we present a word cloud based on the words between them as well as on the words found in the context. Both types of word clouds can be filtered to the specific word fields (words from specific domains) which is helpful in gaining a focused insight into the characters relations.
                    
            
            
               Word Field developments
               We plot the timeline of multiple predefined world fields (specified by word lists) in the text. This feature is helpful in representing how certain fields (
                      e.g., concepts, emotions) develop throughout the narrative (Kim et al., 2017).
                   
            
            
               Implementation
               The tool was developed using Python v.3.6 and the Flask
                     web development framework. The tool outputs a single PDF report. The resulting document contains information from the analysis modules described in the previous section. Network graphs included in the report are generated with 
                    graphviz. Additionally, the tool can generate a CSV file that can be used as input to Gephi. 
                    
            
         
         
            
               Use-case Demonstration
                
            For a use-case analysis, we apply 
                    rCAT to 
                    Die Leiden des jungen Werther by Johann Wolfgang Goethe with the following parameters: X=8, Y=5, stop words removed (previous work focused on this analysis without rCAT, 
                    cf. Murr, 2017).
                
            In Goethe's epistolary novel, the protagonist Werther describes his unhappy love for Lotte, who is engaged to Albert. The characteristic triangular relationship in the novel arises from this constellation (protagonist - beloved woman - antagonist). With 
                    rCAT we expect to identify and characterize this relationship. Figures 1 and 2 show a sample network analysis output (tables are shown only partly).
                
            The protagonist Werther shows a degree of 21, which is the number of characters with whom he interacts. The closest relationship measured by edge weight (Figure 2) is observed between Werther and Lotte (81 interactions). The antagonist Albert has a low degree of 3. However, his weighted degree is 36 (third highest after Werther and Lotte), which confirms his important role in the triangular relationship.
            
               
                  
                  Illustration 1: Degrees and weighted degrees for most important characters of Goethe’s Werther
               
            
            
               
                  
                  Illustration 2: Edge weights
               
            
            
               
                  
                  Illustration 3: Complete network of Goethe’s Werther
               
            
            Highlighted in red is the typical triangular relationship in Goethe’s novel, which corresponds to the three highest weighted degrees. In further steps, we will use 
                    rCAT to analyze the adaptations of Goethe's novel with a focus on this triad.
                
            To better characterize the edges, the tool outputs top-
                    n word clouds sorted by edge weight (
                    n is specified by the user) for character pairs and by degree for single characters. Figure 4 and 5 show examples of the word clouds for character pairs filtered to the words from the emotion domain.
                
            
               
                  
                  Illustration 4: Word clouds for Werther-Lotte
               
               
                  
                  Illustration 5: Werther-Albert
               
            
            The word clouds enable first conclusions about the relationships of the characters. Werther and Lotte's word cloud characterizes their ambivalent relationship. The key words "Leidenschaft" and "Freude" reflect Werther's love, whereas the mentions of "sterben" and "Verblendung" are characteristic of the unrequited love, which leads Werther into his "disease unto death". As Werther and Albert’s word cloud reveals, their relationship is dominated by the "Unruhe" that Werther feels through his adversary. 
            Additionally, the tool plots the development of the narrative (not bound to specific characters) based on the word fields, an example of which is shown on Figure 6. In this case we used words from the emotion domain (with emotion dictionaries by Klinger et al. (2016)).
            
               
                  
                  Illustration 6: Word field development for Goethe’s Werther
               
            
            The word field development can highlight the prevalence of individual emotion domains across the text. The accumulation of the negative emotion words (Wut,Trauer, Furcht) towards the end suggests, for example, that Goethe’s novel has no “happy ending”. The striking rash on “Freude”, however, captures the last happy hours Werther spends with Lotte in the second part of the narration before he kills himself.
            
               
                  Future Work
                    
               The next version of the tool will include a character-oriented word field development calculated and plotted for the main characters of the stories. In addition, future releases will include more analysis features and bulk file processing.
            
         
      
      
         
            
               
            
            
               
            
            
               
                  www.ims.uni-stuttgart.de/data/rcat
               
            
            
               
            
         
         
            
               Bibliographie
               
                  Agarwal, A. / Corvalan, A. / Jensen, J. / Rambow, O. (2012): “Social Network Analysis of Alice in Wonderland”, in: CLfL@ NAACL-HLT 88-96.
               
                  Agarwal, A. / Kotalwar, A. / Rambow, O. (2013): “Automatic Extraction of Social Networks from Literary Text. A Case Study on Alice in Wonderland”, in: IJCNLP 1202-1208.
               
                  Beveridge, A. / Shan, J., (2016): “Network of thrones”, in: Math Horizons, 23(4): 18-22.
               
                  Bondy, J.A. / Murty, U.S.R. (1976): Graph theory with applications (Vol. 290). London: Macmillan.
               
                  Burrows, J.F. (1987): “Word-patterns and story-shapes: The statistical analysis of narrative style”, in: Literary & Linguistic Computing, 2(2): 61-70.
               
                  Elson, D.K. / Dames, N. / McKeown, K.R. (2010): “Extracting social networks from literary fiction”, in: Proceedings of the 48th annual meeting of the association for computational linguistics 138-147. Association for Computational Linguistics.
               
                  Heuser, R., F. Moretti / E. Steiner (2016): The Emotions of London. Technical report. Stanford University. Pamphlets of the Stanford Literary Lab.
               
                  Jayannavar, P. / Agarwal, A. / Ju, M. and Rambow, O. (2015): “Validating Literary Theories Using Automatic Social Network Extraction”, in CLfL@ NAACL-HLT 32-41.
               
                  Jockers, M.L. / Underwood, T. (2016): “Text‐Mining the Humanities”, in: Schreibman, Susan / Siemens, Ray / Unsworth, John (eds.): A New Companion to Digital Humanities 291-306.
               
                  Kim, E. / Padó, S. / Klinger, R. (2017): “Investigating the Relationship between Literary Genres and Emotional Plot Development”, in: Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature 17-26.
               
                  Klinger, R. / Sulliya S.S. / Reiter N. (2016): “Automatic Emotion Detection for Quantitative Literary Studies -- A Case Study on Kafka's ‘Das Schloss’ and ‘Amerika’”, in: Digital Humanities (DH), Conference Abstracts, Kraków, Poland, 2016.
               
                  Michel, J.B. / Shen, Y.K. / Aiden, A.P. / Veres, A. / Gray, M.K. / Pickett, J.P. / Hoiberg, D. / Clancy, D. / Norvig, P. / Orwant, J. / Pinker, S. (2011): “Quantitative analysis of culture using millions of digitized books”, in: science, 331(6014) 176-182.
               
                  Moretti, F. (2005): Graphs, maps, trees: abstract models for a literary history. Verso.
               
                  Moretti, F. (2011). Network theory, plot analysis. Stanford Literary Lab Pamphlet Series 2. Available at: https://litlab.stanford.edu/LiteraryLabPamphlet2.pdf
               
                  Murr, S. / Barth, F. (2017): Digital Analysis of the Literary Reception of J.W. v. Goethe’s ‘Die Leiden des jungen Werthers’, in: Digital Humanities (DH), Conference Abstracts, Montreal, Canada 2017.
               
                  Nalisnick, E.T. / Baird, H.S. (2013): “Extracting sentiment networks from Shakespeare's plays”, in: Document Analysis and Recognition (ICDAR), 2013 12th International Conference on IEEE 758-762.
               
                  Park, G.M. / Kim, S.H. / Cho, H.G. (2013): “Structural analysis on social network constructed from characters in literature texts”, in: Journal of Computers, 8(9): 2442-2447.
               
                  Rydberg-Cox, J., (2011): “Social networks and the language of greek tragedy”, in: Journal of the Chicago Colloquium on Digital Humanities and Computer Science (Vol. 1, No. 3).
               
                  West, D.B. (2001): Introduction to graph theory (Vol. 2). Upper Saddle River: Prentice Hall.
            
         
      
   



      
         
            Einleitung
            Obwohl sich infrastrukturell einiges getan hat, sieht ein typischer Operationsmodus der digitalen Literaturwissenschaft immer noch so aus, dass eine bestimmte Forschungsmethode auf ein oft nur ephemeres Korpus angewandt wird. Im besten Fall ist das Ergebnis 
                    irgendwie reproduzierbar, im schlechtesten Fall gar nicht. Im besten Fall gibt es ein offen zugängliches Korpus in einem Standardformat wie TEI, einer anderen Markup-Sprache oder zumindest als txt-Datei. Im schlechtesten Fall ist das Korpus gar nicht zugänglich, d. h., die Forschungsergebnisse müssen einfach hingenommen werden.
                
            Doch seit kurzem gibt es Anzeichen, dass sich dies ändert. Einige Digital-Humanities-Projekte stellen Schnittstellen zu stabilen Korpora zur Verfügung, über die man mannigfaltige Zugriffsmöglichkeiten bekommt und reproduzierbar arbeiten kann. Eines dieser Projekte ist DraCor, eine offene Plattform zur Dramenforschung, die in diesem Vortrag vorgestellt werden soll (zugänglich unter 
                    bzw. über die Repos und verschiedene Schnittstellen). DraCor transformiert vorliegende Textsammlungen zu ›Programmable Corpora‹ – ein neuer Begriff, den wir mit diesem Vortrag ins Spiel bringen möchten.
                
         
         
            Die Bausteine
            
               Vanillekorpora
               Ähnlich wie die COST Action zu europäischen Romanen (Schöch et al. 2018), versucht das DraCor-Projekt als Basis für eine digitale Komparatistik einen Stamm an multilingualen Dramenkorpora aufzubauen, die in basalem TEI kodiert sind. Ein selbst betriebenes russischsprachiges 
                    () und ein deutschsprachiges Korpus 
                    () dienen dabei als Einstieg. Diese Korpora sind, ähnlich wie die Sammlung »Théâtre classique« von Paul Fièvre, im weitesten Sinne als Vanillekorpora angelegt, die über das notwendige Markup hinaus zunächst kaum weitere spezielle Auszeichnungen enthalten, allerdings frei zur Verfügung stehen und damit fork- und erweiterbar sind. Zur Demonstration, dass auch andere, reicher kodierte Korpora dazugebunden und sofort alle bereits bestehenden Extraktions- und Visualisierungsmethoden der Plattform angeboten werden können, wurden das Shakespeare Folger Corpus sowie das schwedische Dramawebben-Korpus geforkt und angedockt 
                    (bzw. 
                        ). Dramenkorpora in weiteren Sprachen sollen folgen; einzige Voraussetzung dabei ist jeweils, dass diese in TEI vorliegen.
                    
               Die Vorteile von frei auf GitHub gehosteten Korpora liegen auf der Hand. Unabhängig von den letztlich durch die Plattform zur Verfügung gestellten Schnittstellen können die Korpora alternativ durch Klonen oder andere Downloadmethoden, etwa über den SVN-Wrapper von GitHub, direkt bezogen und individuell weiterverarbeitet werden. Ein offen zugängliches GitHub-Repositorium heißt auch, dass Pull Requests zur Fehlerkorrektur und Forks für Erweiterungen möglich und erwünscht sind.
            
            
               XML-Datenbank (eXist-db) und Frontend
               DraCor als Plattform setzt auf die eXist-Datenbank, um die TEI-Dateien zu verarbeiten und Funktionen zur Beforschung der Korpora zur Verfügung zu stellen. Das Frontend wurde mit ReactJS gebaut, ist responsiv und einfach erweiterbar. Der Schwerpunkt liegt aber nicht auf der GUI, sondern auf der API (vgl. generell zur Unterscheidung zwischen beiden Schnittstellenansätzen Bleier/Klug 2018).
            
            
               API und Entwicklungsumgebung
               Um dem Ideal und der Möglichkeit nahe zu kommen, auf einfache Weise »alle Methoden auf alle Texte« anwenden zu können (Frank/Ivanovic 2018), braucht es mehr als offene Korpora. Der zitierte Text von Frank/Ivanovic macht sich hinsichtlich dessen für SPARQL-Endpunkte stark; auch DraCor bietet einen solchen an, besitzt darüber hinaus aber eine reiche API, die über Swagger dokumentiert und erläutert wird 
                    (). In einem Teilbereich der Korpusphilologie, den Digital Scholarly Editions, hat die Diskussion um eine proaktivere Nutzung von APIs bereits begonnen (zur Vorgeschichte vgl. wiederum Bleier/Klug 2018), als Beispiel hierfür diene die Folger Digital Texts API 
                    (), über die man sich spezifische Querys zusammenbauen kann. Der Vorteil einer moderneren Lösung wie Swagger besteht darin, dass API-Querys live und direkt ausgeführt und die Outputs genauer kontrolliert und gesteuert werden können.
                    
               Ein einfaches Use-Case-Szenario sieht dann so aus, dass man etwa im RStudio mit zwei, drei Zeilen Code einen Blick in ein Korpus werfen kann, etwa über die zeitliche Entwicklung der Anzahl der Charaktere im russischen Drama zwischen 1740 und 1940, die in der Metadatentabelle festgehalten sind 
                    (). Diese Datei, beziehbar im JSON- oder CSV-Format, wird in eine Data.Table eingelesen, woraufhin die Werte zweier Spalten (Erscheinungsjahre und Number of Speakers) einfach über ggplot visualisiert werden können (Abb. 1).
                    
               
                  
                     
                     Abbildung 1: Anzahl der Charaktere pro Drama in chronologischer Ordnung (Quelle: RusDraCor).
                  Anhand dieses sehr simplen Beispiels zeigt sich dann recht deutlich, dass sich mit Puschkins an Shakespeare angelehntem historischen Drama »Boris Godunow« (1825), in dem Sprechakte von 79 Charakteren vorkommen, eine strukturelle Diversifizierung der russischen Dramenlandschaft Bahn bricht.
                    
               Die Möglichkeiten beschränken sich aber nicht darauf, vorgefertigte API-Funktionen zu benutzen. Neue Forschungsideen zeitigen immer auch neue Bedarfe an einfach bezieh- und reproduzierbaren Daten und Metriken; die API kann dementsprechend erweitert werden. Dies wird dadurch erleichtert, dass über Apache Ant die gesamte Entwicklungsumgebung auf dem eigenen System nachgebaut werden kann.
               Durch bereits implementierte Funktionen können neben Struktur- und Metadaten etwa auch Volltexte ohne Markup bezogen werden (auch Untermengen von Volltexten wie Regieanweisungen), etwa wenn Methoden wie die Stilometrie oder das Topic Modeling der Endzweck sind, also Methoden, die nach dem »bag of words«-Prinzip arbeiten, für das kein Markup vonnöten ist.
               Insgesamt wird durch den Aufbau und die Dokumentation offener APIs die bisher oft aufwendige Reproduzierbarkeit von Forschungsergebnissen erheblich erleichtert.
            
            
               Shiny App
               Ein Beispiel für die vielseitigen Nutzungsmöglichkeiten der DraCor-API ist die Shiny App, die Ivan Pozdniakov aufgesetzt hat 
                    (). Shiny ist ein auf R basierendes Framework, das es ermöglicht, interaktive Visualisierungen im Browser darzustellen. Die DraCor-Shiny-App tut genau dies und setzt dabei vollkommen auf die DraCor-API für den Datenbezug. So kann zu Lehr- und Forschungszwecken, aber auch zur einfacheren Datenkorrektur, auf Visualisierungen des aktuellen Datenbestandes zugegriffen werden.
                    
            
            
               Didaxe
               Das Markup oder andere Formalisierungen literarischer Texte sind nicht selbsterklärend. Zwar gibt es einige Standards, aber die jeweilige Operationalisierungslösung hängt von der Forschungsfrage ab. Allein das Extrahieren von Figurennetzwerkdaten ist auf viele Arten und Weisen möglich, was dazu führt, dass etwa alle von verschiedenen Forschungsgruppen extrahierten Netzwerke aus Shakespeares »Hamlet« zu leicht verschiedenen Ergebnissen kommen. Selbst für Dramen ist dies also schon ein nicht-trivialer Akt, von Romanen dann ganz zu schweigen (beispielhaft seien Grayson et al. 2016 genannt, die verschiedene Extraktionsmethoden für Romane durchtesten und die Ergebnisse vergleichen). Um diese Erkenntnis schon in der Lehre zu fördern, wurde das Tool »Easy Linavis« 
                    () entwickelt und in die DraCor-Toolchain integriert. Per Hand können Netzwerkdaten aus Texten extrahiert und dabei das Bewusstsein für die Kontingenz dieses Vorgangs geschärft werden, eine wichtige Vorstufe zur Operationalisierung.
                    
               Neben einem Ansatz zur Gamifizierung des TEI-Korrekturvorgangs (Göbel/Meiners 2016) haben wir für Lehrzwecke auch ein Dramenquartett entwickelt, um spielerisch das Verständnis von Netzwerkwerten zu trainieren (Fischer at al. 2018).
               Die aufgezählten, um die Plattform herumgruppierten didaktischen Mittel sind integraler Bestandteil des ganzen Projekts, da sie auf dessen Daten und Operationalisierungen aufsetzen. Wichtig dabei war die Erkenntnis, dass Daten mehrere Gestalten annehmen und für Forschung und Lehre gleichermaßen von Bedeutung sein können.
            
            
               Linked Open Data (LOD)
               Im TEI-Code sind PND- bzw. Wikidata-Identifier sowohl für Autor*innen als auch für die Werke hinterlegt. Auf diese Weise lassen sich verschiedene Realien, die außerhalb der eigenen Korpusarbeit liegen, hinzufügen. Eine automatisch erstellte Autor*innengalerie hat dabei noch eher illustrativen Charakter (de la Iglesia/Fischer 2016).
               Darüber hinaus kann man aber zum Beispiel feststellen, ob es nicht einen unbewussten regionalen Bias im Korpus gibt. Dafür lässt man sich über die Wikidata-Identifier die Verteilung der Geburts- und Sterbeorte der Autor*innen auf einer Karte anzeigen. So konnte dann für das deutschsprachige Korpus GerDraCor ausgeschlossen werden, dass es einen solchen Bias gibt, da sich die Orte relativ gleichmäßig über die (historisch) deutschsprachigen Gebiete verteilen (Göbel/Fischer 2015).
               Ebenso lässt sich über die Wikidata-ID der Stücke herausfinden, wo diese uraufgeführt worden sind (Beispiel-Query: 
                        ), d. h., Aspekte der Aufführungsgeschichte lassen sich zuschalten, obwohl diese gar nicht im Fokus des Kernprojekts liegen. Programmable Corpora verbinden sich also auch mit der Welt um sie herum, was sie u. a. von den nach innen gerichteten Workbenches der Korpuslinguistik unterscheidet.
                    
            
            
               Infrastruktur statt Rapid Prototyping
               Projekte wie DraCor versuchen nichts anderes als den digitalen Literaturwissenschaften eine verlässliche und ausbaufähige Infrastruktur zu geben, damit sie sich stärker auf eigentliche Forschungsfragen konzentrieren und reproduzierbare Ergebnisse hervorbringen können.
               Eine wichtige Folgerung für uns war, dass wir die Weiterentwicklung unserer seit vier Jahren entwickelten all-in-one Python-Skriptsammlung 
                        dramavis aufgeben und uns lieber der Arbeit an der API widmen. 
                        Dramavis (Kittel/Fischer 2014–2018 sowie Fischer et al. 2017) folgte dem in den Digital Humanities nicht untypischen Rapid Prototyping mit direkter Verarbeitung literarischer XML-Daten (Trilcke/Fischer 2018) und einer mittlerweile stark gewachsenen Codebasis, die alles auf einmal kann, deren Maintenance aber immer schwieriger geworden ist und oft genug von den eigentlichen Forschungsfragen weggeführt hat.
                    
            
         
         
            
               Fazit
                
            In Anlehnung an das Projekt »ProgrammableWeb« – das eine Datenbank von offenen APIs unterhält und dessen Slogan lautet: »APIs, Mashups and the Web as Platform« (zugänglich unter 
                    ) – schlagen wir für infrastrukturell-forschungsorientierte, offene, erweiterbare und LOD-freundliche Korpora den Begriff ›Programmable Corpora‹ vor.
                
            Programmable Corpora erleichtern es, Forschungsfragen auf viele Arten und Weisen um Korpora herum programmieren zu können. Es steht zu erwarten, dass sich infrastrukturelle Anstrengungen dieser Art für die gesamte Community auszahlen mit Effekten, wie sie John Womersley in seiner Präsentation auf der ICRI2018 in Wien aufgezählt hat: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction.
            Der Anschlussmöglichkeiten sind viele, egal ob man gar nicht programmieren möchte, sondern nur eine GEXF-Datei für Gephi benötigt, ob ein Korpus über seine Verbindungen zur Linked Open Data Cloud beforscht oder einfach aus R oder Python heraus bestimmte Daten bezogen werden sollen, ohne dass man sich mit dem Korpus und dessen Maintenance und Reproduzierbarkeit selbst kümmern muss (all dies bleibt natürlich aber eine Option). Programmable Corpora erleichtern die Entscheidung, auf welcher Ebene der eigene Forschungsprozess einsetzt.
         
      
      
         
            
               Bibliographie
               
                  Bleier, Roman / Klug, Helmut W. (2018): Discussing Interfaces in Digital Scholarly Editing. In: Digital Scholarly Editions as Interfaces. BoD, Norderstedt, S. V–XV. URL: 
               
               
                  de la Iglesia, Martin / Fischer, Frank (2016): The Facebook of German Playwrights. URL: 
               
               
                  Fischer, Frank / Dazord, Gilles / Göbel, Mathias / Kittel, Christopher / Trilcke, Peer (2017): Le drame comme réseau de relations. Une application de l‘analyse automatisée pour l’histoire littéraire du théâtre. In: Revue d'historiographie du théâtre. № 4. URL: 
               
               
                  Fischer, Frank / Kittel, Christopher / Milling, Carsten / Schultz, Anika / Trilcke, Peer / Wolf, Jana (2018): Dramenquartett – Eine didaktische Intervention. In: Konferenzabstracts zur DHd2018, Universität zu Köln. S. 397 f. DOI: 
               
               
                  Göbel, Mathias / Fischer, Frank (2015): The Birth and Death of German Playwrights. URL: 
               
               
                  Göbel, Mathias / Meiners, Hanna-Lena (2016): Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus. In: Konferenzabstracts zur DHd2016, Bern/CH. S. 140–143. URL: http://www.dhd2016.de/abstracts/vortr%C3%A4ge-007.html
               
               
                  Grayson, Siobhán / Wade, Karen / Meaney, Gerardine / Greene, Derek (2016): The Sense and Sensibility of Different Sliding Windows in Constructing Co-Occurrence Networks from Literature. In: 2nd IFIP International Workshop on Computational History and Data-Driven Humanities. Trinity College Dublin 2016. PDF: 
               
               
                  Kittel, Christopher / Fischer, Frank (2014–2018): dramavis. Python-Skriptsammlung. Repo: 
               
               
                  Schöch, Christoph et al. (2018): Distant Reading for European Literary History. A COST Action [Poster]. In: DH2018: Book of Abstracts / Libro de resúmenes. Mexico: Red de Humanidades Digitales A. C. URL: https://dh2018.adho.org/en/?p=11345
               
               
                  Trilcke, Peer / Fischer, Frank (2018): Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen. In: Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden. Hrsg. von Martin Huber und Sybille Krämer (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3). DOI: 
               
            
         
      
   



      
         Digital Humanities in die Lehre und Ausbildung stärker zu integrieren, ist eine vielfach geäußerte Forderung im Rahmen von DH und geisteswissenschaftlichen Fachverbänden (Sahle 2017). Während in den ersten beiden Jahrzehnten der Digitalisierung vor allem der Wandel von der analogen zur digitalen Erschließung und Präsentation, Open Access, Blended Learning und digitales Publizieren im Mittelpunkt der Forschung stand, haben sich Ansätze und Themen zur universitären Vermittlung von DH-Technologien in der jüngeren Vergangenheit stark verändert. Mit den Digital Humanities ist eine Community entstanden, die innovative neue Methoden, vor allem aber eine Vielzahl von Tools und Werkzeugen bereitstellt. Sie entspringen nicht einem einzelnen fachspezifischen Kontext, sondern setzen auf interdisziplinäre Konzepte und vor allem vertiefte informatische Kenntnisse. Mittlerweile lässt sich in der Fachlandschaft eine Etablierung neuer Formen von Studiengängen und Curricula beobachten, die solche spezifischen DH-Anwendungen vermitteln und so zur Ausbildung der dringend benötigten DH-Spezialisten beitragen. Grundsätzlich deckt diese Form der parallelen Ausbildung von DH-Spezialisten zu den eigentlichen Fachwissenschaften daher einen sehr wichtigen Bedarf ab (Sahle 2013).
         Dennoch wirft diese Entwicklung auch Schwierigkeiten auf, da sie die Entkoppelung von an der DH orientierten Wissenschaftlern und eigentlicher geisteswissenschaftlichen Fachwissenschaft zusätzlich verschärft (Hohls 2017). Da viele DH-Anwendungen heute noch keinem Fachkanon oder Standards unterliegen, bleibt die Einarbeitung in solche Methoden und Tools ein arbeitsintensiver Prozess interdisziplinärer Verständigung, der meist individuell geleistet werden muss. Dies führt häufig zu Schwierigkeiten im Vermittlungsprozess. In der jüngeren Diskussion wird dies gern mit dem Bild des DH-affinen „Hackers“ charakterisiert, der in der Fachwissenschaft dem interessierten „Laien“ gegenübertritt. Die Spannungen zwischen beiden Gruppen (Enthusiasten und Skeptiker) haben sich in den letzten Jahren verschärft. Auf dem letzten Historikertag in Münster (September 2018) war die Skepsis gegen neue Methoden des digitalen Arbeitens in mehreren Sessions genauso fassbar, wie letztlich die Forderung danach, dass die DH auch fachlich für die einzelnen Fachverbände konkrete Antworten liefern müsse und nicht nur eine grundlegende Erschließungsfunktion besitzen dürfe. Innerhalb der einzelnen Fachverbände muss geklärt werden, inwieweit digitale Geisteswissenschaft Bestandteil der klassisch fachbezogenen Ausbildung werden kann und soll (Fickers 2014, S. 27, Schulz 2018, S. 79f.). Das gilt nicht nur für die spezifischen Forschungsrichtungen der Hilfs- und Grundwissenschaften und der Quellenkritik, sondern letztlich für alle Teilbereiche der fachbezogenen Forschung (Schlotheuber/Bösch 2015). Während in den Digital Humanities eher die hilfswissenschaftlichen Traditionen der Erschließung, Annotation, Editorik, Messung und Visualisierung wichtige Dimensionen repräsentieren, bleibt für die Geschichtswissenschaft die digitale Quellenkritik, Datenmodellierung, Analyse und vor allem Methoden- und Algorithmenkritik wesentliche Aufgabe (Rehbein 2015, Schulz 2018). Mittlerweile stehen ausgereifte kommerzielle und OpenSource-Programme bereit, um mittels qualitativer und/oder quantitativer Datenanalyse ganz verschiedene methodische Verfahren anzuwenden. Allerdings ist der zeitliche Umfang des Geschichtsstudiums durch die Einführung der Bachelor- und Masterstudiengänge heute streng limitiert. Viele Studierende verlassen nach dem Bachelorstudium die Universitäten und nehmen eine Arbeit auf. Innerfachlich ist daher der Anteil von historischer Fachkompetenz und digitalem methodischen Wissen bei der Ausbildung des Nachwuchses zu gewichten und es müssen zwingend Vorschläge diskutiert werden, welche grundsätzlichen Bausteine Anschlussfähigkeit zu den Digital Humanties herstellen können und wie diese in den Studienkanon integriert werden können.
         Welche Grundkompetenzen der Geschichtswissenschaft sind für die Basis der Geisteswissenschaft also ausschlaggebend? Für das Fach selbst dürfte die Verbindung von fachlicher Fragestellung, ihre Übertragung in spezifische Methoden und die Modellierung der damit in Zusammenhang stehenden Daten solche Grundkompetenzen beschreiben.
         Im Rahmen des Workshops “Digitale Lehrmethoden und digitale Methoden in der Geschichtswissenschaft. Neue Ansätze für die Lehre” hat die AG Digitale Geschichtswissenschaft im Verband der Historiker und Historikerinnen Deutschlands im Jahr 2018 eine Workshopreihe begonnen, um genau solche Spannungsfelder auszuloten und Angebote für die digitale Vermittlung zwischen DH und Fachcommunities zu leisten (König 2018). Zwar gibt es Summerschools und einzelne Workshops für die Vermittlung von digitalen Methoden, diese bieten aber häufig nur schwerpunktartige Einführungen zu bestimmten Tools und Techniken. Einen breiten Überblick über die vielfältige Landschaft digitaler Methoden, ihre Einbindung in Forschung und Lehre, bzw. ihre Anwendungsmöglichkeiten und -grenzen können diese Formen der Weiterbildung hingegen nicht leisten. Systematisches Wissen steht für die Lehre auf diese Weise bisher nur selten zur Verfügung, wie auch Anreiz- und Gelegenheitsstrukturen für eine tiefergehende digitale Schulung fehlen. Das Digitale wird so schnell zur Last anstatt zur Lust, vor allem wenn hinter den Angeboten auch ausgefeilte didaktische Konzepte und Ideen stecken sollen. Hier braucht es kreative Ideen, um im wissenschaftlichen Alltag Dozierende und Studierende zu erreichen und auf diese Weise, die bereits vorhandenen digitalen Infrastrukturen mit Leben zu füllen.
         Ein solches digitales Angebot möchten wir mit unserem Vortrag vorstellen. Im Rahmen des Workshops „Methoden auf der Testbank. Drei Zugänge zur Hexenforschung im Vergleich“ haben wir anhand von zentralen Thesen zur Hexenforschung einen Korpus von Texten und Daten zusammengestellt, der sich für drei verschiedene methodische Analysen entlang der gleichen historischen Fragestellung nach der Entwicklung, Abgrenzung und Ausdifferenzierung des Zauberei- und des Hexensabbatkonzeptes im 16. Jahrhundert nutzen lässt. Der Workshop richtet sich an Dozierende, die digitalen Methoden eher abwartend gegenüberstehen. Dabei geht es gezielt um die Verbindung fachlicher, methodischer und digitaler Problemstellungen. Aufgegriffen wird daher eine wichtige These der Hexenforschung (Voltmer 2012, Behringer 1997, Dillinger 2007), die bereits eine lange fachliche Diskussion besitzt und für die durch die Lehrkonzeption  auf digitalem Weg Einsichten und Erkenntnisse formuliert werden können. Entwickelt wurde ein Materialkorpus der folgende Materialien bereithält und bis zur Tagung auch frei zur Verfügung gestellt werden soll:
         1.) Lehrkonzept und didaktische Stoffentwicklung zur Thematik der Hexenverfolgung sowie die fachliche Entwicklung und Begründung der Fragestellung. Dieser Baustein des Angebots formuliert nicht nur die Fragestellung, sondern bietet zudem eine Einbindung zentraler fachwissenschaftlicher Texte und Thesen zum Thema, um hier verschiedene Aspekte auch für eine projektorientierte Seminargestaltung zu ermöglichen. So werden etwa Vorschläge unterbreitet, welche Formen der Analyse Studierende anhand eines selbstgewählten Projektes mithilfe der methodischen Ansätze wählen können.
         2.) Textkorpora: Digitalisiert und transkribiert wurden Urgichten und Verhörprotokolle von insgesamt 52 Personen (ca. 500 Digitalisate), die in der Stadt Rostock (Mecklenburg) während des 16. Jahrhunderts aufgrund von Zauberei oder Hexerei angeklagt wurden. Der Textkorpus erlaubt aufgrund des damit eingefangenen - auch sprachwissenschaftlich oder rechtsgeschichtlich sehr interessanten - Beobachtungszeitraums weitergehende Fragestellungen und Analysen. Aufgrund der hervorragenden Quellenüberlieferung bot der Bestand bereits mehrfach Ansatzpunkte zur auszugsweisen Edition (Koppmann 1900, Krause 1915) wie auch grundlegender Erforschung (Ehlers 1986, Moeller 2007, Müller 2017). Langfristig lassen sich hier sogar Untersuchungen zur Editionspraxis verschiedener Zeiten anstellen. Zugleich werden motivgeschichtliche Analysen möglich, da der Bestand etwa in die volkskundlichen Sammlungen und Korpora des 19./20. Jahrhunderts eingegangen ist.
         Diese Texte sind in einem zweimaligen Korrekturprozess und entsprechend der DTA- Transkriptionsrichtlinien transkribiert worden. Bis zur Tagung sollen sie nach Möglichkeit in das Deutsche Textarchiv oder ein anderes Repositorium überführt werden.
         3.) Über die Transkription hinaus erfolgte eine Modellierung der Fragestellung mithilfe eines selbst zu entwickelnden Kategoriensystems, das sich auch zur Implementation (Annotationen) in die Textdateien eignet (hier können praktische Übungen zusätzlich angeboten werden). Zum anderen aber auch in Form von Datenstrukturen zur Analyse mit MAXQDA, einem Statistikprogramm (hier spezifisch SPSS, möglich ist aber auch die Nutzung von OpenSource Software wie R) sowie einem graph- bzw. netzwerkbasierten Datenmodell (hier Gephi) Auswertungen erlaubt. Wir haben uns auf die Anwendungen von Programmen konzentriert, die Lehrenden und Lernenden einen ersten, niedrigschwelligen Zugang zur Anwendung von Methoden im Vergleich ermöglichen und vor allem für das Selbststudium auch genügend ausreichend dokumentiert sind. Über einen didaktischen Aufbau von qualitativer Datenanalyse (orientiert an Kuckartz 2014 und Mayring 2015), statistischer Auswertung und netzwerkorientierter Untersuchung können Studierende hier in der Datenmodellierung vom Konkreten zum Allgemeinen, der Entwicklung von Kategoriensystemen und der Verwendung von Standards geschult werden. Denn grundlegende Probleme in der Lehre sind meist das Verständnis von Datenstrukturen, die Formen der Modellierung, die Entscheidung für eine Methode und die Interpretation von Erkenntnissen aus den erzielten Datenanalysen.
         4.) Die Datenmodellierung wird genau dokumentiert und dient Studierenden und Lehrenden damit ebenso zur Institutionalisierung von zentralen Schritten des Forschungsdatenmanagements. Anhand von eher sozialwissenschaftlichen Dokumentationspraktiken werden Richtlinien formuliert, die nicht nur das Verständnis der Daten in ihrer Überführung von der „Quelle zur Tabelle“ (Manfred Hettling) fördern, sondern ebenso ein Beispiel für die „gute wissenschaftliche Praxis“ der Dokumentation von Forschungsleistungen bieten. Die Daten werden in langfristig speicherbaren, spezifischen Datenrepositorien abgelegt. Auf einer gemeinsamen Plattform der AG Digitale Geschichte und des Historischen Datenzentrums Sachsen-Anhalts werden die einzelnen Komponenten zu einer didaktischen Einheit nachnutzbar versammelt.
         5.) Das Datenset bzw. die Lehrkonzeption ermöglicht es den Studierenden, einen Überblick über die Verwendung von drei verschiedenen methodischen Ansätze zu erlangen, die Methoden kritisch zu vergleichen und Fähigkeiten zur Operationalisierung und Implementierung von Datenmodellierungen zur Beantwortung von Fragestellungen zu erwerben. Überdies wird eine individuelle Aktivierung der Studierenden möglich, die für die didaktische Vermittlung von Fachinhalten heute eine zentrale Rolle in der Lehre spielt (Helmke 2015). Mit Hilfe von selbsterstellten und bereitgestellten Daten können geschichtswissenschaftliche Analysen durchgeführt und die Vor- und Nachteile einzelner Methoden diskutiert werden. Vor allem aber erweist sich, ob Methodenvielfalt eher zur Bestätigung von Thesen oder zu neuen Perspektiven führt. Das Werkzeug bietet damit nicht nur Möglichkeiten der Quellen-, sondern eben auch der Methodenkritik.
         Insgesamt möchten wir damit einen Baustein vorstellen, wie sich digitale Lehre und Methoden der DH unmittelbar mit fachlichen Fragestellungen und Gegenständen der Geschichtswissenschaft verzahnen und bearbeiten lassen und sich der immer wieder beklagte Graben zwischen Digital Humanties und Fachwissenschaften einebnen lässt. Gleichzeitig möchten wir Einschätzungen zum Zeitaufwand, Lehraufwand und zur Ressourcenplanung geben. Die Lehreinheit soll eher unerfahrenen Nutzerinnen und Nutzern der Methoden einen Eindruck vermitteln, was digitale Werkzeuge leisten können und wie sie sich in der alltäglichen, fachspezifischen Lehre einbetten lassen, obwohl sie natürlich klassische Werkzeuge einer allgemeinen digitalen Forschung repräsentieren.
      
      
         
            
               Bibliographie
               
                  Wolfgang Behringer:
                  Hexenverfolgung in Bayern. Volksmagie, Glaubenseifer und Staatsräson in der Frühen Neuzeit, 
                        München 1997.
                    
               
                  Johannes Dillinger:
                  Hexen und Magie. Eine historische Einführung, 
                        Frankfurt/Main 2007.
                    
               
                  Ingrid Ehlers:
                  Über den Glauben an Hexen und Zauberer und ihre Verfolgung im Rostock des 16. Jahrhunderts. 
                        Beiträge zur Geschichte der Stadt Rostock (Neue Folge) 6, 1986, S. 21-40.
                    
               
                  Andreas Fickers:
                  Der ultimative Klick? Digital Humanities, Online-Archive und die Arbeit des Historikers im digitalen Zeitalter,
                        in: Forum für Politik, Gesellschaft und Kultur in Luxemburg 337, 2014, S. 25-29,
                        
                     http://hdl.handle.net/10993/21285.
                  
               
               
                  Andreas Helmke:
                  Unterrichtsqualität und Lehrerprofessionalität: Diagnose, Evaluation und Verbesserung des Unterrichts,
                        Seelze-Velber 2015.
                    
               
                  Rüdiger Hohls:
                  Digital Humanities vs. Digital History: Differenzen und Gemeinsamkeiten, 
                        in: Videoaufzeichnungen der Ringvorlesung "Digital Humanities: Die digitale Transformation der Geisteswissenschaften, Berlin 2017, URL: http://www.bbaw.de/mediathek/archiv-2017/24-10-2017-digital-humanities.
                    
               
                  Mareike König:
                  Workshopreihe 2018: Digitale Lehrmethoden und digitale Methoden in der Geschichtswissenschaft. Neue Ansätze für die Lehre #digigw18, 
                        2018,
                        
                     https://digigw.hypotheses.org/1660.
                  
               
               
                  Karl Koppmann:
                  Aus Hexenprozessen (aus Rostocker Niedergerichtsakten von 1576-1621), 
                        in: Korrespondenzblatt des Vereins für niederdeutsche Sprachforschung 21, 1900, S. 20-29.
                    
               
                  Ludwig Krause:
                  Die Blocksbergfeste der Hexen und Zauberer nach den Rostocker Kriminalakten des 16. Jahrhunderts, 
                        in: Niedersachsen 15, 1915, S. 238-240.
                    
               
                  Udo Kuckartz:
                  Qualitative Inhaltsanalyse. Methoden, Praxis, Computerunterstützung, 
                        Weinheim und Basel 2014.
                    
               
                  Philipp, Mayring:
                  Qualitative Inhaltsanalyse. Grundlagen und Techniken, 
                        Weinheim und Basel 2015.
                    
               
                  Katrin Moeller:
                  Dass Willkür über Recht ginge. Hexenverfolgung in Mecklenburg im 16. und 17. Jahrhundert, 
                        Bielefeld 2007.
                    
               
                  Andreas Müller:
                  Die Magie der Inhaltsanalyse. Entwurf einer  Inhaltsanalyse für den Vergleich von Hexenprozessakten aus Rostock 1584  und Hainburg, 
                        Masterarbeit Universität Wien 2017, 
                        
                     https://www.historicum.net/fileadmin/sxw/Themen/Hexenforschung/
                     
                  
                  Themen_Texte/Magister/hexen_mag_mueller.pdf.
               
               
                  Malte Rehbein:
                  Digitalisierung braucht Historiker/innen, die sie beherrschen, nicht beherrscht, 
                        in: H-Soz-Kult, 27.11.2015,
                        
                     www.hsozkult.de/debate/id/diskussionen-2905
                  
                  .
               
               
                  Patrick Sahle:
                  DH studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities, 
                        Göttingen: GOEDOC, Dokumenten- und Publikationsserver der Georg-August-Universität, 2013 (DARIAH-DE working papers 1).
                    
               
                  Patrick Sahle:
                  Forschung & Karriere. Zur anhaltenden Formierung, Professionalisierung und Professoralisierung der Digital Humanities, 
                        Vortrag Berlin, in: Ringvorlesung des Interdisziplinären Forschungsverbundes Digital Humanities in Berlin, 12.12.2017.
                    
               
                  Eva Schlotheuber / Frank Bösch:
                  Quellenkritik im digitalen Zeitalter. Die Historischen Grundwissenschaften als zentrale Kompetenz der Geschichtswissenschaft und benachbarter Fächer, 
                        auf: VHD-Blog (4 S.; PDF-Version: http://www.historikerverband.de/fileadmin/user_upload/vhd_journal_2015-04_beileger.pdf), 30. Oktober 2015.
                    
               
                  Julian Schulz:
                  Auf dem Weg zu einem DH-Curriculum. Digital Humanities in den Geschichts- und Kunstwissenschaften an der LMU München, 
                        München 2018,
                        
                     https://doi.org/10.5282/ubm/epub.42419
                  
                  .
               
               
                  Rita Voltmer / Walter Rummel:
                  Hexen und Hexenverfolgung in der frühen Neuzeit, 
                        Darmstadt 2012.
                    
            
         
      
   



      
         
            Einleitung 
            
               Methoden der Digital Humanities erfreuen sich zunehmender Akzeptanz in den Geisteswissenschaften - auch in traditionellen Disziplinen wie den Geschichtswissenschaften. Die Werkzeuge der Digital Humanities können hilfreich sein, um die eigenen Daten zu strukturieren, unerwartete Zusammenhänge zu erkennen und Erkenntnisse hervorzubringen, die man ohne den Einsatz von digitalen Methoden nicht erlangt hätte. Vor allem im Bereich der historischen Netzwerkanalyse gibt es seit einigen Jahren neue interessante Entwicklungen. Während man sich anfangs noch an Werkzeugen aus der Sozialwissenschaft bediente
                
               (Jannidis / Kohle / Rehbein 2017: 14), gibt es mittlerweile Software, die nicht nur statistische Ergebnisse ausgibt, sondern auch für historische Fragestellungen geeignet ist. Die Vorteile liegen auf der Hand: einerseits kann man komplexe Beziehungen, deren Reichweite und Auswirkungen ordnen und darstellen; andererseits durch Visualisierungen die Ergebnisse einer breiten Öffentlichkeit zugänglich machen (Düring / Kerschbaumer 2016: 31). Wenn man anfangs mit dem Gedanken spielt, ob man für die Bearbeitung einer geisteswissenschaftlichen Forschungsarbeit überhaupt digitale Hilfsmittel anwenden sollte, wird man mit verschiedenen Fragen konfrontiert:
            
            
               Wie und wo fange ich an? Welches Tool ist sinnvoll? Welche Daten habe ich überhaupt und wie möchte ich diese darstellen? Welchen Mehrwert verspreche ich mir von dem Einsatz digitaler Methoden? Muss ich meine Fragstellungen anpassen um ein zufriedenstellendes Ergebnis zu erhalten? Welche Kenntnisse und Fähigkeiten muss ich mir aneignen? Lohnt sich das überhaupt? Der Posterbeitrag soll an diese Fragen anknüpfen. 
            
         
         
            Netzwerkanalyse für Historiker
            
               Die eigene Recherche hat gezeigt, dass das Angebot und die Initiativen (national wie international) von "Social Network Analysis" (kurz SNA) für Historiker auf den ersten Blick enorm zu sein scheint. Allerdings stellt sich bei näherer Betrachtung meistens heraus, dass nur einige der Tools für das eigene Thema geeignet sind. Schließlich variieren bei geschichtlichen Forschungsfragen die gewählten Zugänge sowie die Quellenlage und der Blickwinkel. Dieses Poster soll einerseits die meist verbreiteten Techniken für Historiker in der Netzwerkanalyse vorstellen; andererseits soll an konkreten Schritten der Entscheidungsprozess für ein geeignetes Tool zur Netzwerkanalyse beispielhaft am eigenen Dissertationsvorhaben nachvollzogen werden. Aufkommende Probleme und mögliche Lösungswege sollen beleuchtet werden.
            
         
         
            Anwendung in der Praxis mit „Gephi“
            
               Mein Forschungsthema befasst sich mit der Darstellung von Palästina in württembergischen Medien des 19. Jahrhunderts. Dafür werte ich politische, religiöse und pädagogische Medien aus. Eine der Hauptfragestellungen ist unter anderem welche Netzwerke gebildet wurden, in denen die Hauptakteure eine gewissen Einfluss übten und dadurch die gegenseitigen Beziehungen beider Länder prägten und förderten. Eine große Rolle spielt die Wissensverbreitung über das damals relativ unbekannte "Heilige Land": wie wurde spezielles Wissen - unter anderem Agrarmethoden, kulturelles und religiöses Leben in Palästina - weitergegeben? Neben den sozialen Beziehungen bestand auch ein reger wirtschaftlicher Austausch zwischen Palästina und Württemberg, der sich unter anderem im Handel äußerte. In diesem Zusammenhang plane ich meine Ergebnisse, die Verbindungen und Verflechtungen, sowie den Transfer zwischen beiden Länder mit der Hilfe von digitalen Tools zu visualisieren. Knotenpunkte wie Personen, Handelsbeziehungen, etc. sollen bei der Analyse im Fokus stehen und die ursprünglich aufgeworfenen Fragen ergänzend bereichern.
            
            
               Diese Forschungsarbeit und Fragestellungen bieten den Rahmen für die nähere Betrachtung der Möglichkeiten der historischen Netzwerkanalyse mit der Open Software "Gephi". Die zahlreichen Tutorials 
               
                  (https://gephi.org/users/),
               
                die aktive Nutzercommunity 
               
                  (https://gephi.wordpress.com/),
               
                die intuitive Nutzermaske und die vielfältigen Projekten, die bereits umgesetzt wurden, sind nur einige der Gründe, die für die Anwendung von „Gephi“ sprechen. Diese und andere Entscheidungskriterien, die in diesem Beispiel zur Wahl von „Gephi“ geführt haben, werden im Posterbeitrag veranschaulicht.
            
         
         
            Zweck des Posters
            
               Der Beitrag soll einen Überblick zu den existierenden Angeboten von Netzwerkanalysen geben sowie Vor-und Nachteile zur Diskussion stellen. Am Beispiel des eigenen Promotionsvorhabens werden Anregungen gegeben, Schritte erläutert und der Entscheidungsprozess begleitet. Das Ziel ist es aus der Perspektive eines Anfängers, Möglichkeiten aufzuzeigen wie man anfängliche Herausforderungen meistern kann. Der Beitrag wendet sich an Ein-und Quereinsteiger und soll durch eine konzeptionelle Gestaltung vermitteln, welches Potenzial und welcher Mehrwert in Netzwerkanalysen steckt. Damit stellt dieses Poster zu gleichen Teilen eine Absichtserklärung und einen Erfahrungsbericht dar. 
            
         
      
      
         
            
               Bibliographie
               
                  Bastian M. / Heymann S., Jacomy M. (2009):
                  Gephi: an open source software for exploring and manipulating networks,
                        International AAAI Conference on Weblogs and Social Media. 
                        https://gephi.org/publications/gephi-bastian-feb09.pdf
               
               
                  Düring, Marten. (2017):
                  Historical Network Research. Network Analysis in the Historical Disciplines.
                  
                     http://historicalnetworkresearch.org/.
                  
               
               
                  Düring, Marten / Kerschbaumer, Florian (2016):
                  Quantifizierung und Visualisierung. Anknüpfungspunkte in den Geschichtswissenschaften, 
                        in: 
                        Düring, Marten / Eumann, Ulrich / Stark, Martin / von Keyserlingk, Linda (eds.):
                  Handbuch Historische Netzwerkforschung. Grundlagen und Anwendungen. 
                        Münster: Lit Verlag 31- 44.
                    
               
                  Grandjean, M. (2015):
                  GEPHI – Introduction to Network Analysis and Visualization. 
                        http://www.martingrandjean.ch/gephi-introduction/
                    
               
                  Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (2017):
                  Digital Humanities. Eine Einführung. 
                        Stuttgart: J. B. Metzler.
                    
               
                  Milligan, I. (2015):
                  From Dataverse to Gephi: Network Analysis on our Data, A Step-by-Step Walkthrough. 
                        https://ianmilligan.ca/2015/12/11/from-dataverse-to-gephi-network-analysis-on-our-data/
                    
               
                  Scott, John. 
                  What Is Social Network Analysis? 
                        London: Bloomsbury Academic, 2013.
                    
            
         
      
   



      
         
            Einleitung
            In diesem Beitrag wird ein Verfahren vorgestellt, das Netzwerkvisualisierungen dramatischer Texte für eine spezifische Form der kommunikativen Interaktion zwischen Figuren fokussiert.
            Es wird gezeigt, inwiefern gewichtete, gerichtete und dynamische Figurennetzwerke narrative Informationsvermittlung in der Figurenrede visualisieren können und auf diesem Weg dramennarratologische Analysen bzw. Annotationen ausgewertet werden.
            Im Gegensatz zu literaturwissenschaftlichen Netzwerkanalysen, die um die automatisierte Analyse 
                    des „kompositorische[n] Grundgerüst[s]“ 
                    
                    (Trilcke 2013: 224) von großen Dramenkorpora 
                    
                    (Piper et al 2017; Trilcke et al. 2015) bemüht sind, steht in diesem Beitrag also die Visualisierung von manuellen Annotationen im Vordergrund.
                
            Darüber hinaus werden mit Rückgriff auf die ermittelten Netzwerkdaten Deutungspotenziale exemplarisch an Kleists 
                    Die Familie Schroffenstein (DFS) diskutiert.1 Das Erkenntnisinteresse zielt also auf zwei Aspekte: (1) Inwiefern lassen sich narrative Redebeiträge, die ein zentrales Element der inneren und äußeren Informationsvermittlung im Drama (Pfister 2001: 20-22) sind, durch Annotationen netzwerkgraphisch visualisieren? (2) Inwiefern stellt die literaturwissenschaftliche Netzwerkanalyse in diesem Kontext einen Mehrwert dar?
                
         
         
            Annotation narrativer Figurenrede
            Ausgangspunkt der vorgestellten Netzwerke ist eine Typologie narrativer Figurenrede bzw. von Binnenerzählungen, die zur Annotation der Dramen Heinrich von Kleists genutzt wurden.2 Dabei wurden über 800 Vorkommnisse narrativer Figurenrede in den Dramen manuell annotiert. In einem ersten Schritt unterscheidet die Typologie zwischen narrativen Äußerungen, mit denen Figuren über ihre eigene Wirklichkeit erzählen, und narrativer Figurenrede, bei der das nicht der Fall ist. Der erste Phänomentyp, die horizontalen Binnenerzählungen, können mit dem narratologischen Kategorieninventar zur Beschreibung anachronen Erzählens (Lahn & Meister 2008: 138-141) genauer beschrieben und annotiert werden:3
            
            
               
                  Binnenerzählungen
               
               
                  Horizontal
                  Analepsen
                  488
               
               
                  
                  Simullepsen
                  123
               
               
                  
                  Prolepsen
                  33
               
               
                  Vertikal
                  Pseudoanalepsen
                  33
               
               
                  
                  Pseudosimullepsen
                  6
               
               
                  
                  Pseudoprolepse
                  29
               
            
            Tabelle 1: Vorkommen narrativer Figurenrede in Kleists Dramen
            Diese manuellen Annotationen sind die Grundlage dafür, dass unterschiedliche Formen der Informationsvermittlung netzwerkgraphisch visualisiert werden können.
         
         
            Netzwerkerstellung
            Unter Rückgriff auf die Annotationen der narrativen Figurenrede und die TEI-Annotationen von Sprecherfiguren und Szenen- sowie Aktwechseln im TextGrid-Korpus wurden chronologisierte Sender-Adressaten-Kanten erstellt. Dazu wurden die vier Sprecherfiguren, die auf eine narrative Äußerung folgen oder ihr vorangehen, als Adressaten berücksichtigt, sofern keine Akt- oder Szenenwechsel zwischen narrativer Figurenrede und potenziellem Adressaten liegt und es sich um unterschiedliche Figuren handelt. Die Anzahl der erzeugten Kanten ist somit deutlich höher als die Anzahl der narrativen Äußerungen (siehe exemplarisch in Tabelle 2 die Kanten 4 und 5 sowie 8-10). 
            
               
                  Id
                  
                     Source (Sprecher)
                  
                  
                     Target (Adressat)
                  
                  
                     Label (Akt)
                  
                  
                     Timeset (Beginn & Ende)
                  
                  
                     Weight
                  
               
               
                  1
                  Rupert
                  Eustache
                  1
                  
                     ""
                  
                  1
               
               
                  2
                  Rupert
                  Jeronimus
                  1
                  
                     ""
                  
                  1
               
               
                  3
                  Rupert
                  Ottokar
                  1
                  
                     ""
                  
                  1
               
               
                  4
                  Jeronimus
                  Ottokar
                  1
                  
                     ""
                  
                  1
               
               
                  5
                  Jeronimus
                  Ottokar
                  1
                  
                     ""
                  
                  2
               
               
                  6
                  Ottokar
                  Jeronimus
                  1
                  
                     ""
                  
                  1
               
               
                  7
                  Jeronimus
                  Kirchenvogt
                  1
                  
                     ""
                  
                  1
               
               
                  8
                  Kirchenvogt
                  Jeronimus
                  1
                  
                     ""
                  
                  1
               
               
                  9
                  Kirchenvogt
                  Jeronimus
                  1
                  
                     ""
                  
                  2
               
               
                  10
                  Kirchenvogt
                  Jeronimus
                  1
                  
                     ""
                  
                  3
               
            
            Tabelle 2: Auszug aus der Kantenliste zu DFS
            Das Kantengewicht 
                (Weight) zwischen einer Sender- und einer Adressatenfigur steigt mit jeder narrativen Äußerung, die eine Senderfigur im ‚Beisein‘ derselben Adressatenfigur äußert (siehe z.B. Kante 4 und 5). 
                
            Die Figurennetzwerke, die auf dieser Grundlage erstellt werden, illustrieren, welche Figuren sich zu welchem Zeitpunkt des Dramenverlaufs narrativ äußern, welche Figuren narrative Informationen bekommen und wie häufig Figuren an narrativem Informationsaustausch beteiligt sind.
         
         
            Visualisierungs- und Analysebeispiele
            Abbildung 1 zeigt dieses Potential der Netzwerkvisualisierung exemplarisch für 
                    Die Familie Schroffenstein.4
            
            
               
               
                  Abbildung 1.
                        Narrative Informationsvermittlung in DFS
               
            
            Die Größe der Knotenbeschriftung repräsentiert hier die betweenness centrality5 der Figuren und damit den Stellenwert bzw. Einfluss der Figur auf die narrative Informationsvermittlung innerhalb des Dramas.
                
            
               
                  Label
                  
                     betweenness centrality
                  
                  
                     weighted indegree
                  
                  
                     weighted outdegree
                  
                  indegree
                  outdegree
                  degree
               
               
                  Jeronimus
                  125,45
                  99
                  82
                  25
                  28
                  53
               
               
                  Gertrude
                  98,74
                  46
                  70
                  15
                  15
                  30
               
               
                  Sylvester
                  85,37
                  107
                  31
                  29
                  14
                  43
               
               
                  Rupert
                  74,47
                  145
                  17
                  36
                  10
                  46
               
               
                  Agnes
                  45,90
                  59
                  85
                  18
                  23
                  41
               
               
                  Eustache
                  17,25
                  37
                  73
                  14
                  19
                  33
               
               
                  Ottokar
                  5,11
                  180
                  51
                  33
                  14
                  47
               
               
                  Ursula
                  3,31
                  1
                  4
                  1
                  4
                  5
               
               
                  Santing
                  2,94
                  10
                  49
                  6
                  12
                  18
               
               
                  Johann
                  2,35
                  4
                  106
                  3
                  15
                  18
               
               
                  Kirchenvogt
                  0,00
                  1
                  80
                  1
                  14
                  15
               
               
                  Ein Diener
                  0,00
                  2
                  2
                  2
                  2
                  4
               
               
                  Sylvius
                  0,00
                  7
                  3
                  4
                  2
                  6
               
               
                  Gärtner
                  0,00
                  2
                  0
                  2
                  0
                  2
               
               
                  Aldöbern
                  0,00
                  1
                  0
                  1
                  0
                  1
               
               
                  Theistiner
                  0,00
                  5
                  15
                  4
                  5
                  9
               
               
                  Der Wanderer
                  0,00
                  0
                  1
                  0
                  1
                  1
               
               
                  Zweiter Wanderer
                  0,00
                  0
                  4
                  0
                  3
                  3
               
               
                  Die Kammerzofe
                  0,00
                  2
                  13
                  2
                  6
                  8
               
               
                  Barnabe
                  0,00
                  1
                  21
                  1
                  8
                  9
               
               
                  Ein Ritter
                  0,00
                  0
                  2
                  0
                  2
                  2
               
            
            Tabelle 3: Betweenness centrality, (weighted) in- und outdegree in DFS
            Schon anhand dieses Beispiels und der netzwerkmetrischen Daten in Tabelle 3 lassen sich einige Vorzüge einer netzwerkgraphischen Annotationsauswertung zeigen:
            
               Die Reichweite einer Figurenerzählung wird im Hinblick auf den Adressatenkreis ermittelt.
               Die narrativen Funktionen der Figuren werden durch ihre Netzwerkeigenschaften erkennbar:
                        
                     
                        Brückenfiguren: hohe betweenness centrality; Verbindung getrennter Netzwerkbereiche 
                                (Trilcke 2013: 217): z.B. Jeronimus (siehe hier und nachfolgend Tabelle 2).
                            
                     
                        Botenfiguren: Geringe betweenness centrality; mind. eine narrative Äußerung: 
                                
                           
                              Botenfiguren i.e.S., die 
                                        nach dem ersten Akt erzählend in Erscheinung treten: z.B. die Wanderer, der Ritter und Barnabe.
                                    
                           
                              Expositionsfiguren, die 
                                        im ersten Akt/Dramenteil erzählend in Erscheinung treten: z.B. der Kirchenvogt.6
                           
                        
                     
                     
                        Zielfiguren: hohe betweenness centrality; hoher gewichteter Eingangsgrad; geringer gewichteter Ausgangsgrad: z.B. Sylvester und Rupert, die häufig die Adressaten, aber selten die Sprecher narrativer Figurenrede sind. (Die Handlung des hier gewählten Beispieltexts legt die These nahe, dass diese Figuren entscheidungsmächtige Figuren sind und daher zahlreiche Informationen bekommen.)
                            
                     
                        Figuren der Informationskontrolle: hohe betweenness centrality; sehr hoher Ausgangsgrad; Netzwerke mit geringer Kantendichte: z.B. Hermann in Kleists 
                                Hermannsschlacht.
                        7
                     
                  
               
               Es lassen sich Figurenpaare und Netzwerkbereiche identifizieren, zwischen denen es keinen oder nur vermittelten Informationsaustausch gibt. Hier sind natürlich Dyaden besonders interessant, bei denen die beiden Figuren eine hohe betweenness centrality aufweisen: z.B. Rupert und Sylvester.
               Die Informationsstrukturen geben Aufschluss über den allgemeinen Grad der Informiertheit der Figuren: z.B. die Kantendichte.8
               
            
            Zudem können unterschiedliche Formen der narrativen Figurenrede netzwerkgraphisch miteinander verglichen werden. Die Abbildungen 3 und 4 zeigen dies exemplarisch. In Abbildung 3 werden Figurenerzählungen visualisiert, in denen sich Figuren in Übereinstimmung mit der fiktionalen Wirklichkeit äußern. Abbildung 4 zeigt narrative Äußerungen, bei denen das Gegenteil der Fall ist. Es handelt sich also um narrative Falschaussagen. Der Vergleich ist in diesem Fall aufgrund der vorhandenen Parallelen 
                    und Unterschiede aufschlussreich. Bei beiden Netzwerken behält Jeronimus die zentrale Position im Netzwerk. Hier schlägt sich nieder, dass er für die Verbreitung von wirklichkeitsgemäßen Informationen ebenso verantwortlich ist, wie für die Verbreitung von falschen Verdächtigungen, Lügen und Vorurteilen. Agnes‘ Netzwerkposition verändert sich hingegen stark (Abb. 4). Sie ist innerhalb ihrer Familie und in der kommunikativen Interaktion mit Ottokar die zentrale Figur bei der Weitergabe falscher Informationen.
                
         
         
            Informationsvermittlung im Dramenverlauf: Dynamische Netzwerke
            Wie 
                    
                    Agarwal et al. (2012: 94) gezeigt haben, hat die Erstellung von dynamischen Netzwerken den Vorteil, die Veränderlichkeit der netzwerkmetrischen Eigenschaften einer Figur, einer Figurengruppe oder eines gesamten Netzwerks im Verlauf eines Roman- oder Dramengeschehens berücksichtigen zu können. Das bestätigen die netzwerkmetrischen Auswertungen der 
                    Familie Schroffenstein in Abbildung 2. Hier wird der gewichtete Ausgangsgrad für vier ausgewählte Figuren aktweise dokumentiert. So tritt der Kirchenvogt narrativ nur im ersten Akt in Erscheinung, was seine Funktion als Expositionsfigur unterstreicht. Auch Barnabes Funktion als Vermittlerin von Anagnorisis-Informationen zum Dramenende spiegelt sich wider. Jeronimus Bedeutung relativiert sich, weil ersichtlich wird, dass er – aufgrund seiner Ermordung am Ende des dritten Akts – nur in den ersten drei Akten als informationsvermittelnde Figur auftritt. Seine hohen Werte in Akt zwei und drei zeigen jedoch, dass er für den Handlungsverlauf entscheidende Informationen äußert. Bei Rupert bestätigt sich seine geringe narrative Aktivität (Tabelle 3) als ein relativ konstantes Verhalten. Der niedrige gewichtete Ausgangsgrad für das gesamte Drama ist also nicht darauf zurückzuführen, dass er nur in wenigen Szenen (erzählerisch) in Erscheinung tritt. 
                
            
               
               
                  Abbildung 2.
                        Gewichteter Ausgangsgrad im Dramenverlauf in DFS
               
            
         
         
            Schluss
            Solange die automatische Annotation narrativer Figurenrede nicht möglich ist, setzt das vorgestellte Verfahren einen relativ großen Annotationsaufwand voraus. Es ermöglicht somit keinen umfassenden Vergleich von Dramen, was unter anderem zur Einordnung der vorgestellten quantitativen Netzwerkanalysen wünschenswert wäre.
            In diesem Beitrag wurde jedoch exemplarisch gezeigt, inwiefern netzwerkgraphische Visualisierungen für die Auswertung narratologischer Annotationen einen analytischen Mehrwert haben können. Die formalen Annotationen können und sollen durch inhaltsbezogene Annotationen angereichert werden. Auf dieser Grundlage könnte netzwerkgraphisch der Informationsaustausch über bestimmte Themen oder Figuren visualisiert werden.
         
         
            Anhang
            
               
               
                  Abbildung 3. Narrative Informationsvermittlung (Horizontale Binnenerzählungen/Wirklichkeitserzählungen) in DFS 
            
            
               
               
                   Abbildung 4.
                        Narrative Informationsvermittlung (Pseudoanalepsen/Falschaussagen) in DFS
               
            
            
               
                  Label
                  
                     betweenness centrality
                  
                  
                     weighted indegree
                  
                  
                     weighted outdegree
                  
                  indegree
                  outdegree
               
               
                  Hermann
                  482,7
                  66
                  47
                  36
                  27
               
               
                  Thuiskomar
                  107,25
                  5
                  10
                  5
                  6
               
               
                  Thusnelda
                  69,62
                  28
                  25
                  13
                  9
               
               
                  Ventidius
                  61,95
                  7
                  27
                  5
                  14
               
               
                  Varus
                  57,8
                  10
                  9
                  6
                  8
               
               
                  Dagobert
                  27
                  3
                  3
                  2
                  3
               
               
                  Zweite Hauptmann
                  21
                  2
                  1
                  2
                  1
               
               
                  Gertrud
                  20
                  7
                  6
                  4
                  3
               
               
                  Marbod
                  16,2
                  11
                  5
                  7
                  4
               
               
                  Wolf
                  11,96
                  5
                  4
                  5
                  4
               
               
                  Aristan
                  8,5
                  1
                  5
                  1
                  4
               
               
                  Rinold
                  6
                  1
                  5
                  1
                  4
               
               
                  Erste Cherusker
                  6
                  1
                  5
                  1
                  4
               
               
                  Der zweite Cherusker
                  5,2
                  1
                  2
                  1
                  2
               
               
                  Gueltar
                  5
                  1
                  2
                  1
                  2
               
               
                  Der Mann
                  4
                  1
                  3
                  1
                  3
               
               
                  Das Volk
                  2
                  2
                  1
                  2
                  1
               
               
                  Erste Älteste
                  0,5
                  1
                  1
                  1
                  1
               
               
                  Scäpio
                  0,33
                  1
                  5
                  1
                  4
               
            
            Tabelle 4: Figuren mit höchster betweenness centrality in Kleist Hermannsschlacht
            
         
      
      
         
             Textgrundlage der Annotationen war Kleists Werkausgabe von Siegfried Streller, die durch das TextGrid Repositorium digital zur Verfügung steht: https://textgrid.de/en/digitale-bibliothek.
             Dazu wurde das Annotationstool CATMA (Meister et al. 2016) verwendet. CATMA bietet die Möglichkeit, mit selbstdefinierten literaturwissenschaftlichen Analysetaxonomien zu annotieren und ist damit für narratologische Forschungsprozesse besonders geeignet.
             Grundlegend für die Annotation ist ein Narrativitätskonzept, das berücksichtigt, dass Texte aller Gattungen narrative Elemente enthalten können, wie es u.a. Wolf (2002) beschreibt. Zu der narratologischen Terminologie vgl. Lahn/Meister 2016: 147-149.
             Alle Visualisierungen und Netzwerkanalysen wurden mit dem Tool GEPHI (Bastian et al. 2008) erstellt.
             Mit der betweenness centrality wird gemessen, für wieviele Knotenpaare ein Knoten den kürzesten Netzwerkpfad darstellt. Eine hohe betweenness centrality in Netzwerken, die Informationsflüsse abbilden, indiziert also einen großen Einfluss der Figur auf die Informationsvermittlung im Netzwerk, da sie als Brückenfigur fungiert.
             Vgl. zum Unterschied Pfister 2001: 280f.
             Kantendichte der Hermannsschlacht: 0,051; Kantendichte Die Familie Schroffenstein: 0,388. Siehe zur Hermannsschlacht Tabelle 4 im Anhang, in der sich Hermanns propagandistische „Überzeugungsarbeit“ (Müller-Salget 2009: 78) widerspiegelt.
             Hier ist zu berücksichtigen, dass die Kantendichte natürlich auch durch andere Faktoren beeinflusst wird (Trilcke 2013: 225).
         
         
            
               Bibliographie
               
                  
                  Agarwal, A. / A. Corvalan / J. Jensen / O. Rambow (2012):
                  Social Network Analysis of Alice in Wonderland, 
                        Proceedings of the Workshop on Computational Linguistics for Literature: 88–96.
                    
               
                  
                  Bastian, M. / S. Heymann / M. Jacomy (2008):
                  Gephi: An open source sofware for exploring and manipulating networks. 
                        AVI 2008 – Proceedings of the Working Conference on Advanced Visual Interfaces.
                    
               
                  
                  Lahn, Silke/ J. C. Meister (2008):
                  Einführung in die Erzähltextanalyse. Stuttgart: Verlag J.B. Metzler.
                    
               
                  Meister, J. C. / M. Petris / E. Gius / J. Jacke (2016): 
                  CATMA 5.0. software for text annotation and analysis.
                    
               
                  
                  Moretti, F. (2011):
                  Network Theory, Plot Analysis. 
                        Stanford Literary Lab Pamphlets 2.
                    
               
                  
                  Müller-Salget, Klaus (2009):
                  Die Herrmannsschlacht, 
                        in: 
                        Ingo Breuer (Hg.):
                  Kleist-Handbuch. Leben – Werk – Wirkung. 
                        Stuttgart: Verlag J.B. Metzler. S. 76-79.
                    
               
                  Piper, Andrew / Mark Algee-Hewitt / Koustuv Sinha / Derek Ruths / Hardik Vala (2017):
                  Studying Literary Characters and Character Networks.
                        Digital Humanities 2017, Conference Abstracts.
                    
               
                  
                  Pfister, Manfred (2001):
                  Das Drama. 
                        München. Wilhelm Fink Verlag.
                    
               
                  Trilcke, P. / F. Fischer / D. Kampkaspar (2015):
                  Digital Network Analysis of Dramatic Texts. 
                        Digital Humanities 2015: Book of Abstracts.
                    
               
                  
                  Trilcke, Peer (2013):
                  Social Network Analysis (SNA) als Methode einer textempirischen Literaturwissenschaft. 
                        Empirie in der Literaturwissenschaft. Hrsg. von Christoph Rauen, Katja Mellmann und Philip Ajouri. Münster: 201–247.
                    
               
                  
                  Wolf, Werner (2002):
                  Das Problem der Narrativität in Literatur, bildender Kunst und Musik: Ein Beitrag zu einer intermedialen Erzähltheorie. 
                        Erzähltheorie transgenerisch, intermedial, interdisziplinär. Hrsg. von Vera Nünning und Ansgar Nünning. Trier: 23–104.
                    
            
         
      
   



      
         
            Abstract
            Die Bedeutung von Social Media in den digitalen Geisteswissenschaften wächst. Nicht nur als Gegenstand der Analyse (z.B. in Gao et al. 2018 oder Reid 2011) sind Social Media für die Digital Humanities von Interesse, sondern auch zunehmend für die Dissemination von Forschungsergebnissen (vgl. Ross 2012). Vor allem in Blogs und Twitter wurde großes Potential für Diskussionen und die Verbreitung von Ergebnissen erkannt (vgl. Puschmann/Bastos 2015, Terras 2012). Auch in der Rezeptionsforschung der Wissenschaftskommunikation zeigt sich, dass Webmedien besonders relevant sind (vgl. Brossard 2013, 14096–14101) und dass diese darum in besonderem Maße zur 
                    „scientific literacy” beitragen könnten (vgl. Schäfer 2017, 283). Generell bietet (informelle) Wissenschaftskommunikation über Webmedien noch viel ungenutztes Potential (vgl. Schäfer 2017, 279–280, Neuberger 2014, Voigt 2012). Unser Beitrag zeigt, wie eine multimediale und multimodale webbasierte Strategie die Dissemination von Digital-Humanities-Methoden unterstützen und die Wissenschaftskommunikation des Forschungsfeldes stärken kann. Die quantitative Analyse der Erfolge dieser Strategie lässt Rückschlüsse darauf zu, welche Methode wem wie vermittelt werden sollte und bildet daher eine wichtige Basis für die Konzeption von Forschungsprojekten und der universitären Lehre.
                
         
         
            Konzeptioneller Rahmen – Multimedialität, Multimodalität und Codierungssysteme in forTEXT
            forTEXT ist ein Vermittlungsprojekt für digitale Methoden der Textanalyse, das sich vor allem an Forschende richtet, die bisher noch nicht mit digitalen Methoden arbeiten (siehe 
                    
                  https://fortext.net).
                Neben der ‘analogen’ Dissemination in Workshops und universitärer Lehre wird auch ein Schwerpunkt auf die online-Vermittlung gelegt, da an wissenschaftlichen Themen Interessierte diese Kanäle häufig als Informationsquelle nutzen (vgl. Brossard 2013, 14098). Die hier vorgestellte webbasierte Strategie als Teil des Disseminationskonzeptes in forTEXT soll darüber hinaus zur DH-Wissenschaftskommunikation beitragen und so die Sichtbarkeit des Forschungsgebiets erhöhen (zur Bedeutung der Geisteswissenschaften in der Wissenschaftskommunikation vgl. Scheu/Volpers 2017). Für die forTEXT-Disseminationsstrategie sind Multimedialität, Multimodalität und multiple Codierungen zentrale Aspekte, die wir wie folgt definieren:
                
            
               Multimedialität: Aufbereitung und/oder Nutzung unterschiedlicher medialer Kanäle. 
                    „Medium” verstehen wir wie Roesler/Stiegler (2005, 150–152) als Vermittlungssystem innerhalb eines Kommunikationsprozesses, bei dem auch das Medium selbst Teil der Vermittlung ist.
                
            
               Multimodalität: Aufbereitung und/oder Nutzung unterschiedlicher Kommunikationsmodi. Dabei verstehen wir 
                    „Modus” als Bezeichnung für eine semiotische Einheit wie z.B. Design oder Sprache (vgl. Bucher 2007, 53).
                
            
               Multiple kulturelle Codierung: Wir übernehmen hier ein semiotisches Verständnis von 
                    „Code
                    ” als Bezeichnung für ein System relevanter Informationseinheiten (vgl. Eco 1985, 58f.). Kulturelle Codes funktionieren als Bedeutungsnetz aus Referenzen auf ein kollektives Wissenskorpus (vgl. Barthes 1976, 25). Um den Begriff klar vom informationstechnologischen (Binär-)Code zu trennen, sprechen wir von Codierung oder Codierungssystem.
                
            Medien, Modi und Codes wirken auf unterschiedlichen Ebenen des Vermittlungsprozesses. Dabei sind Medien und Modi stark miteinander verbunden. Modi können aber als Varianten in andere Medien übertragen werden. Codes sind inhaltliche Elemente, weshalb sie für die Dissemination von Forschungsergebnissen zentral sind. Sie können sich auf einen Modus in einem Medium beziehen oder modi- und medienübergreifend sein: 
            
               
            
         
         
            Arbeitspraxis – die webbasierte Disseminationsstrategie
            
               Die forTEXT-Webseite als Basis medialer Vermittlung von Digital-Humanities-Inhalten
               
                  
               
               Das zentrale Vermittlungsmedium in forTEXT ist die Projektwebseite. Hier werden in Textbeiträgen sowohl Bilder als auch Videos eingebettet. Die forTEXT-Webseite bildet die Basis für die multimediale Web-Strategie, da hier grundlegende Modi und kulturelle Codierungen umgesetzt wurden, die in den sozialen Medien erweitert werden. Die primär genutzten Modi und ihre kulturellen Codierungen sind:
               
                  Design: Gedecktes Farbschema und serifenlose Schrift stehen für Schlichtheit und Sachlichkeit. Nur im Logo gibt es verspielte Elemente, die an eine Handschrift erinnern und die Verbindung von Tradition und Modernität vermitteln.
                    
               
                  Sprache: Die Beiträge erfüllen die Ansprüche wissenschaftlichen Schreibens. Die Wissenschaftlichkeit wird durch die technische Funktionalität zum Zitieren unterstützt.
                    
               
                  Stimme: Grundsätzlich ist die Webseite mehrstimmig angelegt, da hier verschiedene Autor*innen schreiben. Alle nutzen einen sachlichen Tonfall und die implizite Leserin wird stets mit formellem 
                        „Sie” angesprochen.
                    
               
                  Bildlichkeit: Die eingebetteten Bilder sind zumeist digitale Repräsentationen der eingesetzten Tools und scheinen als solche zunächst gegenstandsneutral. Allerdings sind die Bilder häufig auch Visualisierungen der in forTEXT durchgeführten Fallstudien, d.h. sie zeigen nicht nur grafische, sondern auch textliche Elemente und verweisen auf die Modellierung eines Forschungsgegenstandes, die bei der Erstellung der Grafik stattgefunden haben muss.
                    
               Die forTEXT-Webseite richtet sich in erster Linie an drei Zielgruppen, die sich für die forTEXT-Disseminationsstrategie als besonders relevant erwiesen haben:
               
                  Studierende – Lernende der DH-Methodik
                  Nachwuchswissenschaftler*innen – Umsetzende der DH-Methodik
                  Digitale Geisteswissenschaftler*innen – Lehrende der DH-Methodik
               
            
            
               Social Media in forTEXT
               Ausgehend von den Inhalten der Webseite, deren Modi und den entsprechenden Codierungen werden drei soziale Medien zur Vermittlung genutzt. Anders als die Webseite sollen die Social-Media-Kanäle jeweils primär eine Zielgruppe erreichen: YouTube vor allem Zielgruppe 1, Pinterest Zielgruppe 2 und Twitter Zielgruppe 3.
               YouTube
               Auf YouTube erstellen wir eigene Inhalte, die die Artikel der Webseite aufgreifen, weiterführen und ergänzen. Es gibt derzeit zwei Inhaltstypen; Fallstudien und Tutorials. Beide können als Open-Educational-Ressources genutzt werden. In methodischen Fallstudien wird zum Beispiel mittels NER verglichen, welche Bedeutung die Hauptfiguren in Goethes 
                        Werther und in Plenzdorfs 
                        neuem Werther haben. Wir erklären, wie die NER-Machine-Learning-Prozesse funktionieren und verlinken sowohl zur Webseite als auch zu forTEXT-Tutorials. In den forTEXT-NER-Tutorials wird in drei kleinen Einheiten die Installation, Anwendung und das Training eines eigenen NER-Modells gelehrt.
                    
               
                  
               
               Design und sprachliche Elemente der forTEXT-YouTube-Videos richten sich nach den Vorgaben der Webseite. Abbildungen der eingesetzten Tools werden ergänzt von piktografischen Animationen. Diese sind zwar schlicht, befördern jedoch Unvoreingenommenheit und Autodidaktik. Dem Vorurteil einer geringeren Technikaffinität weiblicher Menschen begegnen wir mit einem weiblichen Voice-over (vgl. Schelhowe 2000). Dadurch werden Schwellenängste abgebaut und der Eindruck vermittelt, dass Nutzer*innen und digitale Tutorin sich gemeinsam autodidaktisch an die Methoden heranwagen.
               Pinterest
               
                  
               
               Die Anpassbarkeit in Hinblick auf Design, Stimme und Bildlichkeit der Kommunikationsmodi ist bei Pinterest am geringsten. Hier werden überwiegend fremde Artikel 
                        „gepinnt”, die lediglich mit einer kurzen Beschreibung angereichert werden. Auch führt die Besonderheit von Pinterest als Chimäre zwischen sozialem Medium und Suchmaschine dazu, dass die sprachlichen Elemente nicht nur für die menschliche Wahrnehmung, sondern insbesondere technologisch eine Rolle spielen. Primär werden hier DH-Forschende angesprochen, die Pinnwände für Tools (z.B. Stanford-NER, Carto, Gephi, CATMA), einzelne Methoden (z.B. Netzwerkanalyse, Stilometrie, Topic Modeling), Diskussionen und viele andere Themen der Digital Humanities finden.
                    
               Twitter
               Bei Twitter sind Layout und Typografie der Tweets nicht veränderbar. Jedoch wird bei jedem Tweet das forTEXT-Logo angezeigt. Im Gegensatz zu Pinterest ist auf Twitter die Ausgestaltung der Stimme bedeutsam. Hier steht die Kommunikation mit der eigenen Forschungscommunity im Vordergrund. Die Beschränktheit der Tweets auf 280 Zeichen führt dazu, dass eher Fachbegriffe als Umschreibungen genutzt werden. Hashtags führen zu Themen, die für die Community bedeutsam sind und folgen einem kulturellen Sprachcode. Hier nimmt forTEXT an kollegialen Insider-Gesprächen Teil und betont die forschungsrelevante Seite des Projektes.
            
         
         
            Quantitative Analyse
            Die webbasierte forTEXT-Disseminationsstrategie wird regelmäßig quantitativ ausgewertet. Zusätzlich zur kontinuierlichen Steigerung von Aufmerksamkeit für das Projekt (quantitativ messbar durch Impressionen, Interaktionen, Betrachtungszeiten), werden auch Analysen durchgeführt, die eher konzeptionelle Aspekte der webbasierten Dissemination von Forschungsmethoden in den Fokus rücken. Auf Basis dieser Analysen entwickeln wir eigene Relevanzmetriken, die neben quantitativen auch qualitative Aspekte berücksichtigen, wie bspw. demografische Daten, die anzeigen, welche Zielgruppen über welche Medien, welche Modi und welche Codes tatsächlich erreicht werden können, aber auch Kommentare, Feedback und Interaktionen mit anderen Nutzer*innen.
            Zum jetzigen Zeitpunkt läuft die forTEXT-Social-Media-Arbeit seit drei Monaten. Auf allen medialen Kanälen zeigt sich bereits eine steigende Aufmerksamkeit, auch wenn die Zahlenwerte nach Medium stark differieren. Twitter erzielt mit durchschnittlich 20.000 Impressionen im Monat quantitativ die größte Reichweite. Auch die Interaktionsrate ist mit bis zu 7,4% relativ hoch – die Zielgruppe 3 kann hier sehr gut erreicht werden. Mit Pinterest konnten in den ersten drei Monaten durchschnittlich 1.737 monatliche Impressionen erreicht werden, wobei die einzelnen Monate mit 780 Betrachtern im ersten Monat und 9.300 Betrachtern im dritten Monat stark schwanken. Hier zeigt sich, dass die mit maschinellem Lernen verknüpfte Suchmaschine Pinterest länger braucht, um Inhalte und Interessierte zusammen zu bringen. Neue Inhalte müssen regelmäßig und vergleichsweise hochfrequent (derzeit fünf tägliche Pins) verlinkt werden, damit die Pinterest-Algorithmen ein Profil einzuordnen lernen und anderen Nutzer*innen empfehlen. Eine Einsicht aus der Analyse der Pinterest-Daten ist, dass hier insbesondere Nutzerinnen erreicht werden können. Die Vermittlung von forTEXT-Inhalten über YouTube läuft derzeit erst etwa einen Monat, sodass die Zahlenwerte (140 Impressionen im September) noch relativ gering sind. Qualitative Rückmeldung zeigt aber, dass die Videos bisher vor allem im Rahmen der DH-Lehre auf Interesse stoßen.
            Bereits zu diesem frühen Zeitpunkt zeigt die Fallstudie des forTEXT-Projektes, welche Aspekte einer multimedialen, multimodalen und multipel codierten Vermittlungsstrategie sich als produktiv erweisen. Die Vorannahme, dass auf Twitter vor allem die eigene Community erreichbar ist, hat sich bestätigt. Hingegen deutet der Gender-Gap auf Pinterest an, dass hier weniger Zielgruppe 2, sondern eher Zielgruppe 1 erreicht werden kann, da vor allem die Zielgruppe der Studierenden geisteswissenschaftlicher Fächer meist überwiegend weiblich ist. Aus Feedback zu den forTEXT-YouTube-Videos konnten wir erfahren, dass diese derzeit vor allem für Lehrende von Interesse sind. Neben den vor allem im Marketing üblichen Relevanzkriterien von Impressionen, Engagement und Interaktion (die auch für die wissenschaftliche Impactmessung fruchtbar gemacht werden können, vgl. Herb/Beucke 2013) ist für die Vermittlung von DH-Methoden daher die tatsächlich erreichte Zielgruppe und deren Nutzungsmotivation relevant. So kann forTEXT am Ende nicht nur selbst Social Media produktiv nutzen, sondern auch aufzeigen, welche Medien für welche Ziele der Vermittlung von DH-Methoden besonders bedeutend sind.
         
      
      
         
            
               Bibliographie
               
                  Barthes, Roland (1976): 
                  S/Z. Frankfurt am Main: Suhrkamp.
                    
               
                  Brossard, Dominique (2013): 
                  New media landscape and the science information consumer, 
                        in: PNAS 110 (3), 14096–14101. 
                        
                     https://www.pnas.org/content/pnas/110/Supplement_3/14096.full.pdf
                  , [Zugriff 21.12.2018]. 
                    
               
                  Bucher, Hans-Jürgen (2007): 
                  Textdesign und Multimodalität. Zur Semantik und Pragmatik medialer Gestaltungsformen, 
                        in: 
                        Roth, Kersten Sven / Spitzmüller, Jürgen (eds.):
                  Textdesign und Textwirkung in der massenmedialen Kommunikation. 
                        Konstanz: UVK, 49–76. 
                    
               
                  Eco, Umberto (1985): 
                  Einführung in die Semiotik. 
                        München: Fink.
                    
               
                  Herb, Ulrich / Beucke, Daniel (2013): 
                  Die Zukunft der Impact-Messung. Social Media, Nutzung und Zitate im World Wide Web, 
                        in: Wissenschaftsmanagement. Zeitschrift für Innovation 19 (4), 22–25. 
                        
                     https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/23789/1/Die_Zukunft_der_Impact_Messung_fuer_Reps_fertig.pdf
                   [Zugriff: 21.12.2018].
                    
               
                  Gao, Jin / Nyhan, Julianne / Duke-Williams, Oliver / Mahony, Simon (2018): 
                  Visualising The Digital Humanities Community: A Comparison Study Between Citation Network And Social Network, 
                        in: Digital Humanities 2018. Book of Abstracts. Puentes-Bridges.
                    
               
                  Neuberger, Christian (2014): 
                  Social Media in der Wissenschaftsöffentlichkeit. Forschungsstand und Empfehlungen, 
                        in: 
                        Weingart, Peter / Schulz, Patricia (eds.): 
                  Wissen – Nachricht – Sensation. Zur Kommunikation zwischen Wissenschaft, Medien und Öffentlichkeit. 
                        Weilerswist: Velbrück, 315–368.
                    
               
                  Puschmann, Cornelius / Bastos, Marco (2015):
                  How Digital Are the Digital Humanities? An Analysis of Two Scholarly Blogging Platforms, 
                        in: PLOS ONE 10 (2): e0115035.
                        
                     https://doi.org/10.1371/journal.pone.0115035
                  
                   [Zugriff 2.10.2018].
               
               
                  Reid, Alexander (2011):
                  Social Media Assemblages in Digital Humanities: from Backchannel to Buzz, 
                        in: 
                        Wankel, Charles (ed.): 
                  Teaching Arts and Science with the New Social Media. 
                        West Yorkshire: Emerald Publishing, 321–338.
                    
               
                  Roesler, Alexander / Stiegler, Bernd (eds. 2005): 
                  Grundbegriffe der Medientheorie. 
                        Paderborn: UTB.
                    
               
                  Ross, Claire (2012): 
                  Social media for digital humanities and community engagement, 
                        in: 
                        Warwick, Claire / Terras, Melissa / Nyhann, Julianne (eds.): 
                  Digital Humanities in Practice. 
                        London: Facet Publishing.
                    
               
                  Schäfer, Mike S. (2017): 
                  Wissenschaftskommunikation online, 
                        in: 
                        Bonfadelli, Heinz et al. (eds.): 
                  Forschungsfeld Wissenschaftskommunikation. Wiesbaden: Springer. 
                        
                     https://link.springer.com/content/pdf/10.1007%2F978-3-658-12898-2.pdf
                   [Zugriff 21.12.2018].
                    
               
                  Schelhowe, Heidi (2000): 
                  Informatik, 
                        in: 
                        Braun, Christina von / Stephan, Inge (eds.): 
                  Gender-Studien. Eine Einführung. Stuttgart, Weimar: Metzler, 207–216.
                    
               
                  Scheu, Andreas M. / Volpers, Anna Maria (2017): 
                  Sozial- und Geisteswissenschaften im öffentlichen Diskurs, 
                        in: 
                        Bonfadelli, Heinz et al. (eds.): 
                  Forschungsfeld Wissenschaftskommunikation. Wiesbaden: Springer. 
                        
                     https://link.springer.com/content/pdf/10.1007%2F978-3-658-12898-2.pdf
                   [Zugriff 21.12.2018].
                    
               
                  Terras, Melissa (2012):
                  The Impact of Social Media on the Dissemination of Research: Results of an Experiment, 
                        in: Journal of Digital Humanities 1 (3).
                        
                     http://journalofdigitalhumanities.org/1-3/the-impact-of-social-media-on-the-dissemination-of-research-by-melissa-terras/
                  
                   [Zugriff 2.10.2018].
               
               
                  Voigt, Kristin (2012): 
                  Informelle Wissenschaftskommunikation und Social Media. 
                        Berlin: Frank & Timme.
                    
            
         
      
   



      
         Bevor Kulturerbe digital verfügbar und nachnutzbar ist, stehen komplexe Prozesse an, die als spezifische „invisible work“ (Star/Strauss 1999) bezeichnet werden können. Diese werden kaum außerhalb der engeren Community problematisiert: Wo finde ich in den analogen Katalogen, Zettelkästen, Findbüchern u.ä. die notwendigen Informationen zur Erfassung und Erschließung? Welche Sammlungsbestände sind besonders wichtig und deshalb zu digitalisieren? Ist eine Massen-Digitalisierung mit geringer Detailtiefe der Erfassung oder eine detaillierte Erschließung einiger Teilbestände sinnvoll? Wie können die Vorgehensweisen von unterschiedlichen Personen und Institutionen vereinheitlicht werden? Und wie können Informationen, die bereits digital vorliegen – etwa in Tabellen oder älteren Erfassungssystemen – an internationale Regelwerke und Standards zur Metadatenhaltung angepasst werden?
         Kernanliegen des vorgeschlagenen Beitrags ist es, für die Sammlungserschließung und Digitalisierung – gerade in Museen, aber auch in Archiven und Bibliotheken – Methoden, Tools und Analyseperspektiven der Digital Humanities stärker als bisher zu nutzen. Dafür werden mögliche Synergien und exemplarische Anwendungen in einer ersten Exploration aufgezeigt. Dabei wird auf eigene Projekterfahrungen zurückgegriffen, die im Kontext des Digitalisierungsvorhabens „Digitales Portal Alltagskulturen im Rheinland“ (2013-2017) gesammelt wurden. Diese sind bereichert um Perspektiven des DH-Forschungsverbundes an der Universität Hamburg, „Automatisierte Modellierung hermeneutischer Prozesse (hermA)“ (2017-2020).
         
         
            What (not) to do with 100.000 Pictures. Sammlungserschließung als Sisyphos-Arbeit?
            Im LVR-Institut für Landeskunde lagern umfangreiche Bestände, die nach Sammlungsgeschichte gegliedert und nur rudimentär erschlossen sind. Vor allem die Fotodokumentationen sind für Erschließung und Digitalisierung prädestiniert, zeigen sie doch plurale Facetten immateriellen Kulturerbes seit etwa 1900. Dazu kommen umfangreiche Materialien zu 31 schriftlichen Befragungen aus den 1970er bis 2000er Jahren sowie kleinere Einzelsammlungen. Die Fotobestände liegen in unterschiedlichen Negativformaten sowie als Dias chronologisch sortiert vor. Dazu sind Fotoabzüge auf Karteikarten geklebt, mit Metadaten versehen und thematisch abgelegt in einer über Jahrzehnte gewachsenen Systematik. Negative und Abzüge sind also unterschiedlich archiviert und nur in Einzelfällen einander zuzuordnen. Es bestehen auf Einzelplatzrechnern gepflegte Bestandslisten sowie eine Datenbank mit Teilinventarisierung. Seit 2013 werden im Rahmen der LVR-weiten Digitalisierungsstrategie Digitalisate erstellt, mit angereichert und Metadaten abgelegt, um sie mittelfristig öffentlich zugänglich zu machen. Die Systeme werden aktuell in die Datenbank digiCULT.web übertragen, wodurch mit dem LIDO-Format größere Datenmengen standardkonform publiziert werden können.
                
            Vergleichbare gewachsene Systeme gibt es in vielen Museen, Archiven und Bibliotheken. Gerade in kleineren Museen und Forschungseinrichtungen, deren Hauptaugenmerk auf der Ausstellung bzw. Forschung und weniger auf der Sammlungserschließung liegt, sind die Erfassungen unvollständig und in der Institutionsgeschichte von verschiedenen Akteuren mit spezifischen Wissenshintergründen und Systematiken erfolgt. Diese Systeme funktionieren vor allem aufgrund des Wissens von Archivar*innen und Forschenden (vgl. zu Wissenskonzepten Koch 2006): Sie wissen, welche Materialien wo liegen, welche Strukturen was auffindbar machen, was in welchen Kontexten verwendet wurde. Wenn Stellenwechsel oder Verrentungen anstehen, geht dieses Wissen verloren oder wird bruchstückhaft weitergegeben.
            Wenn analoge Bestände digitalisiert werden sollen, geht es in der Vorbereitung (z.B. der Antragsstellung für Drittmittel) um hohe technische Qualität, Fragen des Workflows sowie der Bereitstellung, einzuhaltende Standards oder eigene Präsentationsoberflächen. Die konkreten Arbeitsschritte zur Umsetzung dieser Zielvorgaben werden oft erst in der Projektrealisierung entwickelt. Gerade die Museumsdatenbanken sind geprägt von einer gewachsenen Pluralität, die in dieser Form nicht als veröffentlichungswürdig gelten und strukturelle Nachbearbeitungen erfordern.
            Für die hier exemplarisch stehenden Fotobestände des LVR-Instituts fiel die Entscheidung, sich nicht an den bestehenden (Ablage-)Systematiken zu orientieren. Neben anderen Argumenten war dabei die unvollständige und heterogene, thematisch abgelegte Erschließung per Karteikarte ausschlaggebend. Um eine Auswahl zur inhaltlichen Erschließung zu ermöglichen und analoge Vorarbeiten gering zu halten, wurden alle Negative und Dias digitalisiert (zur Problematik von Original und Kopie, die sich für Foto-Abzüge spezifisch stellt, vgl. Schönholz 2017). Im Zuge der Auftragsvergabe wurden Bestände erstmals gezählt und liegen als vollständige Digitalisate vor. Ein Meilenstein dieser Fleißarbeit waren die im Vergleich und Überblick erstmals digital verfügbaren Bildbestände.
            Doch was macht man mit 100.000 Fotos, die außer einer Dateibenennung keinerlei Informationen mit sich bringen? Hier werden die eingangs aufgeworfenen Fragen konkret. Vergleichbar mit den Diskussionen um Distant Reading (Moretti 2007 und 2016; Crane 2006) stellt sich ob der Verfügbarkeit der Digitalisate die Frage, wie diese zielführend erschlossen werden können. Wie findet man relevante Bildbestände für eine Tiefenerschließung? Wo helfen die bestehenden Metadaten zielführend?
         
         
            Maschinelle Unterstützung nutzen, aber wie?
            Für diese Arbeitsschritte sind Methoden der Digital Humanities vielversprechend. Zwar wird zunehmend die Frage nach der Nutzung von Digitalisaten als Open Data für die Wissensproduktion diskutiert, oft jedoch erst nach Abschluss der Erschließungsarbeiten. Nicht erst die publizierten Daten digitalen Kulturerbes können mit DH-Verfahren erforscht werden – diese sind bereits in der Erschließung enorm hilfreich. Zwei Verfahren aus dem Bereich des Machine Learning scheinen besonders erfolgsversprechend: maschinelle Bilderkennung sowie die Analyse bestehender Metadaten mittels Text Mining.
                
            Die großen Mengen unerschlossener Fotos können mit maschineller Bilderkennung hinsichtlich ihrer Ähnlichkeit gruppiert werden, wie es etwa PixPlot realisiert: Bildanordnungen im Vektorraum machen Schwerpunkte des Bestandes deutlich, außerdem lassen sich Subkorpora bilden, die mit Massenbearbeitung formal erschlossen werden können. In der Arbeitspraxis ist daneben das Identifizieren von Dubletten relevant: Wenn beispielsweise Abzüge des Archivs in der Vergangenheit abfotografiert wurden, existiert das Foto in unterschiedlichen Kopien im digitalisierten Bestand – manuell eine Suche nach der Nadel im Heuhaufen, automatisiert mit Bildvergleich gut zu identifizieren (vgl. die vielversprechenden Ansätze bei Schneider 2019). Auch ähnliche Aufnahmen, z.B. aus einer Bildserie, sind so zuzuordnen. Die inhaltliche Erschließung wird durch eine Ähnlichkeitssuche ebenfalls deutlich vereinfacht: Hat man etwa eine gute Aufnahme eines Gegenstandes, so lassen sich relativ eindeutig andere Abbildungen dessen im Bestand finden. Hier wäre jedoch eine menschliche Intervention (zumindest zu Beginn eines möglichen Active Learning-Verfahrens) aufgrund der feinen Unterschiede notwendig. Zudem sind zweifelsfrei die Trainingsdaten von großer Relevanz und sollten wo möglich aus bereits erschlossenen, vergleichbaren Kulturerbe-Datensätzen bestehen.
                
            Text Mining-Verfahren würden in GLAM-Datenbanken nicht nur aus dem Museumsbereich präzisiere Suchabfragen und gerade in bestehenden Erfassungen systematische Funde ermöglichen. Ansätze wie facettierte Suchmasken mit Linked Open Data, wie sie in Plattformen zur Datenpräsentation zunehmend realisiert werden, wären im Backend enorme Arbeitserleichterungen. Schon banale Automatisierungen wie Rechtschreibkorrektur und Vereinheitlichungen der Formalerschließung sind momentan in der Regel nicht vorgesehen. Sie kosten viel Zeit und Konzentration, sobald nicht simple Ersetzungen vorzunehmen sind. Gleichzeitig fehlt den Entwickler*innen der eingesetzten Datenbanken die zielgenaue Kollaboration mit entsprechender DH-Forschung zu konkreten Tools und Verfahren sowie der Testung von verschiedenen Funktionen für eine hohe Qualität der Ergebnisse, die vor der Übernahme in die Infrastruktur erfolgen muss. LOD wird zudem aktuell nur in Ausnahmen direkt in die Erfassungssysteme eingebunden – erst so könnte die Arbeit an Ontologien und konkreten Datensätzen gezielt verbunden werden. Dazu kommt die Notwendigkeit, die Erfassungen besser zu vernetzen; auch in Fällen, in denen dies erst nach der Veröffentlichung möglich oder notwendig wird. Hier sollte eine Öffnung für fortlaufende kollaborative Ergänzung und Korrektur von Daten in Verbindung mit der Erfassung von Paradaten (McIlvain 2013) geschaffen werden.
                
            Viel Zeit wird mit der Erzeugung von metacrap (Doctorow 2001) verbracht. Die messy Metadaten, die für viele Erfassungen – oft im Backend, aber auch publiziert – bestehen, sind durch Tools einfach zu identifizieren und automatisch zu beheben: Museum Analytics beispielsweise ermöglicht es, große Mengen von Museumsdaten zu analysieren, ist allerdings für publizierte Metadaten vorgesehen. Gerade in der Migration zwischen Datenbanksystemen sowie für den internen Gebrauch zwecks Qualitätskontrolle, Bestandssichtung und Entscheidung über Nachbearbeitungen vor der Veröffentlichung erlaubt dieses einen anderen Blick auf die Bestände. Das Tool Breve, das Tabellen visualisiert, könnte ergänzende Funktionen übernehmen. Die Entwicklungsvorhaben des Verbundprojektes GND4C oder des Projekts Qrator sind richtungsweisend, leider aber noch nicht verfügbar, und entsprechende Konferenzworkshops zu DH für Gedächtnisinstitutionen (Döhl/Voges 2019) erfreulich.
                
            Erste Ansätze zur Nutzung von DH-Analyseverfahren werden auch von Museumsseite diskutiert, etwa hinsichtlich Netzwerkanalyse der Sammlungsbestände (Werner 2019) oder Möglichkeiten der Visualisierung (Mayr/Windhager 2019), bilden dort jedoch (noch) die absolute Ausnahme. Dies spiegelt sich auch in den Programmen der entsprechenden Tagungen wie „Museums and the Internet“ oder denen der Fachgruppe Dokumentation des Deutschen Museumsbundes. Falls entsprechende Ansätze bereits genutzt werden, so geschieht dies wiederum weitestgehend als „invisible work“ (s.o.) ohne Darstellung in der (Forschungs)Öffentlichkeit. In weiten Teilen der entsprechenden GLAM-Community wird gerade von Museumsseite die DH noch zu wenig als möglicher Kooperationspartner wahrgenommen, um entsprechende Workflows und Implementierungen zu konzipieren.
                
            Die vorgestellten Zugänge könnten im Ergebnis der Implementierung nicht nur aufzeigen, welche Daten in einem Bestand enthalten sind, sondern auch, welche Leerstellen in der Erfassung noch geschlossen werden sollten. Von einer linearen Durchsicht, Überarbeitung und Freigabe der Datenbank-Einträge kann mit entsprechender Tool-Unterstützung – und einhergehender Interoperabilität! – zu einer gezielten Nachbearbeitung von Teilbeständen oder Vereinheitlichung einzelner Metadatenfelder übergegangen werden. So bleibt mehr Zeit für eine inhaltliche Erschließung und Analyse sowie die dringend notwendige epistemologischen Reflexionen dieser Prozesse.
         
         
            Fazit: DH-Verfahren in Sammlungsdatenbanken
            Was fehlt in der Gesamtschau momentan? Vor allem die Öffnung von Erschließungssystemen für die dargestellten Methoden sowie die Öffnung der entsprechenden Communities zueinander.
            Erforderlich ist dabei der frühzeitige Einbezug von bestehenden Tools und Analyseverfahren – lange vor der Veröffentlichung der Datensätze. Gerade in der Exploration weitestgehend nicht erfasster Bestände zur Vorbereitung der Erschließung und in der Qualitätskontrolle von Metadaten liegen große Potentiale, die noch zu wenig genutzt werden. Wenn gleichzeitig bereits tiefenerschlossene Bestände als Trainingsdaten genutzt werden, können die Ansätze auch unabhängig von der Nutzung konkreter Datenbanken gegenseitigen Mehrwert sowohl in der Methodenentwicklung als auch in der Erschließung bringen.
            Eine öffentliche Finanzierung und Weiterentwicklung von den entsprechenden Tools ist dabei dringend notwendig. Statt weiter in gewinnorientierte Software zu investieren, sollten Genossenschaften und Vereine gegründet und ausgebaut werden. Eine Unterstützung der Toolentwicklung durch entsprechend kompetente DHler*innen ist vielversprechend. So wäre etwa ein Hackathon zur Erweiterung von Erschließungssystemen eine Möglichkeit, um über das Tagesgeschäft hinausgehende Innovationen umzusetzen. Entsprechend erweiterte Datenbanken sollten viel häufiger auch in der Forschung verwendet werden, die aktuell noch viel zu oft in Form von Excel-Sheets (weiter-)arbeitet, obwohl Datenbanken mit erweiterten Funktionen existieren. Mit einfachen Import/Exportfunktionen innerhalb der Tools und Verbindungen zu Analyseverfahren könnten Forschungsumgebungen geschaffen werden, in denen kollaboratives Arbeiten gleichzeitig die Datensätze anreichert und Forschungsfragen beantwortet.
         
      
      
         
            
  Projektergebnisse unter https://alltagskulturen.lvr.de/. Das DFG-geförderte Projekt wurde durch die Autorin koordiniert, von Dagmar Hänel geleitet und maßgeblich auch durch den wissenschaftlichen Dokumentar im Projekt, Christian Baisch, vorangetrieben.

            
  Das von Gertraud Koch und Heike Zinsmeister geleitete Verbundprojekt befragt DH-Perspektiven auf Möglichkeiten zur Modellierung hermeneutischer Prozesse. Vgl.
  .

            
  Vgl. Sammlungsbeschreibungen unter
  .

            
  Faust Software, vgl.
  .

            
  Genutzt wird eine Weiterentwicklung von MediaFiler, vgl.
  .

            
  digiCULT.web als entitätsbasierte Online-Datenbank ist vorrangig für Museumsbestände entwickelt und baut auf CIDOC-CRM auf. Vgl.
  .
    Zu LIDO vgl.
  .

            
  Vgl. etwa die Schwerpunktsetzung „Open Data – now what?“ der Sharing is Caring-Konferenz 2019.
  .
  Danke an Samantha Lutz für den Hinweis.

            
  Vgl. 
  .
  Vgl. auch Leonard 2019.

            
  Vgl. etwa Suchfacetten der DDB,
  , mittlerweile auch der Europeana,
  .

            
               .

            
               .

            
  Ziel 3 des Projektes ist die „Bereitstellung von Schnittstellen und Werkzeugen zur Unterstützung nicht-bibliothekarischer Anwendungskontexte.“ Vgl.
  .
  Danke an Axel Vitzthum für den Hinweis.

            
  Vgl.
  .
  GLAM-Institutionen mit digitalem Kulturerbe sind hier ein Anwendungsfall.

            
  Bisher waren entsprechende Beiträge bei der Tagung absolute Ausnahme. Vgl. das Archiv unter
  https://mai-tagung.lvr.de/de/startseite.html.

            
  Das Archiv der Tagungsprogramme und Vortragsfolien lässt nicht auf entsprechende Diskussionen schließen. Vgl. 
  https://www.museumsdokumentation.de/?lan=de&q=Who%20is%20who/FG%20Dokumentation%20im%20DMB/Tagungsarchiv.

         
         
            
               Bibliographie
               
                  Crane, Gregory (2006): “What Do You Do with a Million Books?” In: D
                        -Lib Magazine 12/3. http://www.dlib.org/dlib/march06/crane/03crane.html.
                    
               
                  Doctorow, Cory (2001): “Metacrap. Putting the Torch to Seven Straw-Men of the Meta-Utopia.” https://people.well.com/user/doctorow/metacrap.htm.
                    
               
                  Döhl, Frédéric / Voges, Ramon (2019): „Erklärt und ausprobiert – Digital Humanities für Gedächtnisinstitutionen.“ Workshop im Rahmen der Tagung 
                        Zugang gestalten 2019. https://zugang-gestalten.org/dokumentation-2019/.
                    
               
                  Koch, Gertraud (2006): „Die Neuerfindung als Wissensgesellschaft. Inklusionen und Exklusionen eines kollektiven Selbstbildes.“ In: Hengartner, Thomas; Moser, Johannes (Hg.): 
                        Grenzen & Differenzen. Zur Macht sozialer und kultureller Grenzziehungen. Leipzig, S. 545–559.
                    
               
                  Leonard, Peter (2019): “Large images dataset overtime: PixPlot new features.” In: 
                        Culture Analytics Workshop: Time Series, Digital Humanities 2019. https://dev.clariah.nl/files/dh2019/boa/1079.html und https://github.com/CultureAnalytics/DH2019.
                    
               
                  Mayr, Eva / Windhager, Florian (2019): „Vor welchem Hintergrund und mit Bezug auf was? Zur polykontexturalen Visualisierung kultureller Sammlungen“. Vortrag im Rahmen der Tagung 
                        Objekte im Netz. Wissenschaftliche Sammlungen im digitalen Zeitalter. Folien unter http://objekte-im-netz.fau.de/projekt/sites/default/files/2019-11/Mayr%26Windhager_PolyContext.pdf.
                    
               
                  McIlvain, Eileen (2013): “Paradata. What is Paradata?” In: 
                        NSDL Documentation Wiki. https://wiki.ucar.edu/display/nsdldocs/Paradata.
                    
               
                  Moretti, Franco (2007): “Graphs, Maps, Trees. Abstract Models for Literary History.” London, New York.
                    
               
                  Moretti, Franco (2016): “Distant Reading.” Göttingen.
                    
               
                  Schneider, Stefanie (2019): „Über die Ungleichheit im Gleichen. Erkennung unterschiedlicher Reproduktionen desselben Objekts in kunsthistorischen Bildbeständen.“ In: 
                        DHd 2019. Digital Humanities im deutschsprachigen Raum 2019. Konferenzabstracts, S. 92–94. https://doi.org/10.5281/zenodo.2596095.
                    
               
                  Schönholz, Christian (2017): „Jede Kopie ein Original!. Aspekte eines kulturellen Größenverhältnisses.“ In: Koch, Gertraud (Hg.): 
                        Digitalisierung. Theorien und Konzepte für die empirische Kulturforschung. Konstanz/München 2017, S. 157–182.
                    
               
                  Star, Susan Leigh / Strauss, Anselm L. (1999): “Layers of Silence, Arenas of Voice. The Ecology of Visible and Invisible Work.” In: 
                        Computer Supported Cooperative Work 8/1-2, S. 9–30.
                    
               
                  Werner, Claus (2019): „Die Sammlung als Graph. Gephi als Tool der Sammlungsevaluation. Deutsches Bergbau-Museum Bochum.“ Vortrag im Rahmen der Tagung 
                        Museums and the Internet (Mai-Tagung) 2019. https://mai-tagung.lvr.de/media/mai_tagung/pdf/2019/MAI-2019-Werner.pdf.
                    
            
         
      
   



      
         
            Die Komponenten: DraCor und forTEXT
            
               DraCor
               Mit ELTeC (European Literary Text Collection; 
https://github.com/COST-ELTeC) und DraCor gibt es mittlerweile zwei europäische Initiativen, die eine korpusbasierte Infrastruktur für die digitalen Literaturwissenschaften aufbauen, wobei sich DraCor (Drama Corpora Project; 
https://dracor.org) der Sammlung TEI-kodierter Dramen in verschiedenen Sprachen widmet (vgl. Fischer u.a. 2019). DraCor liefert über seine API etwa Netzwerkdaten zu Dramen aus, die auf der Kookkurrenz von Sprecherïnnen basieren und es ermöglichen, die Kommunikationsstrukturen mithilfe von Network-Analysis-Metriken zu erforschen. Darüber hinaus bietet die Plattform mit ezlinavis (Easy Literary Network Analysis Visualisation) ein didaktisches Tool an, das den Einstieg in die systematische Erhebung von Netzwerkdaten erleichtert. Außerdem wurde aus DraCor heraus mit dem 
Dramenquartett (vgl. Fig. 1) ein Kartenspiel entwickelt, mit dem das Verständnis von Netzwerkmetriken ebenso wie die typologische und historische Vielfalt von Dramennetzwerken spielerisch entdeckt und erlernt werden kann (vgl. Fischer u.a. 2018 und Fischer/Schultz 2019).

               
                  
                  Figure 1: Beispielkarte aus dem Dramenquartett (Rück- und Vorderseite)
               
            
            
               forTEXT
               
                  
                  Figure 2: Die Startseite des Disseminationsportals forTEXT.net
               
               Das DFG-Projekt 
  forTEXT (https://fortext.net; vgl. Fig. 2) bietet in diesem Zusammenhang einen Methodeneintrag (vgl. Schumacher 2018b), eine (Gephi-)Lerneinheit (vgl. Schumacher 2019b), ein Fallstudien-Video, vier Tutorialvideos sowie praktische Anwendungen in der Lehre. Hierbei sind jeweils unterschiedliche Abstraktionsgrade und verschiedene Arten der Vermittlung abgedeckt: Der Methodeneintrag ist eine abstrakte, sprachlich-theoretische Beschreibung der Methode mit dem Schwerpunkt der Anschlussfähigkeit an die traditionelle Literaturwissenschaft. Die Lerneinheit ist eine konkrete Klick-für-Klick-Einführung für Autodidakten aus der Zielgruppe junger Geisteswissenschaftlerïnnen in Form einer Text-Bild-Kombination.
  
               Die Videos vermitteln die Methode über eine Text-Bild-Audio-Kombination: Das Methodenvideo bietet eine Fallstudie zum Figurennetzwerk von 
  Emilia Galotti. Es vermittelt die Methode an eine autodidaktische Zielgruppe geisteswissenschaftlicher Studentïnnen und wählt einen Einstieg über das ,traditionelle’ Thema, taucht in technisch-theoretische Hintergründe ein und schließt dann wieder an das literarische Thema an. Die textbasierte Thematik wird so auf eine Bildebene überführt, die DH-Tool-Grafiken mit strichmännchenartigen figurativen Darstellungen koppelt (vgl. Fig. 3).
  
               
                  
                  Figure 3: Vorschaubild eines forTEXT-Fallstudien-Videos
               
               Angesprochen werden hier theoretische, strukturelle und emotionale autodidaktische Vermittlungsmuster (zur Bedeutung von Emotionen für autodidaktisches Lernen vgl. Mega u.a. 2014). Auf der Tonebene ist ein erklärender Duktus vorherrschend. Die ,selfmade’-Anmutung der Videos vermittelt, dass die autodidaktische Erarbeitung der Inhalte Betrachterïnnen und Erstellerïnnen des Videos miteinander verbindet (vgl. Horstmann & Schumacher 2019).
               Die Tutorial-Reihe schließlich funktioniert ähnlich wie die Lerneinheit als Schritt-für-Schritt-Anleitung und bietet die Möglichkeit, die Arbeit mit Gephi als Screencast zu erlernen. Das Tutorial-Video zur Nutzung des DraCor-Tools ezlinavis verknüpft die praktische Erstellung von Netzwerken mit der Nutzung der Ressource TextGrid Repository (vgl. Horstmann 2018) und den Methoden Named Entity Recognition (vgl. Schumacher 2018a) und Annotation in CATMA (vgl. Jacke 2018 und Schumacher 2019a).
            
            
               Das Dramenquartett als Erweiterung des Disseminationsmodells in forTEXT
               Die Dissemination einer digitalen Methode wie der Netzwerkanalyse durch ein nicht-digitales Kartenspiel bietet Möglichkeiten, die die bisher genannten digitalen Medien nicht abdecken konnten. Die Spielerïnnen werden in einer nicht-digitalen Umgebung mit den funktional reduzierten Ergebnissen einer digitalen Analyse konfrontiert, können diese visuell und haptisch erfahren und spielerisch explorieren. Der empfohlene Spielmodus ist ,Supertrumpf’, bei dem Werte der Netzwerke verglichen werden. Die Spielregeln sind online veröffentlicht
  (https://dramenquartett.github.io/). Neben einem neuen und vor allem kompetitiven Blick auf Dramen – der die relationale Perspektive auf figürliche Kopräsenzen hervorhebt – wird zusätzlich die Neugier auf die den Netzwerken zugrunde liegende Methode geweckt, sodass in didaktisch-produktiver Hinsicht der Prozess einer Art 
  Reverse Engineering im Sinne einer Mustererkennung auf unterschiedlichen Komplexitätsstufen angestoßen werden kann. Der Weg hin zu einem Umgang mit digitalen Ressourcen und Tools wie DraCor, ezlinavis und sogar die Anwendung eines komplexeren Tools wie Gephi ist damit geebnet, die kritische Methodenreflexion kann folgen. Dieser niedrigschwellige Zugang fügt sich in das auf Zugänglichkeit und Benutzerfreundlichkeit konzentrierte Disseminationsmodell von forTEXT ein und erweitert dieses durch den zusätzlichen Abbau von Schwellenängsten oder Vorbehalten gegen digitale Methoden.
  
               Die im Folgenden vorgestellte, reflektierte und erprobte Pipeline geht von einer ersten theoretischen Annäherung durch forTEXT-Tutorials aus, auf die eine spielerische Vertiefung der spezifischen Objektkonstitution qua Netzwerkanalyse und der entsprechenden Metriken mittels des Dramenquartetts folgt. Anschließende Arbeitsphasen könnten, wie in 3. skizziert, z. B. die formalisierte Erstellung, Gestaltung und Analyse von Dramennetzwerken mittels ezlinavis und Gephi oder die konkrete Bearbeitung von literarhistorischen Forschungsfragen mittels DraCor umfassen.
            
         
         
            Epistemische und didaktische Implikationen
            
               Epistemische Dimensionen des Medienwechsels
               Der quantifizierende Zugriff auf Dramentexte kann als „radikale ,Anästhetisierung’ der Objekte” (Trilcke, im Erscheinen) beschrieben werden. Auf die qua Formalisierung erfolgende Anästhetisierung, bei der die ursprüngliche ästhetische Dimension des literarischen Kunstwerks zunächst ausgesetzt wird, folgt jedoch eine reästhetisierende Transformation im Zuge der Diagrammatisierung (vgl. ebd.). Die Dramen werden somit zunächst zwar nicht mehr primär als textuelle Artefakte wahrgenommen, dennoch aber als ästhetische Artefakte in Form ihrer netzwerkartigen Repräsentation, wodurch andere epistemische Dimensionen angesprochen und andere epistemische Praktiken vollzogen werden können (vgl. Trilcke und Fischer 2018). Dabei ist der Weg zurück zum Dramentext vom Kartenspiel über die digitale Darstellung der entsprechenden Netzwerke dadurch geebnet, dass Medien in Form „transkriptiver Bezugnahmen” (Jäger 2010, 301) generell intermedial aufeinander Bezug nehmen und Übersetzungsprozesse somit keine einseitig vorgegebene Richtung haben.
               
               Ein entscheidender Vorteil digitaler Diagramme ist die Möglichkeit der Interaktion (vgl. Horstmann, im Erscheinen): Netzwerke lassen sich je nach Wahl des Layoutalgorithmus unterschiedlich darstellen, ein semantischer Zoom ermöglicht überdies, zusätzliche Informationen des Ausgangsmaterials zu visualisieren. Dramennetzwerke in einer festgelegten (und damit nicht mehr veränderbaren) Form als Spielkarte zu drucken, bedeutet daher in erster Linie eine funktionale Reduktion. Gerade diese funktionale Reduktion eröffnet jedoch didaktische Spielräume: Das Wissen, dass die abgedruckten Netzwerke ebenfalls in digitaler Form vorhanden und dort sogar manipulierbar sind, wird im Laufe des Spielprozesses die Neugier auf diese Funktionsvielfalt steigern, sodass der Übergang in die ,digitale Arbeit’ fließend stattfinden kann und nicht mehr als etwas kategorial anderes empfunden wird. Die Interaktion zwischen Benutzerïnnen und Netzwerken als konzeptioneller Bestandteil digitaler Netzwerkdarstellungen wird übertragen auf die Interaktion zwischen den Spielerïnnen, wodurch nicht zuletzt die von Jenkins (2006, 2) sog. 
  participatory culture im nicht-digitalen Bereich eine Entsprechung erfährt.
  
            
            
               Ansprechen unterschiedlicher Lerntypen 
               Das Kartenspiel entfaltet seinen didaktischen Mehrwert auch, weil es situational gerahmt ist: Es wird in kollektiven Unterrichtsphasen eingesetzt, die darauf abzielen, sich einem abstrakten Unterrichtsgegenstand auf spielerische Weise anzunähern. Da Menschen in ihrer Rolle als 
  visual beings vor allem ihren Sehsinn als einen wichtigen Wahrnehmungskanal nutzen, um Informationen zu verstehen (vgl. Ward et al. 2010), stellen Visualisierungen bei der Präsentation von wissenschaftlichen Erkenntnissen ein wichtiges, den Verstehens- und Erinnerungsprozess begünstigendes Element dar. Der Einsatz des Kartenspiels greift darauf zurück und spricht unter den vier Lerntypen (auditiv, haptisch, kommunikativ und visuell) v. a. visuelle, aber auch kommunikative Lerntypen an, indem das Spiel die Kommunikation über Fachinhalte fokussiert und Sprache als Medium des Lernens einsetzt (vgl. Anselm und Werani 2017).
  
               Im Fokus steht der Versuch, nicht nur kumulatives bzw. assimilatives Lernen zu initiieren, wodurch v. a. begrenztes, anwendungsorientiertes Wissen oder thematisch, anwendungsorientiertes Wissen produziert werden würde (vgl. Illeris 2010). Die – von der konkreten Kenntnis des Spielprinzips ,Supertrumpf‘ unabhängige – spielerische Aktivierung unterschiedlicher Sinneskanäle und die damit einhergehende Diskussion über Fachinhalte zielt auf die Einleitung akkommodativer und transformativer Lernprozesse und darauf, über Fachwissen in relevanten Kontexten frei verfügen zu können.
            
            
               Anwendung in der universitären Lehre und Lehrerïnnenbildung
               Das im Wintersemester 2019/2020 an der Universität Hamburg durchgeführte Seminar „Digitale Literaturwissenschaft und pädagogische Praxis” hat unterschiedliche Standardverfahren und Werkzeuge erprobt, die gegenwärtig in der digitalen Literaturwissenschaft eingesetzt werden. Dieses Feld wird zunehmend auch für Lehrerïnnen insbesondere im gymnasialen Bereich relevant: Bereits die heutige Schülerïnnengeneration zählt zu den 
  digital natives, für die der Umgang mit digitalen Medien und Werkzeugen selbstverständlich ist, die aber zugleich in Schule und/oder Studium in eine vertiefte 
  data literacy eingeführt werden müssen. Der Transfer von Digital-Humanities-Methoden in den schulischen Bereich kann deshalb als wichtige Herausforderung identifiziert werden. Gleichzeitig geht es darum, das vernetzte Denken zu fördern, mithin literaturwissenschaftliche und fachdidaktische Zugänge zu 
  einem Gegenstand stark zu machen. Um in Seminaren kein starres Wissen zu produzieren, auf das die angehende Lehrkräfte in der nächsten Phase ihrer Ausbildung – dem schulischen Alltag – nicht zugreifen können, muss die Kooperation zwischen Fachdidaktik und Fachwissenschaft gefördert werden. Neben der Einarbeitung in die Methoden steht deshalb die Frage der Komplexitätsreduktion und des schulischen Anwendungsbezuges im Zentrum des Seminars, wofür das DraCor-Kartenspiel exemplarisch herangezogen und getestet wird. Die konzeptionelle Einbettung des Kartenspiels in eine didaktische Heranführung an digitale Methoden ergänzend, wurde damit sowohl in diesem als auch im Seminar „Gender modellieren – Genderrollen und -stereotype in der Literatur des 19. Jahrhunderts”, das ebenfalls im Wintersemester 19/20 an der Universität Hamburg angeboten wurde, eine praktische Anwendung durchgeführt, deren Erfolg qualitativ evaluiert wurde. Damit soll auch ein Beitrag zur Evaluation konkreter DH-Lehrformen geleistet werden.
  
            
            
               Erste Ergebnisse
               Um den Effekt des Dramenquartetts auf den Lernerfolg der Studierenden zu untersuchen, wurde eigens ein Testverfahren entwickelt, das die Wissensstände vor und nach dem Einsatz des Quartetts mess- und v. a. vergleichbar macht. Das Verfahren setzt sich aus fünf aufeinander aufbauenden Phasen zusammen:
               (1) 
                        Vorab: Gruppeneinteilung und eigenständige Vorbereitung (Gruppe 1: Methodenbeitrag/Lerneinheit, Gruppe 2: Video-Tutorials)
                    
               Vorbereitend befasst sich ein Teil der Lerngruppe mit schriftlichen forTEXT-Lernmaterialien zur digitalen Netzwerkanalyse, während der andere Teil die Video-Fallstudien und -Tutorials konsultiert.
               
               (2)
                         Praxisphase 1: Erste Umfrage
               
               Ausgangspunkt der Erhebung stellt folglich ein gruppenspezifisch relativ homogener Wissensstand dar, der grundlegende Kenntnisse über die Methode der digitalen Netzwerkanalyse beinhaltet. Um die Wissensstände beider Gruppen vor dem Einsatz des Quartetts zu erfassen, wurde eine Umfrage entworfen und zu Beginn des Seminars in Einzelarbeit mit dem Audience Response System ARSnova durchgeführt. Die Umfragen adressieren mit jeweils neun Fragen drei Anforderungsbereiche (I: Reproduktionsleistung, II: Reorganisation- und Transferleistung, III: Reflexion und Problemlösung). Den Anforderungsbereichen entsprechend beinhalten sie Single-Choice-, Multiple-Choice- sowie Freitextfragen.
               (3)
                         Praxisphase 2: Einsatz des Dramenquartetts
               
               Nach der ersten Quizphase wurde die gesamte Testgruppe in Kleingruppen eingeteilt, die im Supertrumpf-Modus das Dramenquartett spielen.
               (4)
                         Praxisphase 3: Zweite Umfrage
               
               Eine zweite Umfrage erfasst den Wissensstand beider Gruppen, nachdem sie das Dramenquartett gespielt haben.
               (5)
                         Auswertung der Umfrage: Erste Ergebnisse und Ausblick
               
               Die Auswertung des ersten Testdurchlaufs, der mit 11 Teilnehmenden durchgeführt wurde, verweist auf einen lernförderlichen Effekt des Dramenquartetts. Im Rahmen der ersten Quizrunde wurden 43% der Fragen, nach der zweiten Umfrage 52% der Fragen richtig beantwortet. Darüber hinaus verweist ein erster Blick auf die Freitextantworten darauf, dass der spielerische Zugang die intrinsische Motivation, sich über den Seminarkontext hinaus mit digitaler Netzwerkanalyse auseinanderzusetzen, steigert. Das erarbeitete Verfahren zur vergleichenden Lernstandserhebung hat sich bewährt und wird in einem weiteren Seminar eingesetzt, um den Einfluss einer spielerischen Wissensvermittlung auf Kompetenz- und Wissensstand zu untersuchen.
            
         
         
            Ausblick: zukünftig mögliche Arbeitsfelder
            Das Projekt lotet das didaktische Potenzial von Gamification-Ansätzen in den DH konzeptionell und praktisch aus, indem es das DraCor-Kartenspiel mit Tools und Tutorials in einer didaktischen ,Pipeline‘ verbindet und damit in die Disseminationsstrategie von forTEXT integriert. Der damit entwickelte Prototyp eines Konzepts, das auch fachdidaktisch Weiterentwicklungspotenzial birgt, ermöglicht diverse Adaptionen und Transformationen: in Hinblick auf die Netzwerkanalyse literarischer Texte, in Hinblick auf andere Methoden der Digital Humanities sowie in Hinblick auf das didaktische Szenario einer Verzahnung von analogen und digitalen Ansätzen.
            So ließen sich auf der Grundlage der Netzwerkdaten aus anderen DH-Projekten, etwa zu Romanen, andere generische Karten-Sets entwerfen, wobei auch die – durch ezlinavis in Kombination mit Gephi ermöglichte – kollaborative Erstellung eigener Sets denkbar ist. Diese selbstständige Erstellung von Karten-Sets würde nicht zuletzt auch den haptischen Lerntyp ansprechen. Eine Weiterentwicklung der didaktischen Engführung von Analogem und Digitalem ließe sich über eine Verzahnung des Kartenspiels mit der digital-interaktiven Repräsentation der einzelnen Dramen auf DraCor vornehmen (z. B. über QR-Codes). Unter didaktischen Gesichtspunkten bietet sich des Weiteren die Möglichkeit, kreativ-produktionsorientierte Elemente in die skizzierte Pipeline einzubauen, etwa indem die Lernenden Netzwerke ,erfinden‘, die sie zunächst händisch zeichnen und dann – den Schritt in den digitalen Raum machend – mittels ezlinavis formal erfassen müssen.
            Der im Projekt durchgeführte Testlauf soll in diesem Sinne zu einer weiteren Diskussion über didaktische Potenziale sowohl von Gamification-Ansätzen als auch der Verzahnung von analogen und digitalen Lehrmitteln anregen und damit grundsätzlich der Reflexion über didaktische Szenarien dienen, die den spielerischen, kreativen Übergang zwischen lebensweltlich vertrauten Situationen und der Abstraktion digitaler Forschungsprozesse gestalten.
         
      
      
         
            
                Vgl. 
               https://fortext.net/ressourcen/videos/fallstudien/analyse-der-figurennetzwerke-in-lessings-emilia-galotti.
    
            
                Vgl. 
               https://fortext.net/ressourcen/videos/tutorials/netzwerkanalyse-und-literaturanalyse.
    
            
                Vgl. 
               https://de.wikipedia.org/wiki/Supertrumpf.
    
            
      Zum Diagrammatikbegriff vgl. etwa Krämer 2016.
    
            
                Zu intermedialen Übersetzungsprozessen vgl. Schmid, Veits und Vorrath 2018.
            
            
                Beide Seminare richten sich ausdrücklich an Studierende ohne technische Vorkenntnisse. Es ist also davon auszugehen, dass die Personen beider Testgruppen über keinerlei Vorbildung bezüglich Methoden der digitalen Netzwerkanalyse verfügen.
            
         
         
            
               Bibliographie
               
                  Anselm, Sabine / Werani, Anke (2017): 
  Kommunikation in Lehr-Lernkontexten. Bad Heilbrunn: Klinkhardt.

               
                  Fischer, Frank / Kittel, Christopher / Milling, Carsten / Trilcke, Peer / Wolf, Jana (2018): „Dramenquartett – Eine didaktische Intervention“, in: 
  DHd 2018. Kritik der digitalen Vernunft. Konferenzabstracts, 397–398. DOI: 
  https://doi.org/10.6084/m9.figshare.5926363.v1.

               
                  Fischer, Frank / Schultz, Anika (2019): „Dramenquartett – Eine didaktische Intervention“. Unter Mitarbeit von Christopher Kittel, Carsten Milling, Peer Trilcke und Jana Wolf. 32 Blatt in Kartonbox, Farbdruck. Bern: edition taberna kritika 2019. (Spielanleitung: 
  https://dramenquartett.github.io/)

               
                  Fischer, Frank / Börner, Ingo / Göbel, Mathias / Hechtl, Angelika / Kittel, Christopher / Milling, Carsten / Trilcke, Peer (2019): „Programmable Corpora. Die digitale Literaturwissenschaft zwischen Forschung und Infrastruktur am Beispiel von DraCor“, in: Sahle, Patrick (ed.): 
  DHd 2019. Digital Humanities: multimedial & multimodal. Konferenzabstracts, 194–197.

               
                  Horstmann, Jan (2018): „TextGrid Repository“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/ressourcen/textsammlungen/textgrid-repository [letzter Zugriff 12. September 2019].

               
                  Horstmann, Jan (im Erscheinen): „Textvisualisierung: Epistemik des Bildlichen im Digitalen”, in: Huber, Martin / Krämer, Sybille / Pias, Claus (eds.): 
  Wovon sprechen wir, wenn wir von Digitalisierung sprechen? Gehalte und Revisionen zentraler Begriffe des Digitalen, CompaRe: Fachinformationsdienst Allgemeine und Vergleichende Literaturwissenschaft.

               
                  Horstmann, Jan / Schumacher, Mareike (2019): „Social Media, YouTube und Co: Multimediale, multimodale und multicodierte Dissemination von Forschungsmethoden in forTEXT“, in: Sahle, Patrick (ed.): 
  DHd 2019. Digital Humanities: multimedial & multimodal. Konferenzabstracts, 207–211. DOI: 
  10.5281/zenodo.2596095.

               
                  Illeris, Knud (2010): 
  Lernen verstehen. Bedingungen erfolgreichen Lernens. Bad Heilbrunn: Klinkhardt.

               
                  Jacke, Janina (2018): „Manuelle Annotation“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/methoden/manuelle-annotation [letzter Zugriff 12. September 2019].

               
                  Jäger, Ludwig (2010): „Intermedialität – Intramedialität – Transkriptivität: Überlegungen zu einigen Prinzipien der kulturellen Semiosis”, in: Deppermann, Arnulf / Linke, Angelika (eds.): 
  Sprache intermedial: Stimme und Schrift, Bild und Ton. Berlin, New York: de Gruyter 299–324. DOI: 
  10.1515/9783110223613.299.

               
                  Jenkins, Henry (2006): 
  Convergence Culture: Where Old and New Media Collide. New York, London: New York University Press.

               
                  Krämer, Sybille (2016): 
  Figuration, Anschauung, Erkenntnis. Grundlinien einer Diagrammatologie. Berlin: Suhrkamp.

               
                  Mega, Carolina / Ronconi, Lucia / De Beni, Rossana  (2014):
  „What makes a good student? How emotions, self-regulated learning, and motivation contribute to academic achievement”, in:
   Journal of Educational Psychology, 
                  Vol 106(1), 121–131.
               
               
                  Odebrecht, Carolin / Burnard, Lou / Navarro Colorado, Borja / Eder, Maciej / Schöch, Christof (2019): „The European Literary Text Collection (ELTeC)”, in: 
  DH 2019. Complexities. Utrecht University. [Poster.]

               
                  Schmid, Johannes C. P. / Veits, Andreas / Vorrath, Wiebke (eds. 2018): 
  Praktiken medialer Transformationen. Übersetzungen in und aus dem digitalen Raum. Bielefeld: transcript. DOI: 
  10.14361/9783839441145.

               
                  Schumacher, Mareike (2018a): „Named Entity Recognition (NER)“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/methoden/named-entity-recognition-ner [letzter Zugriff 12. September 2019].

               
                  Schumacher, Mareike (2018b): „Netzwerkanalyse“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/methoden/netzwerkanalyse [letzter Zugriff 12. September 2019].

               
                  Schumacher, Mareike (2019a): „CATMA“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/tools/tools/catma [letzter Zugriff 12. September 2019].

               
                  Schumacher, Mareike (2019b): „Netzwerkanalyse mit Gephi“, in: 
  forTEXT. Literatur digital erforschen. URL: 
  https://fortext.net/routinen/lerneinheiten/netzwerkanalyse-mit-gephi [letzter Zugriff 12. September 2019].

               
                  Trilcke, Peer / Fischer, Frank (2018): „Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen”, in: Huber, Martin / Krämer, Sybille (eds.): 
  Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden (Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3). DOI: 
  10.17175/sb003_003.

               
                  Trilcke, Peer (im Erscheinen): „Small Worlds, Change Rates und die Netzwerkanalyse dramatischer Texte. Reflexionen aus dem Rabbit Hole”, in: Jannidis, Fotis / Winko, Simone / Rapp, Andrea / Meister, Jan Christoph / Stäcker, Thomas (eds.): 
  Digitale Literaturwissenschaft. DFG-Symposium Villa Vigoni, 2017. Berlin, New York: de Gruyter.

               
                  Ward, Matthew / Grinstein, Georges / Keim, Daniel (2010): 
  Interactive Data Visualization. Foundations, Techniques, and Applications. Wellesley: Peters.

            
         
      
   



      
         
            Einführung und Forschungsdesign
            Das Poster diskutiert die methodischen Herausforderungen und Erkenntnismöglichkeiten bei der Modellierung und Auswertung heterogener ideengeschichtlicher Netzwerkstrukturen mit den Methoden der Sozialen Netzwerkanalyse. Dies soll im Rahmen der Vorstellung eines Projekts erfolgen, bei dem mittels einer landesgeschichtlichen Perspektive die Spätaufklärung im Fürstentum Lippe untersucht wurde. Grundlage hierfür Listen der innerhalb dieses Territoriums gelesenen und gedruckten Literatur.
            Unter dem Begriff der Aufkärung wird eine Vielzahl von Diskursen zu philosophischen, politischen, religiösen und anthropologischen Ideen gefasst, die alle Lebensbereiche in den Gesellschaften des 18. Jahrhunderts betrafen. An diesen Diskursen waren unterschiedliche Akteure und soziale Gruppen beteiligt, die nicht nur verschiedene inhaltliche Schwerpunkte setzten, sondern auch variierende, zum Teil gegenläufige, Ziele verfolgten (Stollberg-Rilinger 2011: 10). Ausgehend von dieser Bandbreite und Diffusität bewegen sich nahezu alle Studien zu diesem Themenkomplex bei der Wahl ihrer Untersuchungsperspektive in der Nähe von zwei Polen:
            
               Aufklärung wird aus einer 
                        mikroskopischen Perspektive betrachtet, wobei meist ein einzelner aufgeklärter Diskurs intensiv untersucht wird.
                    
               Aufklärung wird aus einer 
                        makroskopischen Perspektive betrachtet. Hierbei wird versucht eine möglichst allgemeine, idealtypische Begriffsbestimmung von Aufklärung zu schaffen, bei der zwar der großen Vielfalt des Themas Rechnung getragen wird, Details jedoch (zwangsläufig) ausgeblendet werden.
                    
            
            Die Bedeutung dieser Perspektiven für die Aufklärungsforschung ist nicht zu bestreiten. Dennoch bleiben mit der Auslassung einer mesoskopischen Sichtweise einige Fragen unbeantwortet. Zwar wurde an vielen Stellen die interne Struktur einzelner Diskurse offen gelegt und zugleich deutlich gemacht, welche Bedeutung diese für die Epoche der Aufklärung insgesamt darstellten. Unklar bleibt jedoch häufig, wie sich die einzelnen Teildiskurse zueinander verhielten; wie sie sich gegenseitig beeinflussten und wo inhaltliche sowie personelle Überschneidungen existierten. Auf Grund des umfassenden, gesamtgesellschaftlichen Durchdringungspotentials der Aufklärung waren diese diskursiven Bezüge jedoch vorhanden (Stollberg-Rilinger 2011: 10). Größere Entwicklungsverläufe in der aufgeklärten Diskurslandschaft lassen sich durch die skizzierten Perspektiven nicht vollständig, sondern nur in Ausschnitten darstellen; ebenso ist die Identifizierung von Teildiskursen oder auch aufgeklärten Denkschulen meist nur unter Bezugnahme auf Meistererzählungen sowie zeitgenössische Deutungen möglich. Eine Adressierung dieser Problemfelder würde jedoch nicht nur unser generelles Verständnis dieser Epoche schärfen, sondern könnte auch allgemeine Erkenntnisse zur Funktionsweise großer sozialer Bewegungen liefern, die von einer ähnlich heterogenen Diskurslandschaft getragen werden.
            In der dem Poster zu Grunde liegenden Arbeit wurde die aufgeklärte Diskurslandschaft in einem Nebenland der Aufklärung – dem Fürstentum Lippe – im Zeitraum zwischen 1796 und 1820 untersucht. Auch die Aufklärungsforschung zu diesem Territorium des Alten Reiches bewegte sich vor allem in der Nähe der oben skizzierten makroskopischen (z.B. Arndt 1992) und mikroskopischen (z.B. Behrisch 2016 oder Wehrmann 1972) Dimensionen. Um die beschriebenen Anforderungen einer mesoskopischen Perspektive auf Aufklärung zu berücksichtigen wurden daher die einzelnen historischen Quellen, Personen und Ideen in ihrer singulären Bedeutung zurückgestellt und sie stattdessen in ihrer Relationalität zueinander untersucht um Rückschlüsse auf die Struktur der aufgeklärten Diskurslandschaft als Ganzes innerhalb des Untersuchungsraumes zu gewinnnen.
                
         
         
            Modellierung ideengeschichtlicher Netzwerkdaten
            Der Modellierung ebendieser Diskurslandschaft in einer Graphenstruktur lag insbesondere eine kulturwissenschaftliche Vorannahme über die betrachteten historischen Konzepte zu Grunde: Die Operationalisierung der Diskursstruktur erfolgte über ein praxeologisches Verständnis von 
                    Aufklärung. Hierbei wurden einerseits das Verfassen von Büchern und Journalartikeln als Teilhabe an aufgeklärten Diskursen sowie andererseits deren Rezeption durch Lesen dieser Beiträge innerhalb von Sozietäten als zentrale Praktiken der Aufklärung identifiziert (Vgl. für eine allgemeine Operationalisierung aufgeklärter Kommunikation auch Bödeker 1987). Diese Praktiken bieten den Rahmen für die Spielräume in denen die verschiedenen, heterogenen Diskurse der Aufklärung öffentlich ausgetragen wurden.
                
            Die wichtigsten Quellengrundlagen waren hierbei zum Einen für die innerhalb des untersuchten Fürstentums Lippe 
                    rezipierten Publikationen die Auktionslisten der lippischen Lesegesellschaft. Zum Anderen bot für die im Untersuchungsraum 
                    publizierten Druckwerke das Verzeichnis des im Untersuchungszeitraum einzigen Verlags Lippes den Beleg (Weißbrodt 1914). Diese Literaturlisten wurden als strukturierte Informationen automatisiert in Netzwerkdaten überführt.
                
            Die in den einzelnen Publikationen behandelten Themen wurden dann als Teildiskurse bzw. 
                    Diskursfelder aufgefasst, die in ihrer Gesamtheit den aufgeklärten Diskurs im Untersuchungsraum abbildeten. Ein Diskursfeld konnte hierbei einzelne Themenbereiche darstellen – etwa die Verbesserung der Situation der Bauern, aufgeklärte Pädagogik oder die Rolle des Adels in der Gesellschaft. Diese Themenfelder stehen somit auch für sich genommen als eigene, diskursanalytisch relevante ideengeschichtliche Entitäten. Ihre Modellierung als Netzwerkknoten erfolgte auf Grundlage der bisherigen Forschung zur Aufklärung. Diese nicht-automatisierte Erstellung der Diskursfelder bedingt dementsprechend besonders stark die Erkenntnismöglichkeiten des Netzwerks. Zugleich wird so bereits bei der Modellierung sichergestellt, dass die Netzwerkdaten im Rahmen der bisherigen Aufklärungsforschung kontextualisiert sind.
                
            Diskursfelder und Publikationen wurden dann als Knoten in einem bimodalen Netzwerk modelliert (Abbildung 1). Die Verknüpfungen zwischen den Publikations- und Diskursfeld-Knoten erfolgte in einem hermeneutischen-interpretativen Prozess auf Grundlage der Titel der Bücher und Journalartikel. Das Netzwerk wurde dann zu einem unimodalen Netzwerk aus Diskursfeldern transformiert um eine Auswertung vornehmen zu können (Abbildung 2). Durch die Verwendung der Publikationen als heuristische Grundlage konnten so nach der Transformation ideengeschichtliche Ähnlichkeiten und Abhängigkeiten zwischen den einzelnen Diskursfeldern abgebildet werden. Um dabei auch Veränderungen innerhalb des Untersuchungszeitraums erfassen zu können wurden die Kanten des Netzwerks mit zeitlichen Informationen versehen. Grundlage waren die Jahre, in denen die einzelnen Bücher und Journalartikel veröffentlicht wurden – in diesem Zeitraum lässt sich durch den Druck einer Publikation ihr Einfluss auf ein Diskursfeld nachweisen.
                
            
               
               Abbildung 1: Bimodales, gerichtetes Netzwerk der innerhalb Lippes erschienenen Publikationen. Diskursfelder sind rot, Publikationen grün dargestellt. Visualisierung mit dem Fruchtermann-Reingold-Algorithmus.
            
            
               
               Abbildung 2: Aus Abbildung 1 transformiertes Diskursnetzwerk der innerhalb Lippes erschienenen Publikationen. Die Dicke der Kanten beschreibt dabei die Anzahl der Publikationen, die sich ein Diskursfeld teilen. Visualisierung mit dem Davidson-Harel-Algorithmus.
            
         
         
            Netzwerkanalytische Auswertung
            Sowohl die quantitative Auswertung mit Methoden der 
                    Sozialen Netzwerkanalyse als auch die Aufbereitung der Daten erfolgte mit der Skriptsprache 
                    R; v.a. unter Zuhilfenahme des Pakets 
                    igraph. Die Verwendung von eigenem Programmcode bietet gegenüber GUI-basierten Lösungen wie 
                    Gephi oder 
                    Pajek eine leichtere Nachnutzbarkeit und damit auch eine bessere Überprüfbarkeit der Ergebnisse.
                
            Sowohl die Vielfalt der einzelnen Wissensfelder, als auch die nahezu unüberschaubare Menge historischer Quellen dieser Zeit sollten berücksichtigt werden. Die quantitative Auswertung der Relationen zwischen den Diskursfeldern mittels netzwerkanalytischer Verfahren, wie Berechnung von Betweenness-Zentralität (Jansen 2003: 134f) oder Community-Analyse (Rosvall 2019) legten unter anderem eine moderate Diversität der aufgeklärten Diskurslandschaft Lippes offen. Über den gesamten Untersuchungszeitraum ließ sich sowohl in der Publikations- als auch in der Rezeptionstätigkeit ein breites thematisches Interesse nachweisen, wie es für die Zeit der Aufklärung als charakteristisch beschrieben wird (Stollberg-Rilinger 2000). In Bezug auf die innerhalb des Fürstentums Lippe erschienen Publikationen existierte diese Vielfalt jedoch nicht durchgängig während des betrachteten Zeitraumes zwischen 1796 und 1820. Vielmehr war der innerhalb Lippes geführte Diskurs von wenigen Spezialthemen geprägt, die einen dauerhaften Bezug zu nahezu allen anderen Diskursfeldern aufwiesen. Hier dominierten insbesondere religiöse und pädagogische Themen im öffentlichen Diskurs. Volksaufklärerische Schriften mit einer großen territorialen Selbstreferenzialität nahmen ebenfalls einen wichtigen Platz im lippischen Publikationswesen ein. Politische Aktivität und Publikationswesen waren im Fürstentum weitgehend voneinander entkoppelt.
         
         
            Methodische Perspektiven
            Aus einer landesgeschichtlichen Perspektive ermöglichte die Untersuchung eine Schärfung des Verständnisses zu ideengeschichtlichen Schwerpunkten und Strukturen der Spätaufklärung im Fürstentum Lippe. Damit nimmt sie die oben skizzierte Mesoperspektive auf die Erforschung von Aufklärung als geistesgeschichtliches Phänomen ein. Hinsichtlich der 
                    Historischen Netzwerkforschung – der Anwendung der 
                    Sozialen Netzwerkanalyse in den Geschichtswissenschaften – erprobte die Untersuchung einen Ansatz zur Operationalisierung und Auswertung von Netzwerken, die nicht aus Personen, sondern aus abstrakten Ideen bestehen. Die Untersuchung nicht-personaler Netzwerke findet in den Historischen Disziplinen bislang nur selten statt (Düring 2016: 38). Daher ist es erforderlich, Möglichkeiten ebenso wie Herausforderungen bei der Quellensammlung, Datenmodellierung, Abgrenzung (Laumann 1992) und Auswertung eines Netzwerks, das aus ideengeschichtlichen Entitäten besteht, auszudifferenzieren und zu diskutieren. 
                
            Auf diese Weise sind auch unausweichlich zentrale Themen der Tagung betroffen, welche nicht nur für das Teilgebiet der 
                    Sozialen Netzwerkanalyse, sondern für die 
                    Digital Humanities im Allgemeinen relevant sind. Die Vorauswahl bestimmter Datenmodelle und Algorithmen wird bei der 
                    Sozialen Netzwerkanalyse besonders evident: Hier beruht das gesamte Forschungsdesign auf einem Denkparadigma, dem ein streng relationales Welt-, beziehungsweise Geschichtsbild, zu Grunde liegt. Erst die Wahl dieses Paradigmas bedingt den Einsatz bestimmter Methoden und computergestützter Werkzeuge. So ist auch das Thema dieses Posters ein Beispiel dafür, wie diese Vorannahmen einerseits die Selektion von Fragestellungen und bestimmten Quellentypen beeinflussen. Andererseits bietet die 
                    Soziale Netzwerkanalyse in Verbindung mit einem praxeologischen Ansatz jedoch auch eine ‚Unvoreingenommenheit’ gegenüber dem historischen Material. Serielle Textkorpora – wie in diesem Fall Publikationsverzeichnisse – können auf Grundlage formalisierender Vorüberlegungen vollständig ausgewertet werden. Dadurch kann auch einer, der manuellen Reduktion eines Korpus inhärenten, Gefahr von Meistererzählungen vorgebeugt werden.
                
         
      
      
         
            
      Eine Veröffentlichung der Forschungsergebnisse, der dabei angefallenen Daten sowie des für deren Aufbereitung und Auswertung entwickelten Programmcodes ist derzeit in Vorbereitung.
    
            
      Das Land Lippe verfügte zwar über eine aufgeklärten Ideen gegenüber sehr aufgeschlossene landesherrliche Regierung, Verlagswesen und kleine Sozietätslandschaft, besaß jedoch kein eigenes geistiges Zentrum in Form einer Universität.
    
            
      Diese sind im Landesarchiv NRW, Abteilung Ostwestfalen-Lippe vollständig erhalten.
  
            
    Wegen ihres Umfangs bieten Literaturtitel aus dem 18. Jahrhunderts sehr ausführliche Informationen zum Inhalt des jeweiligen Werkes.
  
         
         
            
               Bibliographie
               
                  Arndt, Johannes (1992): 
                        Das Fürstentum Lippe im Zeitalter der Französischen Revolution 1770 – 1820. Münster: Waxmann. 
                    
               
                  Behrisch, Lars (2016): 
                        Die Berechnung der Glückseligkeit. Statistik und Politik in Deutschland und Frankreich im späten Ancien Régime (= Beihefte der Francia 78), Ostfildern: Thorbecke.
                    
               
                  Bödeker, Hans Erich (1987): „Aufklärung als Kommunikationsprozeß“, in: 
                        Aufklärung 2: 89–111.
                    
               
                  Düring, Marten / Kerschbaumer, Florian (2016): „Quantifizierung und Visualisierung. Anknüpfungspunkte in den Geschichtswissenschaften“ in: Düring, Marten / Eumann, Ulrich / Stark, Martin / von Keyerlingk, Linda (eds.): 
                        Handbuch Historische Netzwerkforschung. Grundlagen und Anwendungen (= Schriften des Kulturwissenschaftlichen Instituts Essen (KWI) zur Methodenforschung 1), Münster: Lit Verlag 31–43. 
                    
               
                  Füssel, Marian (2015): „Praxeologische Perspektiven in der Frühneuzeitforschung“ in: Brendecke, Arndt (ed.): 
                        Praktiken der Frühen Neuzeit. Akteure, Handlungen, Artefakte (= Frühneuzeit-Impulse 3), Köln / Weimar / Wien: Böhlau 21–33. 
                    
               
                  Jansen, Dorothea (2003): 
                        Einführung in die Netzwerkanalyse. Grundlagen, Methoden und Forschungsbeispiele. Opladen: Leske + Budrich. 
                    
               
                  Landwehr, Achim (2018): 
                        Historische Diskursanalyse. Frankfurt a. Main: Campus Verlag.
                    
               
                  Laumann, Edward O. / Marsden, Peter V. / Prensky, David (1992): „The Boundary Specification Problem in Network Analysis“ in: Freeman, Linton C. / White, Douglas R. / Romney, Antone Kimball (eds.): 
                        Research Methods in Social Network Analysis. New Brunswick / New Jersey: Taylor & Francis 61–88. 
                    
               
                  Lemercier, Claire (2015a): „Formal network methods in history: why and how?“ in: Fertig, Georg (ed.): 
                        Social Networks, Political Institutions, and Rural Societies (= Rural History in Europe 11), Turnhout : Brepols 281–310. 
                    
               
                  Dies. (2015b): „Taking time seriously. How do we deal with change in historical networks?“ in: Gamper, Markus / Reschke, Linda / Düring, Marten (eds.): 
                        Knoten und Kanten III. Soziale Netzwerkanalyse in Geschichts- und Politikforschung (= Sozialtheorie), Bielefeld: transcript 183–212. 
                    
               
                  Rosvall, Martin / Bergstrom, Carl T. (2008): „Maps of random walks on complex networks reveal community structure“, in: 
                        Proceedings of the National Academy of Sciences 4 1118–1123. 
                    
               
                  Stollberg-Rilinger, Barbara (2000): 
                        Europa im Jahrhundert der Aufklärung, Stuttgart: Reclam.
                    
               
                  Dies. (2011): 
                        Die Aufklärung. Europa im 18. Jahrhundert, Stuttgart: Reclam.
                    
               
                  Trappmann, Mark / Hummell, Hans J. / Sodeur, Wolfgang (2011): 
                        Strukturanalyse sozialer Netzwerke. Konzepte, Modelle, Methoden (Studienskripte zur Soziologie). Wiesbaden: Springer VS.
                    
               
                  Wehrmann, Volker (1972): 
                        Die Aufklärung in Lippe. Ihre Bedeutung für Politik, Schule und Geistesleben (= Lippische Studien 2). Detmold: Landesverband Lippe. 
                    
               
                  Weißbrodt, Ernst (1914): 
                        Die Meyersche Buchhandlung in Lemgo und Detmold und ihre Vorläufer. Festschrift zum 250jährigen Bestehen der Firma am 12. Juni 1914. Detmold: Meyer. 
                    
            
         
      
   

