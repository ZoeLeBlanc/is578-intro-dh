

        
            Aufgrund der technischen Entwicklungen in den letzten Jahrzehnten sind GeisteswissenschaftlerInnen in ihrem Forschungsalltag vor neue Herausforderungen gestellt. Während es für den wissenschaftlichen Nachwuchs in Österreich eine Reihe von curricularen Angeboten sowie Summer Schools im Bereich der Digitalen Geisteswissenschaften gibt
                
                    Vgl. Liste von DH-Studiengängen im deutschsprachigen Raum verfügbar unter 
                        http://www.cceh.uni-koeln.de/Dokumente/BroschuereWeb.pdf [letzter Zugriff 29.10.2015]; Sahle, Patrick (2013): "DH Studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities". DARIAH-DE Working Papers Nr. 1. Göttingen: DARIAH-DE. URN: urn:nbn:de:gbv:7-dariah-2013-1-5, Anhang BA- und MA-Studiengänge, 30-31 [letzter Zugriff 29.10.2015]; Dariah.eu Digital Humanities Course Registry
                         
                        https://dh-registry.de.dariah.eu/ [letzter Zugriff 29.10.2015]; Tabellarischer Vergleich der Studiengänge. In: Thaller, Manfred (Eds.), (2015). Digital Humanities als Beruf. Fortschritte auf dem Weg zu einem Curriculum. Akten der Dhd Arbeitsgruppe „Referenzcurriculum Digital Humanities“ vorgelegt auf der Jahrestagung 2015, Graz 24.-27. Februar 2015, 123-125.
                    
                , sind erfahrene Forschende meist auf sich gestellt, wenn es darum geht, sich forschungsrelevante ICT-Kompetenzen für individuelle Fragestellungen anzueignen: Gemäß der von DARIAH
                
                    Digital Research Infrastructure for the Arts and Humanities, 
                        http://dariah.eu/ [letzter Zugriff 29.10.2015]
                    
                 initiierten Umfrage (Schneider / Scholger in Druck) im Jahr 2014/2015 gaben mehr als 50 Prozent der befragten Forschenden in Österreich an, dass Weiterbildungsangebote zum Thema DH-Methoden und Werkzeuge und wie diese ihre eigene Forschung verbessern könnten und tendenziell wichtig oder sehr wichtig für ihre Arbeit wären.
            
            Das Austrian Centre of Digital Humanities (ACDH) an der Österreichischen Akademie der Wissenschaften sieht es als eine seiner Aufgaben, DH-Inhalte und -kompetenzen zu vermitteln und die Forschenden dabei zu unterstützen, das Potenzial digitaler Methoden und Werkzeuge für ihre konkreten Forschungsprojekte zu nützen. Aus diesem Grund werden am ACDH unterschiedliche Formate der Wissensweitergabe erprobt und evaluiert. In unserem Kurzvortrag stellen wir eines dieser Formate, die ACDH Tool Gallery
                
                    
                        https://acdh.oeaw.ac.at/acdh/en/acdh-tool-gallery [letzter Zugriff 29.10.2015]
                    
                , als Fallstudie vor und berichten über deren Konzeption und Etablierung als außeruniversitäres Weiterbildungsangebot. 
            
            Anders als in herkömmlichen Vortragsreihen, in denen praktischen Aspekten weniger Bedeutung zugemessen wird, stellt die ACDH Tool Gallery das Experimentieren mit eigenen Daten in den Mittelpunkt. Das Konzept der ACDH Tool Gallery besteht darin, zwei Gruppen von Expertinnen und Experten zusammenzubringen: einerseits jene, die sich mit der Entwicklung von Tools beschäftigen und diese bereitstellen, und andererseits jene, die in ihrer geisteswissenschaftlichen Fachdisziplin ausgewiesen sind, und diese Tools in Zukunft verwenden möchten. Bei der Auswahl der Kurzreferate am Vormittag wird diesem Konzept insofern Rechnung getragen, als dass sowohl IT-ExpertInnen als auch GeisteswissenschaftlerInnen, die diese Tools bereits für ihre Forschung einsetzen, als Vortragende eingeladen werden. Die eintägigen Veranstaltungen sind jeweils einem Themenkomplex gewidmet: Die Tools, die am Vormittag in Einführungsvorträgen und Präsentationen vorgestellt werden, können nachmittags von den TeilnehmerInnen erprobt werden, indem diese Schritt für Schritt von der Installation bis zur Anwendung begleitet werden. Idealerweise hat jeder der TeilnehmerInnen ein eigenes, individuelles Set an Daten am Laptop vorbereitet, an dem die Tools getestet werden. Mit der Teilnahme an der Veranstaltung kann eine Kursbestätigung erworben werden. 
            Die ACDH Tool Gallery versteht sich als Angebot für EinsteigerInnen im Bereich der Digital Humanities und vermittelt einen Überblick sowohl zu bewährten als auch zu neuesten Tools, die zur Verfügung gestellt werden. Im Vordergrund steht der Austausch zwischen AnbieterInnen und potentiellen AnwenderInnen: Gemeinsam kann eine Einschätzung darüber erfolgen, ob und inwieweit ein jeweiliges Tool zur Beantwortung individueller Forschungsanliegen geeignet ist. Ausreichend Zeit zur (spontanen) Diskussion ist die Voraussetzung zum Gelingen dieses innovativen Formats. 
            Die Zielgruppe sind ForscherInnen aller geisteswissenschaftlichen Disziplinen sowie zum Teil MitarbeiterInnen von wissenschaftlichen Einrichtungen wie Bibliotheken und Archiven. Die ACDH Tool Gallery ist ein unentgeltliches Angebot, das sich nicht nur an ForscherInnen der Österreichischen Akademie der Wissenschaften richtet, sondern allen Interessierten offensteht. Damit dieses Format gelingt, braucht es intensive Vorbereitung auf allen Seiten: 1. liegt es an den TeilnehmerInnen zu überlegen, an welchen Daten sie das jeweilige Tool zu testen beabsichtigen; 2. leisten die Tool ExpertInnenen Vorarbeit, indem sie einen Zugang zu ihren Tools schaffen; und 3. gibt das ACDH die konzeptionellen und organisatorischen Rahmenbedingungen für dieses Format vor. Da es sich um ein neues Angebot handelt, wird die ACDH Tool Gallery intensiv über die ACDH-Website, die dha-Website, die ÖAW-Website, diverse Mailinglisten und Social Media (Facebook and Twitter) beworben. 
            Die ACDH Tool Gallery wird dreimal im Jahr angeboten und hat bislang bereits zu den Themen “Automated Recognition and Transcription of Handwritten Documents”, “Basic Text Enrichment - TreeTagger for DH-Application” und “Semantic Web Tools” stattgefunden. In einem Bewertungsbogen wurden die TeilnehmerInnen jeweils nach der Veranstaltung befragt, wie sie dieses neue Format einschätzen, mit welchem Vorwissen sie daran teilgenommen haben, was sich verbessern ließe und welche weiteren Tools relevant für ihre Forschungen wären, um künftig im Rahmen der Tool Gallery vorgestellt zu werden. Im Fragebogen wurden außerdem anonyme personenbezogene Daten (z.B. Alter, Geschlecht, Beruf, Herkunftsinstitution) erfasst. 
            Die Ergebnisse dieser Fallstudie werden im Kurzvortrag präsentiert. Vor dem Hintergrund dieser Erfahrungen möchten die Autorinnen diskutieren, was daraus abgeleitet werden kann und Empfehlungen geben, wie ein attraktives, außeruniversitäres Weiterbildungsangebot, das den disziplinären Anforderungen
                
                    Vgl. die Empfehlungen zur Implementierung eines österreichweiten Schulungsprogrammes zum Thema Forschungsdaten speziell im Bereich der Geisteswissenschaften bei Bauer, Bruno; Ferus, Andreas; Gorraiz, Juan; Gründhammer, Veronika; Gumpenberger, Christian; Maly, Nikolaus; Mühlegger, Johannes Michael; Preza, José Luis; Sánchez Solís, Barbara; Schmidt, Nora; Steineder, Christian (2015): Forschende und ihre Daten. Ergebnisse einer österreichweiten Befragung – Report 2015. Version 1.2, S. 70. DOI: 10.5281/zenodo.32043. Online auch unter: http://phaidra.univie.ac.at/o:407513
                 unterschiedlicher Forschenden gerecht wird, gestaltet werden könnte. 
            
        
        
            
                
                    Bibliographie
                    
                        Bauer, B., Ferus, A., Gorraiz, J., Gründhammer, V., Gumpenberger, Ch., Maly, N., Mühlegger, Johannes M. Preza, J. L. Sánchez Solís, B., Schmidt, N., Steineder, Ch. (2015).
                        Forschende und ihre Daten. Ergebnisse einer österreichweiten Befragung – Report 2015. Version 1.2, DOI: 10.5281/zenodo.32043. Online auch unter: http://phaidra.univie.ac.at/o:407513 (letzter Zugriff 29.10.2015).
                    
                    
                        Sahle, P. (2013). DH Studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities. DARIAH-DE Working Papers Nr. 1. Göttingen: DARIAH-DE. URN: urn:nbn:de:gbv:7-dariah-2013-1-5, Anhang BA- und MA-Studiengänge, pp. 30-31 (letzter Zugriff 29.10.2015).
                    
                    
                        Schneider, G., Scholger, W. (2015). In: Dallas and N. Chatzidiakou (Eds.), 
                        DARIAH survey on scholarly practices and needs of European humanities researchers in the digital environment 2014-15, Technical report, Athens: Digital Curation Unit. Forthcoming, in Druck: Austria.
                    
                
            
        
    

      
         Annotation natürlicher Sprachdaten aus sozialen Medien zur
        Erforschung zeitgenössischer Szenen, zur Sprach- und Trendanalyse und zur
        Weiterentwicklung von Sprachtechnologien gewinnt mit der zunehmenden Verfügbarkeit
        großer Datenbestände weiter an Bedeutung (Farzindar / Inkpen 2015). Zeitgenössische
        Kommunikation in sozialen Medien verfügt über inhaltliche und strukturelle
        Besonderheiten und ist von umgangssprachlicher Ausdrucksform geprägt. Beiträge, die
        im Kontext internetbasierter Diskussionskulturen in Foren entstehen, stellen eine
        wichtige Forschungsquelle dar. Diese nutzergenerierten Texte, in Form von semi- oder
        unstrukturierten Kommentaren, repräsentieren Meinungen und Bewertungen einer
        Gemeinschaft zu einem Thema, Produkt oder Werk und beziehen sich in der Regel auf
        inhaltliche, technische oder ästhetische Aspekte. Die Autoren verwenden dabei
        Sprachmittel wie Metaphern, Analogien, Ambiguität, Humor und Ironie sowie
        metalinguistische bildhafte Mittel wie Emoticons oder andere graphische Zeichen
        (Reyes et al. 2012).
         Vor diesem Hintergrund adressiert dieses Projekt Herausforderungen, die bei der linguistischen und statistischen Verarbeitung von realen web-basierten Daten entstehen. Es wird ein Ansatz semi-automatischer Annotation zur Extraktion von Begriffen für die ontologiebasierte Beschreibung von computergenerierten audiovisuellen Kunstwerken einer digitalen Kunstszene präsentiert. Forschungsgegenstand ist die Diskussionskultur der Demoszene, einer spezialisierten Computerkunstszene. Bisher sind die zahlreichen Beiträge der Gemeinschaft, die sich auf ästhetische und technische Aspekte der Kunstwerke beziehen, nicht erschlossen. Bei diesen Beiträgen handelt es sich um informelle, emotionale, kurze und unstrukturierte Kommentartexte. Das verwendete Vokabular ist mehrsprachig und beinhaltet fachspezifische Terminologien, exklusive Neologismen und einen eigenen szenespezifischen orthographischen Stil. Diese Beiträge bieten detaillierte Einblicke in die Charakteristika der Werke, weshalb ihre Erschließung deren Verständnis fördert und eine gezielte Recherche einzelner Werke ermöglicht. Das Projekt befasst sich mit der Fragestellung, in wieweit sich aktuelle Verfahren der natürlichen Sprachverarbeitung (NLP), die auf grammatikalisch korrekte Schriftformen optimiert und auf Zeitungskorpora trainiert sind, anwenden lassen. Somit leistet das präsentierte Projekt einen Beitrag im Bereich der Entwicklung von Ansätzen zur Aufbereitung großer textbasierter Datenbestände sowie der Erforschung des Sprachgebrauchs zeitgenössischer digitaler Kunstszenen, aber auch hinsichtlich Nutzung semantischer Technologien.
         Die Anwendung von NLP-Verfahren für textbasierte Kommunikation
          in soziale Medien bedarf einiger Anpassungen an die sprachlichen Besonderheiten
          (Maynard 2012). Die Nutzung standardisierter Techniken ist bisher nur wenig
          erfolgversprechend (Gimpel 2011; Finin 2010). Bestehende Frameworks, wie das Natural
          Language Toolkit (NLTK, vgl. Bird et al. 2015), bieten die Möglichkeit der
          Implementierung eines individuellen NLP-Prozesses, bei dem verschiedene
          Verarbeitungsschritte modular integriert und miteinander kombiniert werden können.
          Für das vorliegende Projekt wurde eine Pipeline konzipiert und implementiert, die
          die Generierung von Annotationsebenen, begonnen mit der Tokenisierung und
          Part-of-Speech Tagging bis hin zur Extraktion von relevanten werkbeschreibenden
          Begriffen umfasst. Zur Evaluation des entwickelten Ansatzes wird ein regelbasiertes
          überwachtes Experiment mit einer definierten Teilmenge von 1255 Kommentaren
          durchgeführt. Es lässt sich feststellen, dass Emoticons und Partikeln falsch
          verarbeitet werden. Darüber hinaus werden auch Nomen, Verben und Adjektive,
          insbesondere Gerundien häufig falsch annotiert. Das Experiment zeigt, dass die
          konzipierte Pipeline für das vorliegende Kommentarkorpus iterativ optimiert werden
          muss. Der generierte Index werkbeschreibender Terminologie wird ferner für die
          Erweiterung einer domainspezifischen Ontologie zur Unterstützung semantischer
          Annotation verwendet. Hierfür wird ein Ansatz für das Lernen von Ontologien aus
          Texten verfolgt, wobei die ermittelten Begriffe als Kandidaten für Instanzen
          beschrieben werden. Als Referenzontologie wird eine auf CIDOC CRM-basierte Adaption
          verwendet (Hastik et al. 2013).
         Dieses Projekt präsentiert einen innovativen Ansatz, um mit NLTK Kommentartexte aus Onlineforen der Demoszene zu annotieren. Das Standard-Tagset muss jedoch angepasst werden. Die Erweiterung der CIDOC CRM-basierten Ontologie auf Basis des generierten Indexes ermöglicht die semantische Beschreibung der Werke.
      
      
         
            
               Bibliographie
               
                  Bird, Steven / Klein, Ewan / Loper, Edward (2015):
              Natural Language Processing with Python. NLTK
              Book http://www.nltk.org/book/
              [letzter Zugriff 15. Februar 2016].
               
                  Farzindar, Atefeh / Inkpen, Diana (2015): Natural Language Processing for Social Media. San
              Francisco: Morgan & Claypool.
               
                  Finin, Tim / Murnane, Will / Karandikar, Anand / Keller,
                Nicholas / Martineau, Justin (2010): "Annotating Named Entities in
                Twitter Data with Crowdsourcing", in: Proceedings of the
                NAACL HLT 80–88. 
               
                  Gimpel, Kevin / Schneider, Nathan / O'Connor, Brendan /
                  Dipanjan, Das / Mills, Daniel / Eisenstein, Jacob / Heilman, Michael /
                  Yogatama, Dani / Flanigan, Jeffrey / Smith, Noah A. (2011):
                  "Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments",
                  in: Proceedings of the 49th Annual Meeting of the
                  Association for Computational Linguistics 42-47. 
               
                  Hastik, Canan / Steinmetz, Arnd / Thull, Bernhard
                  (2013): "Ontology based Framework for Real-Time Audiovisual Art", in: IFLA World Library and Information Congress. 79th
                  IFLA General Conference and Assembly: Audiovisual and Multimedia with
                  Cataloguing http://library.ifla.org/87/1/124-hastik-en.pdf [letzter Zugriff
                  15. Februar 2016]. 
               
                  Maynard, Diana / Bontcheva, Kalina / Rout, Dominic
                  (2012): "Challenges in Developing Opinion Mining Tools for Social Media",
                  in: Proceedings of @NLP can u tag #usergeneratedcontent?!
                  Workshop at International Conference on Language Resources and
                  Evaluation (LREC 2012) 8.
               
                  Reyes, Antonio / Rosso, Paolo / Buscaldi, Davide
                  (2012): "From Humor Recognition to Irony Detection: The Figurative Language
                  of Social Media", in: Data Knowledge Engineering.
                  Applications of Natural Language to Information Systems 74: 1-12. 
            
         
      
   



      
         
            Projekt
            Das DFG-Projekt Future Publications in den Humanities (Fu-PusH)
        untersuchte die Potenziale des digitalen Publizierens in den
        Geisteswissenschaften und erarbeitete anhand von Szenarien Handlungsempfehlungen
        für akademische Infrastruktureinrichtungen wie insbesondere
        Universitätsbibliotheken und Rechenzentren, um Publikationsprozesse zu
        unterstützen und dabei den funktionalen Anforderungen unterschiedlicher
        geisteswissenschaftlicher Fachrichtungen gerecht zu werden.
            Auf dem Poster werden zum einen die Ergebnisse der Studie beschrieben und zum anderen ein speziell in diesem Projekt entwickeltes Recherche-Tool zur Auswertung qualitativer Interviews (Statement Finder) vorgestellt, das als niedrigschwelliges Open-Source-Tool der Community zur Verfügung gestellt wird. Abschließend werden eine Reihe von Handlungsempfehlungen formuliert für die an digitalen Publikationsprozessen beteiligten Akteursgruppen, namentlich für geisteswissenschaftliche Fachgemeinschaften darunter insbesondere die Digital-Humanities-Community, für Infrastruktureinrichtungen wie Bibliotheken und Rechenzentren, für Wissenschaftsverlage sowie für Förderinstitutionen und für die Wissenschaftspolitik.
         
         
            Ergebnisse
            Die Ergebnisse des Fu-PusH-Projektes zeigen sehr deutlich die Unterschiede im Forschungs- und Publikationsverhalten sowohl zwischen den Geisteswissenschaften und den Naturwissenschaften als auch innerhalb des disziplinären Spektrums der Geisteswissenschaften selbst. Dies betrifft insbesondere die Zurückhaltung gegenüber der Nutzung digitaler Publikationsmedien, auch angesichts ihrer anerkannten Potenziale.
            Das
          Publikationsverhalten in den Geisteswissenschaften orientiert sich nach wie vor weitgehend an traditionellen Formen aus der Printkultur wie Monografien, Sammelbandbeiträge, Zeitschriftenaufsätze sowie Rezensionen. Wo digital publiziert wird, folgt man den etablierten Modellen der Verlagspublikation in einem dem Printparadigma möglichst ähnlichen Format. Hier sind auch perspektivisch nur geringe oder selektive Änderungen und Optimierungen zu erwarten. Die Einbindung von multimedialen Erweiterungen wird auf der Materialebene durch urheberrechtliche Bedingungen und auf der technischen Ebene durch den Mangel an Standards und niedrigschwelligen Lösungen eingeschränkt. Bisher lässt sich am ehesten die Form des Bloggens als dauerhafte zusätzliche Variante für die wissenschaftliche Kommunikation bestimmen.
        
            Das Publizieren nach dem
          Open-Access-Prinzip scheint in den Geisteswissenschaften geringer ausgeprägt als in den Naturwissenschaften. Dafür lassen sich mehrere Gründe identifizieren. Zum einen fehlen an vielen Stellen bislang fachwissenschaftlich etablierte Infrastrukturen. Zum anderen genießen rein digitale Publikationen nach wie vor keinen guten Ruf, was sich beispielsweise auf die Kreditierung des Forschungsoutputs auswirkt. Schließlich wirkt im Vergleich zu naturwissenschaftlichen Publikationen der Zugangsdruck zu Neuerscheinungen an vielen Stellen durch längere Forschungszeiträume und geringere Kosten weniger stark.
        
            Auffällig ist, dass sich stärker in Schnittstellen mit Naturwissenschaften
          befindliche und internationalisierte Disziplinen (z. B. Sprachwissenschaften,
          Archäologie) deutlich aktiver in dieser Richtung entwickeln, als die vorwiegend
          hermeneutisch-interpretativ arbeitenden Fächer. Eine Nutzung von frei
          zugänglichen Materialien erfolgt dagegen fächerübergreifend. Es existiert beim
          Open Access also eine Diskrepanz zwischen Publikations- und Rezeptionsverhalten. 
            In einigen Bereichen vor allem unter dem Einfluss der
            Digital Humanities finden sich jedoch auch stärker digital orientierte Entwicklungen. Eine Erklärung lautet, dass viele Formen dieser Wissenschaft überhaupt erst durch digitale Technologien realisierbar werden. Dort wo größere Datenmengen flexibel verarbeitet werden müssen, etwa in der Editionswissenschaft oder der Computerlinguistik, finden sich bereits stärker etablierte Formen der digitalen Forschung und des digitalen Publizierens, die – sehr selektiv – auch auf anderen Fachbereiche inspirierend einwirken. Eine Zwischenform zwischen Publikation und Forschung, der vergleichsweise viel Potential zuerkannt wird, ist das digitale
            Annotieren. Damit zusammenhängend wird das größte Zukunftspotenzial des digitalen Publizierens im Bereich der
            digitalen Editionen gesehen, die häufig zugleich als mögliche Hybridausgaben zur differenzierten Rezeption wie auch als digitales Forschungsdatum zur weiteren Verarbeitung gesehen werden.
          
            Die Nutzung von
            Social-Media-Anwendungen scheint sich in vielen Bereichen der Geisteswissenschaften weitgehend auf die Vernetzung durch soziale Wissenschaftsnetzwerke oder Kurznachrichtendienste (Twitter) zu beschränken. Mit Hypotheses.org etabliert sich allerdings nach und nach eine Blogplattform, die sich durchaus ein gewisses Renommee aufbaut. Das ist insofern der relevante Schritt, weil eine zentrale Hürde bei der Nutzung solcher Medien die bisher fehlende Kreditierbarkeit für wissenschaftliche Karrieren darstellt. Zudem existiert die Sorge, dass frei auf solchen Wegen zum Beispiel vor einer “ordentlichen” Publikation publizierte Ergebnisse von Anderen übernommen und verwertet werden.
          
            Bei vielen Aspekten vernetzter und digitaler Forschung bzw. des interaktiven Publizierens zeigt sich, wie sehr wissenschaftskulturelle Aspekte der Nutzung bestimmter technologischer Formen entgegenstehen. Das betrifft insbesondere den Aspekt der Kollaboration, der Voraussetzung für den sinnvollen Einsatz
            virtueller Forschungsumgebungen ist. Hier findet sich nur eine geringe Nutzungsbereitschaft. Es ist zu vermuten, dass sowohl wissenschaftskulturelle Gepflogenheiten als auch eine vergleichsweise komplexe Nutzbarkeit die Akzeptanz und Nutzung solcher Angebote bremsen. Zweckmäßiger erscheinen hier einfache, modularisierte und miteinander verknüpfbare Lösungen.
          
            Herausforderungen werden generell bei Fragen der technischen
            Standardisierung zur Gewährleistung von
            Interoperabilität deutlich. Dies betrifft sowohl die Werkzeuge als auch die digitalen Forschungsdaten. Zudem zeigen sich wahrgenommene Risiken, die generell von Technologien im Kontext der Digital Humanities ausgehen. Zum einen liegen bisher kaum Erfahrungswerte vor, mit denen sich eine tatsächliche Relevanzbewertung von Informationsinfrastrukturen bzw. Publikationsszenarien vornehmen lässt. Zum anderen besteht die Gefahr, dass neue technische Dispositive bestimmte Forschungs- und Erkenntnispraxen begünstigen und dafür andere weniger angemessen berücksichtigen.
          
         
      
      
         
            
               Bibliographie
               
                  Universitätsbibliothek der Humboldt-Universität zu
              Berlin (o. J.): Fu-PusH. Future Publications
              in the Humanities https://www2.hu-berlin.de/fupush/ [letzter Zugriff 06. Januar
              2016].
            
         
      
   



      
         
            Einleitung
            Die Datenverarbeitung innerhalb der Geisteswissenschaften ist sehr eng mit den
          gegenwärtigen technologischen Entwicklungen verbunden und dementsprechend auch
          stark davon abhängig. Ein sehr gutes Beispiel dafür ist das Gebiet der
          Dialektologie / Dialektometrie. Klassische Dialektometrie ist eine
          Forschungsrichtung innerhalb der Linguistik, die sich mit der Erforschung
          möglichst hochrangiger Ordnungsstrukturen in sprachgeographischen Netzen
          beschäftigt. Diese Aufgabe wurde bislang hauptsächlich durch die Analyse
          gesprochener Sprache (z. B. akustische Aufnahmen) oder der sogenannten
          Fragebögen (z. B. gezielt abgefragte, schriftliche Daten) bewältigt. Ein
          Nachteil dieser ist allerdings, dass die erhobenen Daten stark beeinflusst oder
          nicht schriftlich sind. Durch die gegenwärtigen Entwicklungen in der
          Informationstechnologie sind Sammlungen von neuartigen Dialektdaten erreichbar
          (die ohne äußeren Einfluss, gesammelt wurden und darüber hinaus in schriftlicher
          Form als Datensatz vorhanden sind), womit in der Dialektometrie neue Wege
          gegangen werden können. Ein Beispiel dafür sind neue Medien, wie z. B.
          Wikipedia, Twitter, digitale Zeitschriften, etc., in denen außerdem
          Veränderungen in der Gesellschaft schnell abgebildet werden.
             Allein in Wikipedia ist eine große Anzahl an Dialekten vertreten, wie zum
            Beispiel die italienischen Dialekte Lombardisch (31.986 Artikel)
            1, Sizilianisch (25.273 Artikel), Neapolitanisch (14.346 Artikel) etc., die
            fortlaufend mit neuen Artikeln erweitert werden, die nicht nur von einem,
            sondern von mehreren Autoren editiert werden. Aus diesen Artikeln kann eine
            bisher nicht vorhandene Art Korpus erstellt werden, dessen Untersuchung die
            Beantwortung völlig neuer Fragestellungen möglich werden lässt. 
            Die Größe dieser neuen Korpora ermöglicht nicht nur neuartige Fragestellungen in der Dialektometrie, sondern auch einen zeitgenössischen und automatisierten Vergleich für die Analyse von Dialekten und ihren linguistischen Eigenschaften (basiert auf statistische Ansätze). Für solche Verfahren ist allerdings nicht nur die vorhandene Datenmenge wichtig, sondern auch die leichte Erreichbarkeit von qualitativen Annotationen und Analysetools. Diese wurden bislang hauptsächlich für die Standardsprachen entwickelt, für Dialekte existieren diese bis jetzt nur in wenigen Ausnahmefällen.
            Ein solches Analysetool für die Standardsprache Italienisch ist AnIta (Tamburini
              / Melandri 2012), ein morphologisches Finite-State-Analysetool, welches bisher
              nur für das Italienische verwendet werden kann. In AnIta können aber auch viele
              empirische Belege für Dialekte integriert werden, sodass die maschinelle
              Bearbeitung vieler italienischer Dialekte möglich wird. Die neuen
              Dialektwikipedias ermöglichen auch einen halb automatisierten Ansatz dafür.
         
         
            
                SiMoN
              
            
               Überblick
               In unserer Softwaredemonstration möchten wir eine vorläufige Erweiterung von AnIta vorstellen, die mit vielen regelmäßigen Verbparadigmen des sizilianischen Dialekts erweitert wurde - SiMoN (Sizilianische Morphologie für NLP-Anwendungen). Die Version der Softwaredemonstration ist schon
                  online erreichbar. Aus Einträgen der sizilianischen Wikipedia wurden Verblemmata (368 sizilianische Lemmata) für das Lexikon von AnIta automatisch extrahiert anhand von dem Auftreten regulären sizilianischen Verbendungen und einer Liste von Verben im Italienischen. Da sich die Verben des Sizilianischen in nur zwei Typen aufteilen (statt wie im Italienischen in drei), sind nur Verbeinträge mit Endungen auf
                  -ari und auf
                  -iri vorhanden. Die gesamte Zahl, der durch Flexionsparadigmen erfassten Verbformen beläuft sich auf ca. 24.700. Damit bietet SiMoN einen ersten Grundstock für die Entwicklung einer computergestützten, sizilianischen Morphologie.
                
            
            
               Dokumentierte Paradigmen
               Der Fokus der zu untersuchenden Paradigmen liegt in dieser Arbeit auf den Konjugationsmustern regelmäßiger Verben. Das vorderste Ziel ist es hier, eine Grundlage für die Verbanalyse für Sizilianisch zu schaffen. Im Gegensatz zum Italienischen gibt es für einige Verben eine große Zahl an Wahlmöglichkeiten für Endungen konjugierter Formen, die regional unterschiedlich verbreitet und gleichermaßen gültig sind. Bonner und Cipolla (2001) dokumentieren für die regelmäßigen Verben einiger Zeiten und Modi alternative Formen, die wir verfolgen. Diese Alternativformen gehören alle zum selben Paradigma. Daher gibt es im jeweiligen Lexikon der beiden Verbtypen in SiMoN teilweise mehrfache Einträge zur Konjugation der ersten, zweiten oder dritten Person. Eine vorläufige Analyse des gewonnenen Wikipedia-Korpus zeigte ebenfalls, dass die verschiedenen Varianten der Verben in der Praxis verwendet werden. Stammveränderungen in der sizilianischen Verbgrammatik existieren ebenfalls, diese Fälle werden allerdings mit SiMoN im Moment noch nicht abgedeckt.
               
                  
                  
                     Tabelle 1: Die regelmäßigen Konjugationsformen, die in
                SiMoN integriert wurden.
               
               In Tabelle 1 sind die regelmäßigen Konjugationsformen (die in SiMoN vorhanden
                  sind) am Beispiel der sizilianischen Verben parrari
                  (Deutsch - reden) und battiri (Deutsch - schlagen)
                  aufgeführt. Die Formen beider Verbtypen in den Flexionskategorien Indikativ,
                  Imperativ und Subjunktiv, sowie Konditional und Gerundium sind jeweils
                  vorhanden. Die Paradigmen der unregelmäßigen Hilfsverben essiri (Deutsch - sein) und aviri (Deutsch
                  - haben) sowie das sehr häufig verwendete fari
                  (Deutsch - machen) wurden ebenfalls in SiMoN in die Liste der Lemmata
                  aufgenommen, um Partizipkonstruktionen u. ä. zu erkennen. 
            
         
         
            Ausblick
            Unserer Ziel ist vorerst anhand den Texten der Wikipedia für Standard Italienisch und alle andere Dialektwikipedias weiterhin automatisch dialektspezifische Verben zu extrahieren und damit SiMoN zu erweitern. Damit können zusätzliche Dialekte auch behandelt und entwickelt werden. SiMoN würde dann eine automatisierte morphologische Analyse für reguläre italienische Dialektparadigmen ermöglichen, was wir bis jetzt nur für Sizilianisch anbieten können. Weiterhin ist es geplant auch irreguläre Dialektparadigmen manuell zu integrieren.
         
      
      
         
            Die Zahlen sind von Wikipedia
                  entnommen worden (Stand: August 2015). 
         
         
            
               Bibliographie
               
                  Bonner, J. K. "Kirk" / Cipolla, Gaetano (2001): Introduction to Sicilian Grammar. Brooklyn, NY:
                      Legas. 
               
                  Tamburini, Fabio / Melandri, Matias (2012): „AnIta: A
                      Powerful Morphological Analyser for Italian“, in: Proceedings of the Eight International Conference on Language Resources
                      and Evaluation (LREC’12), Istanbul, Turkey 941-947.
            
         
      
   



      
         
            Zusammenfassung der Sektion
            Die Vermessung der Welt mittels digitaler Medien hat längst begonnen. Von der Durchdringung der Gesellschaft zeugen nicht nur Street View und weltweit verfügbare Satellitenaufnahmen, sondern auch Twitter und Facebook und nicht zuletzt die Auswirkungen auf die Wissenschaftskulturen, insbesondere auf die der Geisteswissenschaften. Hierfür hat sich der Begriff der Digital Humanities etabliert, der schillernd und komplex zugleich ist. Während zunächst historisches Material und Artefakte digitalisiert wurden, rückte in den letzten Jahren vor allem die Annotation von Digitalisaten und die Anlage und Aufbereitung von Datenbanken ins Zentrum des Interesses. Derzeit fächert sich das Spektrum der Digital Humanities weiter auf. 
            Anne Burdick, Johanna Drucker und andere (Burdick et al. 2012) weisen in ihrem
          intensiv rezipierten und vielfach zitierten Buch zum Konzept der Digital
          Humanities darauf hin, dass die Möglichkeiten und Chancen der Digital Humanities
          quasi einer Erweiterung der Geisteswissenschaften gleichkommen, die sowohl
          Werte, interpretative Praxis und Strategien der Bedeutung als auch die
          Ambiguitäten der menschlichen Existenz betreffen. Innerhalb der Sektion soll der
          Blick insbesondere auf zwei auch von Burdick und Drucker thematisierte Aspekte
          gelenkt werden: Zum einen ermöglicht die Erweiterung der Digital Humanities neue
          Wege der transmedialen Erforschung durch interdisziplinäre Kooperationen. Zum
          anderen darf nicht nur die Anwendung von digitalen Werkzeugen und Datenbanken im
          Fokus stehen, sondern auch Konzeption, Entwicklung und Nutzung können und müssen
          neue Wege beschreiten. 
            Innerhalb dieser Sektion soll die kooperative, interdisziplinäre und interfakultäre Konzeption und Entwicklung einer mobilen Anwendung exemplarisch diese neuen Wege und Prinzipien als eine wichtige Option kulturwissenschaftlicher und informatischer Verschränkung vorgestellt werden. 
            An der Universität Paderborn hat sich vor zwei Jahren eine Forschergruppe
            innerhalb des akademischen Mittelbaus gebildet, die ein auf mehrere Semester
            angelegtes interdisziplinäres und interfakultäres Forschungs- und Lehrprojekt
            entwickelt hat. An diesem Projekt, das im September 2015 mit dem Forschungspreis
            der Universität Paderborn ausgezeichnet wurde, sind derzeit die Fächer
            germanistische Mediävistik und Linguistik, Geschichte, Informatik und
            Kunstgeschichte beteiligt. Die ‚Historisches Paderborn‘-App (kurz HiP-App) zeigt
            die neuen Verknüpfungen, die durch das Konzept der Digital Humanities in den
            letzten Jahren proklamiert wurden, geradezu idealtypisch auf: Denn in der
            HiP-App ist die Informatik nicht lediglich Dienstleister für die
            Kulturwissenschaften und sind die Kulturwissenschaften nicht ausschließlich
            Content-Lieferanten für die Informatik. Vielmehr widmet sich das Projekt
            übergreifenden Forschungsfragen und ermöglicht darüber hinaus ebenso
            Individualforschung. In unserem Projekt fokussieren wir Fragestellungen, die
            sich aus der Konzeption, der Entwicklung und der Nutzung von digitalen
            Anwendungen (Apps) für mobile Endgeräte wie Smartphones oder Tablets
            ergeben.
            Innerhalb dieser Sektion liegt der Fokus auf drei essenziellen Schwerpunkten des Projekts: (I.) auf dem Potenzial kooperativer Exploration und Konzeption mobiler Anwendungen für die Vermittlung wissenschaftlicher Inhalte in einem außeruniversitären Kontext, (II.) auf multimodaler Kommunikation und Raumwahrnehmung und (III.) auf der evolutiven Software-Entwicklung unter Berücksichtigung einer mensch-zentrierten Entwicklung, die in der interdisziplinären Kooperation zwischen Kulturwissenschaften und Informatik innerhalb unseres Projektes verwirklicht werden kann.
         
         
            Ins Leben gerückt. Zum Potential mobiler Anwendungen für die Vermittlung vormoderner Artefakte 
            
               Markus Greulich, Nicola Karthaus
              und Ariane Schmidt (Universität Paderborn)
            
            Insbesondere die historischen Geisteswissenschaften, im ganz besonderen Maße die
                mediävistischen Fächer, gelten manchem als längst überholte
                Wissenschaftsdisziplinen, aus denen weder ein Wert für die Gegenwart noch für
                die Zukunft zu erwarten ist. ‚Das‘ Mittelalter gilt als gut erforscht, die
                Einträge in online-Ressourcen vermitteln ein abgeschlossenes Bild: Wir wissen,
                wie unsere Vergangenheit war. Doch immer wieder gibt es Irritationen: Da
                geistern Pergamentfragmente durch die Tagesschau, widmen sich Regisseur_innen
                mit großem Erfolg den Lebensgeschichten von Kaiser_innnen und Heiligen, zeigen
                internationale Serien, dass die Päpste des Mittelalters und der Renaissance ein
                nur begrenzt katholisches Leben pflegten und immer wieder geraten wertvolle
                Handschriften und Kunstobjekte des Mittelalters in den Fokus der Öffentlichkeit.
                So wie unsere Gegenwart nie als geschlossenes Bild vor uns stehen kann, so
                verändert sich auch ‚unser‘ Blick auf ‚das ‘ Mittelalter. Diesen Blick zu
                schärfen, vormoderne Artefakte lesbar zu machen und ihre eigene Geschichte als
                Teil ‚unserer‘ Geschichte, als Teil der Gegenwart erfahrbar zu machen – dies
                soll das interdisziplinäre Forschungs- und Lehrprojekt ‚Historisches
                Paderborn‘-App, kurz HiP-App, leisten. 
            Die HiP-App ist eine Anwendung für mobile Endgeräte, die
                auf ansprechende Weise detaillierte und wissenschaftlich sinnvoll aufbereitete
                Materialien zur selbstständigen historischen Erkundung der Stadt Paderborn
                anbietet. Sie wird derzeit von Mittelbau-Vertreter_innen der Universität
                Paderborn aus den Bereichen Informatik, germanistische Mediävistik und
                Linguistik, Geschichte und Kunstgeschichte entwickelt. Die HiP-App ist zugleich Forschungsgegenstand und -infrastruktur, die
                durch ihren grundlegend evolutiven Charakter vielfältige Forschungsfragen
                erzeugt (Burdick et al. 2012). Existierende Denkmäler dienen als Ausgangspunkt,
                um germanistische, historische und kunsthistorische Inhalte zu erläutern. Das
                interaktive Front-End der HiP-App, d. h. die für den
                Benutzer sichtbare Oberfläche, wird u. a. durch aktuellste Präsentationsformen
                historischer Artefakte im Bereich der Augmented Reality, d. h. der virtuell
                erweiterten Realität, gestaltet. Hierbei können Kunstwerke und andere Objekte
                nicht nur schriftlich oder mündlich erläutert, sondern auch singuläre Details
                hervorgehoben oder verlorene Sinnzusammenhänge visualisiert werden. Sogar der
                ursprüngliche Kontext eines Werkes kann so rekonstruiert werden. Auch ist es
                möglich, kunsthistorische Vergleiche zu ziehen, verwandte Werke zu zeigen,
                zusätzliche Materialien anzubieten und eine kulturhistorische Kontextualisierung
                vorzunehmen. Die neuen technischen Möglichkeiten der medialen Aufbereitung von
                Kulturgeschichte sind verbunden mit neuen sozialen Praktiken des Wahrnehmens an
                der Schnittstelle zwischen physischem und digitalem Raum (Buschauer / Willis
                2013; Schüttpelz 2006). Die so gestalteten historischen Aussagen werden im
                Hinblick auf eine für die Gegenwartsorientierung relevante (stadt)geschichtliche
                Sinnbildung (Rüsen 1996) motiviert. Somit soll die Neugier der Nutzer_innen
                geweckt und durch eine veränderte Wahrnehmung des urbanen Umfelds eine
                weiterführende A useinandersetzung mit dem Paderborner Kulturraum gefördert
                werden. 
            Zentrales gemeinsames Forschungsanliegen ist es, am Beispiel der HiP-App
                  Methoden, Prozesse und Analysen im Bereich der Digital Humanities zu entwickeln.
                  Forschungsgegenstand und Grundlage hierfür bilden Genese, Entwicklung, Betrieb
                  und Pflege der HiP-App, aber auch die kritische Reflexion
                  der Kopplung physischer und digitaler Räume mithilfe mobiler Apps, die von der
                  menschlichen Körperorientierung ausgehend den städtischen Raum diachron lesbar
                  machen. So sollen durch Analyse der Dateneingabe etwa auch Aspekte der
                  Software-Usability und der Entwicklung multimodaler Kommunikationsformate in den
                  Blick genommen werden. Bezogen auf die Front-End-Entwicklung stehen verschiedene
                  Formen des epistemologischen Präsentmachens durch Visualisierung, auditive
                  Aufbereitung und weitere empirische Anknüpfung durch materielle Spurensuche auf
                  dem Prüfstand (Kesselheim 2010). Die interdisziplinäre Kooperation lässt
                  gleichzeitig Methoden der verschiedenen Disziplinen produktiv zusammenwirken.
                  Entwicklung und Betrieb von Front- und Back-End finden aktuell in Form agiler
                  Softwareentwicklung statt, in die die beteiligten Akteure aus den
                  Kulturwissenschaften (insbesondere auch die Studierenden) fest eingebunden sind.
                  Weitere Schwerpunkte liegen auf der leichten Nutzbarkeit von Technologien,
                  beispielsweise einer einfachen Pflege der eingestellten Daten innerhalb eines
                  Web-Back-Ends sowie einer flankierenden kulturwissenschaftlichen Reflexion der
                  raumgenerierenden Potenziale neuer mobiler Systeme. Die Motivation des Projekts
                  liegt somit auch darin, die aus der Informatik heraus entwickelten neuen Formen
                  der Überblendung und Verkopplung physischer, kartographierter und medialer Räume
                  in ihren produktiven Momenten der Verräumlichung (Habscheid / Reuther 2013) und
                  der historischen Narrativierung kulturwissenschaftlich und experimentell
                  (Back-End-Editieren, Nutzerverhalten) zu begleiten. 
            Im Rahmen der ersten Projektphase werden derzeit drei historische Stadtrundgänge zum Heiligen Liborius, zu Kaiser Karl dem Großen und zu Bischof Meinwerk von Paderborn entwickelt. Diese historischen Persönlichkeiten sind für die Genese und Entwicklung der Stadt Paderborn im Frühmittelalter von besonderer Bedeutung gewesen. Sie sind auch heute noch in vielfältiger Weise im Stadtbild präsent. Unter anderem wird dies in einem weiteren Rundgang thematisiert, der sich historischen Orts- und Straßennamen widmet, denn auch in der Namensgebung von Orten und Straßen artikuliert sich das kulturelle Gedächtnis. Ein zentrales Anliegen unseres Projekts ist es, den Stadtraum historisch erfahrbar zu machen. Ziel ist es, unter anderem die Ungleichzeitigkeit des Gleichzeitigen herauszuarbeiten: Die in der heutigen Präsenz als in sich geschlossene Einheit sichtbaren Objekte sollen als historisch gewachsen erfahren werden, ihre einzelnen Elemente als aus unterschiedlichen
                    Epochen stammend. 
            Ein gutes Beispiel hierfür ist der Paderborner Dom (einführend: Quednau 2011),
                      der sowohl von vielen Ortsansässigen als auch von Touristen mit seinen
                      zahlreichen Artefakten und religiösen Objekten als gegebenes Bauwerk
                      wahrgenommen wird. Dass dieses Bauwerk aber keinesfalls statisch, sondern
                      vielmehr historisch gewachsen ist (u. a. Lobbedey 1986) und seit der
                      Grundsteinlegung unter Karl dem Großen im Jahre 777 im Laufe der Jahrhunderte
                      vielfältige bauliche Veränderungen, Erweiterungen und Überformungen erfahren
                      hat, ist nur wenigen bewusst. Hier setzt die HiP-App an.
                      Mit ihrer Hilfe ist es möglich, den Dom in seiner ganzen historischen Dimension
                      und Vielschichtigkeit für den Betrachter sichtbar und erfahrbar zu machen. Als
                      ein ganz konkretes Beispiel für die Ungleichzeitigkeit des Gleichzeitigkeit
                      bietet sich das Paradiesportal des Paderborner Doms an: 
            Ursprünglich als romanische Vorhalle konzipiert, wurde das Paradies im Zuge eines Umbaus des Westquerhauses mit einem Figurenportal ausgestattet. Dieses wies zunächst die Struktur eines rein ornamentalen Portals auf, dem in der Konzeptionsphase Sandsteinfiguren hinzugefügt wurden. Die monumentalen gotischen Gewändefiguren entsprechen dem zeitgenössischen Geschmack des 13. Jahrhunderts und orientierten sich an Skulpturen der französischen Gotik, beispielsweise denen der Kathedralen von Paris und Reims (Sauerländer 1971). Auf diese Weise 'modernisierte' also die Paderborner Dombauhütte das Hauptportal des Sakralbaus bereits im Hochmittelalter, und zwar in Anlehnung an die damals aktuelle und hochwertige Bauskulptur des französischen Königreichs. Die beiden romanischen Holzskulpturen des Portals, die den hl. Kilian und den hl. Liborius darstellen, stammen hingegen aus dem 12. Jahrhundert, wurden jedoch erst deutlich später, nämlich 1815, an den Portaltüre
                        n angebracht. Diese Veränderung und Ausgestaltung des Domportals eignet sich in geradezu idealer Weise, um den Nutzer_innen mit Hilfe von Augmented Reality innerhalb des Front-Ends die historische Dimension nicht nur des Portals, sondern exemplarisch auch des gesamten Doms vor Augen zu führen. 
            Für interessierte Nutzer_innen hält die HiP-App aber auch
                        vertiefende Informationsangebote bereit: So z. B. erläuternde Texte zu einzelnen
                        Skulpturen des Portals, die etwa typische, in der App farbig hervorgehobene
                        Attribute der Heiligen erklären, oder aber kulturhistorische Einordnungen mit
                        Blick auf die Nutzung (Tack 1958) oder die religiöse Praxis (Bawden 2014). Auch
                        können Brücken und Verbindungen zwischen verschiedenen Fachdisziplinen
                        geschlagen werden: So illustriert die zentrale Marienfigur des Trumeaus die
                        hochmittelalterliche Marienverehrung anhand des Melker Marienlieds, eines wenig
                        bekannten Textes aus dem 12. Jahrhundert. Die Nutzer_innen erfahren hier nicht
                        nur Wissenswertes zum Text, sondern sehen mit Hilfe der App auch - vielleicht
                        erstmals - eine mittelalterliche Handschrift. Dass Maria im Melker Marienlied
                        als Himmelspforte (porte des paradyses) bezeichnet wird, schlägt dabei eine
                        Brücke zur Portalsymbolik und gibt - über die Einzelbetrac htung des Portals
                        hinaus - einen Einblick in die Kulturgeschichte und spezieller noch in die
                        Liturgie. War die Nutzung des Portals - u. a. als erzbischöflicher Zugang und
                        als Gerichtsort - bereits zur Bauzeit vielschichtig, so erfuhr sie im Laufe der
                        nachfolgenden Jahrhunderte noch weitere Veränderungen und Ergänzungen. So bildet
                        das Domportal etwa noch heute den monumentalen Rahmen für die feierliche
                        Liborius-Prozession: Sie erinnert alljährlich an die Translation der Gebeine des
                        Heiligen aus dem damals westfränkischen, heute französischen Le Mans und an ihre
                        Ankunft in Paderborn im Jahre 836. Darüber hinaus werden in der App regionale
                        und überregionale Vergleichsobjekte (Sauerländer 1971; Lobbedey 1999)
                        vorgestellt, die eine kunsthistorische Einordnung in die Portalskulptur geben.
                        Damit macht die App eben gerade nicht nur die konkrete Stadt- und Baugeschichte
                        Paderborns anschaulich erlebbar, sondern greift darüber hinaus. Es geht gerade
                        auch darum, den Nutzer_innen der stadtgeschichtlichen App ein exemplarisches
                        Wissen zu vermitteln, durch das Sehgewohnheiten verändert und historische
                        Artefakte selbständig les- und erfahrbar werden.
         
         
            Digitalisierung von geschichtlichem Wissen im Raum und raumgebundener Erinnerungskultur – am Beispiel von Straßennamen 
            
               Kristina Stog (Paderborn)/ Nicole
                          M. Wilk (Paderborn) 
            
            
               Idee: Vervielfältigung der Lesarten durch neue Visualisierungsmethoden
               Wissensarten sind an Darstellungsformate gebunden. Informationen über das
                              Medienmaterial, seine Gestaltung, Beschaffenheit und Platzierung gehen bei
                              der Verarbeitung von Daten im Zuge der Digitalisierung größtenteils verloren
                              oder werden isoliert vom Textkorpus z. B. in Form von Metadaten gespeichert.
                              Dieser Verlust wird in den interpretierenden Disziplinen oft in Kauf
                              genommen, da quantitativ motivierte Fragestellungen bereits an diese
                              Reduktionssituation angepasst sind. Doch es setzt sich gleichzeitig in
                              linguistischen und (sozial)semiotischen Forschungskontexten die Erkenntnis
                              durch, dass Wissen immer situiertes Wissen in materiellen und
                              institutionellen Umgebungen ist (Fix 2008), und dass mit Blick auf
                              multimodale Gebrauchsmuster die Wahl der semiotischen Ressource (O'Halloran
                              2004) und nicht zuletzt die Raumbasiertheit von Kommunikation semantische
                              und Diskurs strukturierende Effekte haben können (Habscheid / Reuther 2013). 
               Kaum ein „Text“ ist so eng mit seinem Ort verknüpft wie ein aufgedruckter
                                „Name“. Am Beispiel der Namen für Straßen, Gebäude und Plätze zeigen wir in
                                unserem Beitrag Digitalisierungsmöglichkeiten auf, die die Verknüpftheit von
                                stadtgeschichtlichem Wissen mit Orten auf (technisch) verschiedene Weise
                                modellieren. Hierfür wird mit dem Smartphone ein mobiles Instrument gewählt,
                                das über eine App Schnittstellen zwischen materiellem und digitalem Raum
                                erzeugt (Weber 2012), um Stadtgeschichte in unterschiedlichen Deutungsrahmen
                                (visuell, auditiv) verfügbar zu machen. Der Raum erweist sich dabei als
                                interaktive Ressource (Hausendorf / Mondada / Schmitt 2012), auf die die
                                kulturellen Sinnangebote ausgerichtet sind und die sie selbst als solche
                                hervorbringen (reflexiver Raumbegriff). 
               Mit den neuen technologischen Verfahren erschöpft sich die interaktive Dimension nicht in der Aufmerksamkeitslenkung durch schriftbasierte oder bildliche Information, mithilfe standortbezogener Informationen durch Location-based Services kann Nutzer_innen vielmehr an konkreten Orten durch eine mittelalterliche Geräuschkulisse oder eine visuelle Anreicherung des Stadtbilds ein Einstieg in historische Szenarien geboten werden. 
            
            
               Hintergrund: Straßennamen als Kondensate kulturellen Wissens und der Inanspruchnahme von Geschichte
               Namen von Straßen, Plätzen und anderen Örtlichkeiten (Toponyme) dienen der
                                  Orientierung. Sie erschließen den Raum und strukturieren ihn physisch und
                                  historisch, zugleich können Toponyme als Verweise auf die Geschichte sowie
                                  auf das Geschichtsbewusstsein einer Stadt gelesen werden. Anders als
                                  Denkmäler, die aufgrund ihrer erinnernden Funktion bewusst aufgesucht
                                  werden, stellen sie – in ihrer Sekundärfunktion – „Medien kultureller
                                  Erinnerung“ (Pöppinghege 2005: 10) dar, die von den Rezipient_innen im
                                  urbanen Raum täglich genutzt werden. Vor allem die frühen Orts- und
                                  Straßennamen, die aufgrund von gemeinsamen Gewohnheiten, Bedürfnissen oder
                                  Wahrnehmungen in der Interaktion (bereits im Mittelalter) gewachsen sind
                                  (Fuchshuber-Weiß 1996) geben Hinweise auf Vergangenes, im heutigen Stadtbild
                                  möglicherweise nicht mehr Sichtbares: Geographische bzw. topographische
                                  Merkmale oder Besonderheiten des Ortes, nennenswerte Gebäude in der
                                  Umgebung, eine bestimmte Nutzung des Bezugsbereiches (wie etwa dort
                                  angesiedeltes Gewerbe) oder soziokulturelle Tatbestände vor Ort
                                  (Fuchshuber-Weiß 1996). 
               Namen stellen dabei keinen schlichten Spiegel tatsächlicher Gegebenheiten dar, sondern geben Einblick in die kollektiven Sicht- und Vorstellungsweisen ihrer Nutzer und bilden „in ihrer auswählenden und akzentuierenden Thematisierung des Stadtraumes Dokumente einer Mentalitätsgeschichte des Sehens“ (Glasner 1999: 320). In dieser Hinsicht gibt auch die heutige Benennungspraxis Aufschluss über das Geschichtsbewusstsein einer Stadt: So spiegelt sich etwa in der Vergabe von Namen, die sich auf Historisches „vor Ort“ beziehen, auch das „kultur- und alltagsgeschichtliche Verständnis“ (Pöppinghege 2005: 10) einer Stadt. Lokalen Ereignissen, Personen, aber auch Intentionen oder Vorstellungen, mit denen sich eine Stadt identifiziert, werden in Form von Straßen(-namen) begehbare „Zeichen gesetzt“. 
               In der Erarbeitung toponymischen Wissens ergibt sich eine Herausforderung daraus, dass verschiedene Wissenssorten zusammenkommen: Legendenbildungen, Volksetymologien, Geschichtswissen der Historiker und ein zeitabhängiges Geschichtsbewusstsein, das teilweise mit einer intensiven Geschichte der Umbenennung einhergeht. Beobachtungen zur thematischen Verarbeitung von Namensgebung und Namensgeschichte in bestehenden Apps zur Stadtgeschichte belegen das allgemeine Bedürfnis nach „Lesbarkeit“ von urbanen Räumen: Namen werden als Spuren geschichtlicher Zusammenhänge aufgeschlossen. Doch wie werden durch sie Diskurse räumlich materialisiert? Wie können Wissenssorten auch durch mediale Operationen reflektiert werden? Das Beispiel der in Entwicklung befindlichen 'Historisches Paderborn'-App soll aufzeigen, wie die doppelte Situiertheit des stadtgeschichtlichen Wissens in einem metakommunikativen und in einem räumlichen Sinne durch Visualisierungstechniken darg
                                    estellt werden kann. 
            
            
               Ausgangslage: Typonyme in bisherigen Kommunikationsangeboten und Apps zur Stadtgeschichte
               Um in mobilen Geräten Geschichtliches auf neue experimentelle Weise darzustellen, müssen zunächst die traditionellen Repräsentationen geschichtlichen Wissens im Stadtraum hinsichtlich ihrer raumstiftenden Qualitäten erschlossen werden. Straßen- und Gebäudenamen haben trotz des in ihnen sedimentierten impliziten Wissens über Ereignisse der Stadtgeschichte für viele Bewohner und Stadtbesucher einen primär pragmatischen Sinn und dienen der räumlichen Orientierung. Selbst Legenden und Volksetymologien, die sich um die Namen im Stadtraum ranken, drohen verloren zu gehen. Auf diese Situation reagieren ortsfeste und ortsgebundene digitale Kommunikationsangebote zur Stadtgeschichte, die den urbanen Raum als Medium des kollektiven Gedächtnisses und mit ihm eine völlig neue städtische Erzählkultur etablieren. In einer medienlinguistischen Studie zur Musterhaftigkeit ortsfester Kommunikationsangebote zur Stadtgeschichte konnten zwei wesentliche Tendenzen in
                                      der Entwicklung internetbasierter Formate und Stadtgeschichts-Apps festgestellt werden: der Ausbau einer dialogischen Sequenzierung stadtgeschichtlichen Wissens und die Narrativisierung urbaner Erzählsequenzen („Histörchen“). Ohne Angabe von Quellen und ohne Verweise auf die Deutungsvielfalt der historischen Dokumente werden Brauchtümer und Motive sprachlich so dargelegt als seien die Namen Repräsentationen einer zu allen Zeiten und eindeutig herauszulesenden historischen Faktizität (vgl. Wilk 2015). Der Einsatz multimodaler Darstellungsformen verspricht hier Möglichkeiten, Namen und Namenswandel exemplarisch in einem Spurkonzept zu modellieren (vgl. Müller 2012), das Typonyme im linguistischen Sinn weniger als objektive Zeugen eines geschichtlichen Geschehens aufschließt als vielmehr anhand der Namensgebung den Kampf um historische Lesarten und ihre Orientierung für die Zukunft verdeutlichen. Aufgabe der Medienlinguistik ist es dabei, anhand konkreter Textentwicklunge
                                      n zu beschreiben, wie unter Nutzung verschiedener Daten aus den Visualisierungen eines durch historische Szenarien erweiterten Stadtraums unterschiedliche historische Interpretationen hervorgehen. 
            
            
               Das Beispiel Paderborn – Motiviertheit und sozialer Sinn hinter den Spuren
               Wie Namen als Spuren von Vergangenem im heutigen Stadtbild gelesen werden
                                        können, lässt sich anhand einiger Straßennamen in der Paderborner Innenstadt
                                        beispielhaft zeigen: Sie können Hinweise auf das historische Stadtbild
                                        geben, wie etwa der Name Grube, der als einer der ältesten Straßennamen in
                                        der Altstadt auf die heute nicht mehr sichtbare Grube, die nach Auffüllung
                                        eines Steinbruchs südlich der Domburg im 12. Jahrhundert zu sehen war (vgl.
                                        Liedtke 1999: 101), verweist. Namen wie Im Düstern oder Krummer Ellenbogen
                                        geben darüber hinaus Einblicke in die Wahrnehmung des städtischen Raums aus
                                        der Perspektive ihrer Nutzer_innen. Neben Anwohnergruppen (Weberberg)
                                        spiegeln sich in Namen bestimmte Nutzungsweisen von Straßen (wie etwa Kühe
                                        durch die Kuhgasse zur Tränke an die Pader zu treiben (vgl. Liedtke 1999)).
                                        Auch Spuren des Niederdeutschen, das als gesprochene Alltagssprache in
                                        Paderborn kaum noch existiert, finden sich in Namen wie Abtsbrede (Brede
                                        bezeichnet einen breiten Acker) oder Börnepader (börnen: tränken). 
               Nach Nübling (2012: 244) lassen sich aus diesen primären Straßennamen, die in engem Zusammenhang mit den Straßen, die sie bezeichnen, entstanden sind, „Topographie und Sozialgeschichte einer Stadt hervorragend rekonstruieren“. In einer App zum Historischen Paderborn bilden sie nicht nur einen Anknüpfungspunkt für die Auseinandersetzung mit dem historischen Raum, sondern auch mit den kollektiven Sicht- und Vorstellungsweisen ihrer Nutzer_innen. Dies gilt auch für die sekundären Straßennamen, die administrativ vergeben werden: Die Wahl der regionalen und überregionalen Personen und Ereignisse, nach denen eine Stadt ihre Straßen benennt, gibt Aufschluss über ihr (Stadt-)Geschichtsbewusstsein (vgl. Pöppinghege 2007). So deutet etwa die Benennung der Straßen eines Viertels in Paderborn, in dem neben der Karlsstraße und dem Karlsplatz auch Albin-, Gerold-, Einhard- und Widukindstraße auf historische Zusammenhänge und Personen im Umfeld Karls des
                                          Großen verweisen, auf die Bedeutung hin, die dieser im Selbstverständnis der Stadt einnimmt. 
               Politische und gesellschaftliche Umbrüche werden vor allem in der Auseinandersetzung mit den Umbenennungen von Straßen oder Plätzen sichtbar. In ihnen spiegeln sich die Vorstellungen, Ideen und Ideologien, zu deren Verbreitung Toponyme seit dem 18. Jahrhundert genutzt werden (vgl. Fuchshuber-Weiß 1994: 1472). Deutlich wird dies am Beispiel des Le-Mans-Walls in Paderborn, der bis 1938 als Wilhelmstraße, in der Zeit des Nationalsozialismus als Horst-Wessel-Wall und nach 1945 erneut als Wilhelmstraße bezeichnet wurde, bis 1967 mit der Städtepartnerschaft die Umbenennung nach der französischen Stadt Le Mans folgte (vgl. Liedtke 1999: 151). Mit der Benennung, die sich nun zugleich an mittelalterlichen Ereignissen orientierte, wurde der Straße – als Weg, über den 836 der Zug mit den Reliquien des Hl. Liborius von Le Mans in Richtung Dom geführt haben soll – somit auch eine größere Bedeutung innerhalb der Stadtgeschichte Paderborns zugesprochen. 
               In der Hip-App lässt sich die Geschichte dieser
                                          Umbenennung und der Vereinnahmung historischer Persönlichkeiten für die
                                          städtische Identität multiperspektivisch visualisieren, so dass anschaulich
                                          wird, wie zu verschiedenen Zeitpunkten Historisches in städtischen
                                          Strukturen repräsentiert (worden) ist. Diese Repräsentationen der
                                          historischen Traditionen schließen zudem die Konsequenzen für das
                                          (Geschichts-)Bild der gegenwärtigen Stadt und der Stadt der Zukunft auf. 
               Hierbei sollen epistemischen Effekte, d.h. insbesondere
                                            komplexitätsreduzierende Wissenseffekte verschiedener Darstellungsweisen
                                            (Karten, archäologische Modelle, Einblendungen) exemplarisch erfasst werden.
                                            Diese variieren mit der Auswahl der Kategorien, der Relationierung markanter
                                            Ereignisse und nicht zuletzt der Herstellung von Bezügen zum materiellen
                                            Raum. Die Differenzierung von Erzählzeit und erzählter Zeit reflektiert
                                            dabei die Variabilität historischer Sinnstiftung: So können historische
                                            Figuren (Könige, Ritter, Pilger etc.), die z. B. mit Pferdegetrappel an
                                            ausgewählten Stellen des Stadtrundgangs die Wege der Nutzer_innen kreuzen
                                            und damit historische Situationen auf dem Hellweg simulieren, eine
                                            mittelalterliche Vergangenheit einerseits behaupten. Andererseits lassen sie
                                            sich anschließen an eine stadttypische Rezeptionsgeschichte
                                            mittelalterlicher Quellen und Schriften. In der (zusätzlichen) Darstellung
                                            gewandelter städtischer Identitätsdiskurse z. B. über kartografierte
                                            Wissensbezüge (Bezug zum Mittelalter, zur niederdeutschen Varietät, zur
                                            Geografie etc.) lässt sich ein jeweils zeitabhängiges Geschichtsbewusstsein
                                            veranschaulichen.
            
         
         
            Auf dem Weg zu einer experimentellen und evidenzbasierten Softwareentwicklung in den Digital Humanities
            
               Björn Senft, Simon Oberthür – SICP
                                            – Software Innovation Campus Paderborn, Universität Paderborn
            
            Das interdisziplinäre Projekt ‚Historisches Paderborn'-App, kurz HiP-App, ist ein gutes Beispiel für die sich verändernden
                                              Anforderungen an den Software-Entwicklungsprozess im DH-Kontext bzw. im Kontext
                                              des digitalen Wandels (Digital Transformation). Klassische Entwicklungsmodelle
                                              wie das Wasserfallmodell wurden für Situationen entworfen, in denen
                                              Funktionsumfang und Aufbau der Software zu Beginn der Entwicklung relativ genau
                                              festgelegt werden können. 
            Für die HiP-App ist dies jedoch aus mehreren Gründen nicht
                                              möglich. Aufgrund der Verwendung neuer Technologien in der App und der Raum- und
                                              Zeitgebundenheit der Inhalte ergeben sich vollkommen neue Wege, mit Wissen
                                              umzugehen. Das Dilemma ist nicht gerade selten: Informatiker verfügen über das
                                              Wissen um moderne Technologien, Kulturwissenschaftler verfügen über das Wissen
                                              der zu vermittelnden Inhalte. Ein sinnvoller und innovativer Einsatz neuer
                                              Technologien kann aber nur in enger Verzahnung mit konkreten Inhalten erfolgen.
                                              Oft kann auch eine Verzahnung vorab nicht hinreichend beurteilt werden, sondern
                                              muss beispielsweise experimentell bewertet werden. Die sinnvolle Anwendung neuer
                                              Technologien ist deshalb ein Forschungsdesiderat und muss durch geeignete
                                              Entwicklungsmethoden und -abläufe unterstützt werden. 
            Weil in unserem Kontext die Anforderungen vor der Implementierung nicht genau
                                                spezifiziert werden können, muss erforscht werden, welche Faktoren zielführend
                                                für die Lösung der zu bewältigenden Entwicklungsaufgabe sind. Da die Informatik
                                                mit dieser Situation relativ häufig konfrontiert ist, wurden hierfür Methoden
                                                wie etwa die agile Entwicklung gebildet. Das ist allerdings nur ein möglicher
                                                Ansatz zur Lösung dieses Problems, da die Methoden nur einen lockeren Rahmen
                                                bieten und nicht genauer darauf eingehen, wie mit der Software experimentiert
                                                werden kann, geschweige denn, wie eine systematische Extraktion und Evaluation
                                                in diesem Kontext aussehen könnte. Sinnvoll wäre beispielsweise ein
                                                Softwareleitstand, der Experimente mit verschiedenen Nutzergruppen (Betatester,
                                                Endnutzer, Experten, etc.) bzw. Ebenen (Simulation, Menschen, etc.) ermöglicht.
                                                Erschwerend kommt dabei hinzu, dass Prozesse verschiedener Domänen
                                                (Kulturwissenschaften, kulturelle Institutionen, etc.) in den eigentlichen
                                                Softwareentwicklungsprozess integriert werden müssen.
            
               Lösungsansatz
               In unserem Projekt entwickeln wir ein mensch-zentriertes Prozessmodell (siehe
                                                    Abbildung 1), dass das DevOps-Prinzip (Hüttermann 2012; Sharma / Coyne 2015)
                                                    mit dem Führungskreislauf des St. Galler Management Modells (Ulrich / Krieg
                                                    1974) kombiniert, um so die vielschichtige Verzahnung von Technologien,
                                                    Inhalten, Domänen und Akteuren (Informatiker, Kulturwissenschaftler,
                                                    Usability-Experten, Nutzer, etc.) zu gewährleisten. Die Grundidee hinter
                                                    diesem Modell ist die Aufteilung in generellere Phasen, um so
                                                    Verzahnungspunkte für die unterschiedlichen Prozesse zu definieren. In der
                                                    Ermittlungsphase werden mit Methoden der Informatik und der
                                                    Kulturwissenschaft Daten aus der Realität (Verwendung des Prototyps,
                                                    Interviews, Unit-Tests, ‚ausgelieferte‘ Software, etc.) ermittelt, um so ein
                                                    Lagebild zu erstellen, das die Grundlage für die Weiterarbeit in der
                                                    Strategie- und Analysephase bildet. In dieser kommen die unterschiedlichen
                                                    Akteure zus ammen und analysieren gemeinsam die ermittelten Daten und
                                                    beraten über die zukünftige Strategie, die in der Realisierungsphase
                                                    (Implementierung, Entwickeln eines konkreten Interviews, etc.) umgesetzt und
                                                    anschließend erneut in der Realität eingesetzt und evaluiert wird. Wichtig
                                                    ist dabei - da es kein definiertes Ende gibt -, die einzelnen Phasen
                                                    kontinuierlich und iterativ zu durchlaufen, da wir nach Drucker et al.
                                                    (2012) davon ausgehen, dass sich die Software, aufgrund der hieraus
                                                    entstehenden neuen Erkenntnisse, ständig weiterentwickeln wird: 
                „Digital Humanities work embraces the iterative, in which experiments are
                                                      run over time and become objects open to constant revision. Critical design
                                                      discourse is moving away from a strict problem-solving approach that seeks
                                                      to find a final answer: Each new design opens up new problems
                                                      and—productively—creates new questions.“ (Drucker et al. 2012: 22).
               
                  
                  
                     Abb. 1: Verwendetes Prozessmodell der HiP-App-Entwicklung im Digital Humanities Kontext
               
            
            
               Erfahrungen und Erkenntnisse
               Der bisher entwickelte Teil des Lösungsansatzes basiert auf den Erfahrungen,
                                                        die bislang bei der Entwicklung der HiP-App gemacht
                                                        wurden. Eine studentische Projektgruppe der Informatik entwickelte in
                                                        stetiger Rückkopplung mit den Kulturwissenschaften das Backend zum
                                                        Einpflegen der Daten. Die Studierenden entwickeln die Software nach Ansätzen
                                                        der agilen Softwareentwicklung (Beck et al. 2001) und nach dem
                                                        DevOps-Prinzip (Hüttermann 2012; Sharma / Coyne 2015), die für uns
                                                        Schlüsselfaktoren sind, um einen kontinuierlichen mensch-zentrierten
                                                        Softwareentwicklungsprozess (Mayhew / Follansbee 2012) mit explorativen
                                                        Möglichkeiten zu erreichen.
               Dass eine enge Kooperation von Informatik und Kulturwissenschaften im Sinne der Digital Humanities notwendig ist, hat sich bereits in den ersten Arbeitsphasen des Projekts bestätigt. Wie bereits erwähnt, können sinnvolle Anwendungen nur in enger Verzahnung von Inhalten und Technologien entstehen. Informatik und Kulturwissenschaften müssen deshalb interagierend Daten auswerten und die Strategie anpassen. Diese Erkenntnis ist eine Quintessenz aus unserer Projekterfahrung. 
               Den involvierten Kulturwissenschaftlern fehlten anfangs Bewusstsein und
                                                          Wissen über die notwendige Spezifizität, über den Realisierungsaufwand und
                                                          auch über die Nachhaltigkeit der Softwareentwicklung, die für eine
                                                          zielgerichtete Entwicklung qualitativ hochwertiger Software jedoch
                                                          essenziell sind. Die beteiligten Informatiker verloren sich dagegen allzu
                                                          schnell in technologischen Herausforderungen anstatt die Nutzeranforderungen
                                                          zu fokussieren. Es bedarf deshalb eines gemeinsamen Verständnisses und einer
                                                          gemeinsamen Strategie, welche Merkmale einer Anwendung welche Priorität
                                                          haben und wann diese implementiert werden sollen oder aber mit Hilfe anderer
                                                          Frameworks zu realisieren sind. Um wichtige technologische Entscheidungen
                                                          treffen zu können, müssen sich die Anforderungen herauskristallisieren, die
                                                          sich im Detail aus den Prozessen und Inhalten ergeben. Daher erscheint uns
                                                          ein Verhältnis Dienstleister (Informatik) und Content-Lieferant
                                                          (Kulturwissenschaften) für die Entwicklung von Software im Kontext von
                                                          Digital Humanities wenig sinnvoll, ja sogar kontraproduktiv. 
               Festzuhalten ist, dass unser Prozessmodell möglichst kurze Durchläufe erlaubt, um so frühzeitig neue Erkenntnisse zu gewinnen, die dann zeitnah in die weitere Softwareentwicklung einfließen können. Die kurzen Wiederholungen im Prozessmodell helfen, nicht-verbalisierbares Nutzerwissen verfügbar zu machen. Solches Wissen ist beispielsweise grundlegend, um die Lösungen bestmöglich auf die Bedürfnisse der Nutzer auszurichten.
               Unser Lösungsansatz ermöglicht ein Experimentieren, das nicht nur auf Softwareprototypen bezogen ist. Beim gemeinsamen „Design Thinking“ (Uebernickel et al. 2015) von Informatikern und Kulturwissenschaftlern haben wir die Erfahrung gemacht, dass erst und vor allem das häufige Evaluieren und Experimentieren dabei hilft, sich von technologischen und organisatorischen Restriktionen zu lösen und stattdessen sinnvolle Anwendungen zu identifizieren. Es müssen zudem technologische Konzepte entwickelt werden, die sowohl ein Experimentieren mit verschiedenen Varianten als auch eine Evolution von Software-Architektur und -Design und ein Reagieren auf fehlerhaften Code (Resilience) erlauben.
            
            
               Abgrenzung vom aktuellen Stand der Forschung und Technik
               Bisherige Ansätze in der Softwaretechnik sind vor allem mit Blick auf das
                                                            Extrahieren und Experimentieren unzureichend. Ein klassisches
                                                            Entwicklungsmodell in der Informatik ist das Wasserfallmodell (Royce 1970),
                                                            in dem bestimmte Phasen wie Anforderungserhebung, Systementwurf und
                                                            Implementierung lediglich einmal durchlaufen werden. Dieses Modell ist vor
                                                            allem für solche Entwicklungen geeignet, die bereits existente Prozesse
                                                            digitalisieren sollen. Wenn jedoch neue digitale Prozesse entwickelt werden
                                                            sollen, wirkt sich bei diesem Modell nachteilig aus, dass Fehlentwicklungen
                                                            erst am Ende des Prozesses sichtbar werden, also erst dann, wenn die
                                                            Software als Ganzes bereits fertig ist. Um schneller auf sich ändernde
                                                            Anforderungen reagieren zu können, wurden deshalb agile Methoden entwickelt,
                                                            deren Rahmen mithilfe des agilen Manifest (Beck et al. 2001) definiert
                                                            werden. Zur Grundidee der agilen Softwareentwicklung gehören kurze, feste
                                                            Iterationen mit de m Ziel, möglichst frühzeitig lauffähige Produktinkremente
                                                            auszuliefern. So können öfter Rückmeldungen vom Kunden eingeholt und
                                                            Fehlentwicklungen frühzeitig erkannt werden. Vor allem aber ist dieser
                                                            Prozess auch transparenter für den Kunden, da er regelmäßig Fortschritte
                                                            sieht. Da dieser Ansatz davon ausgeht, dass man zwangsläufig ‚scheitern‘
                                                            wird, soll das Scheitern im Kleinen stattfinden, um so Potenzierungseffekte
                                                            zu minimieren. Die agilen Methoden haben jedoch den Nachteil, dass sie
                                                            lediglich einen Rahmen bilden und keine Spezifika bieten, wie z. B. konkret
                                                            experimentiert werden kann oder soll. Als Anforderungen werden im agilen
                                                            Ansatz Scrum User Stories verwendet, die aus Nutzersicht die gewünschten
                                                            Funktionalitäten beschreiben. Um eine gute Produktqualität zu erreichen,
                                                            muss Scrum (Sutherland / Schwaber 2007) mit klassischen Ansätzen kombiniert
                                                            werden. 
               Im Gegensatz zu den hier erläuterten Ansätzen bietet das von uns vorgestellte
                                                              Modell durch den ständig wiederkehrenden Dialog der domänenübergreifenden
                                                              Akteure sowie das Experimentieren die Möglichkeit, die Entwicklung neuer
                                                              Methoden und Werkzeuge systematisch zu unterstützen. Gestützt von Prozessen
                                                              können so neue Technologien experimentell auf ihre Anwendbarkeit untersucht
                                                              werden. Ein Aspekt, der sich aktuell in der Projektarbeit der HiP-App bereits bestätigt hat.
            
            
               Übergeordnete offene Fragestellungen
               In ihren Anfängen zeichneten sich die Digital Humanities hauptsächlich durch die Übertragung bewährter Konzepte der Informatik aus. Es ist aber zu fragen, ob die Informatik nicht stärker von den Kulturwissenschaften lernen kann? Wäre es für die Informatik beispielsweise nicht hilfreich, verstärkt auch soziologische Methoden (qualitative Methoden wie Experteninterviews, quantitative Verfahren, etc.) für den Prozess der Anforderungserhebung und des Experimentierens zu adaptieren? Wie aber könnte das in der Softwareentwicklung praktikabel und systematisch angewandt werden? Diese offenen Fragestellungen gilt es weiterhin im Auge zu behalten.
            
         
      
      
         
            
               Bibliographie
               
                  Bawden, Tina (2014): Die Schwelle
                                                              im Mittelalter (= Sensus 4). Köln / Weimar / Wien:
                                                              Böhlau-Verlag.
               
                  Beck, Kent et al. (2001): Manifesto
                                                              for agile software development
                  http://agilemanifesto.org
                                                              [letzter Zugriff 20. Januar 2016].
               
                  Burdick, Anne / Drucker, Johanna / Lunenfeld, Peter /
                                                                Presner, Todd / Schnapp, Jeffrey (2012): Digital_Humanities. Cambridge: The MIT Press / Massachusetts
                                                                Institute of Technology.
               
                  Buschauer, Regine / Willis, Katharine S. (2013): Locative Media. Medialität und Räumlichkeit.
                                                                Multidisziplinäre Perspektiven zur Verortung der Medien. Bielefeld:
                                                                Transcript.
               
                  Fix, Ulla (2008): "Nichtsprachliches als Textfaktor.
                                                                Medialität, Materialität, Lokalität", in:  Zeitschrift für
                                                                Germanistische Linguistik 36, 3: 343–354. 
               
                  Fuchshuber-Weiß, Elisabeth (1996): "Straßennamen:
                                                                deutsch / Street Names: German / Noms de rues: domaine allemand", in:
                                                                Eichler, Ernst / Hilty, Gerold / Löffler, Heinrich / Steger, Hugo / Zgusta,
                                                                Ladislav (eds.): Namenforschung. Ein internationales
                                                                Handbuch zur Onomastik / Name Studies. An
                                                                International Handbook of Onomastics / Les noms
                                                                propres. Manuel international d'onomastique (= Handbücher zur
                                                                Sprach- und Kommunikationswissenschaft 11,2). Berlin: De Gruyter 1465-1468. 
               
                  Glasner, Peter (1999): "Ein sprachhistorischer
                                                                Beitrag zur Semiotik der Stadt: das Pilotprojekt 'Kölner Straßennamen'", in:
                                                                Muttersprache 109: 316-330. 
               
                  Habscheid, Stephan / Reuther, Nadine (2013):
                                                                "Performatisierung und Verräumlichung von Diskursen. Zur soziomateriellen
                                                                Herstellung von ‚Sicherheit‘ an öffentlichen Orten", in: Felder, Ekkehard
                                                                (ed.): Faktizitätsherstellung in Diskursen. Die Macht
                                                                des Deklarativen (= Sprache und Wissen 13). Berlin / New York: de Gruyter
                                                                127–145.
               
                  Hausendorf, Heiko / Mondada, Lorenza / Schmitt, Reinhold
                                                                  (eds.) (2012): Raum als interaktive Ressource
                                                                (= Studien zur Deutschen Sprache 62). Tübingen: Narr. 
               
                  Hüttermann, Michael (2012): DevOps
                                                                for Developers. New York: Apress. 
               
                  Kesselheim, Wolfgang (2010): "'Zeigen, erzählen und
                                                                dazu gehen': Die Stadtführung als raumbasierte kommunikative Gattung", in:
                                                                Costa, Marcella / Müller-Jacquier, Bernd (eds.): Deutschland als fremde Kultur. Vermittlungsverfahren in
                                                                Touristenführungen. München: Iudicum 244–271. 
               
                  Liedkte, Gerhard (1999): Abbestraße
                                                                bis Zwetschenweg. Straßennamen in Paderborn. Paderborn: H&S
                                                                Verlag. 
               
                  Lobbedey, Uwe (1986): Die
                                                                Ausgrabungen im Dom zu Paderborn 1978/80 und 1983 (= Denkmalpflege
                                                                und Forschung in Westfalen 11). Bonn. Habelt.
               
                  Lobbedey, Uwe (1999): Romanik in
                                                                Westfalen. Würzburg: Zodiaque-Echter.
               
                  Mayhew, Deborah J. / Follansbee, Todd J. (2012):
                                                                "User Experience Requirements Analysis within the Usability Engineering
                                                                Lifecycle", in: Jacko, Julie A. (ed.): The
                                                                Human-Computer-Interaction-Handbook. Boca Raton, FL, USA: CRC Press
                                                                945–953.
               
                  Müller, Marcus (2012): "Geschichte als Spur im Text",
                                                                in: Jacko, Julie A. / Bär, Jochen A. (eds.): Geschichte
                                                                der Sprache – Sprache der Geschichte. Probleme und Perspektiven der
                                                                historischen Sprachwissenschaft des Deutschen. Berlin 159–179.
               
                  Nübling, Damaris (2012): Namen. Eine Einführung in die Onomastik. Tübingen: Narr. 
               
                  O'Halloran, Kay L. (ed.) (2004): Multi-modal Discourse Analysis. Systemic Functional Perspectives.
                                                                London / New York. 
               
                  Pöppinghege, Rainer (2005): Geschichte mit Füßen getreten: Straßennamen und Gedächtniskultur in
                                                                Deutschland (= Paderborner Universitätsreden 94). Paderborn:
                                                                Universitätsverlag Paderborn. 
               
                  Pöppinghege, Rainer (2007): Wege
                                                                des Erinnerns. Was Straßennamen über das deutsche
                                                                Geschichtsbewusstsein aussagen. Münster: Agenda. 
               
                  Quednau, Ursula (2011): Handbuch
                                                                der deutschen Kunstdenkmäler. Nordrhein-Westfalen II. Westfalen.
                                                                Berlin / München: Deutscher Kunstverlag.
               
                  Royce, Winston W. (1970): "Managing the development
                                                                of large software systems", in: Proceedings of IEEE
                                                                WESCON 26, 8: 328-388.
               
                  Rüsen, Jörn (1996): "Historische Sinnbildung durch
                                                                Erzählen. Eine Argumentationsskizze zum narrativistischen Paradigma der
                                                                Geschichtswissenschaft und der Geschichtsdidaktik im Blick auf
                                                                nicht-narrative Faktoren", in: Internationale
                                                                Schulbuchforschung 18: 501–543.
               
                  Sauerländer Willibald (1971): "Die
                                                                kunstgeschichtliche Stellung der Figurenportale des 13. Jahrhunderts in
                                                                Westfalen", in: Westfalen 49: 1-76. 
               
                  Schüttpelz, Erhard (2006): "Die
                                                                medienanthropologische Kehre der Kulturtechniken", in: Engell, Lorenz /
                                                                Siegert, Bernhard / Vogl, Joseph (eds.): Kulturgeschichte
                                                                als Mediengeschichte (oder vice versa?). Weimar: Universitätsverlag
                                                                Weimar 87–110. 
               
                  Sharma, Sanjeev / Coyne, Bernie (2015): DevOps for Dummies. 2nd IBM Limited Edition. Hoboken:
                                                                John Wiley & Sons, Inc. 
               
                  Sutherland, Jeff / Schwaber, Ken (2007): The Scrum Papers. Nuts, Bolts, and Origins of an
                                                                Agile Method. Boston: Scrum, Inc.
               
                  Tack, Wilhelm (1958): "Die Paradies-Vorhalle des
                                                                Paderborner Domes und die Wallfahrt nach Santiago de Compostela", in: Alte und Neue Kunst im Erzbistum Paderborn 8:
                                                                27-62.
               
                  Uebernickel, Falk / Brenner, Walter / Pukall, Britta /
                                                                  Naef, Therese / Schindlholzer, Bernhard (2015): Design Thinking – Das Handbuch. Frankfurt am Main: Frankfurter
                                                                  Allgemeine Buch.
               
                  Ulrich, Hans / Krieg, Walter (³1974): St. Galler Management-Modell. Bern: Haupt.
               
                  Weber, Heike (2012): "Urbanisierung und Umwelt: Ein
                                                                  Plädoyer für den Blick auf Materialitäten, Ressourcen und urbane
                                                                  ,Metabolismen'", in: IMS. Informationen zur modernen
                                                                  Stadtgeschichte 2: 28-35.
               
                  Wilk, Nicole M. (2015): "'Gebäude erzählen
                                                                  Geschichte(n)'. Medienlinguistische und diskursgrammatische Untersuchung zur
                                                                  multimodalen Herstellung historischer Stadt-Räume durch Schilder, Pulte,
                                                                  Stelen, Mobile Tagging und Apps", in: Networks. Die
                                                                  Online-Schriftenreihe des Projekts mediensprache.net 72: http://www.mediensprache.net/networx/networx-72.pdf [letzter
                                                                  Zugriff 20. Januar 2016].
            
         
      
   



      
         Public History (vgl. einführend Zündorf 2010) ist im deutschsprachigen Raum ein noch junges Feld, die erste Professur wurde erst Ende 2012 in Heidelberg eingerichtet. Die Disziplin ist zurückzuführen auf die doppelte Erkenntnis, dass die Mehrheit der Fachstudierenden nicht in der Geschichtswissenschaft wird arbeiten können (und dementsprechend zielgerichtet in Vermittlungskompetenzen aller Art geschult werden muss) und dass die meisten HistorikerInnen sich zwar über mangelnde Aufmerksamkeit für ihr Fach nicht beklagen können, demgegenüber aber kaum wissenschaftlich valide Werkzeuge für den Umgang mit der Öffentlichkeit entwickelt wurden.
         Paradoxerweise scheint die Public History trotz ihres modernen Selbstanspruchs den Fehler der herkömmlichen Geschichtswissenschaft zu wiederholen: Die Digitalisierung ihrer Arbeit bleibt weit hinter den technischen Möglichkeiten zurück und beschränkt sich größtenteils auf die Erleichterungen einer erweiterten Schreibmaschine. Doch Öffentlichkeiten, die sie schon ihrem Namen nach im Blick hat, migrieren zusehends in den digitalen Raum der sozialen Netzwerke und sollten genau dort angesprochen werden.
         Eine Möglichkeit, die digitale Teilöffentlichkeit zu erreichen, bietet das soziale Netzwerk Twitter. Seit ungefähr sechs Jahren werden dort historische Ereignisse in je maximal 140 Zeichen zeitgenau nacherzählt, was unter den Bezeichnungen „Re-Entweetment“ oder auch “Twhistory” bekannt geworden ist. Dieses Potential des Medium wurde bislang fast ausschließlich von Laien genutzt, so über die Accounts 
                @TitanicRealTime und das MDR-Projekt 
                @9Nov89live, das über einen Tag eine fiktive Geschichte des Mauerfalls zeichnete. In jüngerer Zeit wird es aber zunehmend auch von einer geringen Zahl von (Public) Historians aktiv angeboten, beispielsweise für 
                @NRWHistory und das Zweitweltkriegsprojekt 
                @DigitalPast, zu dem parallel das Sachbuch “Als der Krieg nach Hause kam” (Hoffmann 2015) veröffentlicht wurde. Wahrscheinlich besser als jede andere Medienform bietet Twhistory die Möglichkeit der Erzählung in Echtzeit als nicht-textlichem Inhalt, über den Geschichte lebendig gemacht und vorhandenes historisches Interesse (re-)aktiviert werden kann.
            
         Insbesondere die Zeichenbegrenzung ist für das Re-Entweetment Chance und Risiko zugleich: die Einstiegsschwelle ist im Vergleich zu herkömmlichen Darreichungsformen (Buch, Museum) äußerst gering, zugleich besteht die Gefahr der Simplifizierung sowie der Falschdarstellung von Geschichte als Aneinanderkettung von Einzelereignissen. Trotz der mittlerweile international steigenden Projektzahl hat sich noch keine Best Practice ergeben, um diesen Risiken zu begegnen. Dadurch ist auch die Zahl der digitalen Tools für diesen Bereich noch sehr klein, die Liste der Desiderate an die Digital Humanities aber lang und äußerst divers. Beispielsweise sind für die Planung, die Sammlung, die Gesamtschau und die Quellenreferenzierung von Inhalten Datenbanken oder zumindest tabellarische Aufstellungen notwendig, für die noch keine Möglichkeit bestand, die aggregierten Inhalte auch automatisch mit der Twitter-Plattform zu verknüpfen.
         Dies hat sich mit der Bereitstellung der Software autoChirp geändert, die an der Kölner Informationsverarbeitung entwickelt wurde, um die Umsetzung entsprechender Twhistory-Projekte zu unterstützen. Zum einen vereinfacht autoChirp die Arbeit für die ErstellerInnen von Twitter-Timelines historischer Ereignisse, indem es eine Schnittstelle zum automatischen Upload von tabellarischen Sammlungen unterschiedlichen Formats anbietet. Dabei können neben dem gewünschten Datum, der genauen Uhrzeit und den Tweet-Text auch Bilder und Geolocations für den Tweet angegeben werden (vgl. Abb. 1). Auch können ganze Gruppen von Tweets per Mausklick auf eine neue Referenzzeit geschedulet werden.
         
            
         
         Abb. 1: Sceenshot des autoChirp-Web-Clients, mit dem eine Reihe von Tweets automatisch aus einer Tabelle geschedulet wurde. Das Web-Application-Frontend interagiert mit einer redundant angelegten Datenbank, um die Sicherung der in den verschiedenen Projekten generierten Tweets auch jenseits der Twitter-Plattform nachhaltig zu gewährleisten. 
         Die autoChirp-App wird zur Zeit mindestens von den Twitter-Projekten @DigitalPast (
                http://digitalpast.de/), @NRWHistory (
                http://nrwhistory.de/)und 
                @goals_from_past genutzt und dabei unter anderem auch in der Lehre eingesetzt. Dabei stehen die EntwicklerInnen im engen Austausch mit den AnwenderInnen, um das Potential für Weiterentwicklungen abzuwägen. Aktuell wird die Integration von autoChirp in das Tiwoli-Projekt (vgl.Fischer & Strötgen 2015) realisiert, was zeigt, dass nicht nur historische, sondern auch literaturwissenschaftliche Vorhaben von einer Unterstützung im Zugang zur Twitter-Plattform profitieren können. 
            
         
            Für einen niederschwelligen Einstieg läuft eine Instanz von autoChirp als Web-Application zur freien Nutzung unter 
                
               https://autochirp.spinfo.uni-koeln.de/
            . Dort finden sich auch ausführliche Tutorials zur Benutzung. Für Weiterentwicklungen steht der dokumentierte Code im Github-Verzeichnis 
                
               https://github.com/spinfo/autoChirp
             zur Verfügung.
            
      
      
         
            
               Bibliographie
               
                  Fischer, Frank / Strötgen, Jannik (2015):
                        „Wann ﬁndet die deutsche Literatur statt? Zur Untersuchung von Zeitausdrücken in großen Korpora“,
                        in: 
                        DHd 2015: Von Daten zu Erkenntnissen.
                    
               
                  Hoffmann, Moritz (2015): 
                        Als der Krieg nach Hause kam.
                        Berlin: Ullstein.
                    
               
                  Strötgen, Jannik / Gertz, Michael (2012):
                        „Temporal Tagging on Different Domains: Challenges, Strategies, and Gold Standards“,
                        in 
                        Proceedings of LREC 2012 3746–3753. 
                    
               
                  Zündorf, Irmgard (2010):
                        „Zeitgeschichte und Public History, Version: 1.0“,
                        in: 
                        Docupedia-Zeitgeschichte, 11.2.2010 
                        http://docupedia.de/docupedia/index.php?title=Public_History&oldid=68731 [letzter Zugriff 24. August 2016].
                    
            
         
      
   



      
         
            Einleitung
            Im vorliegenden Abstract stellen wir eine Methode sowie erste Ergebnisse der Analyse von Entitäten-Assoziationen realer Leserinnen und Leser vor.
            Literaturwissenschaftliche Rezeptions-, Lese- und Lesertheorien gehen seit ihren hermeneutischen und wirkungsästhetischen Anfängen (Schleiermacher 1838, insb. 309f.; Iser 1976) von professionellen (Dijkstra 1994), informierten (Fish 1970, 86), Modell- (Eco 1979) oder sogar idealen (Schmid 2005) Lesern aus (vgl. Willand, 2014). Diesen wird die Kompetenz zugeschrieben, idealerweise sämtliche Textmerkmale referentialisieren zu können, wobei je nach literaturtheoretischer Provenienz unterschiedliche Kontexte die Grundlage der Zuschreibungen an den Text bilden. Dazu gehören u.a. Informationen über den Autor oder über die sozialhistorischen Bedingungen der Textproduktion, über die Rezeptionsbedingungen, über Vorgänger- oder zeitgenössische Texte oder über Wissen aus dem Bereich der Literaturwissenschaftlerin bzw. des Lesers selbst. 
            An bestimmte Wissensbestände dieser 
                    realen Leserinnen und Leser literarischer Texte können wir uns durch eine computergestützte empirische Analyse von Rezeptionszeugnissen aus sozialen Medien annähern. Konkret ist unser Ziel die Rekonstruktion und Analyse der von literarischen Texten ausgelösten Assoziationen. Dabei beschränken wir uns auf die Assoziationen, die reale oder fiktive Entitäten betreffen, also etwa Personen des öffentlichen Lebens oder Figuren aus fiktionalen Werken.
                
            Die Plattform Goodreads bietet Leserinnen und Lesern die Möglichkeit des freien schriftlichen Austauschs über literarische Texte in einer großen Community. 55 Mio. Mitglieder haben bis 2017 über 50 Mio. Reviews geschrieben, wobei die Besprechungen die Inhalte der Bücher selbst und nicht - wie etwa bei Verkaufsplattformen wie Amazon - die Distribution, den Preis o.ä. fokussieren (Piper et al. 2015). 
         
         
            Verarbeitung
            Als Grundlage unserer Analysen  wurden die Reviews in einer lokalen Datenbank gespeichert. 
            Die Datenbank enthält 1,3 Millionen englischsprachige Reviews zu 5.481 besprochenen Texten. Die Reviews umfassen insgesamt etwa 150 Mio. Tokens, d.h. uns steht eine große Datenmenge zur Extraktion der Entitäten zur Verfügung. In einem ersten Schritten wurden die Reviews bereinigt: HTML-Tags wurden entfernt und Wiederholungen von mehr als dreimal dem gleichen Zeichen oder Wort auf drei reduziert. 
            Zur Extraktion der Entitäten aus den Reviews haben wir den Stanford Named Entity Recognizer (Finkel et al., 2005) verwendet. Der Tagger klassifiziert die gefundenen Entitäten in mehrere Klassen. Für uns ist die Klasse „PERSON“ relevant, da diese alle gefundenen Entitäten von Personen enthält.
            Im nächsten Schritt disambiguieren wir die extrahierten Entitäten, da z.B. ein Name wie “Harry” auf viele mögliche Träger des Namens verweisen kann. Mit Hilfe von UKB (Agirre et al., 2009) und UKB-wiki (Agirre et al., 2015) können den Entitäten Wikipedia-Seiten zugeordnet werden, welche die möglichen Entitäten repräsentieren. Für diese Disambiguierung verwendet UKB den PageRank-Algorithmus (Page et al. 1999), der Dokumente nach ihrem Verlinkungsgrad bewertet. Sobald Namen wie „Ron“ und „Dumbledore“ im selben Kontext erwähnt werden, wird die Wahrscheinlichkeit größer, dass mit “Harry” 
                    Harry Potter, mit “Ron” 
                    Ron Weasly und mit “Dumbledore” 
                    Albus Dumbledore aus der Romanreihe 
                    Harry Potter gemeint sind, weil diese Entitäten aus dem selben Kontext kommen und dies in der Wissensbasis Wikipedia durch Verlinkungen explizit ablesbar und quantifizierbar ist.
                
            UKB-wiki stellt einen herunterladbaren Graphen zur Verfügung, der Wikipedia-Seiten und Links auf andere Wikipedia-Seiten repräsentiert. In einem mitgelieferten Wörterbuch sind Entitäten mit allen möglichen Entitäten (Verweise auf Wikipedia Seiten) aufgeführt. 
            Die auf diese Weise gewonnen Wikipedia-Einträge wurden anschließend hinsichtlich des ontologischen Status der referenzialisierten Entität kategorisiert, also ob es sich um eine reale Person oder fiktionale Figur handelt. Dazu wurde die Wissensbasis DBpedia
                     verwendet, die die Daten aus Wikipedia strukturiert und maschinenlesbar kodiert. Da die Disambiguierung Wikipedia-Einträge liefert, können wir anhand dieser die auf den zugehörigen DBpedia-Eintrag zugreifen. Über DBpedia lassen sich neben ontologischen Kategorien  auch andere Eigenschaften extrahieren, die für eine Analyse ggf. interessant sind, etwa Geschlecht oder Relationen zu anderen Figuren.
                
            Die extrahierten Daten werden zunächst als Tabelle gespeichert und erlauben somit eine flexible weitergehende Verarbeitung, etwa in einem Netzwerk. Eine Zeile der Tabelle besteht aus dem Werktitel, der disambiguierten Entität (Verweis auf Wikipedia Seite), einer Liste der extrahierten Entitäten aus den Reviews, einer Liste von Review-IDs, um nachvollziehen zu können in welchen Reviews der Name erwähnt wird, der Anzahl der Erwähnungen und der Angabe ob es sich um eine Figur handelt oder nicht.
            
               
                  Titel
                  Disambiguierte Entität (Verweis auf Wikipedia Seite)
                  Extrahierte Entität
                  Review IDs
                  Anzahl der Erwähnungen
                  Handelt es sich um eine fiktionale Figur?
               
               
                  The Hound of the Baskervilles
                  Agatha_
                            Christie
                  christie, agatha_
                            christie, agatha_
                            christy
                  4707841
                            20, …, 1886
                            08568
                  20
                  False
               
               
                  The Hound of the Baskervilles
                  Spock
                  spock
                  429714
                            73
                  1
                  True
               
               
                  The Hound of the Baskervilles
                  Robert_Downey,_Jr.
                  robert_downey_jr, robert_downey
                  107754
                            3609, …, 125
                            0976986
                  18
                  False
               
               
                  The Hound of the Baskervilles
                  Ann_Radcliffe
                  ann_radcliffe
                  435380
                            655
                  1
                  False
               
            
            
               Tabelle 1: Auszug aus den extrahierten Daten. Die extrahierten Entitäten stammen aus den Reviews zu
                     The Hound of the Baskervilles.
                
            
               Zwischenergebnisse
               Um ein exemplarisches Resultat zu präsentieren, haben wir Reviews zu “The Hound of the Baskervilles” (deutsch: “Der Hund von Baskerville”) analysiert. Unter den häufig erwähnten Entitäten finden sich erwartungsgemäß Sherlock Holmes, Dr. Watson, sowie der Autor Arthur Conan Doyle. Weitere häufig erwähnte Figuren aus der fiktionalen Welt des Sherlock Holmes' sind James Mortimer und Charles Baskerville. Aber auch Professor Moriarty wird häufig erwähnt, obwohl er in diesem Buch der Sherlock-Reihe gar nicht auftaucht. Das System erzeugt jedoch auch Fehler. Beispielsweise wird der Antagonist Stapleton zwar sehr oft erwähnt, da zu ihm aber kein eigener Wikipedia-Eintrag existiert, wird er fälschlicherweise mit dem Footballspieler Frank Stapleton verknüpft. Henry Baskerville, der Sohn von Charles und Erbe des Anwesens, wird im Buch fast durchgehend als Sir Henry bezeichnet, und kommt mit diesem Namen ebenfalls häufig in den Reviews vor. Da auch für ihn kein eigener Wikipedia-Eintrag existiert und der Name Henry extrem mehrdeutig ist, werden eine Reihe klar falscher Entitäten verknüpft: Henry II. von Frankreich; Henry County (Alabama); oder Henry I. von England.
               Bemerkenswert sind insbesondere jedoch die referenzialisierten extra-textuellen Entitäten, also diejenigen, die nicht aus der fiktionalen Welt Sherlocks stammen. Es finden sich etwa 
                        Hercule Poirot und 
                        Agatha Christie unter den erwähnten Entitäten, was als klares Zeichen dafür gesehen werden kann, dass die Leserinnen und Leser den Text vor dem Hintergrund eines starken Gattungsbewusstseins rezipieren. Dafür spricht auch, dass mit 
                        Benedict Cumberbatch, 
                        Robert Downey, Jr., 
                        John Barrymore und 
                        Jeremy Brett gerade die Schauspieler unter den assoziierten Referenzen vertreten sind, die in einer der vielen Verfilmungen die Rolle des Sherlock Holmes verkörpert haben.
                    
            
            
               Fehleranalyse
               Das am häufigsten auftretende Problem ist das Fehlen eines Wikipedia-Eintrages für eine Figur. In der englischsprachigen Wikipedia sind fiktionale Figuren zwar nicht per se davon ausgeschlossen -- Richtschnur hier ist deren “Notability”. Viele Figuren sind jedoch nur auf den Einträgen des entsprechenden Werks erwähnt. Da der Algorithmus nicht in der Lage ist, 
                        keinen Eintrag zu liefern, wird in solchen Fällen eben ein anderer Eintrag verwendet, auch wenn dieser relativ weit entfernt sein mag. Eine technische Lösung wäre sicher, nur ab einem gewissen Schwellwert eine Disambiguierung vorzunehmen, und die nicht-disambiguierten Einträge zumindest als solche erkennen zu lassen. Eine andere Möglichkeit läge in der (zusätzlichen) Verwendung von Literaturlexika, die (womöglich) eine größere Abdeckung zu fiktionalen Figuren aufweisen. Beide Optionen werden wir in zukünftigen Arbeiten genauer untersuchen. 
                    
               Da es sich bei den Reviews letztlich um Inhalte aus einem sozialen Medium handelt, kommt es auch vor, dass Namen falsch geschrieben werden oder gar der gesamte Text schriftsprachliche Konventionen übergeht. Prima vista sind diese Fälle im Vergleich zu Buchrezensionen zwar häufig anzutreffen, wir können das Problem aber umgehen, indem wir nur diejenigen Erwähnungen berücksichtigen, die mehr als einmal vorkommen. Festzuhalten bleibt aber ebenfalls, dass die Texte im Vergleich zu z.B. Twitter-Daten deutlich sauberer sind.
               Eine weitere mögliche (jedoch noch nicht tatsächlich beobachtete) Fehlerquelle liegt in der Natur des PageRank-Algorithmus: Wenn eine Figur in einem Werk existiert, ein Leser oder eine Leserin jedoch explizit z.B. eine Person des öffentlichen Lebens mit dem gleichen Namen erwähnt, wird der Algorithmus diese Erwähnung eher der Figur zuschlagen, da diese dichter mit anderen Figuren verknüpft ist. 
            
         
         
            Auswertung als Netzwerk
            Die oben extrahierten Daten erlauben Auswertungen auf vielfältige Weise. Exemplarisch konzentrieren wir uns hier auf eine Form, in der von Lesern zugeschriebene Gemeinsamkeiten zwischen literarischen Texten untersucht werden. Die Texte und die ihnen zugeschriebenen Assoziationen werden dabei als Knoten in einem Netzwerk repräsentiert. Ein Text ist also mit allen ihm zugeschriebenen Assoziationen verbunden, wobei das Gewicht der Kante die Anzahl der Reviews angibt, in denen eine bestimmte Assoziation auftaucht. 
            Durch diesen Aufbau ergeben sich Kerneigenschaften des Netzwerkes, die bei der Analyse zu beachten sind: Ein Teil der erwähnten Entitäten sind 
                    intratextuelle Referenzen, d.h. Figuren aus dem jeweiligen Text selbst (Veldhues, 1995). Auch wenn diese keine 
                    intertextuellen Assoziationen und damit nur sekundäres Extraktionsziel sind, behandeln wir sie als gleichwertige Assoziationen
                    . 
                
            Figuren, die in mehr als einem Werk auftauchen (z.B. 
                    Sherlock Holmes oder 
                    Harry Potter) bilden eine hoch gewichtete Verbindung zwischen den Texten einer literarischen Reihe, wobei Reihen durch die von ihnen geteilte fiktionale Welt markiert sind. Als gemeinsamer Assoziationsraum sind sie aufgrund der hohen Gewichtung auch angemessen im Netzwerk repräsentiert.
                
            Durch die gemeinsame Darstellung der Werke und assoziierten Entitäten ergeben sich -- bei Auswahl eines geeigneten Layout-Algorithmus z.B. in Gephi
                     -- eng zusammenhängende Gruppen von Werken. Das hier exemplarisch angeführte Resultat eines engen Zusammenhangs repräsentiert jedoch nicht bestimmte Texteigenschaften selbst, sondern lediglich von Leserinnen und Lesern gemeinsam gemachte Zuschreibungen an diese Texte.
                
            Das hier beschriebene Netzwerk wird im Zuge der Konferenz frei zugänglich gemacht.
            
               
                  
                  Abbildung 1: Assoziationen zu Conan Doyles The Hound of the Baskervilles, extrahiert aus den Reviews von Benutzern. Die Abbildung zeigt zur Illustration sämtliche assoziierte Entitäten, unabhängig von der Häufigkeit.
               
            
         
         
            Nächste Schritte
            Durch den Zugriff auf bisher undenkbar große Rezeptionsdatenmengen erhält die empirische Leseforschung einen sie fundamental erweiternden Impetus, war sie methodisch betrachtet bisher überwiegend auf Fragebögen
                     und peripheriephysiologische Messungen angewiesen, jüngst gestützt durch bildgebende Verfahren. Computerlinguistische Methoden der Sprach- und Korpusverarbeitung versprechen nicht nur die Analyse unlesbarer Mengen an Rezeptionszeugnissen, sondern auch eine Modellierung leserattribuierter Kontexte literarischer Texte und somit einen ersten Einblick in die bisher unbeantwortete Frage, mit welchem Vorwissen echte Leser eigentlich lesen.
                
            In diesem Sinne präsentiert das eingereichte Paper erste, jedoch bereits substanzielle Ergebnisse. 
            Die nächsten Schritte leiten sich direkt aus der oben diskutierten Fehleranalyse ab. Zum einen soll die Wissensbasis um fiktionale Figuren aus den Werken erweitert werden (was z.B. über 
                    named entity recognition über den Volltexten machbar wäre). Zum anderen soll der Algorithmus in die Lage versetzt werden bestimmte (fehlerhafte) Zuweisungen zurückzuweisen, etwa mit einem zu definierenden 
                    threshold.
                
         
      
      
         
            http://wiki.dbpedia.org/
            Das Filtern von innertextuellen Figuren ist technisch möglich (Beck, 2017), aber zeitaufwändig und für die hier vorgestellte Nutzung als Explorationswerkzeug letztlich unnötig.
            https://gephi.org
            Groeben 1979; Baurmann 1981; Funke 2003; Christmann u. Schreier 2003; Wübben 2009 u.v.m.
         
         
            
               Bibliographie
               
                  Agirre, Eneko / Soroa, Aitor (2009): “Personalizing PageRank for Word Sense Disambiguation”, in: Proceedings of the 12th conference of the European chapter of the Association for Computational Linguistics (EACL-2009). Athens, Greece.
                    
               
                  Agirre, Eneko / Barrena, Ander / Soroa, Aitor (2015): Studying the Wikipedia Hyperlink Graph for Relatedness and Disambiguation. http://arxiv.org/abs/1503.01655
                    
               
                  Beck, Jens (2017): How do People Read Literature? - Detection and Identification of Names in Book Reviews. Bachelor’s thesis, Institut für Maschinelle Sprachverarbeitung, Universität Stuttgart.
                    
               
                  Baurmann, Jürgen (1981). „Textrezeption empirisch. Wege zu einem ziel, behelfsbrücken oder holzwege?". Rezeptionspragmatik. Beiträge zur Praxis des Lesens. Uni-Taschenbücher. Band 1026. Hrsg. v. Gerhard Köpf, 201–218. München.
                    
               
                  Christmann, Ursula / Margrit Schreier (2003). „Kognitionspsychologie der Textverarbeitung und Konsequenzen für die Bedeutungskonstitution literarischer Texte". Regeln der Bedeutung. Zur Theorie der Bedeutung literarischer Texte. Revisionen. Hrsg. v. Fotis Jannidis, Gerhard Lauer, Matías Martínez & Simone Winko, 246–284. Berlin.
                    
               
                  Dijkstra, Katinka (1994): Leseentscheidung und Lektürewahl. Empirische Untersuchungen über Einflussfaktoren auf das Leseverhalten. Berlin.
                    
               
                  Dimitrov, Stefan / Zamal, Faiyaz / Piper, Andrew / Ruths, Derek (2015): “Goodreads vs Amazon: The Effect Of Decoupling Book Reviewing And Book Selling", in: International Conference on Web and Social Media (ICWSM-14).
                    
               
                  Eco, Umberto (1979): The Role of the Reader. Explorations in the Semiotics of Texts. Bloomington, IN.
                    
               
                  Finkel, Jenny Rose / Grenager, Trond / Manning, Christopher (2005): “Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling”, in: 
                        Proceedings of the 43nd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pp. 363-370.
                    
               
                  Fish, Stanley E. (1970): „Literature in the Reader: Affective Stylistics“, in: 
                        New Literary History 1(2): 123–162.
                    
               
                  Funke, Mandy (2003). „Das Abenteuer der Fragebögen. Aspekte zur empirischen Wirkungsforschung in der DDR". Wissenschaft und Systemveränderung. Rezeptionsforschung in Ost und West – Eine konvergente Entwicklung? Euphorion. Band 44. Hrsg. v. Wolfgang Adam, Holger Dainat & Gunther Schandera, 119–126. Heidelberg.
                    
               
                  Groeben, Norbert (1979). „Zur Relevanz empirischer Konkretisationserhebungen für die Literaturwissenschaft". Empirie in Literatur- und Kunstwissenschaft. Grundfragen der Literaturwissenschaft. Hrsg. v. Siegfried J. Schmidt, 43–82. München.
                    
               
                  Iser, Wolfgang (1976). Der Akt des Lesens. Theorie ästhetischer Wirkung. Band 636. München.
                    
               
                  Page, Lawrence / Brin, Sergey / Motwani, Rajeev / Winograd, Terry (1999): “The PageRank Citation Ranking: Bringing Order to the Web”
                        , technical Report. Stanford InfoLab.
                    
               
                  Schleiermacher, Friedrich (1838): Hermeneutik und Kritik mit besonderer Beziehung auf das Neue Testament. Aus Schleiermachers handschriftlichem Nachlasse und nachgeschriebenen Vorlesungen herausgegeben von Friedrich Lücke. In: Friedrich Schleiermacher’s sämmtliche Werke. Berlin: Reimer.
                    
               
                  Schmid, Wolf (2005): Elemente der Narratologie. Narratologia. Band 8. Berlin.
                    
               
                  Veldhues, Christoph (1995): "Gleich- und Gegenüberstellung".Intratextuelle und intertextuelle Bedeutung in der Literatur. Zeitschrift für französische Sprache und Literatur 40/3 (1995), 243-267.
                    
               
                  Willand, Marcus (2014): Lesermodelle und Lesertheorien. Historische und systematische Perspektiven. Narratologia. Band 41. Berlin.
                    
               
                  Wübben, Yvonne (2009). „Lesen als Mentalisieren? Neuere kognitionswissenschaftliche Ansätze in der Leseforschung". Literatur und Kognition. Bestandsaufnahmen und Perspektiven eines Arbeitsfeldes. Poetogenesis. Band 6. Hrsg. v. Martin Huber & Simone Winko, 29–44. Paderborn.
                    
            
         
      
   



      
         Alle, die mit maschinellen Methoden Sprache analysieren, erleben momentan einen tiefgreifenden methodologischen Wandel. 
                Einerseits erfreut man sich vielleicht als Geisteswissenschaftler/in am immer stärkeren Interesse der Ingenieurtechniken und der Informatik für Sprache. Dies kann durchaus als Erfolg der Linguistik betrachtet werden, zurückgehend auf den Linguistic Turn, der viele andere Disziplinen schon seit Jahrzehnten beeinflusst. Unternehmen interessieren sich für ihre Reputation im massenmedialen Diskurs oder sind der Überzeugung, ihr in unzähligen Dokumenten versprachlichtes Wissen besser verwalten zu können, wenn sie es nach sprachlichen Kriterien neu ordnen. Das Geschäftsmodell von Internetunternehmen basiert ganz erheblich darauf, sprachliche Kommunikation maschinell zu verarbeiten um daraus Wissen aufzubauen und Vorhersagen über das Handeln von Kunden zu machen. Auch in der Politik ist die Analyse von Sprachgebrauch ein wichtiger Faktor, um Wahlkämpfe zu gewinnen.
            
         Andererseits beschert einen dieses Interesse eine Vielzahl von neuen Methoden für die maschinelle Analyse von Text, die auch für geisteswissenschaftliche Fragestellungen interessant sind. Die Digital Humanities sind ein Beispiel für eine Disziplin, die sich den Experimenten mit diesen Methoden verschrieben hat. Auch die Korpuslinguistik profitiert maßgeblich von diesen neuen Methoden.
         Aktuell erfahren in der Computerlinguistik und generell im Data Mining neuronale Netze großen Zuspruch, die den Prozess des maschinellen Lernens nach dem Modell des menschlichen Gehirns gestalten. Solche Systeme, „Deep Learning“-Systeme genannt, sind in der Lage, Muster in den Daten zu erkennen, ohne dass vorher explizit die Eigenschaften festgelegt werden, die getestet werden sollen. Zudem findet das Lernen auf mehreren verborgenen Ebenen statt, so dass das Lernen nicht beobachtet und damit auch die Frage, welche Eigenschaften nun welchen Einfluss auf das gelernte Modell haben, kaum beantwortet werden kann.
         In der Computerlinguistik wurden bereits für viele Probleme Deep-Learning-Algorithmen eingesetzt, meist mit Erfolg. Erfolg bedeutet, dass die statistischen Modelle besser den Goldstandard voraussagen können, aber nicht, dass das grundlegende linguistische Problem (z.B.: Sentiment-Analyse: wie werden Gefühle und Meinungen ausgedrückt; Textklassifikation: wie drückt sich Stil, Autorschaft, Textsorte, Thema etc. aus) besser gelöst wäre.
         Überall wo Sprachgebrauch quantitativ und maschinell analysiert wird, gibt es einen starken Trend, möglichst ohne linguistischen Kategorien und Theorien auszukommen und Black-Box-Systeme zu verwenden. Das ist nachvollziehbar, da es in den meisten Fällen darum geht, ein System zu bauen, das eine klar definierte Aufgabe sehr gut lösen kann. Obwohl diese Ansätze natürlich auch für die Korpuslinguistik interessant sind, genügen sie linguistischen Forschungsinteressen eigentlich nicht, da sie keinen Beitrag dazu leisten, sprachliche Phänomene zu verstehen und erklären zu können.
         Viel dramatischer ist jedoch, dass die Linguistik offensichtlich nicht in der Lage ist, einen nützlichen Beitrag zur Lösung der Probleme der maschinellen Textanalyse zu leisten. Die Linguistik scheint für die quantitative Analyse von Text weitgehend bedeutungslos zu werden.
         Um der Bedeutungslosigkeit zu entgehen, muss die Linguistik ein kritisches Verhältnis zur Forschungslogik in den ingenieurstechnischen Disziplinen pflegen und auf zwei Prinzipien bestehen: 1) Mehr linguistische Theorie. 2) Ergebnisse von quantitativen Analysen müssen gedeutet werden.
         Zu 1): Nicht nur für die Linguistik, sondern für alle geistes- und sozialwissenschaftlichen Disziplinen gilt: Eine theoretische Fundierung der Analysekategorien ist essentiell. Dafür werden valide Analysekategorien benötigt, die deutbar sind. Dieses Prinzip richtet sich jedoch keinesfalls gegen datengeleitete Verfahren, im Gegenteil: Sie sind es, die die theoretischen Modelle herausfordern und schärfen können. Aber das Ziel aller Analysen muss darin liegen, ein Puzzleteil zu einem besseren Verständnis sprachlicher Strukturen, von Sprachgebrauch oder gesellschaftlichen und kulturellen Bedeutungen von Sprache zu führen. Wir benötigen White-Box-, nicht Black-Box-Systeme.
         Das Problem der fehlenden Validität zeigt sich z.B. im Feld der sog. „Authorship Attribution“, also der Zuordnung eines Textes X zu einem Autor A, B, C, …. Um dies zu tun, stehen Texte zur Verfügung, von denen die Autorschaft bekannt ist. Die Frage ist dann also, ob über die sprachlichen Merkmale des Textes X automatisch bestimmt werden kann, wer der Autor/die Autorin (aus der Menge der möglichen Autoren/innen) von Text X ist. Genauer lautet die Frage aber, ob und wie sich persönlicher Schreibstil sprachlich niederschlägt.
         Besonders erfolgreich für diese Aufgabe sind Methoden maschinellen Lernens, die das Problem als Klassifikationsaufgabe auffassen und anhand von Trainingskorpora typische sprachliche Merkmale der Texte der jeweiligen Autor/innen lernen. Dabei zeigt sich, dass „low-level features like character n-grams are very successful for representing texts for stylistic purposes” (Stamatatos, 2009, S. 24). 
                
                Das bedeutet, solche Modelle, die auf der Distribution von Buchstaben-N-Grammen beruhen, sind, gemessen an einem Goldstandard, am erfolgreichsten. Allein: Solche Modelle lassen sich nicht linguistisch deuten, da völlig unklar ist, was sie eigentlich messen. Ist es Stil, Thema, Textsorte, …? Es handelt sich also weder um eine valide, noch um eine deutbare Kategorie (insbesondere, wenn das statistische Modell nicht einsehbar ist). Für spezifische Aufgaben der Autorschaftsattribution mag das ausreichend sein, aber bereits für forensische Anwendungen, beispielsweise vor Gericht, ist eine solche Modellierung fragwürdig und gefährlich. Und für eine linguistische Deutung des Phänomens Autorschaftsstil ist sie gänzlich unbrauchbar.
         
         Die Kritik geht jedoch nicht nur in Richtung des Textminings und der Computerlinguistik, manchmal nicht-valide Kategorien einzusetzen (was zudem oft für die dortigen Zwecke auch sinnvoll ist), sondern auch in die Richtung der Linguistik: Die Computer- und die Korpuslinguistik zeigen beide gleichermaßen, wie wichtig es ist, auch abstrakte Kategorien so zu definieren versuchen, dass überhaupt eine Chance besteht, sie für eine quantitative Analyse operationalisierbar zu machen. Wenn eine linguistische Kategorie so vage ist, dass sich selbst (geschulte) Menschen uneinig darüber sind, wenn sie an authentischem Sprachegebrauch angewendet werden, scheitert die quantitativ-maschinelle Lösung unweigerlich.
         2) Die Ergebnisse von quantitativen Analysen sind nicht Antworten auf Fragestellungen, sondern neue Daten, die vor einem geistes- und sozialwissenschaftlichen Hintergrund genauso hermeneutisch gedeutet werden müssen, wie einzelne Texte. Das ist vielleicht das größte Missverständnis, wenn Textminer und Computerlinguistinnen mit Korpuslinguistinnen zusammenarbeiten: Erstere wollen, dass ein Werkzeug ein Ergebnis hervorbringt, das an einem Goldstandard evaluiert werden kann. Das Ergebnis ist dann im Einzelfall richtig oder falsch und in der Gesamtheit genügend präzise oder nicht. Das Ergebnis ist dann auch im Idealfall die Lösung der Forschungsfrage. Bei den meisten geistes- und sozialwissenschaftlichen Fragestellungen beginnt auf der Grundlage dieser Ergebnisse jedoch ein Interpretationsprozess, um (meist in Kombination mit weiteren Analysen) eine plausible Deutung zu ermöglichen – eine vorläufige Deutung. Die Stärke der Geistes- und Sozialwissenschaften liegt dabei ja gerade darin, dass in ihrer Methodologie ein Zweifeln inhärent ist, mit dem die „gegenwärtig besiegelten Bedeutungen jeweils eingeklammert oder angezweifelt [werden], um zu prüfen, inwiefern sich nach rationalem Ermessen nicht bessere Lösungen, überlegenere Interpretationen oder zustimmungsfähigere Regelungen finden lassen“ (Honneth, 2016, S. 312). 
                
            
         Neben der Suche nach validen Analysekategorien und dem Hochhalten geisteswissenschaftlicher Prinzipien der Deutung sehe ich einen weiteren Aspekt, der helfen sollte, der Korpuslinguistik eine deutliche linguistische Prägung zu verleihen. Es ist der Versuch, korpuslinguistisches Arbeiten als „diagrammatisches Operieren“ aufzufassen. Mit dem Diagramm-Begriff folge ich Krämer (2016), 
                
                die deutlich macht, dass Diagramme als Formen der Visualisierung von Daten „Denkzeuge“ sind, mit denen operiert wird: Ich kann Daten in einem Diagramm darstellen (auf einer Karte, in einem Netzwerkgraph, einem Punkteplot, …) und danach damit operieren, um neue Erkenntnisse daraus zu ziehen. Wenn man einem breiten Diagramm-Begriff folgt, wird deutlich, dass auch Listen, Tabellen und dergleichen diagrammatischen Charakter haben (Siegel, 2009; Steinseifer, 2013). 
                
                Dies sind nun aber Formen, die in der Korpuslinguistik zentral sind: Die Keyword in Context-Liste (zurückgehend etwa auf Zettelkästen im 16. Jahrhundert) etwa kann als Keimzelle eines völlig neuen Textverständnisses angesehen werden, mit dem die Einheit des Textes zerstört wird, um eine neue Sicht auf Textdaten zu gewinnen. Viele weitere Formen der Anordnung von Textdaten spielen ebenfalls wichtige Rollen, entscheidend etwa die Überführung von Textdaten in den Vektorraum, in dem operiert werden kann (z.B. in Form geometrischer Operationen – Lagen von Vektoren und ihren Winkeln zueinander). Aber auch die Erfindung der Partiturdarstellung bei Gesprächstrankripten, mit der überhaupt erst eine moderne Gesprächslinguistik möglich wurde, zeigt die Kraft von diagrammatischen Umformungen, um Daten neu lesbar zu machen. Hinter diesen diagrammatischen Umformungen stecken diagrammatische Grundfiguren (Bubenhofer im Druck b), die in den Geisteswissenschaften generell wirkmächtig sind.
            
         Ich meine, es lohnt sich, korpuslinguistisches Arbeiten unter diagrammatischer Perspektive zu reflektieren, um die Mechanismen und Möglichkeiten der Gegenstandskonstitution besser zu verstehen. Die Digitalität der Daten und Methoden erlaubt dabei neue Transformationen und macht Daten, egal welcher Modalität, miteinander verrechenbar. Aber die diagrammatischen Grundfiguren führen zu unterschiedlichen Gegenständen: Repräsentiert in einem Vektorraum geben die gleichen Daten einen völlig anderen Gegenstand ab als dargestellt in einer Keyword in Context-Liste. Und es müsste vordringliches Ziel sein, noch ganz andere Formen der diagrammatischen Darstellung von Text zu finden, um damit andere Gegenstandskonstitutionen und Fragestellungen zu ermöglichen. Die algorithmische Repräsentation der Daten folgt dabei ebenfalls den diagrammatischen Transformationen (Beispiel Vektorraum) und kann daher nicht unabhängig davon gedacht werden. Für eine hermeneutische Deutung brauchbare Analysekategorien zu erarbeiten, bedeutet deshalb auch, die damit verbundenen diagrammatischen Operationen zu reflektieren. Dafür nötig sind semiotische und natürlich auch wissenschaftstheoretische Überlegungen, die für alle Disziplinen, die mit maschineller Textanalyse befasst sind, relevant sein müssten.
      
      
         
             Dieses „extended Abstract“ ist eine verkürzte und angepasste Fassung des stärker linguistisch ausgerichteten Beitrages von Bubenhofer (im Druck a).
             Vgl. für eine aktuelle linguistisch motivierte Diskussion von stilometrischen Messmethoden für die Autorschaftsattribution Büttner et al. (2017). 
         
         
            
               Bibliographie
               
                  Bubenhofer, Noah (im Druck a): Wenn „Linguistik“ in „Korpuslinguistik“ bedeutungslos wird. Vier Thesen zur Zukunft der Korpuslinguistik. In: Osnabrücker Beiträge zur Sprachtheorie (OBST).
               
                  Bubenhofer, Noah (im Druck b): Visual Linguistics: Plädoyer für ein neues Forschungsfeld. In: Bubenhofer, Noah / Kupietz, Marc (Hg.): Visualisierung sprachlicher Daten. Heidelberg: HeiUP.
               
                  Büttner, Andreas / Dimpel, Friedrich Michael / Evert, Stefan / Jannidis, Fotis / Pielström, Steffen / Proisl, Thomas / Reger, Isabella / Schöch, Christof / Vitt, Thorsten (2017): »Delta« in der stilometrischen Autorschaftsattribution. In: Zeitschrift für digitale Geisteswissenschaften. text/html Format. DOI: 10.17175/2017_006.
               
                  Honneth, Axel (2016): Denaturierung der Lebenswelt. Vom dreifachen Nutzen der Geisteswissenschaften. In: Panteos, A./Rojek, T. (Hrsg.): 
                        Texte zur Theorie der Geisteswissenschaften, 
                        Reclams Universal-Bibliothek. Stuttgart : Reclam, S. 283–315
                    
               
                  Krämer, Sybille (2016): 
                        Figuration, Anschauung, Erkenntnis: Grundlinien einer Diagrammatologie. Frankfurt/Main: Suhrkamp Verlag.
                    
               
                  Nakov, Preslav/Ritter, Alan/Rosenthal, Sara/Stoyanov, Veselin/Sebastiani, Fabrizio (2016): SemEval-2016 Task 4: Sentiment Analysis in Twitter. In: 
                        Proceedings of the 10th International Workshop on Semantic Evaluation, 
                        SemEval ’16. San Diego, California : Association for Computational Linguistics.
                    
               
                  Siegel, Steffen (2009): 
                        Tabula: Figuren der Ordnung um 1600. Berlin / Boston : Akademie-Verlag.
                    
               
                  Stamatatos, Efstathios (2009): A Survey of Modern Authorship Attribution Methods. In: 
                        J. Am. Soc. Inf. Sci. Technol. Bd. 60, Nr. 3, S. 538–556
                    
               
                  Steinseifer, Martin (2013): Texte sehen – Diagrammatologische Impulse für die Textlinguistik. In: 
                        Zeitschrift für germanistische Linguistik Bd. 41, Nr. 1, S. 8–39
                    
            
         
      
   



      
         
            Hintergrund und Zielsetzung
            Am 13. Oktober 2016 gab die Schwedische Akademie bekannt, dass sie den Nobelpreis in Literatur an Bob Dylan „für seine poetischen Neuschöpfungen in der großen amerikanischen Songtradition“ verleihen werde. 
                    
                    Dass Dylan als Musiker und Songwriter den Literaturnobelpreis erhielt wurde mitunter sehr kontrovers diskutiert. Auf Kritik stieß z.B. die unzulässige Herauslösung von Dylans Texten aus der Musik und die Deutung seiner Lieder als Gedichte. 
                    
                    Unbestritten ist nichtsdestotrotz Bob Dylans Rolle als einer der einflussreichsten Musiker des 20. Jahrhunderts. 
                    
                
            Die (welt-)politischen Entwicklungen, die das Schaffen Dylans inspirierten, sind im Kontext seines Wirkens umfassend diskutiert worden, unter anderem in „Bob Dylan und die sechziger Jahre: Aufbruch und Abkehr“, 
                   
                    und liefern noch immer Diskussionsstoff, wie etwa eine in jüngerer Zeit erschienene Arbeit von Taylor & Israelson (2015) 
                    
                   über Dylans politische Einflüsse zeigt. Erfolgte die Beschäftigung mit Dylans Werk bislang allenfalls episodisch, so muss eine systematische Analyse des Gesamtwerks als Desiderat gelten, welches in gewisser Weise bereits von Bob Dylan selbst formuliert wurde:
                
            All these people who say whatever it is I’m supposed to be doing – that’s all gonna pass, because, obviously, I’m not gonna be around forever. That day’s gonna come when there aren’t gonna be any more records, and then people won’t be able to say ‘Well, this one’s not as good as the last one.’ 
                    They’re gonna have to look at it all (eigene Hervorhebung). And I don’t know what the picture will be, what people’s judgement will be at that time. I can’t help you in that area. 
                    – Bob Dylan 
                
            
            Dieser Beitrag erprobt, inwiefern mithilfe digitaler Methoden im Sinne des 
                    Distant Reading-Paradigmas ein neuer Zugang zu Dylans Gesamtwerk geschaffen werden kann. Bezugnehmend auf das Konferenzmotto einer „Kritik der Digitalen Vernunft“ soll untersucht werden, wo die Grenzen und Möglichkeiten eines solchen digitalen Analyseansatzes liegen, indem überprüft wird, ob sich bestehende qualitative Einteilungen von Dylans Werk in unterschiedliche Schaffensperioden auch anhand statistisch signifikanter Wörter (Rayson, Berridge & Francis 2004) und N-Gramme (Evert 2005) belegen lassen. 
                
         
         
            Stand der Forschung
            Bereits vor der Auszeichnung Dylans mit dem Nobelpreis in Literatur, waren seine Texte Gegenstand wissenschaftlicher Betrachtungen im Sinne des 
                    Close Reading (vgl. etwa Brown 2014). Brown unterteilt Dylans Werk in einzelne Phasen und verknüpft diese jeweils mit allgemeinen, zeitgeschichtlichen Ereignissen sowie biographischen Meilensteinen des Künstlers. Taylor & Israelson (2015) gehen einen ähnlichen Weg, versuchen jedoch Dylans Werk abseits verbreiteter politischer Einordnungen zu betrachten. Etwas anders ausgerichtet ist die Arbeit von Wissolik et al. (1994): Hierbei handelt es sich um eine Art Wörterbuch, in dem Namen und Gegenstände, die in Dylans Texten auftauchen, erläutert werden. 
                
            Eine umfassende Untersuchung Dylans Werks mithilfe computerbasierter Methoden fand sich bis zum Abfassungszeitpunkt des vorliegenden Texts nicht. Allerdings sind quantitative Verfahren zur stilistischen und inhaltlichen Analyse von Liedtexten in den Digital Humanities durchaus verbreitet. So beschreiben etwa, 
                    
                    wie mithilfe von N-Gramm-Modellen ein Liedtext-Korpus anhand der Merkmale Textlänge, Textstruktur, Wortschatz und Semantik analysiert werden kann, um eine automatisierte Genrezuordnung vornehmen zu können. Daneben existieren stilometrische Untersuchungen von Liedern oder Gedichten, die sich mit der Berechnung von autoren- und genrespezifischen Merkmalen befassen, wie z.B. Suzuki & Hosoya (2014), die japanische Pop-Songs analysieren.
                
         
         
            Forschungsmethodik
            
               Bezugsrahmen der Analyse: Schaffensphasen Bob Dylans
               Den analytischen Bezugsrahmen dieser Studie stellt die phasenweise Einteilung von Dylans Schaffen nach Brown (2014)
                         
                        dar. 
                         
                        Brown unterscheidet dabei neun unterschiedliche Phasen, die mit „Becoming Bob Dylan“ (1960-1964) beginnen und vorläufig mit „Bob Dylan Revisited“ (2000-2012) enden. Diese Stilphasen umfassen z.B. Dylans Hinwendung zum Christentum oder seine elektronische „Folk Rock“-Phase (vgl. Brown, 2014).
                    
            
            
               Korpus und Datenaufbereitung
               In dieser Arbeit wurde ein Korpus bestehend aus 452 Liedtexten mit einem Umfang von 133.045 Tokens untersucht, die Bob Dylan zwischen den Jahren 1962 und 2016 auf Studio-Alben veröffentlicht hat. Die Liedtexte und Metainformationen wie etwa Titel, Album und Jahr stammen von der Plattform 
                        LyricsWikia
                  . Da es sich bei 
                        LyricsWikia um ein community-gestütztes Projekt handelt, erfolgte vorab ein stichprobenartiger Abgleich einzelner Lieder mit den offiziellen Texten nach, 
                        
                        wobei keinerlei Abweichungen festgestellt werden konnten. 
                    
               Das Korpus wurde weiterhin mit Methoden der Computerlinguistik aufbereitet, insbesondere unter Verwendung des 
                        Python Natural Language Toolkits (NLTK). Die Verarbeitung des Korpus umfasst die grundlegende Lemmatisierung mit dem 
                        WordNet-Lemmatizer (Teil des NLTK) und eine Stoppwortbereinigung (NLTK-Stoppwortliste für Englisch mit eigener Erweiterung) sowie die Wortartenannotation mithilfe des 
                        Stanford Log-linear Part-of-Speech-Taggers.
                        
                        Da Dylan in seinen Texten häufig umgangssprachliche Formulierungen wie etwa verkürzte Gerundformen (bspw. „savin“, „swimmin“) verwendet, wurde für den POS-Tagger ein Modell verwendet, welches auf der Grundlage von Twitter-Texten trainiert wurde und gute Ergebnisse für Texte mit nicht-standardisiertem Vokabular und Slang liefert. 
                        
                    
            
            
               Korpusvergleich – Assoziationsmaße und Referenzkorpus
               
                  Assoziationsmaße
                  Ein etabliertes Verfahren, um aus einem Korpus spezifische Wörter zu extrahieren, ist ein direkter Korpusvergleich mit dem 
                            Log-Likelihood-Test, 
                            
                            der sich zum Vergleich von Korpora unterschiedlicher Größe besonders eignet (Rayson, Berridge, & Francis, 2004). Damit können Wörter, die im untersuchten Korpus mit einem signifikanten Frequenzunterschied zum Referenzkorpus auftreten, als Schlagworte betrachtet werden. Dies kann besonders aussagekräftige Ergebnisse liefern, wenn zusätzlich eine POS-Filterung erfolgt, womit sich beispielsweise signifikante Nomen oder Verben eines Korpus berechnen lassen. Darüber hinaus wurde eine Berechnung von N-Grammen in Form von Bi- und Trigrammen umgesetzt. Die berechneten N-Gramme lassen sich in der Web-App unter der Wahl eines Assoziationsmaßes, wie dem 
                            Chi Quadrat-Test, dem Jaccard-Test, dem Poisson-Stirling-Test, dem Likelihood Ratio-Test sowie dem Pointwise Mutual Information-Test anzeigen. Dabei liefert jedes Verfahren zur N-Gramm-Berechnung eigene spezifische Ergebnisse. Dieser Freiraum wird ganz bewusst erhalten, um die verschiedenen Facetten eines Texts, die ein Assoziationsmaße jeweils anzeigt, für die spätere Datenanalyse nutzen zu können.
                        
               
               
                  Referenzkorpus 
                  Als Referenzkorpus dient das mündliche Subkorpus des 
                            Open American National Corpus (OANC; American National Corpus Project),
                            
                            welches insgesamt 3.862.172 Tokens umfasst. Das Korpus enthält viele Belege aus der mündlichen Kommunikation 
                             
                            und eignet sich dadurch in besonderer Weise als Vergleichskorpus für Dylans Texte, die wie bereits beschrieben einen hohen Anteil umgangssprachlicher Formulierungen und Slang-Ausdrücke enthalten.
                        
                  Beim Korpusvergleich kann entweder das gesamte Dylan-Korpus mit dem Referenzkorpus verglichen werden, oder mit den jeweiligen Dylan-Subkorpora, also bspw. all seinen Texten aus den 1970er-Jahren oder aus der ersten Schaffensperiode „Becoming Bob Dylan“ (1960-1964). Ein Vergleich der einzelnen Dylan-Subkorpora zum Gesamtwerk ist ebenso möglich. Letztere Option wird z.B. genutzt, um anhand jeweils signifikanter Wörter die einzelnen Schaffensperioden nach Brown (2014) zu überprüfen und damit die grundsätzliche Eignung solch quantitativer Verfahren zur Identifikation thematischer Verschiebungen zu untersuchen. Die Ergebnisse dieses Korpusvergleichs sind, zusammen mit allen anderen Ergebnissen der angewandten Analyseverfahren, in einer interaktiven Webanwendung über unterschiedliche Visualisierungen (Balkendiagramm, 
                            treemap, wordcloud, Tabelle) für weitere Interpretationen zugänglich. Wie schon bei den Assoziationsmaßen, so gilt auch hier, dass jede Visualisierungsform eine bestimmte Perspektive auf die Berechnungsergebnisse eröffnet.
                        
               
            
         
         
            Ergebnisse
            Im direkten Vergleich des gesamten Dylan-Korpus (1962-2016) mit dem OANC-Referenzkorpus treten einige interessante, signifikant-häufige Wörter im Werk Dylans hervor. Die von Bob Dylan verwendeten Adjektive erzeugen in der Gesamtschau tendenziell eher eine bedrückende Stimmung (blind, weary, lonely, drunken, scared, restless, ragged). Bei den Substantiven mischen sich unter viele Personen- und Ortsnamen auch religiöse Begriffe (soul, heaven, devil, eden, prayer, paradise). Viele der übrigen Begriffe sind erwartungsgemäß typisch für Folk-Musik (levee, rooster, train), was sich wiederum durch die Wahl des Referenzkorpus, das verschiedenartige mündliche Textquellen enthält, erklären lässt (Rayson, Berridge, & Francis, 2004: 8).
                
            Die Analyse signifikant-häufiger Wörter für die einzelnen Schaffensphasen Dylans liefert Ergebnisse mit hoher Aussagekraft. So fällt etwa für die Phase „The Changing of the Guard" (1978-1981), in der sich Dylan dem Christentum hinwendet, auf, dass das Vokabular tatsächlich viele christliche Motive aufweist (lord, Jesus, devil, altar, faith, confession, grace, power, serve). Insgesamt nimmt der Anteil an „düsterem“ Vokabular in dieser Phase ab, verschwindet jedoch nicht komplett (bspw. 
                    shot, destruction). Der Anteil an hoffnungsvollen Wörtern nimmt hingegen zu (bspw. 
                    beginning, ready, arise, wake, thank). Bei den übrigen Schaffensphasen fallen die Ergebnisse jedoch mitunter wesentlich weniger deutlich aus.
                
            Ein differenziertes Bild ergibt sich für die N-Gramm-Analyse, was einerseits der Vielfalt an verfügbaren Methoden zur Berechnung 
                     
                    und andererseits den unterschiedlichen N-Gramm-Längen geschuldet ist. Die Ergebnisse für Bigramme mit Hilfe des 
                    Pointwise-Mutual-Information-Tests (PMI) erschienen dabei am geeignetsten, um die thematischen Schwerpunkte von Dylans Schaffensphasen nach 
                     
                    nachzuvollziehen. So findet das PMI-Verfahren im Subkorpus der Phase „The Changing of the Guard“ (1978-1981) Bigramme wie 
                    close prayer, name lucifer, jesus good, jesus bone oder arise upon, die eindeutig religiöse Bezüge in Dylans Texten dieser Phase veranschaulichen. Generell fällt jedoch die Dominanz von Refrain-Versen in den Liedern bedeutend ins Gewicht (z.B. 
                    knock heaven door), was die Qualität der Ergebnisse insbesondere bei den Trigrammen beeinflusst. 
                
         
         
            Diskussion
            Im Sinne einer Kritik der Digitalen Vernunft bleibt demnach festzuhalten, dass sich Methoden der computergestützten Textanalyse und des statistischen Korpusvergleichs grundsätzlich dafür eignen, einen inhaltlichen Gesamtüberblick zu einem Liedtext-Korpus zu erhalten. Es können damit diachrone Entwicklungen des Wortschatzes und Verlagerungen thematischer Schwerpunkte als grobe Tendenzen aufgezeigt werden, um das Bild des Gesamtwerks zu ergänzen. Ein solcher Ansatz eignet sich demnach gut für die initiale Thesengenerierung und kann in gewisser Weise die Funktion eines Empfehlungs- bzw. Hinweissystems für erklärungsbedürftige Stellen
                     in den Geisteswissenschaften übernehmen.
                
            Die Identifikation konkreter Schaffensperioden, ausschließlich auf Basis signifikant häufiger Wörter ist aber – zumindest für das Werk Dylans – nicht ohne Weiteres erfassbar. Bei den N-Grammen zeigt sich, dass im Falle von Dylans Texten methodenübergreifend und mit zunehmender N-Gramm-Länge meist keine brauchbaren Ergebnisse erzielt werden konnten. Dies ist ein Hinweis darauf, dass die hier präsentierten Analysemethoden, die für andere Textsorten wie bspw. Parlamentsprotokolle bereits erfolgreich eingesetzt werden konnten (vgl. Sippl et al. 2016), auf Liedtexte nur eingeschränkt anwendbar sind. Ein möglicher Kritikpunkt am hier beschriebenen Vorgehen mag zudem das verwendete OANC-Referenzkorpus sein, welches trotz hoher Anteile mündlicher Kommunikation doch nur beschränkt vergleichbar mit der Textsorte „Liedtext“ ist. Für künftige Vergleichsstudien böte sich ggf. ein Vergleich mehrerer unterschiedlicher Künstler und deren Liedtexte an, also bspw. Bob Dylan vs. Johnny Cash.
         
      
      
         
             http://lyrics.wikia.com, alle Hyperlinks dieses Dokuments wurden zuletzt abgerufen am 10.01.2018
             Verfügbar unter http://www.nltk.org/
             Filterung von Stoppwörtern, wie „hey“, „ah“, „yeah“, und Verkürzungen, wie „‘ve“, „‘s“ etc.
             https://www.colin-sippl.de/dylan (Klick auf den Analyse-Button rechts oben)
             Diesen Gedanken äußerte Hubertus Kohle auf der #DigiCampus-Tagung im Juni 2017 in München, vgl. https://twitter.com/8urghardt/status/876725916487036928.
         
         
            
               Bibliographie
               
                  American National Corpus Project (2015a): 
                        American National Corpus. Frequency Data. http://www.anc.org/data/anc-second-release/frequency-data/ [Letzter Zugriff 10. März 2017].
                    
               
                  American National Corpus Project (2015b): 
                        The Open American National Corpus (OANC). http://www.anc.org/ [Letzter Zugriff 10. März 2017].
                    
               
                  Brown, Donald (2014): 
                        Bob Dylan: American troubadour. Lanham, Md. [u.a.]: Rowman & Littlefield.
                    
               
                  Cott, Jonathan (2006): 
                        Bob Dylan, the essential interviews. New York: Wenner Books.
                    
               
                  Derczynski, Leon et al. (2013): "Twitter part-of-speech tagging for all: Overcoming sparse and noisy data", in: 
                        Proceedings of the Recent Advances in Natural Language Processing September, 198–206. http://www.derczynski.com/sheffield/papers/twitter_pos.pdf [Letzter Zugriff 9. März 2017].
                    
               
                  Dylan, Bob (2016): 
                        The lyrics: 1961-2012. New York: Simon & Schuster.
                    
               
                  Evert, Stefan (2005): "The Statistics of Word Cooccurrences, Word Pairs and Collocations", in: 
                        Unpublished doctoral dissertation Institut fur maschinelle Sprachverarbeitung Universitat Stuttgart 98: August 2004, 353. http://en.scientificcommons.org/19948039 [Letzter Zugriff 3. März 2017].
                    
               
                  Fell, Michael / Sporleder, Caroline (2014): "Lyrics-based Analysis and Classification of Music", in: 
                        International Conference on Computational Linguistics 25: 23–29, 620–631.
                    
               
                  Geisel, Sieglinde (2016): 
                        Bob Dylan - Literaturnobelpreisträger wider Willen. Deutschlandradio Kultur. http://www.deutschlandradiokultur.de/bob-dylan-literaturnobelpreistraeger-wider-willen.1005.de.html?dram:article_id=373494 [Letzter Zugriff 7. März 2017].
                    
               
                  Rayson, Paul / Garside, Roger (2000): "Comparing corpora using frequency profiling", in: 
                        Proceedings of the workshop on Comparing Corpora 1–6.
                    
               
                  Rayson, Paul / Berridge, Damon / Francis, Brian (2004): "Extending the Cochran rule for the comparison of word frequencies between corpora", in: 
                        JADT 2004: 7es Journées internationales d’Analyse statistique des Données Textuelles: 1–12.
                    
               
                  Schmidt, Mathias R. (1983): 
                        Bob Dylan und die sechziger Jahre: Aufbruch und Abkehr. Frankfurt am Main: Fischer Taschenbuch Verlag.
                    
               
                  Sippl, Colin / Burghardt, Manuel / Wolff, Christian / Mielke, Bettina (2016): Korpusbasierte Analyse österreichischer Parlamentsreden. In: 
                        Netzwerke: Tagungsband des 19. Int. Rechtsinformatik Symposions IRIS 2016: 25.- 7. Feb. 2016, Univ. Salzburg, S. 139-148.
               
               
                  Suzuki, Takafumi / Hosoya, Mai (2014): "Computational Stylistic Analysis of Popular Songs of Japanese Female Singer-songwriters", in: 
                        Digital Humanities Quarterly 8: 1, .
                    
               
                  Svenska Akademien (2016): 
                        Der Nobelpreis in Literatur des Jahres 2016.
                    
               
                  Taylor, Jeff / Israelson, Chad (2015): 
                        The Political World of Bob Dylan: Freedom and Justice, Power and Sin. New York: Palgrave Macmillan.
                    
               
                  Toutanova, Kristina / Klein, Dan / Manning, Christopher D (2003): "Feature-rich part-of-speech tagging with a cyclic dependency network", in: 
                        Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL ’03), 252–259. http://nlp.stanford.edu/~manning/papers/tagging.pdf [Letzter Zugriff 3. März 2017].
                    
               
                  Wissolik, Richard David / McGrath, Scott. / Colaianne, A. J. (1994): 
                        Bob Dylan’s words: a critical dictionary and commentary. Greensburg, PA: Eadmer Press.
                    
            
         
      
   



      
         
            Digital Medievalist is an international web community for medievalists working with digital media. Established in 2003 by a group of volunteers
                 and before the arrival of Facebook and Twitter
                , the goal of 
                Digital Medievalist would also become that of the popular social networks: to connect people around the world providing them with an exchange platform. But where Facebook and Twitter were driven by relationships, 
                Digital Medievalist was driven by interest. 
            
         The very first Digital Humanities disciplinary-focus community of practice, 
                Digital Medievalist sought to meet the increasingly sophisticated demands faced by creators of digital projects working with medieval content. Among its initial community-building activities, 
                Digital Medievalist started an homonymous open access scholarly journal, which is still active today, and commissioned the publication of short tutorials on its website to guide interested scholars through the basics of text encoding, web development and manuscript digitisation, to mention but a few. The benefits brought by 
                Digital Medievalist became so evident that other, similar web communities began to emerge, such as the 
                Digital Classicist
             (Mahony, 2017) and 
                Digital Victorianist.
            
         Over time, and as new technologies rapidly developed, 
                Digital Medievalist’s didactic component was superseded by free online courses, moodles and web tutorials, shifting its focus toward the dissemination of scholarly research to the widest possible audience. The 
                Digital Medievalist community has continued to gather importance since its founding and today serves a number of disciplinary fields, including Digital Humanities, Medieval Studies and Auxiliary Sciences, Cultural Heritage, Archaeology, Literary Studies, History, Linguistics, and Museum and Archival Science.
            
         Membership to 
                Digital Medievalist is open to anyone with an interest in its subject matter, regardless of skill or previous experience in Digital Humanities or medieval studies. Participants range from novices contemplating their first project to many of the pioneers in the field. The entire 
                Digital Medievalist community counts over 1,500 members worldwide. 
            
         The current activities and assets of 
                Digital Medievalist include: 
            
         
            The Digital Medievalist mailing list: 1,272 (as of Sept. 14th 2017) list members use this platform to ask for advice, discuss problems, and share any kind of information related to the field of medieval studies. The list’s collegial atmosphere encourages a variety of conversations.
                
            The Digital Medievalist journal: The community’s online, open access, refereed journal publishes original research and scholarship, notes on technological topics (standards, tools, software, etc.), commentary pieces discussing developments in the field, bibliographic and review articles, and project reports. The journal is funded in part through grants provided by the University of Lethbridge School of Graduate Studies and recently joined the Open Library of Humanities (OLH), a non-profit organisation dedicated to publishing open access scholarship with no author-facing article processing charges. Funded by an international consortium of libraries OLH has built a sustainable business model in order to make scholarly publishing fairer, more accessible, and rigorously preserved for the digital future.
                
            The Digital Medievalist website: The community’s online presence provides comprehensive information about the organisation including membership, structure and bylaws. It also provides announcements and an up-to-date list of recent and upcoming conferences, colloquia, workshops and training events relevant to (digital) medieval studies. The website also invites members to write blog-posts for several thematic series.
                
            The Digital Medievalist Facebook group with over 1,600 members and a Twitter presence to widen the scope and impact of scholarly communication, and to disseminate best practice, data and knowledge pertaining to digital medieval studies (Ross, 2012; Terras, 2012).
                
         
         In 2017, Digital Medievalist joined the European Alliance for Social Sciences and Humanities (EASSH) as a learned society in order to increase the visibility of the 
                Digital Medievalist community. 
            
         DHd 2018 provides the ideal venue to expose 
                Digital Medievalist to a large German-speaking community of scholars. The 
                Digital Medievalist website averages 2,760 views per day from Germany alone; the 
                Digital Medievalist conference representatives are eager to speak to practitioners in Germany to better understand how 
                Digital Medievalist is meeting their needs and how it can improve. Additionally, we think that 
                Digital Medievalist can still serve as an example for community building. Its history and current state demonstrates how the interest in digital methods intersects with the use of digital communication tools, and is thus maybe an archetypical example of Digital Humanities.
            
         The poster will outline the aforementioned activities and will serve as a conversation starter to establish connections with relevant initiatives, start reflection on the role of this and similar activities, collect feedback and continue fostering as wide a geographical coverage as possible. 
      
      
         
             Daniel Paul O'Donnell, Peter Baker, James Cummings, Martin Foys, Murray McGillivray, Dot Porter, Roberto Rosselli Del Turco, and Elizabeth Solopova. 
                        Digital Medievalist was founded with direct financial support from the Faculty of Arts and Science at the University of Lethbridge, the Curriculum Redevelopment Centre (now the Teaching Centre) at the University of Lethbridge, the
                        Image, Text, Sound, and Technology (ITST) programme of the Social Sciences and Humanities Research Council of Canada (SSHRC).
                    
             Facebook was founded in 2004 and Twitter in 2006.
             For a discussion on the relationship between 
                        Digital Medievalist and 
                        Digital Classicist, see Bodard and O’Donnell (2008).
                    
         
         
            
               Bibliography
               
                  Bodard, G., O’Donnell, D. (2008) ‘We are all together: On publishing a Digital Classicist issue of the Digital Medievalist journal’, 
                        Digital Medievalist, 4. DOI: http://doi.org/10.16995/dm.18
                    
               
                  Digital Medievalist website: 
                        https://digitalmedievalist.wordpress.com/
               
                  Digital Medievalist journal: 
                        https://journal.digitalmedievalist.org/
               
                  Digital Medievalist mailing list: 
                        https://digitalmedievalist.wordpress.com/mailing-list/
               
                  Digital Medievalist on Facebook: 
                        https://www.facebook.com/groups/49320313760/
               
                  Digital Medievalist on Twitter: 
                        https://twitter.com/digitalmedieval
               
                  European Alliance for Social Sciences and Humanities: http://www.eassh.eu/
               
                  Mahony, S. (2017) ‘The Digital Classicist: Building a Digital Humanities Community’, 
                        Digital Humanities Quarterly, 11(3). At: http://www.digitalhumanities.org/dhq/vol/11/3/000335/000335.html
                    
               
                  Open Library of Humanities: https://www.openlibhums.org/
               
                  Open Scholarly Communities on the Web, ISCH COST Action A32. At: http://www.cost.eu/COST_Actions/isch/A32 
               
                  Ross, C. (2012) ‘Social media for digital humanities and community engagement’, In C. Warwick, M. Terras and J. Nyhan (eds.) 
                        Digital Humanities in Practice. Facet Publishing, pp. 23-46.
                    
               
                  Terras, M. (2012) ‘The Impact of Social Media on the Dissemination of Research: Results of an Experiment’, 
                        Journal of Digital Humanities, 1(3). At: http://journalofdigitalhumanities.org/1-3/the-impact-of-social-media-on-the-dissemination-of-research-by-melissa-terras/
                    
            
         
      
   



      
         
            Einleitung
            Twitter ist ein soziales Netzwerk bzw. ein Mikroblogging-Dienst, welches das Senden von Textnachrichten („Tweets“) ermöglicht. Es ist das derzeit am schnellsten wachsende soziale Netzwerk, worin die Twitter-Benutzer untereinander stark vernetzt sind und ihre eigenen Meinungen sowie Gefühle zu aktuellen Themen ausdrücken. Die textuellen und sprachlichen Inhalte der einzelnen Tweets sind nicht normativ, da die enthaltenen Informationen von umgangssprachlichen Ausdrücken, Abkürzungen, Emoticons und von Grammatikfehlern durchsetzt sind, sodass kein standardisiertes / automatisiertes Auswertungsverfahren zur Sentiment Analyse angewendet werden kann. Den Tweets können zudem Anhänge wie Bilder, Videos oder Hyperlinks beigefügt werden. Memes sind beliebte Vertreter solcher Bildanhänge in Tweets: 
                    „I define an Internet meme as: (a) a group of digital items sharing common characteristics of content, form, and/or stance, which (b) were created with awareness of each other, and (c) were circulated, imitated, and/or transformed via the Internet by many users.” (Shifman L., 2013, S. 41) Auf Basis dieser Definition von L. Shifman zählen somit auch Videos zur Gattung der Memes. Memes haben die Eigenschaft Bild und Text zu kombinieren, dadurch eine polysemische Nachricht zu generieren, welche die Rezipienten anspricht und Emotionen auslösen. 
                    „Ein Internet-Meme ist die humoristische/sarkastische Reaktion der Internetgemeinde auf ein (mediales) Ereignis.“ (Marx und Weidacher, 2014, S. 143) Zudem sind Memes in politischem Kontext aktuell wenig empirisch erforscht 
                    (Shifman L., 2013, S. 119). Gerade aus diesen Gründen wurden Memes, speziell Memes in politischem Kontext, als Untersuchungsgegenstand für diese Sentiment Analyse gewählt.
                
            Die vorliegende Analyse basiert auf zwei Memes: Das erste Meme steht im Kontext zur Wahl des US-Präsidenten inkl. des Vizepräsidenten vom 8. November 2016, das Zweite im Zusammenhang der Ersten 100 Amtstage von Präsident Donald John Trump. Parallel zur Präsidentschaftswahl verbreitete sich bereits zwei Tage nach dem Wahlstichtag ein statisches Bild der Simpsons, worin schon im Jahr 2000 der Wahlausgang mit der geografischen Karte der Wahlergebnisse prophezeit wurde 
                    (siehe Abbildung 1). Als Reaktion auf die Ersten 100 Tage von D. Trump im Amt wurde am 26. April 2017 ein Video von den Simpsons 
                    (siehe Abbildung 2) auf YouTube veröffentlicht, welches genau diesen Zeitraum seiner Amtshandlungen parodiert. Bereits einen Tag danach setzte die Diskussion zum Video auf Twitter ein. Die vorliegende Arbeit analysiert die aus diesen beiden Memes entstandenen emotionalen Diskussionen und Reaktionen auf die beiden politischen Ereignisse via Twitter.
                
            
               
               Abbildung 1: Simpson Prediction
               
            
            
               
               Abbildung 2: Donald Trump's First 100 Days In Office | Season 28 | THE SIMPSONS
               
            
         
         
            Forschungsstand
            Dass die Meinungen anderer Mitmenschen unsere eigenen Entscheidungen beeinflussen, ist schon lange aus der Psychologie bekannt 
                    (Friedkin, 1990). Auch die Sammlung und Auswertung von Meinungen wird schon lange betrieben. Mit dem Aufkommen des Web 2.0 ergeben sich viel bessere Möglichkeiten, große Datenmengen gezielt auf Meinungen hin zu analysieren. Die Sentiment Analyse zielt auf die Ergründung der Haltung, Stimmung, Meinung und die generelle Einstellung von Personen in Bezug auf ein speziell ausgewähltes Produkt, andere Personen, Dienstleistungen oder aktuellen Themen ab. Dieses Forschungsgebiet fällt in den Bereich der 
                    Computerlinguistik bzw. der 
                    linguistischen Datenverarbeitung (Natural Language Processing), welche Untergebiete des 
                    Text Minings sind. Die Sentiment Analyse ist als Klassifikationsproblem von Texten und dessen Polaritätserkennung zu verstehen. Für die Polaritätserkennung müssen Indikatoren im Text identifiziert werden, welche Rückschlüsse auf das sogenannte Sentiment zulassen. Dabei handelt es sich um sprachspezifische Ausdrücke, die aufgrund ihrer Wortbedeutung bereits positiv, neutral oder negativ vorbelegt sind. Diese Stimmungsinformation lässt sich aus sogenannten Sentimentlexika (bzw. Sentiment-Wörterbüchern) der jeweiligen Sprachen entnehmen. Hier werden stimmungstragende Ausdrücke - häufig Adjektive - als solche gekennzeichnet. Meist wird von deren kontextunabhängigen Polaritätsausprägung ausgegangen, die binär (positiv/negativ bzw. +/-) oder verhältnisskaliert (wie beispielsweise beim Wörterbuch 
                    „SentiWordNet“) kodiert wird. Die Vorgehensweise bei einer Sentiment Analyse kann entweder 
                    Lexikon basiert, anhand eines Wörterbuchs erfolgen oder 
                    lernbasiert, wo man sich auf die Algorithmen aus dem Fachgebiet des maschinellen Lernens stützt. Bei der Durchführung der Sentiment Analyse untergliedert man die drei Ebenen (
                    Level): 
                    Dokumenten-Ebene, 
                    Satz-Ebene und die 
                    Aspekt-Ebene. Bei der 
                    Dokumenten-Ebene wird der gesamte Inhalt eines Dokuments in die Analyse mit einbezogen um eine generelle Stimmung zu deuten. Genauere Ergebnisse liefert die Betrachtung der 
                    Satz-Ebene, da hier die ausgedrückte Meinung für jeden einzelnen Satz berechnet wird. Die genausten Ergebnisse erhält man bei der Analyse der 
                    Aspekt-Ebene, da hier zu jedem Aspekt einer bestimmten Entität, die Stimmung des Meinungsvertreters zu einem bestimmten Zeitpunkt betrachtet wird.
                
            Nach der Definition von 
                    Liu Bing besteht eine Meinung aus einem 5-Tupel: 
                
            
               opinion = (e,a,s,h,t)
            
            Wobei 
                    e eine Entität (Objekt), 
                    a einen Aspekt (Feature) der Entität, 
                    s die subjektiv positive, negative oder neutrale Stimmung (Sentiment) des Aspekts der Entität, 
                    h den Meinungsvertreter (Opinion holder) und 
                    t den Zeitpunkt der Meinungsäußerung darstellt.
                
         
         
            Forschungsdesign und Methode 
            Aufbauend auf dem Forschungsstand untersucht diese Arbeit die inhaltlichen Diskussionen zu beiden eingangs erwähnten Memes auf Twitter hinsichtlich deren Emotionen und stellt diese in einem direkten Vergleich gegenüber. 
                    „Communication messages such as tweets, emails, and digital images are by definition memes, because they are replicable transmitters of cultural meanings.” (Spitzberg B. H., 2014, S. 312) In dieser Arbeit findet ein 
                    Lexikon basierter Ansatz für die Sentiment Analyse der Tweets Anwendung, da im Gegensatz zu den 
                    lernbasierten Methoden des maschinellen Lernens, die Lexikon basierten Verfahren für Bereiche eingesetzt werden können, für die keine Trainingsdaten existieren (Kennedy und Inkpen, 2006). Da für die vorliegende Sentiment Analyse lediglich ein relativ kleiner Untersuchungskorpus von 167 Tweets zur Verfügung steht, kommt die Anwendung der lernbasierten Methoden nicht in Frage. Des Weiteren können bei den Lexikon basierten Methoden kontextbedingte Ambivalenzen und andere sprachliche Konstrukte leichter berücksichtigt werden, da linguistische Aspekte eines Textes in Betracht gezogen werden können (Brooke et al., 2009). Dies ist vor allem zur Analyse von Mikroblogging-Einträgen geeignet, weil die Texte sehr kurz gehalten sind (Twitter erlaubt maximal 140 Zeichen pro Tweet). Grundsätzlich sind allerdings die Methoden des maschinellen Lernens bei Sentiment Analysen im Hinblick auf die Genauigkeit und Präzision der Klassifizierung meist effektiver als die Lexikon basierten Ansätze (Kennedy und Inkpen, 2006).
                
            Es existieren zahlreiche Wörterbücher, wie beispielsweise 
                    „MPQA Subjectivity Lexicon“, „Bing Liu and Minqing Hu Sentiment Lexicon“, „SentiWordNet“, „VADER Sentiment Lexicon“, „SenticNet“, „LIWC“, „Harvard General Inquirer“, „ANEW“ usw. um nur einige Beispiele zu nennen. Die meisten dieser beispielhaft genannten Wörterbücher liefern allerdings nur die tendenziellen Stimmungen 
                    positiv, 
                    negativ oder 
                    neutral. Das hier verwendete Wörterbuch 
                    „SentiWordNet 3.0“ liefert vertiefend die einzelnen Gewichte der Stimmungen („Score“) zu einzelnen Wörtern.
                
            Das Ziel dieser Sentiment Analyse ist die 
                    Bestimmung der emotionalen Färbungen der einzelnen Tweets und in weiterer Folge die 
                    Ermittlung der generellen Stimmungshaltung der Diskussionen bezüglich des Statischen im Vergleich zum bewegten Bild. Die Datenerhebung der einzelnen Tweet-Ströme (Tweets + Retweets) erfolgt über die Twitter API mit Hilfe des Web-Tools 
                    „FollowTheHashtag“
               . Die Datensätze werden direkt in Microsoft Excel exportiert und dort weiterverarbeitet. Zur Grundgesamtheit gehören alle Tweets (exkl. Retweets) die im Zuge der US-Präsidentschaftswahl oder im Zuge der Ersten 100 Tage nach Amtsantritt von D. Trump via Twitter weltweit abgesetzt wurden und beide Hashtags 
                    „#thesimpsons“ und 
                    „#trump“ beinhalten. Retweets werden bei der vorliegenden Sentiment Analyse nicht berücksichtigt, da diese keine neuen Aussagen, Meinungen oder Emotionen enthalten, sondern lediglich eine intendierte Wiederholung eines vorangegangenen Tweets darstellen.
                
            Die Analyseeinheit ist der jeweils im Tweet enthaltene Text 
                    („Tweet Content“). Der textuelle Inhalt eines Tweets kann Hashtags (#), Taggings (@) oder (Medien-) Links enthalten. Alle Texte, deren Aussagen nicht in Relation mit den beiden Memes stehen, werden als Spam klassifiziert und aus dem Datensatz bereinigt. Die beiden Memes an sich werden nicht untersucht, sondern stellen nur den Auslöser der Diskussion dar.
                
            Die Untersuchungszeiträume betragen jeweils sieben Tage ab dem Stichtag des Absetzens des ersten Tweets zu einem der beiden definierten Memes. Der Datensatz des statischen Bildes beläuft sich somit auf den Untersuchungszeitraum vom 
                    10.11.2016 bis zum 
                    16.11.2016 und beinhaltet 
                    N = 94 Tweets (exkl. Retweets). Der Datensatz des Videos beläuft sich auf den Untersuchungszeitraum vom 
                    27.04.2017 bis zum 
                    04.05.2017 und beinhaltet 
                    N = 73 Tweets (exkl. Retweets). Von der Kombination des Lexikon basierten Ansatzes mit lernbasierten Methoden wird aufgrund der geringen Datenmenge 
                    (N = 167 relevante Tweets) abgesehen.
                
            Nach der Datenerhebung und –bereinigung folgt manuell der Prozess der 
            Textnormalisierung nach dem Konzept von 
                    Tajinder Singh and Madhu Kumari, gefolgt von der manuellen Vorverarbeitung inkl. Satztypen Erkennung der textuellen Einheiten nach dem Konzept von 
                    Lei Zhang et al. Die einzelnen Tweet Contents werden hinsichtlich ihrer Satz- und Wortebene unterteilt, wobei für jede textuelle Einheit die 
                    positive, 
                    negative oder 
                    neutrale Stimmung aus einem Wörterbuch entnommen wird. Die einzelnen Tweets werden nach den Satztypen 
                    deklarativ, 
                    imperativ und 
                    interrogativ kategorisiert. Interrogativsätze fließen nicht in die Auswertung ein, da dieser Satztyp keine informativen Meinungen, sondern lediglich Fragestellungen zum Thema oder zu vorausgehenden Tweets ausdrückt.
                
            Danach folgt die Betrachtung der Wortebene, wobei nun alle Wörter mit emotionaler Stimmung aus den textuellen Einheiten extrahiert werden. Die Gewichtung der Stimmung jedes dieser Wörter wird mit Hilfe des Wörterbuchs 
                    „SentiWordNet 3.0“ bestimmt. Für die einzelnen Abfragen aus dem Wörterbuch wird der freie Programmcode von 
                    Petter Törnberg adaptiert. Nach der Abfrage aller Gewichte erfolgt die Auswertung der Daten. Hierzu wird der 
                    „Score“ je Tweet (bestehend aus einzelnen bzw. mehreren Sätzen) durch die Summe der einzelnen Gewichte der Stimmungen der Wörter der textuellen Einheiten eines Tweets ermittelt. 
                
            
               score = score(pos(Wort)) + score(neg(Wort))
            
            Wobei 
                    score(pos(Wort)) die Summe der Scores aller positiven Gewichte der relevanten Wörter eines Tweets enthält. 
                
            
               Score(neg(Wort)) stellt analog die Summe aller negativen Gewichte dar. Durch obige Formel wird der Score des Tweets berechnet. 
                
            Punktationen, wie beispielsweise ;-), Smileys, Emoticons oder ähnliche nicht textuelle Ausdrucksformen von Stimmungen werden in der Sentiment Analyse nicht berücksichtigt.
         
         
            Ergebnisse 
            Die Ähnlichkeiten und Unterschiede der emotionalen Diskussionen beider Memes konnten ermittelt und beschrieben werden. Dabei stellte sich insbesondere heraus, dass sich die emotionalen Richtungen der Diskussionen bezüglich des statischen Bilds im Vergleich zum Video erheblich voneinander unterscheiden. Die Auswertung der Häufigkeiten der emotionalen Färbungen der Tweets zum statischen Bild 
                    (N = 86) unterteilt sich in 
                    19 positive, 
                    51 neutrale und 
                    16 negative Äußerungen. Die Ergebnisse für das Video 
                    (N = 70) beinhalten 
                    18 positiv, 
                    36 neutral und 
                    16 negativ gestimmte Tweets.
                
            Vergleicht man rein die Häufigkeiten des Auftretens der einzelnen Stimmungen, sieht man, dass die Verteilung fast ähnlich ist. Betrachtet man die Scores für das statische Bild 
                    (score = 1,339) bzw. das Video 
                    (score = -.153), sprich die Summe aller positiven und negativen Gewichte der Wörter aller Tweets je Datensatz, so zeigt sich, dass trotz ähnlicher Häufigkeitsverteilungen die finale Stimmung für das statische 
                    Bild in Summe positiv gehalten ist. Beim 
                    Video fällt die Stimmung negativ aus. Ob bzw. in wie weit beispielsweise das Unterhaltungserlebnis oder die (ironischen) Inhalte des Videos im Gegensatz zum statischen Bild einen Einfluss auf die Emotionalität der Twitter-Nutzer beim Verfassen der einzelnen Texte der Tweets hat, wird im Rahmen dieser Forschungsarbeit nicht betrachtet.
                
            Die Forschungsarbeit dient als Anwendungsbeispiel für eine Social Media Sentiment Analyse auf Twitter Daten und bildet einen thematisch übergreifenden Forschungsansatz für die Disziplinen der Informatik, der traditionellen Geisteswissenschaften und der Digital Humanities aufgrund der Kombination von 
                    „User-Generated-Content“ in sozialen Netzwerken, über eine Programmierung bis hin zum Untersuchungsgegenstand der Memes, welcher wiederum typisch für die Geistes- und Sprachwissenschaften ist.
                
         
         
            Schlussfolgerungen
            
               Die Plattform Twitter ist ein stark genutztes soziales Netzwerk bzw. ein Mikroblogging-Dienst, wodurch dessen Nutzer ihre eigenen Meinungen sowie Gefühle zu Themen aller Art ausdrücken. Die Twitter-Benutzer kommentierten das Thema der US-Wahl und die ersten 100 Amtstage von D. Trump. Die Auslöser dieser Diskussionen stellten die beiden eingangs gezeigten Memes dar inkl. ihrer transportierten Nachrichten, welche durch das Zusammenspiel von Text und Bild vermittelt werden. Memes können durchaus Emotionen in der Internetgemeinde erzeugen - unter anderem auch in politischen Kontexten – ansonsten hätten sich diese beiden Memes nicht binnen kürzester Zeit via Twitter verbreiten können.
               Bezüglich der inhaltlichen Ausgestaltung der einzelnen Tweet Contents kann man sagen, dass nahezu alle Tweets Abkürzungen und/oder Emoticons beinhalteten. Die Polaritäten der Emoticons wurden allerdings nicht berücksichtigt, da hier auf keinen entsprechenden wissenschaftlichen Ansatz zurückgegriffen werden konnte.
               Für manuell durchgeführte Sentiment Analysen auf kleinem Untersuchungskorpus eignen sich Lexikon basierte Ansätze hervorragend. Ob eine Kombination mit Methoden des maschinellen Lernens herangezogen wird, hängt vom Untersuchungsgegenstand und von der Größe des Untersuchungskorpus ab.
               Grundsätzlich existieren zahlreiche Wörterbücher für Sentiment Analysen. Allerdings liefern die meisten Wörterbücher lediglich die tendenziellen Wortbedeutungen 
                        positiv, 
                        negativ oder 
                        neutral. Das Wörterbuch 
                        „SentiWordNet“ hingegen skaliert die stimmungstragenden Wörter auf dem Intervall [-1, 1] und verleiht den Wörtern spezifische Gewichte, wobei „-1“ negativ und „1“ positiv bedeuten. Die verhältnisskalierten Stimmungsinformationen zu den einzelnen Wörtern konnten größtenteils über dieses Wörterbuch bestimmt werden, allerdings gab es einige Wörter die selbst dieses Wörterbuch nicht beinhaltete. Das Wörterbuch „
                        SentiWordNet“ ist somit kein vollständiges Sentimentlexikon.
                    
               Bei der reinen Auswertung der Häufigkeiten der emotionalen Färbungen der Tweets zeigt sich, dass die Verteilung fast ident ist zwischen dem statischen Bild 
                        (N = 86, davon 
                        19 positiv, 
                        51 neutral und 
                        16 negativ) und dem Video 
                        (N = 70, davon 18 positiv, 
                        36 neutral und 
                        16 negativ). Deshalb wurden zusätzlich die verhältnisskalierten Stimmungsinformationen erhoben und einzelnen Gewichte betrachtet. Hierdurch lässt sich zeigen, dass die finale Stimmung für das statische 
                        Bild in Summe positiv (score = 1,339) gehalten ist. Beim 
                        Video fällt die Stimmung negativ (score = -.153) aus. Die Deutung/Interpretation der Ergebnisse einer Sentiment Analyse ist somit stark von der verwendeten Methode und dessen Ansatz abhängig.
                    
            
         
      
      
         
            Quelle Abbildung 1: 
                 https://img.buzzfeed.com/buzzfeed-static/static/201611/9/16/asset/buzzfeed-prod-fastlane02/sub-buzz18441-1478727536-5.png?downsize=715:*&outputformat=auto&output-quality=auto (zuletzt aufgerufen: 30.06.2017) 
             
            Quelle Abbildung 2: 
                 https://www.youtube.com/watch?v=Qo3fT0xPeHs (zuletzt aufgerufen: 30.06.2017)
             
            
               http://www.followthehashtag.com/ (zuletzt aufgerufen: 26.07.2017)
             
            Programmcode verfügbar unter GNU General Public License. Quelle: 
                 https://github.com/mserrate/twitter-streamingapp/blob/master/twitter-stormtopology/src/main/java/analysis/SentiWordNet.java (zuletzt aufgerufen: 30.06.2017) 
             
         
         
            
               Bibliographie
               
                  Baccianella, S., Esuli, A., & Sebastiani, F. (2010). 
                        SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining. LREC, Vol. 10, S. 2200-2204. 
                    
               
                  Brooke, J.; Tofiloski, M.; Taboada, M.: Crosslinguistic sentiment analysis: From english to spanish. In: Proceedings of the 7th International Conference on Recent Advances in Natural Language Processing, Borovets, Bulgaria, 2009, S. 50–54.
               
                  Friedkin, N.E. & Johnsen, E.C. (1990) 
                        Social influence and opinions. J. Math. Soc. 15. pp. 193 – 206. 
                    
               
                  Kennedy, A.; Inkpen, D.: Sentiment Classification of Movie and Product Reviews Using Contextual Valence Shifters. In: Computational Intelligence 22 (2006), Nr. 2, S. 110–125.
               
                  Liu, B. (2010):
                        „Sentiment Analysis: A Multifaceted Problem“. IEEE Intelligent Systems, S. 76-80. 
                    
               
                  Marx, K. & Weidacher, G. (2014). 
                        Internetlinguistik. Ein Lehr- und Arbeitsbuch. Tübingen: Narr. 
                    
               
                  Shifman, L. (2013). 
                        Memes in Digital Culture. MA: MIT Press. 
                    
               
                  Singh, T. und Kumari, M. (2016). 
                        Role of Text Pre-processing in Twitter Sentiment Analysis. Procedia Computer Science, 89, 549-554. 
                    
               
                  Spitzberg, B. H. (2014). 
                        Toward A Model of Meme Diffusion (M3D). Communication Theory 24. S. 311–339. 
                    
               
                  Wilson, T., Wiebe, J., & Hoffmann, P. (2005). 
                        Recognizing contextual polarity in phrase-level sentiment analysis. In: Proceedings of the conference on human language technology and empirical methods in natural language processing. S. 347–354. 
                    
               
                  Zhang, L., et al. (2011). Combining Lexicon-based and Learning-based Methods for Twitter Sentiment Analysis. HP Laboratories, Technical Report HPL-2011-89. 
            
         
      
   



      
         
            Projektvorstellung
            Die Methoden der Digital Humanities sind ebenso zahlreich wie die Fachbereiche, in denen sie zur Anwendung kommen. Ob Archäologie oder Literaturwissenschaft, Soziologie oder Geschichte: Unser Podcast will über Theorien und praktische Anwendungsbereiche der Digital Humanities informieren und diskutieren; er will neugierig machen und die Angst vor der Anwendung neuer, noch ungewohnter Forschungsmethoden nehmen. 
            Die Rezeptionsform „Podcast“ soll Studierende auf eine noch weitgehend ungewohnte Weise ansprechen: Wir möchten sie in ihrem Alltag erreichen, sie zur Entdeckung neuer Forschungsmethoden anregen und gleichzeitig den kreativen Umgang mit ihrem je eignen Forschungsthema fördern.
            Umgekehrt soll die Aufmerksamkeit für bereits existierende DH-Projekte und -Studiengänge erhöht werden, um den Forschungsnachwuchs neugierig zu machen und DH-Vorhaben mehr Reichweite zu ermöglichen. Dabei soll der Eindruck, dass die DH in weiten Teilen eine algorithmen- und werkzeuggetriebene Wissenschaft sei, dem kritischen Anspruch der Geisteswissenschaften gegenübergestellt werden. Wir führen u. a. Interviews mit Wissenschaftler/-innen und Praktikern und bereiten Forschungsdiskurse auf.
         
         
            Themen
            Wir möchten einen umfassenden Einblick in die Methoden und Anwendungsbereiche der Digital Humanities geben und die Forschungswerkstätten beleuchten. In Form von Interviews werden in den ersten Folgen Mitarbeitende verschiedener Universitäten und Institute interviewt und zu ihren aktuellen Forschungsprojekten befragt. Weitere Kooperationen sind in Planung. Doch auch "Praktiker" wie Editionswissenschaftler, Bibliothekare, Data Scientists etc. sollen nach ihren berufspraktischen Erfahrungen gefragt werden. Wie können hier digitale Methoden über die universitäre Forschungspraxis hinaus zum Einsatz kommen? Ziel ist zudem, laut über mögliche Kritikpunkte nachzudenken: Welche Potenziale haben digitale Methoden, welche Defizite des Analogen können damit ausgeglichen werden? Welche Gefahren ergeben sich? Geht der Methode überhaupt eine Theorie voraus?
            Ergänzend zum Podcast bauen wir ein Glossar in Form des sogenannten "Wissensblogs" auf: Parallel zu den Audio-Beiträgen informieren wir hier ganz grundlegend über konkrete Werkzeuge und Methoden. Die hier gelieferten Informationen werden in den einzelnen thematisch passenden Folgen ggf. aufgegriffen und dem Publikum vorgestellt. Hier geben wir dem wissenschaftlichen Nachwuchs Tools an die Hand, die fachübergreifend nützlich sein können. So beleuchten wir zum Beispiel Fragen der Lizenzierung von Online-Veröffentlichungen, digitale Präsentationsmöglichkeiten, einheitliche Daten-Referenzsysteme etc.
            Eine weitere Informationsquelle soll die Rubrik "Empfehlungen" darstellen, die ebenfalls über die Webseite zu erreichen ist. Hier werden hilfreiche Bücher und andere Publikationen rezensiert.
            Der Podcast wird kostenlos über die üblichen Kanäle wie iTunes etc. und unsere Website verbreitet.
            Das Poster soll eine Kostprobe geben und vor allem Lust aufs Zuhören machen. Ziel ist es, Aufmerksamkeit zu schaffen und ggf. neue Interviewpartner zu finden. Zudem sind kurze Hörproben geplant.
            
               
            
         
         
            Key Facts
            
               Dauer pro Folge: 45 Minuten
               6 Folgen pro Jahr
               Arbeitsaufwand pro Folge: ca. 25 Stunden
               Webauftritt: 
                        http://www.digitale-wissenschaft.de
               
               Twitter: 
                        @DigiWissen
               
               Verwendete Technik/Software: Blue Yeti Podcasting Mikrofon, Ultraschall
               Reichweite über: iTunes, Webseite, Twitter, Facebook, DH-Liste
               Finanzierung: Crowdfunding via Patreon
            
            
               Aufbau
               
                  Dialog zwischen Jens-Martin Loebel und Carolin Hahn
                  30 Minuten Interview mit einer dritten Person (je nach Rubrik)
                  15 Minuten Vor- und Nachbesprechung
                  Einführung in ein zum Thema passendes Wissensgebiet im Bereich Digital Humanities, das im Blog ausführlicher nachgelesen werden kann
               
            
            
               Zielgruppe
               Als Informationsportal adressieren wir explizit Studierende und interessierte Laien. 
            
            
               Relevante Informationen auf dem Poster
               
                  Key Facts, Beteiligte, Konzept, Distibutionskanäle und Zielgruppe
                  Rubriken und Programm 2018
                  Audio-Effekt: Einbau einer dezenten Tonspur im Poster per Lautsprecher, Hörproben zum Mitnehmen
               
            
         
      
      
         
            
               Bibliographie
               
                  Geoghegan, Michael / Dan Klass (2007): Podcast Solutions. The Complete Guide to Audio and Video Podcasting. New York: Springer.
                    
               
                  Hagedorn, Brigitte (2006): Podcasting: Konzept | Produktion | Vermarktung. Köln: mitp Verlags GmbH.
                    
               
                  Podcast der Helmholtz-Gemeinschaft: https://resonator-podcast.de/ [letzter Zugriff 15. September 2017].
                    
               
                  Podcast der Universität Wien: http://medienportal.univie.ac.at/uniview/podcast-audimax/ [letzter Zugriff 15. September 2017].
                    
               
                  Physik-Podcast im ORF: http://www.physikalischesoiree.at/ [letzter Zugriff 15. September 2017].
                    
            
         
      
   



      
         
            Einleitung
            In diesem Beitrag wollen wir ein Vorhaben zur Diskussion stellen, das an zwei zentralen Herausforderungen in den Digital Humanities ansetzt: Der Erstellung adäquater Annotationsrichtlinien für geisteswissenschaftlich relevante textuelle Konzepte und der Schnittstelle in der Kooperation zwischen beteiligten Wissenschaftlerinnen und Wissenschaftlern aus Geisteswissenschaft und Informatik. Für DH-Projekte sind Kooperationen unerlässlich, wenn fortgeschrittene Techniken zur Textanalyse eingesetzt werden und/oder es um eine Zusammenführung von Konzepten oder Zugangsweisen geht, die bereits intradisziplinär als komplex gelten. Dabei wird ein signifikanter Anteil der Projektlaufzeit auf die Entwicklung einer “gemeinsamen Sprache” und die Identifikation der exakten, gemeinsamen wissenschaftlichen Fragestellung verwendet. Dies ist zweifellos ein produktiver Prozess, dessen erfolgreiche Durchführung allerdings voraussetzt, dass auf beiden Seiten Forscherinnen und Forscher beteiligt sind, die sich auf das interdisziplinäre Vorgehen voll einlassen und auch den nötigen Zeitaufwand tragen. 
            Methodisch-technisch ist ein substanzielles Nadelöhr bei der Entwicklung automatischer Werkzeuge das Fehlen von annotierten Goldstandards, an/auf denen Werkzeuge trainiert, verglichen und feinjustiert werden können. Das Fehlen der Goldstandards ist jedoch eigentlich ein nachgelagertes Problem, wie sich z.B. in narratologisch orientierten Projekten zeigt (heureCLÉA: Bögel et al., 2015; Propp annotation: Fisseni et al., 2014): Die Umsetzung narratologischer Theorien als Annotationen ist alles andere als trivial, da narratologische Konzepte nicht im Hinblick auf Annotation entwickelt wurden. Leerstellen in den Definitionen müssen gefüllt, Voraussetzungen geklärt und Unterkategorien geklärt werden. Die Annotation solcher Kategorien ist also kein reiner Umsetzungs- oder Implementierungsprozess, sondern einer bei dem sich tiefe, konzeptionelle Fragen stellen. Als Ergebnis solcher Prozesse stehen dann Annotationsrichtlinien, die die Brücke zwischen Theorie und Praxis schlagen. Erst wenn Annotationsrichtlinien für ein Phänomen (oder eine Gruppe von Phänomenen) etabliert sind, können größere Annotationsprojekte mit Aussicht auf Erfolg durchgeführt werden.
            Das von uns vorgeschlagene Vorgehen erlaubt den Beteiligten Forscherinnen und Forschern ihre Expertise einzubringen, ohne in einem gemeinsamen Projektkontext zu arbeiten. Die Schnittstelle zwischen D und H wird hierbei von annotierten Daten und Annotationsrichtlinien gebildet, wobei die Richtlinien ohne Kompromisse bezüglich möglicher Automatisierungen erstellt werden. Das Vorhaben gibt somit auch narratologisch/literaturwissenschaftlich anspruchsvoller Konzeptentwicklung und damit Theoriebildung einen Rahmen. Verfügbare annotierte Daten wiederum erlauben Informatikerinnen und Informatikern ohne Expertise in narratologischen Fragen die Entwicklung von Werkzeugen für komplexe technische Probleme.
         
         
            Ein 
                    shared task zur Erstellung von Annotationsrichtlinien
                
            Shared Tasks sind in der Computerlinguistik weit verbreitet und haben für viele Bereiche gezeigt, dass sie ein geeignetes Instrument sind, Forschungsbemühungen verschiedener Gruppen zum gleichen Thema zu bündeln und zu verstärken. In einem 
                    shared task versuchen verschiedene Arbeitsgruppen mit verschiedenen Methoden dieselbe, klar definierte Aufgabe zu lösen, z.B. Word Sense Disambiguation (z.B. Mihalcea et al., 2004), Sentiment Analysis (z.B. Nakov et al., 2013) oder Named Entity Recognition (z.B. Sang and/De Meulder 2003). Auch wenn bisweilen im Rahmen von NLP-shared tasks Annotationsstandards neu entwickelt werden, liegt der Fokus hier auf der Verbesserung der Vorhersagequalität automatischer Systeme. Damit in einem solchen Vorgehen literaturwissenschaftlich relevante und interessante Konzepte und Phänomene bearbeitet werden, muss literaturwissenschaftliche Expertise bei der Erstellung der Annotationsrichtlinien einfließen.
                
            Als Rahmen für die Entwicklung von Annotationsrichtlinien organisieren wir einen shared task der sich genau auf dieses Ziel konzentriert (Phase 1: Erstellung von Guidelines). Sind die Richtlinien etabliert, kann anschließend ein großes Korpus annotiert werden, das wiederum in einem NLP-shared task eingesetzt werden kann, um Verfahren zu erproben, die die annotierten Phänomene automatisch finden (Phase 2: Automatisierung).
            Als Phänomen haben wir uns dabei auf Erzählebenen in englischen und deutschen Texten festgelegt, da diese für zahlreiche, komplexere literaturwissenschaftliche Fragestellungen hilfreich sind, ohne selbst (für einen ersten 
                    shared task) zu komplex zu sein. Zudem sind sie als Phänomen omnipräsent: Praktisch jeder narrative Text enthält mehr als eine Erzählebene, und sie sind auch in nicht-textuellen Medien wie z.B. Filmen verbreitet. Die Existenz verschiedener Theorien zur Analyse von Erzählebenen in literarischen Texten belegt, dass es dabei auch konzeptionellen, theoretischen Entwicklungsbedarf gibt. Erzählebenen bilden darüber hinaus eine wichtige Segmentierungsstufe für die weitere automatische semantische Verarbeitung von Texten: z.B. sollte Koreferenzresolution von der vorher erfolgten Erkennung von Erzählebenen profitieren,da Koreferenzketten in heterodiegetischen eingebetteten Erzählungen nicht ebenenübergreifend sein sollten.
                
            Während Details zum Gesamtaufbau des Shared Tasks bereits in einem anderen Artikel beschrieben wurden (Reiter et al., 2017), fokussieren wir uns in diesem Beitrag auf die genauere Beschreibung der ersten Phase des 
                    shared tasks. 
                
         
         
            Geplanter Ablauf
            
               Erstellung von Annotationsrichtlinien 
               (bis Mitte Juni 2018)
               Im ersten Schritt wird allen Teilnehmerinnen und Teilnehmern ein 
                        development corpus bestehend aus ca. 20 Texten zugänglich gemacht. Die Texte liegen auf deutsch und englisch vor und decken verschiedene Genres und Epochen ab. Die Texte enthalten verschiedene Arten von Erzählebenen, gemäß eines etwas vagen Vorverständnisses.
                    
               Die Texte können und sollen von den Teilnehmerinnen und Teilnehmern benutzt werden, um Richtlinien für die Annotation von Erzählebenen zu entwickeln und zu testen. Ob die Texte in einer oder in beiden Sprachen verwendet werden, ist dabei den Teilnehmerinnen und Teilnehmern überlassen. Sie sollten dabei das Ziel verfolgen, eine möglichst breite Anwendbarkeit der Richtlinien sicherzustellen (auch jenseits des 
                        development corpus). Außerdem sollen die Richtlinien vollständig und selbsterklärend sein, so dass kein Expertenwissen zur Anwendung vorausgesetzt wird. Um mehrsprachige Anwendung zu ermöglichen, sollen die Richtlinien auf Englisch formuliert sein, sie dürfen aber sprachspezifische Beispiele enthalten.
                    
               Wie genau die Gruppen dabei vorgehen, bleibt ihnen überlassen. In vergangenen Annotationsprojekten (mit und ohne Bezug zu Literaturwissenschaft bzw. literarischen Texten) hat sich aber ein iterativer Prozess als fruchtbar erwiesen. Sobald eine erste Version der Richtlinien erstellt wurde, werden sie auf neuen Texten getestet, um ihre Definitionslücken oder Vagheiten zu identifizieren. Aus dem Schließen der Lücken ergibt sich dann eine weitere Version der Richtlinien, die wiederum auf Texten getestet werden können.
            
            
               Anwendung eigener Guidelines
               (bis Ende Juni 2018)
               Im zweiten Schritt sollen die Arbeitsgruppen ihre eigenen Richtlinien auf neuen Texten testen. Nach dem Einreichen ihrer Richtlinien erhalten die Teilnehmerinnen und Teilnehmer hierzu sechs neue literarische Texte, die vom Organisationsteams des 
                        shared tasks ausgesucht wurden. Die Annotation dieser Texte muss dabei in einem Web-basierten, frei zugänglichen, von den Organisatoren bereitgestellten Annotationstool durchgeführt werden, um die automatisierte Auswertung der Annotationen und ihren Vergleich zu ermöglichen.
                    
            
            
               Anwendung von Guidelines anderer Teilnehmer
               (bis Mitte Juli 2018)
               Im dritten Schritt erhält jede teilnehmende Gruppe Richtlinien anderer Gruppen, auf deren Basis Erzählebenen in den sechs Texten erneut annotiert werden, wobei alle Richtlinien von uns zuvor anonymisiert werden. Zusätzlich wird auch eine vom Organisationsteam betreute Gruppe von studentischen Hilfskräften alle eingereichten Annotationsrichtlinien auf den sechs Texten anwenden.
            
            
               Evaluation aller vorgeschlagener Guidelines
               (August/September 2018)
               Im letzten Schritt der ersten Phase des 
                        shared tasks werden alle eingereichten Annotationsrichtlinien verglichen und evaluiert. Dafür treffen sich die Teilnehmerinnen und Teilnehmer zu einem Workshop, auf dem sie ihre eigenen Richtlinien vorstellen und gemeinsam Qualität und Komplexität bewertet werden. Das Ziel des Workshops ist außerdem, basierend auf der Diskussion und den Informationen bezüglich der Inter-Annotator-Agreements im Plenum und möglichst konsensual die Annotationsrichtlinien zu bestimmen, die dann in der zweiten Phase des 
                        shared tasks verwendet werden. Auf deren Basis werden dann Methoden und Systeme entwickelt, die automatisch Erzählebenen in Texten identifizieren können.
                    
               Zur vergleichenden Evaluation von Annotationsrichtlinien sind bisher Ansätze aus der Computer- und Korpuslinguistik zur quantitativen Messung des Inter-Annotator-Agreement (IAA) bekannt (vgl. Artstein, 2017), die im Bereich der Digital Humanities angewendet wurden und werden. Da es aber bei der Erstellung von Annotationsrichtlinien für narratologische Phänomene eben nicht 
                        nur um die Umsetzung und Erklärung einer klar spezifizierten Theorie geht, sondern eben 
                        auch um die (Weiter-)Entwicklung narratologischer Konzepte, bedarf es eines weitergehenden Blickes. Dabei sollen drei Aspekte Berücksichtigung finden: Die 
                        Anwendbarkeit von Annotationsrichtlinien kann durch quantitatives IAA gemessen werden. Hier stellen sich durch möglicherweise unterschiedliche theoretische Zugänge vor allem Fragen der Vergleichbarkeit. Der Aspekt der begrifflichen 
                        Abdeckung bezieht sich darauf, welche (bekannten) narratologischen Ebenenkonzeptionen  in der konkreten Ausgestaltung vollständig oder teilweise enthalten sind. Dies wird sich nur durch qualitative Analyse und wissenschaftliche Diskussion basierend auf theoretischen Vorstudien klären lassen, für die der Workshop einen Rahmen bieten soll. Die 
                        Nützlichkeit von Annotationsrichtlinien kann bei narrativen Ebenen dahingehend bewertet werden, ob sie interpretativ wertvolle Beschreibungen erlauben. Leitgedanke ist hier, dass narratologische Annotationen eine deskriptive Basis für literaturwissenschaftliche Interpretationen liefern sollen. Unterschiedlichen Annotationsrichtlinien zu folgen hieße also zu unterschiedlichen Text-Deskriptionen zu kommen, die wiederum unterschiedliche Interpretationen zulassen.
                    
            
         
         
            Conclusions
            Im Rahmen des Vortrags wollen wir insbesondere zwei der o.g. Aspekte in den Fokus rücken und diskutieren: Die iterative Entwicklung von Annotationsrichtlinien als verteiltes, kollaboratives Projekt sowie die Evaluation und Vergleichbarkeit von Annotationsrichtlinien für literarische Phänomene.
         
      
      
         
            
               Bibliographie
               
                  Artstein, Ron (2017): “Inter-annotator Agreement”, in: Ide, Nancy / Pustejovsky James (eds.): 
                        Handbook of Linguistic Annotation. Dordrecht: Springer. DOI 10.1007/978-94-024-0881-2.
                    
               
                  Bögel, Thomas / Gertz, Michael / Gius, Evelyn / Jacke, Janina / Meister,  Jan Christoph / Petris, Marco / Strötgen, Jannik (2015): “Collaborative text annotation meets machine learning: heurecléa, a digital heuristic of narrative”, in: DHCommons 1.
                    
               
                  Fisseni, Bernhard / Kurji, Aadil / Löwe, Benedikt (2014): “Annotating with Propp’s morphology of the folktale: Reproducibility and trainability”, in: 
                        Literary and Linguistic Computing 29(4):488–510, 1093/llc/fqu050
                    
               
                  Mihalcea, Rada /  Chklovski, Timothy / Kilgarriff, Adam (2004): “The Senseval-3 English Lexical Sample Task”. In 
                        Proceedings of SENSEVAL-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain.
                    
               
                  Nakov, Preslav / Rosenthal, Sara / Kozareva, Zornitsa / Stoyanov, Veselin / Ritter, Alan / Wilson, Theresa (2013): “SemEval-2013 Task 2: Sentiment Analysis in Twitter”. In 
                        Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval ‘13, Atlanta, Georgia, USA.
                    
               
                  Reiter, Nils / Gius, Evelyn / Strötgen, Jannik / Willand, Marcus (2017): “A Shared Task for a Shared Goal - Systematic Annotation of Literary Texts
                        ”. In Digital Humanities 2017: Conference Abstracts, Montreal, Canada.
                    
               
                  
                  Sang, Erik F. Tjong Kim / de Meulder, Fien (2003): “Introduction to the CoNLL-2003 Shared Task: Language-independent Named Entity Recognition”, in 
                        Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003 - Volume 4 (CONLL '03).
                    
            
         
      
   



      
         
            Thematik und Ziel
            
               „Der Schlaf der Vernunft gebiert Monster“, wusste Francisco de Goya. Ein 
                    Schlaf der Quellenkritik auch. Deshalb ist eine dem Digitalen angepasste, auf Daten erweiterte quellenkrititische Methodik üblich in den Digital Humanities und verwandten Fächern. Akzeptanz für eine kontextuell orientierte Quellenkritik im Digitalen ist auch im erweiterten Diskurs detektierbar, wenn auf einer abstrakteren Ebene für kulturkritische Perspektiven mit verstärkt ganzheitlichen Sichtweisen plädiert wird (Liu 2012, Presner 2015). In diesem Zusammenhang steht das Ziel des Vortrags, der sich in zwei Blöcke gliedert: Einer Analyse von Quellenspezifika im Digitalen folgt, vergleichend und übertragend, die Skizze eines digital-quellenkritischen Leitfadens, der Kriterien der 
                    exakt historischen Methode auf Born Digital spiegelt. Damit nimmt das transdisziplinäre Experiment methodisch Anleihe an der volkskundlichen 
                    Münchner Schule, die wiederum auf “Klassiker” der Quellenkritik zurückgreift, etwa Johann Gustav Droysen. Diese dezidiert 
                    historische Sichtweise wird eingenommen, da auch digitale Quellen historisch bedingt sind und ihre Deutung - im Sinne einer Ganzheitlichkeit - dem Rechnung tragen sollte. Es erscheint sinnvoll, eine Systematik anzuwenden, die hilft, Kontexte entsprechend zu identifizieren und transparent in hermeneutische Prozesse miteinzubeziehen. 
                
         
         
            Stand
            Die historische Dimension des Digitalen (das Internet: Brügger 2017; Born Digital, Webseiten: z. B. Nanni 2017; Twitter: Sternfeld 2014) ist ebenso Gegenstand in den Digital Humanities wie die kulturwissenschaftliche (Klawitter et al 2012). Quellenkritikim Digitalenpräzisieren Handbücher(Crompton et al 2016; Griffin, Hayler 2016), Angebote wie „compas. Strukturiertes Forschen im Web“ (infoclio.ch, Baumann/Hügi 2017) führen niederschwellig ein in„Quellenkritik bei Quellen aus dem Internet“. Zwei Beiträge seien hier herausgestellt: Eva Pfanzelter (2010)vergleicht explizit historische Quellenkritik („innere/äußere Kritik“, Pfanzelter 2010: 43) mit Quellenkritik im Digitalen und beleuchtet den daraus notwendig resultierenden „kritischen Umgang mit digitalen Ressourcen“. Peter Haber Peter Haber (2011)rekurriert in „Digital Past“ explizit auf Droysens Methodik. 
         
         
            Spezifika
            Vorangestellt sei ein Diktum von Alan Liu, 
                    „(…) the virtual is indeed fully material“ (Liu 2014: 276). Dem wird zugestimmt 
                    und aufgezeigt, dass das Ungreifbare Folgen hat für die Einordnung von Inhalt und Kontext: Digitale Quellen weisen spezifische Eigenschaften auf, die sich auf Autorschaft, Stoffliches und Zeitliches beziehen. Diese Kriterien werden anhand digitaler Quellenarten herausgearbeitet und in Bezug auf kritische Methodik betrachtet, um dann die methodische Übertragung zu zeichnen.
                
            
               Daten und Autorschaft
               Konventionell bezieht sich Autorschaft im Digitalen auf sekundäre oder primäre Quellen, bei denen Publizierende konservativ verzeichnet sind, sowie auf Schwarmprodukte mit fließenden Autorschaften, Schichtungen und Intentionen, deren Identifikation synoptische Auswertungsprozesse verlangt. 
                        Da Autorschaft und Intentionalität als kritisches Moment eng hängen zusammenhängen, ist die Frage der Autorschaft im gegebenen Kontext zu erweitern auf Daten: Datenkritik. Das „Ethos der Statistik“, das sich auf Erfassungsparameter und Algorithmen genauso wie auf Fragestellungen und Operationalisierungen bezieht, ist im quellenkritischen Sinn zu erweitern auf hermeneutische Interpretationen. Am Beispiel von Malte Rehbeins (2017) Kritik des Projekts 
                        Charting Culture wird der Wert dieser kritischen Verschränkung deutlich. 
                    
            
            
               Digitalisat und Stofflichkeit 
               Digitalisate bedürfen als vom Analogen ins Digitale transformierte Quellen besonderer Kritik, sowohl bezüglich des Objekts als auch der Metadaten. Im Analogen bildet die Dualität von Medium und Text Information aus, bei Daten als Träger von Information fallen Medium und Botschaft im McLuhanschen Sinne zusammen. Deshalb wohnt digitalen Repräsentationen immer ein Informationsverlust inne, dem Erfassung und Modellierung lediglich entgegenwirken. So teilt ein digitales Faksimile mehr über die 
                        kritische Physis der analogen Quelle mit (z. B. Alterung) als es die Homogenität eines OCR-prozessierten Textes vermag (immanente Schriftinformationen). 
                    
            
            
               Born Digital und Zeitlichkeit
               Born Digital hat keine Rückbindung an Greifbares und ist selbst potentiell ungreifbar. Ihrem Wesen nach sind diese Quellen fluid: Zum einen unterliegen sie ständigen Alternierungsprozessen, die der Rezipient bestenfalls passiv zur Kenntnis nehmen kann. Folgen für das Erfassen und Tradieren, die Domäne der Webarchivierung, sind Selektion, motiviert durch permanente „Vervielfältigung“ der sich im Turnus oder unregelmäßig verändernden Quellen, daraus resultierende Lücken sowie Probleme bei Datenspeicherung bzw. -vorhaltung. Zum anderen oszilliert Born Digital zwischen Ewigem Leben (vgl. Recht auf Vergessen) und spontanem Verschwinden. Diese Eigenschaften haben in Summe Konsequenzen für Korpusvalidität, Datierungen bzw. Ordnungen (Zeugenschaften), die Kritik von Inhalten sowie für die potentielle geschichtliche Dimension der Quellenart als solcher. Aufgrund der besonderen Bedeutung als „Quelle der Zukunft“ und ihrer komplexen Beschaffenheit stellt Born Digital eine besondere Herausforderung dar. 
            
         
         
            Methode
            Das präzise Sondieren dynamischer kulturhistorischer Phänomene ist sowohl den Digital Humanities als auch der Volkskunde eigen, in der der hier diskutierte methodische Bezugspunkt Mitte der 1950er Jahre gesetzt wurde. Hans Moser und Karl-Sigismund Kramer initiierten die als 
                    Münchner Schule bezeichnete Perspektive. Sie trug zu einer Neuaufstellung nach der NS-Zeit bei, in der etliche Fach-Akteure die Blut-und-Boden Ideologie mitgestaltet hatten. Anstelle der Suche nach Absolutem im (germanischen) Vergangenem („Ursprungsforschung“) trat die 
                    exakt historische Methode als „exakte Geschichtsschreibung der Volkskultur“ mit definierten Quellen, Räumen und Zeiten. 
                    Damals teils polarisierend, forderte Hermann Bausinger (
                    Tübinger Schule) zeitnah eine Orientierung am Aktuellen, der „technischen Welt“ – in Kombination führten u. a. diese beiden Ansätze zu einer Art vektorialen Denkens in der Volkskunde: Heutige Phänomene methodisch 
                    historisch zu lesen.
                
         
         
            Prozess
            Die Historische Quellenkritik staffelt sich zuerst in „äußere“und „innere Kritik“. Der„äußeren Kritik“(vgl. zum Begriff: Pfanzelter 2010: 43) zuzuordnen sind Aspekte der Multimodalität - das Zusammenspiel von Text, Bild, Audiovisuellem, Interaktion - was imFolgenden nicht dezidiert vertieftwird; derBlick geht vielmehr 
                    exakt historisch von Außen nach Innen. Karl-Sigismund Kramer formuliert 1968 modellhaft Kriterien der Quellenkritik, die Übertragung folgt dieser Systematik. 
                
            Der Quellenkritik vorgelagert ist eine Material-Kritik zur Unterscheidung „objektiven oder subjektiven Zeugniswerts“ bzw. von „Mischlagen“, was an der individuellen Quelle zu beurteilen ist. Übertragen auf Born Digital, erscheinen komplexere Formen wie Blogs und Foren, die ausgeprägt durch „Mischlagen“ charakterisiert sind, probat: „Objektiv“ bezieht sich auf inhaltlich definierte Themenkomplexe, „subjektiv“ auf eine erste Grobordnung nach Tendenzen. 
            Es folgen die drei Stufen der Quellenkritik (nach Droysen; vgl.: Haber):
            
               „1. Kritik der Echtheit“; dies verlangt den kritischen Abgleich von Traditionen bezüglich falscher Sachverhalte. Z. B.: Das Erzeugen einer Authentizitäts-Anmutung, die Optimierungsprozessen geschuldet ist und von einem spezialisierten Microtask-Markt mitgetragen wird.
               „2. Kritik des Früheren und Späteren“; das Prüfen zeitlicher Schichtung hat bei der Dynamik der gegebenen Quellen zufolge, dasslineare Vergleiche nur anhand systematisch eingehegter Archivierung möglich sind – diese Stufe der Kritik verweist auf die Notwendigkeit einer solchen zur Herstellung der Arbeitsbasis.
               „3. Kritik des Richtigen, d. h. die Frage nach dem Grad der Verzeichnung [eines objektiven Verhalts, d. Verf.], die (...) besonders durch subjektive und tendenziöse Verfärbung eingetreten sein kann“; aufbauend auf der bereits erfolgten Material-Kritik werden Themenkomplexe weiter aufgesplittet und granularer „objektiv“ kategorisiert. Dieses Extrapolieren von Konnotationen benötigt eine intermediale, dezidiert historische Lesart. 
                        
               
            
            Auf dieser Basis kann die Interpretation in vier Stufen vorgenommen werden:
            
               „1. Pragmatische Interpretation, d. h. die Herstellung des sachlichen Zusammenhanges innerhalb des Forschungsgegenstandes (ob Einzelerscheinung oder Gesamtaspekt), wie er sich aus dem kritisch geordneten Material ergibt.“ Hier wird Verlinkung im Kontext der Korpusvalidität angesprochen – wie ist „Gesamtheit“ im Terrain von Born Digital bewertbar? 
               „2. Interpretation der Bedingungen, (…) Umwelteinflüsse im engeren Umkreis der lokalen, wirtschaftlichen, rechtlichen, sozialen, technischen und allgemein geistigen Bedingungen, die auf das Werden der Erscheinung eingewirkt haben und ihre Funktionen bestimmen“. Das Beispiel Fake News verweist auf Fragen, die hier Relevanz haben.
               „3. Psychologische Interpretation, d. h. Versuch der schärferen Erkenntnis der seelischen Konstitution der Umwelt, in der die Erscheinung beheimatet ist, und des Willens und der Gefühle der aktiv oder passiv beteiligten Personen und Gruppen.“ Bezugsrahmen für Subjektives, das psychosozial eingeordnet und kontextuell decodiert wird, ist hier Twitter.
               „4. Interpretation nach den bewegenden sittlichen und politischen Mächten, (...) überindividuelle und [auf] den engeren Umkreis übergreifenden Impulse, die auf das Volksleben einwirken, es bewegen und gestalten“: Hier erfolgt die kulturkritische Einbettung in größere gesamtgesellschaftliche bzw. gobale Kontexte und theoretische wie empirische Metaperspektiven.
            
         
         
            Fazit
            Das transdisziplinäre Experiment versteht sich als „synkretistischer“ Versuch, eine tradierte Denkschule auf den Raum des Digitalen zu projizieren. Die Übertragung gibt Impulse für methodische Vertiefungen (konzeptionelle Verdichtung, Use Cases) und für Adaptionen in der Lehre (geisteswissenschaftliche Grundlagen und Analytik, Kritikfähigkeit).
         
      
      
         
            
               Bibliographie
               
                  Baumann, Jan / Hügi, Jasmin (2017): „compas. Strukturiertes Forschen im Web. Ein Projekt von infoclio.ch.“; ebd.:„2. 5. 2. Quellenkritik bei Quellen aus dem Internet“. 25.04.2017 http://www.compas.infoclio.ch/de/kompas/2-5-2-quellenkritik-bei-quellen-aus-dem-internet/164 [letzter Zugriff 11. September2017].
               
                  Brückner, Wolfgang (1985): „Hans Mosers Bedeutung für die Volkskunde“, in: Moser, Hans (1985): Volksbräuche im geschichtlichen Wandel. Ergebnisse aus fünfzig Jahren volkskundlicher Quellenforschung. Berlin-München: Deutscher Kunstverlag X-XI
                    
               
                  Crompton, Constance / Lane, Richard J. / Siemens, Ray(eds.) (2016) : „Doing Digital Humanities. Practice, Training, Research“. London: Routledge 
               
                  Nanni, Federico (2017): „Reconstructing a website’s lost past. Methodological issues concerning the history of Unibo.it.“, in: 
                        Digital Humanities Quarterly 2017 Volume 11 Number 2
                    
               
                  Gerndt, Helge (ed.) (1987): Volkskunde und Nationalsozialismus. Referate und Diskussionen einer Tagung der Deutschen Gesellschaft für Volkskunde, München 23. bis 25. Oktober 1986. München: 
                        Münchner Vereinigung für Volkskunde, 1989² 
                    
               
                  Griffin, Gabriele / Hayler, Matt (2016): Research methods for reading digital data in the digital humanities. Edinburgh: Edinburgh University Press
                    
               
                  Haber, Peter (2011): Digital Past. Geschichtswissenschaft im digitalen Zeitalter. München: Oldenbourg Verlag 104-112
                    
               
                  Kaschuba, Wolfgang (1999/2003): Einführung in die Europäische Ethnologie. München: Beck 2006³ 83-85
               
                  Klawitter, Jana / Lobin, Henning / Schmidt Torben(2012): „Kulturwissenschaftliche Forschung – Einflüsse von Digitalisierung und Internet“, in: Diess. (eds.): Kulturwissenschaften digital. Neue Forschungsfragen und Methoden. Frankfurt am Main: Campus Verlag 9-29
                    
               
                  Köstlin, Konrad: „Historische Methode und regionale Kultur“ in: Ders.(ed.) (1987): Historische Methode und regionale Kultur. Karl-S. Kramer zum 70. Geburtstag. Regensburger Schriften zur Volkskunde, B. 4. Berlin-Vilseck: Tesdorpf Verlag 7-23
                    
               
                  Kramer, Karl-Sigismund(1968): „Zur Erforschung der historischen Volkskultur“, in: 
                        Rheinisches Jahrbuch für Volkskunde, 1968, 19. Jahrgang. Bonn: Ferdinand Dümmler Verlag 7-41
                    
               
                  Liu, Alan (2012): „Where Is Cultural Criticism in the Digital Humanities?“, in: Gold, Matthew K. (ed.): Debates in the Digital Humanities. Minneapolis-London: University of Minnesota Press 490-509. http://dhdebates.gc.cuny.edu/debates/text/20 [letzter Zugriff 11. September 2017].
                    
               
                  Liu, Alan (2014): „The Big Bang of Online Reading“, in: Arthur, Paul Longley / Bode, Katherine (eds.): Advancing Digital Humanities. Research, Methods, Theories. Basingstoke: Palgrave Macmillan 275-290 
                    
               
                  Mittler, Elmar (2012): „Wissenschaftliche Forschung und Publikation im Netz. Neue Herausforderungen für Forscher, Bibliotheken und Verlage“, in: Füssel, Stephan (ed.): Medienkonvergenz – Transdisziplinär. Media Convergence, Band 1. Berlin-Boston: Walter de Gruyter Verlag 31-80 
                    
               
                  Pfanzelter, Eva (2010): „Von der Quellenkritik zum kritischen Umgang mit digitalen Ressourcen“, in: Gasteiner, Martin / Haber, Peter (eds.): Digitale Arbeitstechniken für die Geistes- und Kulturwissenschaften. Wien: UTB 39-49
                         
               
               
                  Presner, Todd (2015): „Critical Theory and the Magle of Digital Humanities“, in: Svensson, Patrik (ed.):Between humanities and the digital. Cambridge, Mass: MIT Press 55-67
                    
               
                  Rehbein, >Malte (2015): Forum: „Digitalisierung braucht Historiker/innen, die sie beherrschen, nicht beherrscht“, in: H-Soz-Kult, 27.11.2015 http://www.hsozkult.de/debate/id/diskussionen-2905 [letzter Zugriff 11. September 2017].
                    
               
                  Rehbein, Malte (2017): „Geschichtsforschung im digitalen Raum. Über die Notwendigkeit der Digital Humanities als historische Grund- und Transferwissenschaftwissenschaft“, in: Herbers, Klaus / Trenkle, Viktoria (eds.): Papstgeschichte des hohen Mittelalters: Digitale und hilfswissenschaftliche Zugangsweisen zu einer Kulturgeschichte Europas (im Druck).
                    
               
                  Schaller, Martin (2015): „Arbeiten mit digitalisierten Quellen. Herausforderungen und Chancen“, in: Schmale, Wolfgang (ed.): Digital humanities. Praktiken der Digitalisierung, der Dissemination und der Selbstreflexivität. Stuttgart: Steiner 15-30 
                    
               
                  Schich, Maximilian / Song, Chaoming / Ahn, Yong Yeol /Mirsky, Alexander / Martino, Mauro / Albert Barabási, Albert László / Helbing, Dirk (2014): „A network framework of cultural history“, in: 
                        Science 345 (6196). DOI: 10.1126/science.1240064. 558–562
                    
               
                  Sternfeld, Joshua (2014): „Historical Understanding in the Quantum Age“, in: 
                        Journal of Digital Humanites Vol. 3, No. 2 Summer 2014
                    
               
                  Weber, Matthew S. (2017): „The tumultuous history of news on the web“, in: Brügger, Niels / Schroeder, Ralph (eds.): The Web as History. Using Web Archives to Understand the Past and the Present. London: UCL Press 83-100
                    
            
         
      
   



      
         
            Sentiment Analyse und Dramenanalyse
            Sentiment Analyse (SA) beschreibt eine Reihe von computergestützten Methoden zur Prädiktion der Polarität eines Texts, versucht also vereinfacht gesagt automatisiert herauszufinden, ob ein Text ein positives oder negatives Gefühl ausdrückt (Liu 2016). Darüber hinaus werden teilweise auch komplexere emotionale Kategorien (wie z.B. Zorn und Freude) betrachtet (Mohammad & Turney 2010). Zentrale Anwendungsfelder der SA sind bislang vor allem die Analyse von Online-Reviews (McGlohan, Glance & Reiter 2010) und Social Media-Daten (Kouloumpis, Wilson & Moore 2011). 
            Zur Analyse von literarischen Texten mittels SA-Techniken finden sich bislang nur wenige Studien, z.B. zu Märchen (Alm, Roth & Sproat 2005) und Romanen (Kakkonen & Kakkonen 2011; Elsner 2012; Jannidis et al. 2016). Auf größeren Textkorpora wurde getestet, inwiefern SA-Werte eines Textes und Emotionskurven von Texten zur Genreklassifikation verwendet werden können (Kim, Padó & Klinger 2017) und wie begriffsgeschichtliche Bedeutungsverschiebungen in literarischen Texten mithilfe von erweiterten SA-Methoden erforscht werden können (Buechel, Hellrich & Hahn 2017). In Dramentexten hat man bisher die Verteilung von emotionalen Kategorien (Mohammad 2011) oder die Entwicklung von Figurenbeziehungen (Nalisnick & Baird 2013) in Shakespeare-Dramen untersucht. Auch der vorliegende Beitrag beschäftigt sich mit dem Einsatz von SA im Bereich der Dramenanalyse. Es werden erstmals systematisch verschiedene Methoden der SA für Dramen getestet und evaluiert. Zudem wird exploriert, inwiefern bisher in der Literaturwissenschaft erforschte Aspekte von Dramen mithilfe der SA erfasst werden und inwiefern die SA auch für die Gewinnung neuer literaturwissenschaftlicher Erkenntnisse eingesetzt werden kann.
            Das im Rahmen dieser Studie verwendete Lessing-Korpus umfasst ein mit Strukturinformationen annotiertes Dramenkorpus mit 11 Dramen, bestehend aus insgesamt 8224 Einzelrepliken. Sämtliche Dramen wurden über die Plattform 
                    TextGrid
                bezogen, so dass alle im Rahmen dieses Beitrags entwickelten Tools auch auf andere 
                    TextGrid-Dramen anwendbar sind. Mit dem am besten evaluierten SA-Verfahren wurde eine webbasierte Anwendung zur Analyse und Visualisierung von Sentiment-Verteilungen und -Verläufen implementiert.
                
         
         
            Evaluation unterschiedlicher SA-Verfahren
            
               Lexikonsbasierte SA
               Innerhalb der SA unterscheidet man zwei wesentliche Ansätze: (1) die Nutzung maschinellen Lernens und (2) die Verwendung lexikonbasierter Verfahren. Für das erstgenannte Vorgehen ist typischerweise ein mit Sentiment-Informationen annotiertes Trainingskorpus notwendig (D‘Andrea et al. 2015), welches für die Dramenanalyse bislang nicht vorliegt. Aus diesem Grund werden in der vorliegenden Arbeit lexikonbasierte Verfahren eingesetzt. Ein Sentiment-Lexikon ist dabei eine Wortliste, in der für jedes Wort Sentiment-Informationen angegeben sind (Liu 2016: 10), also z.B. ob es positiv oder negativ konnotiert ist und in welchem Ausmaß (Polaritätsstärke). Ein derartiges Wort nennt man auch 
                        sentiment bearing word (SBW; Liu 2016: 189).
                    
            
            
               SA-Parameter
               Folgende SA-Optionen wurden in unterschiedlichen Kombinationen systematisch evaluiert: 
               
                  i) Lexika – Es wurden fünf zentrale Sentiment-Lexika für den deutschsprachigen Bereich herangezogen: 
                        SentiWortschatz (SentiWS; Remus, Quasthoff & Heyer 2010), die 
                        Berlin Affective Word List – Reloaded (Bawl-R; Vo et al. 2009), die deutsche Version des 
                        NRC Emotion-Association Lexicon (NRC, Mohammad & Turney 2010), ein Lexikon von Clematide & Klenner (2010; im folgenden CK genannt) und das 
                        German Polarity Clues (GPC; Waltinger 2010). SentiWS, Bawl-R und CK enthalten Polaritäten und Polaritätsstärken, das NRC und GPC nur Polaritätsangaben. Das NRC enthält des Weiteren Annotationen zu acht unterschiedlichen Emotionen (Zorn, Furcht, Erwartung, Freude, Vertrauen, Ekel, Traurigkeit, Überraschung).
                    
               
                  ii) Historisch-linguistische Varianten – Über ein Tool des Deutschen Text-Archivs von Jurish (2011) wurde die Option der Lexikon-Erweiterung mit historischen linguistischen Varianten der Originalwörter untersucht.
                    
               
                  iii) Stoppwortlisten – Analog zu Saif et al. (2014) wurde der Einfluss der Verwendung von insgesamt drei unterschiedlichen Stoppwortlisten auf die Qualität der SA untersucht. Grund hierfür ist, dass durch verschiedene Kombination der Verfahren Sentiment-tragende Stoppwörter entstehen. Neben herkömmlichen Stoppwörtern wurden dabei auch Listen mit hochfrequenten Wörtern des Korpus untersucht. Dadurch wird der Einfluss von Wörtern analysiert, die zwar als sentiment-tragend in SA-Lexika ausgezeichnet werden, aber aufgrund der häufigen Nutzung im Korpus ein ungleichmäßiges Sentiment-Gewicht erzeugen (z.B. Herr, Fräulein).
                    
               
                  iv) Lemmatisierung – Eine weitere untersuchte Verarbeitungsform für die SA ist die Lemmatisierung. Als Lemmatisierer werden der 
                        Pattern-Lemmatisierer (De Smedt & Daelemans 2012) der Python-Bibliothek 
                        textblob und der Python-Wrapper des 
                        treetagger-Tools (Schmid 1995) evaluiert. Viele SA-Lexika enthalten lediglich Grundformen. Aufgrund der Probleme und Schwierigkeiten der Lemmatisierung im Deutschen (Eger, Gleim & Mehler 2016) soll vergleichend untersucht werden, welcher Lemmatisierer die besten Ergebnisse in Kombination mit Lexika erzielt. Ferner enthalten einige SA-Lexika manuell angegebene flektierte Wortformen. Es wird somit auch die automatische Lemmatisierung mit der manuellen Erweiterung verglichen.
                    
            
            
               SA-Metriken
               Alle nachfolgenden Berechnungen wurden bezüglich aller kombinatorischen Möglichkeiten der soeben beschriebenen SA-Parameter durchgeführt. Dabei werden die jeweiligen SA-Metriken nach Term-Zähl-Methodik (Kennedy & Inkpen 2006) berechnet, d.h. ein Text wird hinsichtlich vorhandener SBWs untersucht, positive und negative Wörter ausgezählt und für einen Polaritätswert die positive von der negativen Zahl subtrahiert. SA-Metriken wurden auf folgenden Ebenen über die jeweils zugehörigen Texte kalkuliert: Drama, Akte, Szenen, Repliken sowie Sprecher und Sprecherbeziehungen pro Drama, Akt, Szene und Replik. Die Beziehungen zwischen den Figuren wurden nach einer Heuristik von Nalisnick & Baird (2013) berechnet. 
            
            
               Erstellung des Gold Standards
               Zur systematischen Evaluation der Prädiktionsleistung der verschiedenen SA-Ansätze wurde ein Evaluationskorpus bestehend aus 200 Repliken erstellt. Bei der Auswahl der Repliken wurde darauf geachtet, dass die dramenspezifische Verteilung berücksichtigt wird, längere Dramen sind also mit mehr Repliken vertreten. Ferner wurden nur solche Repliken aufgenommen, die mindestens 19 Wörter umfassen. Diese Länge entspricht etwa -25% des Mittelwerts des Gesamtkorpus und vermeidet damit die Selektion von zu kurzen Repliken. Es wurde insgesamt auf eine gleichmäßige Längenverteilung geachtet.
               Die Repliken wurden von insgesamt fünf Personen (4 weiblich, 1 männlich; alle jeweils mit Deutsch als Muttersprache) jeweils unabhängig voneinander bezüglich deren Polaritätswirkung bewertet. Die Polarität jeder Replik wurde jeweils sechswertig (sehr negativ, negativ, neutral, gemischt, positiv, sehr positiv) und binär (positiv, negativ) bewertet. Die Annotationen wurden bezüglich des Übereinstimmungsgrades analysiert. Dazu wurden das Übereinstimmungsmaß Fleiss‘ Kappa (Fleiss 1971) sowie der Durchschnittswert der prozentualen Übereinstimmung aller Annotatoren und Annotatorinnen berechnet (vgl. Tabelle 1).
               
                  
                  
                     Tabelle 1. 
                            Annotator agreement.
                        
               
               Man erkennt eine geringe Übereinstimmung für die Bewertungsskala mit sechsstufiger Polarität und eine moderate Übereinstimmung für die binäre Variante. Die Ergebnisse verhalten sich konform zu verwandten Studien bei der Interpretation literarischer Texte (Alm & Sproat 2005). Als finale Annotation für eine Replik wird die binäre Polarität gewählt, die die Mehrheit der Annotatoren und Annotatorinnen ausgewählt haben (Endresultat: 139 negativ, 61 positiv).
            
            
               Evaluationsmaße 
               Als Evaluationsmaße wurden Genauigkeit (accuracy), Recall, Precision und F-Werte (Gonçalves et al. 2013) herangezogen. Abb. 1 zeigt einen Ausschnitt aus den je fünf besten Kombinationen pro Lexikon, geordnet nach Genauigkeit.
                        
               
               
                  
                  
                     Abbildung 1: Ausschnitt aus der detaillierten Ergebnistabelle zur Evaluation der SA-Kombinationsmöglichkeiten.
                        
               
            
            
               Ergebnisse der Evaluation
               Nachfolgend erfolgt eine überblicksartige Zusammenstellung einiger zentraler Ergebnisse aus der Evaluation:
               
                  Eine explizite Lemmatisierung führt zu einer verbesserten Leistung. Beide Lemmatisierer erzielen dabei meist ähnliche Ergebnisse. Die Lexikonerweiterung durch historische Varianten macht die explizite Lemmatisierung jedoch weitestgehend unnötig, da hierbei auch eine grundlegende Lemmatisierung inkludiert ist. 
                  Es zeigt sich eine konsistente Verbesserung durch die Lexikonerweiterung mittels der Wort-Varianten aus dem Tool von Jurish (2011). 
                  Stoppwortlisten haben nur auf vereinzelte Lexika (GPC, CK) einen merklich positiven Einfluss. 
                  Lexika mit Polaritätsstärken sind meist besser als reine Term-Zähl-Verfahren desselben Lexikons. 
                  Das Lexikon, dass die höchsten Genauigkeiten für die SA erzielt, ist SentiWS 
                  Die beste Leistung (unter Analyse aller Metriken) erzielt das erweiterte SentiWS mit den Polaritätsstärken, lemmatisiert mittels Pattern-Lemmatisierer und ohne Stoppwortliste (Genauigkeit = 0,67; F-Wert = 0,64). Die Erkennungsrate ist besser als die random baseline von 0,576 aber schlechter als viele Erkennungsraten auf anderen Anwendungsgebieten der SA (Vinodhini & Chandrasekran 2012). 
               
               Aufgrund der Tatsache, dass hier ein verhältnismäßig simpler SA-Ansatz gewählt wurde und bereits menschliche Annotatoren und Annotatorinnen Schwierigkeiten mit der Polaritätsbestimmung haben, sind die Ergebnisse insgesamt durchaus positiv zu bewerten.
            
         
         
            Online-Tool
            Abschließend wurde auf Basis des besten SA-Ansatzes ein Web-Tool für die SA bei Dramen entwickelt. Dieses bietet interaktive Visualisierungen der Sentiment-Verteilungen und -Verläufe für alle berechneten Ebenen. Neben den SentiWS-Metriken wurden auch die Emotionskategorien des NRC integriert. Über das Tool kann man erste Fallstudien auf Dramen-, Akt-, Szenen-, Repliken-, Sprecher- und Sprecherbeziehungsebene durchführen. Die SA-Komponente ist online verfügbar.
                    
            
            Trotz der historischen Differenz stimmen die Ergebnisse der automatischen SA tendenziell mit dem überein, was man in der Dramengeschichte über Bewertungen von Figuren und deren Verhalten weiß. Zusätzlich ist aber ein wichtiger heuristischer Mehrwert zu beobachten: eine Analyse allein auf der Basis von Sentiment-Zuschreibungen führt dazu, dass man das Augenmerk gezielt auf Fakten des Textes richtet, die bisher nicht berücksichtigt wurden. 
            Im Folgenden einige Beispiele für die Bestätigung bekannter Ergebnisse und für Entscheidungen von Analysefragen: 
            
               Fallstudie: Minna von Barnhelm
               Die Analyse von Minna von Barnhelm zeigt, dass die negativen emotionalen Bewertungen insgesamt gegenüber den positiven deutlich überwiegen (vgl. Abb. 2). Dieser Befund bestätigt die bekannte Erkenntnis, dass Lessing das Schema des rührenden Lustspiels verwendet hat. Während die Komik im Stück eher das Ergebnis von Schlussprozessen ist, geht es auf der wörtlichen Ebene überwiegend um ernste Vorwürfe und drohenden Identitäts- und Beziehungsverlust.
               
                  
                  
                     Abbildung 2: Polaritätsverteilung im Drama – 
                            Minna von Barnhelm
                  
               
               Es ist verschiedentlich behauptet worden (Saße 1993), Minna und nicht Tellheim sei die lächerliche Figur des Stücks. Die Sympathielenkung auf der wörtlichen Ebene des Textes, die in der unten stehenden Sentimentverteilung pro Akt abgebildet ist, kann dazu herangezogen werden, diese Frage negativ zu bescheiden (vgl. Abb. 3). Es ist eine auffällige Abweichung der Polarität im zweiten Akt erkennbar. In diesem Akt tritt Minna von Barnhelm zum ersten Mal auf, Tellheim jedoch nicht.
               
                  
                  
                     Abbildung 3: Polaritätsverlauf pro Akt – 
                            Minna von Barnhelm
                  
               
            
            
               Fallstudie: Emilia Galotti
               Die letzte Visualisierung kann genutzt werden die Frage zu diskutieren, warum Emilia in Lessings Drama „Emilia Galotti“ sterben muss (vgl. Abb. 4). Auffällig ist hier die starke negative Bewertung Emilias im zweiten Akt. Entgegen bisheriger Interpretationen, in denen nur die Intrige des Prinzen und Marinelli dafür verantwortlich gemacht werden, dass Emilia um ihre Tugend fürchten und ihren Vater dazu bringen muss, sie umzubringen, wird dadurch die Abwertung allein durch die Avancen des Prinzen sichtbar, die später sowohl Emilias als auch für Odoardos Einschätzung der Ehrbarkeit Emilias in ihrem zukünftigen Leben bestimmen.
               
                  
                  
                     Abbildung 4: Polaritätsverlauf von Sprechern pro Akt – 
                            Emilia Galotti
                  
               
            
            
               Fazit 
               Insgesamt sind die ersten Analyse-Ergebnisse über das Web-Tool sehr vielversprechend. Dabei ist zu bedenken, dass über die Verwendung von SA-Lexika ein sehr einfacher SA-Ansatz gewählt wurde. Über ML- oder Hybrid-Ansätze können Besonderheiten der poetischen und veralteten Sprache möglicherweise besser beachtet werden. Ferner ist fraglich, ob eine Reduktion auf das sonst in der SA übliche binäre System positiv/negativ ausreichend ist für komplexe Interpretationen von Emotionen in Dramen.
               Durch Optimierung des SA-Verfahrens, Ausbau der Funktionen im Front-End und Erweiterung des Tools mit zusätzlichen Dramen sollen künftig Möglichkeiten und Nutzen der SA in der Dramenanalyse weiter exploriert werden.
            
         
      
      
         
             https://textgridrep.org/repository.html; Hinweis: alle im Beitrag erwähnte URLs wurden zuletzt am 12.1.2018 überprüft
             Die vollständige Tabelle ist online verfügbar unter https://drive.google.com/open?id=1cvyqiiLJ03XT1VNaWgSDoajeTE3wgeqxxr2PXp-VM4w
             http://lauchblatt.github.io/QuantitativeDramenanalyseDH2015/FrontEnd/sa_selection.html
         
         
            
               Bibliographie
               
                  Alm, Cecilia Ovesdotter / Sproat, Richard (2005): "Emotional sequencing and development in fairy tales.", in:
                         International Conference on Affective Computing and Intelligent Interaction 668-674.
                    
               
                  Alm, Cecilia Ovesdotter / Roth, Dan / Sproat, Richard (2005): "Emotions from text: machine learning for text-based emotion prediction.", in: 
                        Proceedings of the conference on human language technology and empirical methods in natural language processing 579-586.
                    
               
                  Buechel, Sven / Hellrich, Johannes / Hahn, Udo (2017): “The Course of Emotion in Three Centuries of German Text – A Methodological Framework.”, in: 
                        Digital Humanities 2017 176-179.
                    
               
                  Clematide, Simon / Klenner, Manfred (2010): "Evaluation and extension of a polarity lexicon for German.", in: 
                        Proceedings of the First Workshop on Computational Approaches to Subjectivity and Sentiment Analysis 7-13.
                    
               
                  D’Andrea, Alessia et al. (2015): "Approaches, tools and applications for sentiment analysis implementation.", in 
                        International Journal of Computer Applications 125.3: 26-33.
                    
               
                  De Smedt, Tom / Daelemans, Walter (2012): "Pattern for python.", in: 
                        Journal of Machine Learning Research 13: 2063-2067.
                    
               
                  Eger, Steffen / Gleim, Rüdiger / Mehler, Alexander. (2016). “Lemmatization and Morphological Tagging in German and Latin: A Comparison and a Survey of the State-of-the-art.”, in: 
                        LREC 1507–1513.
                    
               
                  Elsner, Micha (2012): "Character-based kernels for novelistic plot structure.", in: 
                        Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics 634-644.
                    
               
                  Fleiss, Joseph L. (1971): "Measuring nominal scale agreement among many raters.", in: 
                        Psychological bulletin 76.5: 378-382.
                    
               
                  Gonçalves, Pollyanna, et al. (2013): "Comparing and combining sentiment analysis methods.", in: 
                        Proceedings of the first ACM conference on Online social networks 27-33.
                    
               
                  Jannidis, Fotis, et al. (2016): "Analyzing Features for the Detection of Happy Endings in German Novels.", in: 
                        arXiv preprint arXiv:1611.09028
                    
               
                  Jurish, Bryan (2011): 
                        Finite-state canonicalization techniques for historical German. Diss. Universitätsbibliothek der Universität Potsdam.
                    
               
                  Kakkonen, Tuomo / Kakkonen, Gordana Galić (2011): "SentiProfiler: creating comparable visual profiles of sentimental content in texts.", in: 
                        Language Technologies for Digital Humanities and Cultural Heritage 62-67.
                    
               
                  Kennedy, Alistair / Inkpen, Diana (2006): "Sentiment classification of movie reviews using contextual valence shifters.", in: 
                        Computational intelligence 22.2: 110-125.
                    
               
                  Kim, Evgeny / Padó, Sebastian / Klinger, Roman (2017): “Investigating the relationship between Literary Genres and Emotional Plot Development.”, in: 
                        Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature 17–26.
                    
               
                  Kouloumpis, Efthymios / Wilson, Theresa / Moore, Johanna D.  (2011): "Twitter sentiment analysis: The good the bad and the omg!.", in: In 
                        Proceedings of the Fifth International Conference on Weblogs and Social Media 538-54.
                    
               
                  Liu, Bing (2016): 
                        Sentiment analysis: Mining opinions, sentiments, and emotions. New York: Cambridge University Press.
                    
               
                  McGlohon, Mary / Glance, Natalie S. / Reiter, Zach (2010) "Star Quality: Aggregating Reviews to Rank Products and Merchants.", in: 
                        Proceedings of the International Conference on Weblogs and Social Media (ICWSM-2010) 114-121.
                    
               
                  Mohammad, Saif (2011): "From once upon a time to happily ever after: Tracking emotions in novels and fairy tales.", in: 
                        Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities 105-114.
                    
               
                  Mohammad, Saif M. / Turney, Peter D. (2010): "Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon.", in: 
                        Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text 26-34.
                    
               
                  Nalisnick, Eric T. / Baird, Henry S. (2013): "Character-to-character sentiment analysis in shakespeare’s plays.“, in:
                         Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics 479–483.
                    
               
                  Remus, Robert / Quasthoff, Uwe / Gerhard, Heyer (2010): "SentiWS-A Publicly Available German-language Resource for Sentiment Analysis.", in: 
                        LREC 1168-1171.
                    
               
                  Saif, Hassan, et al. (2014): "On stopwords, filtering and data sparsity for sentiment analysis of twitter.", in: 
                        Proc. 9th Language Resources and Evaluation Conference (LREC) 810-817.
                    
               
                  Saße, Günter (1993): 
                        Liebe und Ehe: oder, wie sich die Spontaneität des Herzens zu den Normen der Gesellschaft verhält. Lessings Minna von Barnhelm. Tübingen: Niemeyer.
                    
               
                  Schmid, Helmut (1995): "Improvements in part-of-speech tagging with an application to German.", in: 
                        Proceedings of the acl sigdat-workshop.
                    
               
                  Vinodhini, G. / Chandrasekaran, R. M. (2012): "Sentiment analysis and opinion mining: a survey.", in: 
                        International Journal of Advanced Research in Computer Science and Software Engineering 2.6: 282-292.
                    
               
                  Võ, Melissa LH, et al. (2009): "The Berlin affective word list reloaded (BAWL-R) ", in: 
                        Behavior research methods 41.2: 534-538.
                    
               
                  Waltinger, Ulli (2010): "Sentiment Analysis Reloaded-A Comparative Study on Sentiment Polarity Identification Combining Machine Learning and Subjectivity Features.", in: 
                        Proceedings of the 6th International Conference on Web Information Systems and Technologies (WEBIST '10).
                    
            
         
      
   



      
         „Als Grundwissenschaft erwirken die Digital Humanities das so elementar wichtige 
                Nutzenkönnen digitaler Methoden und Daten, wie die Paläographie uns das 
                Lesenkönnen unserer Quellen sicherstellt“. (Rehbein 2015) Wie Malte Rehbein hier andeutet, scheint es hinsichtlich des Stellenwertes, der den Historischen Hilfs- oder Grundwissenschaften (HGW) wie auch den Digital Humanities (DH) eigentlich beigemessen werden sollte, durchaus Ähnlichkeiten zu geben. „Sollte“! Denn sowohl bei den HGW als auch bei den DH ist ihr Status als eigenständiger wissenschaftlicher Zweig nicht gänzlich unumstritten. Beide werden aktuell häufig als reine Zulieferer-Wissenschaften oder Dienstleister gegenüber der „richtigen“ Forschung wahrgenommen und ihr eigener wissenschaftlicher Wert in Zweifel gezogen.
            
         Die zum Kanon der traditionellen HGW gehörenden, teils sehr unterschiedlichen Teildisziplinen – neben der bereits genannten Paläographie zählen unter anderem auch Kodikologie, Epigraphik, Heraldik, Sphragistik oder Diplomatik dazu – arbeiten allesamt quellennah und betreiben damit wertvolle Grundlagenforschung. Ob der Breite dieses Kanons fällt es nicht ganz leicht, die HGW in Patrick Sahles „3-Sphären-Modell zur Kartierung der Digital Humanities als Schnittmenge, Brücke und eigenständigem Bereich zwischen (ausgewählten) traditionellen Disziplinen“ (Sahle 2015) zu verorten. In vielen Aspekten scheinen sie den DH im Hinblick auf Interdisziplinarität, Methoden, Stellenwert etc. jedoch sogar näher zu stehen als die Geschichtswissenschaft, unter die sie im Allgemeinen subsumiert werden. Ja, es gab sogar eine Phase, in der die Historische Fachinformatik als neue Teildisziplin der HGW galt.
                
         
         Während Professuren mit einer DH-Ausrichtung oder Denomination auf dem Vormarsch zu sein scheinen – in seinem Beitrag „Zur Professoralisierung der Digital Humanities“ zählt Sahle mittlerweile 53 Ausschreibungen (Stand: Januar 2018) im deutschsprachigen Raum mit allerdings äußerst diversen Ausrichtungen (Sahle 2016) – ist in den letzten Jahren die Zahl der Universitätsstandorte, die HGW im Programm haben, zunehmend kleiner geworden, so dass diese heute mit zu den strukturprekären Disziplinen gehören.
                 (Arbeitsstelle Kleine Fächer) Diese Situation war Ende 2015 Anlass für die Formulierung des Positionspapieres „Quellenkritik im digitalen Zeitalter. Die Historischen Grundwissenschaften als zentrale Kompetenz der Geschichtswissenschaft und benachbarter Fächer“ von Eva Schlotheuber und Frank Bösch (Schlotheuber / Bösch 2015), welches eine breite Diskussion auf „H-Soz-Kult“ in Gang setzte, bei der (teils beiläufig) auch immer wieder das Verhältnis von HGW und DH thematisiert wurde. Trotz aller Differenzen, die bei diesem – mitunter durchaus kontrovers geführten – Austausch zutage kamen, herrschte hinsichtlich eines Aspektes mehrheitlich Einigkeit: Der Wegfall von Professuren, Studiengängen und Lehrveranstaltungen, die das notwendige methodische, grundwissenschaftliche Rüstzeug an heutige und künftige Generationen von Studierenden weitergeben, resultiert in einem Mangel an entsprechenden Fachkompetenzen. In einer Zeit, in der im Zuge zunehmender Digitalisierung historische Quellen in großer Zahl allgemein und jederzeit verfügbar geworden sind, führt dies zu der grotesken Situation, dass das Auffinden von Quellen und der Zugriff auf sie heute zwar deutlich einfacher geworden ist, die Mittel, mit diesen adäquat umzugehen, vielen Personen aber nicht mehr (oder noch nicht) zur Verfügung stehen. Dass der Zugang zu Datenbanken, die beispielsweise bei der Datierung, Verortung oder Einordnung von Einbänden, Wasserzeichen, Initialen etc. helfen, die Arbeit der Grundwissenschaftler|innen und auch anderer Forschenden heute erleichtert und ökonomisiert, wird von den Nutzer|inne|n niemand bestreiten. Das Gros des heutigen in den HGW beschäftigten Lehrpersonals ist allerdings selbst oft nicht ausreichend geschult, um die notwendigen Kenntnisse im Umgang mit diesen Ressourcen an die Studierenden zu vermitteln. 
            
         Dennoch fordern aktuelle Ausschreibungen von Bewerber|innen häufig ausgeprägte grundwissenschaftliche Kompetenzen und gleichzeitig Kenntnisse im Bereich der DH. Die wenigsten Absolvent|inn|en deutscher Hochschulen können diesem Profil heute wirklich gerecht werden. Personen, die im Rahmen ihres Studiums noch eine tiefergehende Ausbildung im erstgenannten Bereich genossen haben, stehen oft vor der Problematik, dass sie sich ihr Wissen im Bereich der DH mühevoll im Selbststudium oder in den zahlreich angebotenen Summer Schools erarbeiten müssen. 
         Neben diesen praktischen Problemen der Zugänglichkeit zu entsprechenden Weiterbildungsangeboten, differieren aber auch die grundlegenden Auffassungen darüber, welche (technischen) Kompetenzen überhaupt notwendig sind, um das „digital“ Dargebotene hinsichtlich seiner Wissenschaftlichkeit und Vollständigkeit hinreichend bewerten zu können.
                 Auch herrscht weiterhin Uneinigkeit darüber, wie diese Kompetenzen (an Studierende und Lehrende) überhaupt vermittelt werden können. Sind die Grundwissenschaften in der Pflicht, ihre Vermittlungskonzepte auf die veränderte Situation anzupassen? (Vogeler 2015) Definitiv! „[S]ind digitale Techniken und Methoden nicht nur eine Chance, sondern vielleicht auch die einzige Möglichkeit für eine sinnvolle Weiterentwicklung der Hilfswissenschaften“? (Hiltmann 2015) Wahrscheinlich ja! Doch wie kann diese Weiterentwicklung ganz konkret aussehen? (Wie) Können die hergebrachten grundwissenschaftlichen Kompetenzen erhalten, und dabei gleichzeitig neue Kompetenzen aufgebaut werden, die für das heutige und zukünftige wissenschaftliche Arbeiten benötigt werden? Wie müsste eine Neuausrichtung grundwissenschaftlicher Curricula, die stärker digitale Methoden in die Lehre integrieren, aussehen? Welche Umsetzungsversuche gibt es hier bereits? Welche Kompetenzen werden gebraucht und wo kann Kompetenzaufbau (auch für Graduierte) stattfinden? Welche Maßnahmen sind hierfür notwendig? Welche konkreten Maßnahmen wurden in den vergangen zwei bis drei Jahren vielleicht auch schon in Gang gesetzt, um den Bereich der HGW zu erhalten bzw. zu neuem Leben zu erwecken?
                 Wie kann das unzweifelhaft vorhandene Potenzial bestmöglich genutzt werden, um die grundwissenschaftliche Forschung zu befördern? Aber auch: Wo liegen vielleicht grundlegende Probleme in der Kollaboration von HGW und DH?
            
         Bei der Aushandlung des Stellenwertes bzw. der Verortung der DH ging es bisher meist um das Verhältnis zwischen angewandter Informatik und traditionellen Geisteswissenschaften. Der Dialog zwischen HGW und DHs erscheint aber besonders für einen fruchtbaren Austausch geeignet, da beide „Disziplinen“ teils mit sehr ähnlichen Problemen zu kämpfen haben und aus sich heraus schon interdisziplinär arbeiten. Andererseits stellen aber die DH gerade aus Sicht einiger Grundwissenschaftler|innen vermehrt ein Feindbild dar, da sie vermeintlich die Existenz des eigenen Faches bedroht sehen und/oder sich den neuen Herausforderungen nicht gewachsen fühlen. 
         Das Panel, welches diesmal Vertreter|inn|en der traditionellen HGW 
                UND Digital Humanists als dezidierte „Grenzgänger“ an einen Tisch bringt, soll also zum einen dazu dienen, Vorurteile abzubauen und die bereits begonnenen Diskussionen am Leben zu halten, zu konkretisieren und weiterzuführen, zum anderen auch Gelegenheit bieten, beispielsweise Projekte mit grundwissenschaftlicher Ausrichtung sichtbar zu machen und damit gleichzeitig deren Ansätze, Methoden und Umsetzung zur allgemeinen Diskussion zu stellen. 
            
         Die folgenden Personen (in alphabetischer Reihenfolge) haben ihre Teilnahme am Panel zugesagt:
         Jun.-Prof. Dr. Étienne Doublier, Juniorprofessur für Historische Hilfswissenschaften (Bergische Universität Wuppertal)
         Jun.-Prof. Dr. Torsten Hiltmann, Juniorprofessur für die Geschichte des Hoch- und 
                Spätmittelalters / Historische Hilfswissenschaften (Westfälische Wilhelms-Universität Münster)
            
         Prof. Dr. Andrea Stieldorf, Institut für Geschichtswissenschaft, Abteilung für Historische Hilfswissenschaften und Archivkunde, Rheinische Friedrich-Wilhelms-Universität Bonn
         Prof. Dr. Georg Vogeler, Zentrum für Informationsmodellierung in den Geisteswissenschaften, Karl-Franzens-Universität Graz
            
         Der geplante Ablauf ist wie folgt: 
         
            Nach einer knappen Einleitung in die Thematik des Panels und der Motivation zu dessen Organisation, erhalten die Diskutant|inn|en zunächst Gelegenheit zu einer individuellen Stellungnahme. Der Verlauf der anschließenden Diskussion wird nicht – wie sonst üblich – durch vorgegebene Fragen seitens der Moderatorin vorgegeben, sondern „von außen“ bestimmt, so dass dieser für alle Beteiligten nicht vorhersehbar ist. Mit „von außen“ ist hier zum einen das Plenum der Anwesenden gemeint, unten denen sich hoffentlich auch zahlreiche Vertreter der Studierendenschaft und des wissenschaftlichen Nachwuchses befinden, zum anderen aber gerade auch Personen, die selbst nicht an der Veranstaltung teilnehmen können. Hierzu wurde Ende 2017 unter anderem über Twitter ein Aufruf gestartet, (unter 
            
               #
               dhdp3a
            ) Fragen und Diskussionsthemen einzureichen, die dann vor Ort besprochen werden können. Dieses Vorgehen soll sicherstellen, dass 1) in der Diskussion tatsächlich jene Themen aufgegriffen werden, die von allgemeinem Interesse sind, 2) nicht bereits vielfach geführte Debatten lediglich repliziert werden, und 3) 
                ein wirklicher Dialog zwischen „Betroffenen“ und anderen Interessierten stattfinden kann, da diesmal nicht exklusiv auf professoraler Ebene diskutiert wird.
         
      
      
         
             http://www.hgw.geschichte.uni-muenchen.de/ueber_uns/faecher/fachinformatik/index.html
             Die innerdeutsche Verteilung gestaltet sich entsprechend recht übersichtlich: 
                
                  Ruprecht-Karls-Universität Heidelberg (Früheres Mittelalter und historische Grundwissenschaften) 
                  Eberhard-Karls-Universität Tübingen (Geschichtliche Landeskunde und Historische Hilfswissenschaften)
                  Otto-Friedrich-Universität Bamberg (Historische Grundwissenschaften)
                  Friedrich-Alexander-Universität Erlangen-Nürnberg (Mittelalterliche Geschichte und Historische Hilfswissenschaften)
                  Ludwig-Maximilians-Universität München (Historische Grundwissenschaften und Historische Medienkunde)
                  Universität Passau (Mittelalterliche Geschichte und historische Hilfswissenschaften)
                  Universität Regensburg (Historische Hilfswissenschaften)
                  Julius-Maximilians-Universität Würzburg (Mittelalterliche Geschichte und historische Hilfswissenschaften)
                  Christian-Albrechts-Universität zu Kiel (Mittelalterliche Geschichte und historische Hilfswissenschaften)
                  Ruhr-Universität Bochum (Historische Hilfswissenschaften)
                  Rheinische Friedrich-Wilhelms-Universität Bonn (Historische Hilfswissenschaften und Archivkunde)
                  Universität zu Köln (Historische Hilfswissenschaften)
               
            
             Das Gleiche gilt auch für die schon zahlreich verfügbaren Tools zu deren vereinfachten Be- oder Verarbeitung.
             Exemplarisch können hier – neben der noch aktuellen Ausschreibung einer (wenn auch befristeten) W2 Professur für „Historische Grundwissenschaften unter besonderer Berücksichtigung der Digital Humanities“ – auch der Zusammenschluss historisch arbeitender Wissenschaftler|inne|n zur „Arbeitsgemeinschaft Historische Grundwissenschaften“ (AHiG, 
                        https://www.ahigw.de/) genannt werden, sowie die Gründung des „Netzwerk Historische Grundwissenschaften“ (NHG, 
                        https://www.ahigw.de/nachwuchsnetzwerk/). Bei letzterem handelt es sich um einen Zusammenschluss von Nachwuchswissenschaftler|inn|en verschiedener Disziplinen und Qualifikationsstufen, die neben der Organisation einer jährlichen Konferenz mit grundwissenschaftlicher Ausrichtung, auch auf anderen Wegen versuchen, sich produktiv in die aktuellen Diskussionen einzuschalten und Entwicklungen voran zu treiben – so z.B. auch mit der Organisation dieses Panels. 
                    
         
         
            
               Bibliographie
               Arbeitsstelle Kleine Fächer, Fachstandort der Historischen Hilfswissenschaften. URL: 
                        http://www.kleinefaecher.de/historische-hilfswissenschaften/ [letzter Zugriff 24.09.2017]
                    
               
                  Hiltmann, Torsten (2015): „Hilfswissenschaften in Zeiten der Digitalisierung“, in: H-Soz-Kult, 14.12.2015. URL: 
                        www.hsozkult.de/debate/id/diskussionen-2936 [letzter Zugriff 24.09.2017]
                    
               
                  Rehbein, Malte (2015): „Digitalisierung braucht Historiker/innen, die sie beherrschen, nicht beherrscht“, in: H-Soz-Kult, 27.11.2015. URL: 
                        www.hsozkult.de/debate/id/diskussionen-2905 [letzter Zugriff 24.09.2017]
                    
               
                  Sahle, Patrick (2015): „Digital Humanities? Gibt’s doch gar nicht! in: Grenzen und Möglichkeiten der Digital Humanities (Sonderband der Zeitschrift für digitale Geisteswissenschaften 1). DOI: 
                        http://dx.doi.org/10.17175/sb001_004 [letzter Zugriff 13.01.2017]
                    
               
                  Sahle, Patrick (2016): „Zur Professoralisierung der Digital Humanities“, in: DHd-Blog, 23. März 2016. URL: 
                        http://dhd-blog.org/?p=6174 [letzter Zugriff 24.09.2017]
                    
               
                  Vogeler, Georg (2015): „Digitale Quellenkritik in der Forschungspraxis“, in: H-Soz-Kult, 28.11.2015. URL: 
                        www.hsozkult.de/debate/id/diskussionen-2893 [letzter Zugriff 24.09.2017]
                    
               
                  Schlotheuber, Eva / Bösch, Frank (2015): „Quellenkritik im digitalen Zeitalter. Die Historischen Grundwissenschaften als zentrale Kompetenz der Geschichtswissenschaft und benachbarter Fächer“, in: H-Soz-Kult, 15.11.2015. URL: 
                        www.hsozkult.de/text/id/texte-2890 [letzter Zugriff 24.09.2017]
                    
            
         
      
   



      
         Soziale Medien spielen weltweit im politischen Meinungsbildungsprozess eine immer wichtigere Rolle. Sowohl Onlineangebote von Zeitungen als auch Twitteraccounts von Organisationen oder Personen können quasi in Echtzeit über aktuelle Geschehnisse informieren.
                Die Kommentar- und Replyfunktionen bieten zudem einen digitalen Ort für den öffentlichen Austausch. Damit können auch ‚normale’ Nutzer ganz gezielt Nachrichten wie auch persönliche Kommentare oder Gerüchte in einem Ausmaß verbreiten wie es im vor-digitalen Zeitalter kaum möglich war. Das Entstehen eines vollkommen neuen digitalen Kommunikationsraumes, der sowohl Grenzen überschreitet als auch potenziell neue Grenzen schafft („Echokammern“) kann zum einen positiv im Sinne einer Demokratisierung öffentlicher Meinungsbildung gewertet werden (Mossberger et al. 2007), birgt aber auch Risiken (Mancini, 2013, Sarcinelli, 2014).
            
         Die Analyse der Rolle von Sozialen Medien im öffentlichen Meinungsbildungsprozess ist inzwischen ein aktives Forschungsfeld (z.B. Törnberg & Törnberg, 2016, Eilders, 2013). Für eine umfassende Auswertung des Datenmaterials sind jedoch mächtige Analyseverfahren notwendig (Sentimentanalyse, Netzwerkanalyse, Diskursanalyse, Bot-Erkennung etc.), die zur Zeit noch nicht in adäquatem Maß zur Verfügung stehen (s. Mohammad et al., 2015). So ist zum Beispiel die Sentimentanalyse relativ gut erforscht, beschränkt sich aber in der Regel auf das Englische sowie auf die Textsorte „Rezension“. In drei Pilotstudien haben wir untersucht, mit welchem Aufwand sich Methoden der Sentimentanalyse und Bot-Erkennung auf neue Sprachen anpassen lassen und wo mögliche Grenzen dieser Verfahren liegen. Im weiteren Projektverlauf, soll ergründet werden, inwieweit sich Prozesse der öffentlichen Meinungsbildung in sozialen Medien mit den zur Zeit zur Verfügung stehenden Verfahren nachvollziehen lassen. Die Pilotstudien untersuchen den öffentlichen Diskurs im Kontext von Wahlen und decken zwei verschiedene Sprachen (französisch, deutsch) und Textsorten (Kommentare auf Nachrichtenseiten, Tweets) ab und variieren hinsichtlich der ausgewerteten Datenmenge und Herangehensweise (gemischt qualitativ-quantitative stilistische Auswertung vs. primär quantitative Polaritätsanalyse vs. Bot-Erkennung).
         
            Pilotstudie 1: Sentimentanalyse zur Bundestagswahl
            In der ersten Studie stehen Tweets zur Bundestagswahl (BTW) im Vordergrund, für die ein Sentimenttagger entwickelt wurde, um das in den Tweets ausgedrückte Sentiment im Hinblick auf Themen, Parteien und Personen zu analysieren. Zwischen Mai und September 2017 wurden mehr als 5.4 Millionen Tweets gesammelt und nach Schlagworten und Hashtags zur BTW und zu den Parteien gefiltert. 600 Tweets wurden von 3 AnnotatorInnen manuell als `positiv', `negativ', 'neutral' oder 'irrelevant' (kein Bezug zur Bundestagswahl) klassifiziert. Um die Anforderungen eines (bisher nicht verfügbaren) Twitter-Sentimenttaggers für das Deutsche im Hinblick auf die Fragestellung zu ermitteln, wurden in einer Vorstudie 100 Tweets von fünf AnnotatorInnen auf ihr Sentiment hin untersucht. Dabei wurde deutlich, dass das Sentiment zum Teil aus dem Kontext inferiert werden muss (z.B. angehängte Bilder), was besonders die automatische Analyse erschwert. Zudem werden oft mehrere Sentiments ausgedrückt werden (s. Mohammad, 2016). Das Inter-Annotator-Agreement für alle 3 AnnotatorInnen lag bei 61% (94,5% bei Übereinstimmung von 2 AnnotatorInnen). Eine Sichtung der annotierten Daten zeigt, dass Tweets zur BTW überwiegend sentiment-behaftet sind (91%) und negatives Sentiment vorherrscht (81% neg., 10% pos, 9% neut). Für den Sentimenttagger wurden verschiedene überwachte maschinelle Lernverfahren auf den annotierten Daten trainiert und getestet (10-fache Kreuzvalidierung). Verwendet wurden dabei Unigramme sowie weitere Informationen wie das Vorkommen von Emoticons oder bestimmten Satzzeichen. Der beste Klassifikator (Naive Bayes) erreichte 71% F-Score (69% Prec., 75% Rec.). Besonders indikativ für negatives Sentiment in unserem Datenset sind die Unigramme „Schulz“ und „Merkel“, während positives Sentiment durch „!“ angezeigt wird. Hier ist sicher eine weitergehende Analyse notwendig.
         
         
            Pilotstudie 2: Französische Präsidentschaftswahl
            In einer gemischt qualitativ-quantitativen Analyse der französischen Präsidentschaftswahl wurden Kommentare unter Onlineartikeln der französischen Tageszeitung
                Le Monde
                 im Zeitraum zwischen dem ersten (23.04.2017) und dem zweiten (07.05.2017) Wahltag manuell und automatisch analysiert. Hierfür wurden zunächst themenähnliche Artikel ausgewählt, die sich mit den zwei Präsidentschaftskandidaten der zweiten Wahlrunde befassten. Für die weitere Analyse wurden sechs repräsentative Onlineartikel und die zugehörigen Userkommentare ausgewählt. Diese ausgewählten Textdateien wurden mithilfe der Topic Modelling Software MALLET
                 auf ihre Hauptthemen hin analysiert. Als Trainingsdaten für das Topic Modelling wurden ähnliche Onlineartikel anderer Zeitungen und Wahlprogramme der zwei Hauptparteien der zweiten Wahlrunde verwendet. Da es auch für das Französische keinen frei verfügbaren Sentimenttagger gibt, wurde ein regelbasiertes System entwickelt, dass auf einer Kombination von verschiedenen Sentimentlexika beruht, u.a. auch multi-linguale Ressourcen (NRC Emotion Lexicon, Mohammad & Turney, 2013), die auf der einen Seite die Datengrundlage vergrößern, auf der anderen Seite aber auch potenziell Fehler (z.B. fehlerhafte Übersetzungen, Lesartenambiguitäten) beitragen.
                
         
         
            Pilotstudie 3: Erkennung von Social Bots
            In der dritten Studie geht es um die Unterscheidung von menschlichen und künstlichen Akteuren. Um die potenziellen Auswirkungen von Social Bots auf die politische Meinungsbildung zu quantifizieren ist es notwendig, künstliche Agenten automatisiert erkennen zu können. Bisherige Verfahren (z.B. Varol et al., 2017) können simple Bot-Accounts zuverlässig erkennen, scheitern aber an fortgeschrittenen Bots, die sich nicht mehr offensichtlich von echten Menschen unterscheiden. In einem weiterentwickelten Verfahren analysieren wir speziell Accounts dieser Art. Als Datengrundlage hierfür wird der MIB-Datensatz verwendet (Cresci et al., 2017). Zur Bot-Erkennung wird mit überwachten machinellen Lernverfahren experimentiert. 
         
      
      
         
            
               Bibliographie
               
                  Cresci, Stefano / Di Pietro, Roberto / Petrocchi, Marinella / Spognardi, Angelo / Tesconi, Maurizio 
                        (2017): „The Paradigm-Shift of Social Spambots: Evidence, Theories, and Tools for the Arms Race.“ in:
                        
                     Proceedings of the 26th International Conference on World Wide Web Companion
                  
                   (WWW '17 Companion).
               
               
                  Eilders, Christiane
                         (2013): „Öffentliche Meinungsbildung in Online-Umgebungen. Zur Zentralität der normativen Perspektive in der politischen Kommunikationsforschung.“ In: Karmasin, M. et al. (Eds.):
                        Normativität in der Kommunikationswissenschaft, Wiesbaden: Springer, 329-351.
                    
               
                  Mancini, Paolo
                         (2013): "Media Fragmentation, Party System, and Democracy."
                        The International Journal of Press/Politics 18 (1): 43-60.
                    
               
                  Mohammad, Saif
                         (2016): „A Practical Guide to Sentiment Annotation: Challenges and Solutions.“ in:
                        Proceedings of the NAACL 2016 Workshop on Computational Approaches to Subjectivity, Sentiment, and Social Media (WASSA), June 2014, San Diego, California.
                    
               
                  Mohammad, Saif / Kiritchenko, Svetlana / Zhu, Xiaodan / Martinet, Joel
                         (2015): „Sentiment, Emotion, Purpose, and Style in Electoral Tweets“, in: 
                        Information Processing & Management, 51:4, 480–499.
                    
               
                  Mohammad, Saif / Turney, Peter
                        (2013): Crowdsourcing a Word-Emotion Association Lexicon, Computational Intelligence, 29 (3), 436-465, 2013.
                    
               
                  Mossberger, Karen / Tolbert, Caroline J. / McNeal, Ramona S.
                         (2007): 
                        Digital Citizenship. The Internet, Society, and Participation, Cambridge MA/London: MIT Press.
                    
               
                  Sarcinelli, Ulrich (2014): „Von der Bewirtschaftung der Aufmerksamkeit zur simulativen Demokratie?“ in:
                        Zeitschrift für Politikwissenschaft 24 (3), 329 - 339.
               
               
                  Schmid, Helmut
                   (1994): „Probabilistic Part-of-Speech Tagging Using Decision Trees.“
                         in: Proceedings of International Conference on New Methods in Language Processing, Manchester, UK. 
               
               
                  Törnberg, Anton / Törnberg, Petter
                         (2016): „Combining CDA and topic modeling: Analyzing discursive connections between Islamophobia and anti-feminism on an online forum“, in:
                         Discourse and Society  27:4, 401-422.
                    
               
                  Varol, Onur / Ferrara, Emilio / Davis, Clayton A. / Menczer, Filippo / Flammini, Alessandro
                        (2017): „Online Human-Bot Interactions: Detection, Estimation, and Characterization.“ in:
                         Proceedings of ICWSM'17
               
            
         
      
   



      
         
            Einleitung: Digitales Publizieren in den Geisteswissenschaften
            
               Mit der zunehmend selbstverständlichen Nutzung digitaler Ressourcen und der Etablierung der Digital Humanities rückt auch die Frage nach Formen des digitalen Publizierens in der Wissenschaft ins Blickfeld: Während zunehmend digitale Methoden der Erfassung, Erschließung und Analyse zur Anwendung kommen, bleiben jedoch die Publikationswege häufig noch traditionell und analog geprägt. Dabei bieten digitale Veröffentlichungsformen Potenziale für offene und innovative wissenschaftliche Erkenntnisprozesse sowie eine direktere Wissenschaftskommunikation. Die zunehmende Etablierung von (Open)-Peer-Review-Verfahren wirkt gegen das Vorurteil der vermeintlich geringeren Qualität von digitalen Publikationen; auch wissen die Wissenschaftlerinnen die freie und mobile Verfügbarkeit von digitalen Publikationen zunehmend zu schätzen.
            
            
               Die sich im Wandel befindenden medialen Bedingungen wirken direkt auf die Akteurinnen im (digitalen) Publikationsprozess ein (DHd-Arbeitsgruppe 2016). Die Rolle und das Zusammenspiel von Urheberinnen, Autorinnen, Verlag und Rezipientinnen werden daher grundlegend in Frage gestellt (Fitzpatrick 2011: 50). Gleichfalls unterliegt die wissenschaftliche Publikation selbst einem Prozess der Neudefinition: Traditionelle Formen wie Monographie oder Zeitschriftenartikel verlieren ihren Ausschließlichkeitsanspruch, da zunehmend digitale Präsentationsformen im wissenschaftlichen Diskurs als vollwertige wissenschaftliche Publikationen angesehen werden (Kohle 2017: 199). Digitale Publikationen interagieren weit mehr als ihre analogen Vorbilder mit anderen mediale Formen, sei es durch die Einbettung von multimedialen Inhalten (Maciocci 2017), Social Media und Forschungsdaten oder durch Verweise auf andere online verfügbaren Ressourcen im Sinne von Linked Open Data (W3C 2017). Die Integration von crossmedialen Inhalten ist technisch bereits möglich, es fehlen allerdings noch Anwendungskonzepte und Best Practice Beispiele. 
            
            
               Die oftmals ungefilterte Offenheit digitaler Medien wirft jedoch auch kritische Fragen der Qualitätssicherung auf, da nicht alle aus dem Kontext der gedruckten Publikation gewohnte Mechanismen greifen (Herb 2012). Dennoch sind Vorteile und Mehrwert des Digitalen evident: Digitale Texte sind leicht aufzufinden, durchsuchbar und im Idealfall schrankenlos kopierbar. Sie begünstigen damit die breite Distribution und Rezeption sowie die Nachnutzung durch digitale (z B. analytische) Verfahren. Anders als im Druck erschienene Publikationen können digitale Publikationen fortgeschrieben werden, ohne ihre Referenzierbarkeit verlieren zu müssen (durch Versionierung). Sie lassen sich mit anderen Texten verknüpfen (Hypertext) und können auf der Basis geeigneter Vokabulare bzw. Ontologien in eine maschinell auswertbare semantische Beziehung mit anderen Dokumenten und Gegenständen treten (Semantic Web). Die bei digitalen Dokumenten favorisierte Trennung von Struktur- und Layoutschicht ermöglicht es, Texte nicht mehr einem starren Präsentationsregime zu unterwerfen, sondern nach Wünschen der BenutzerInnen neue Ansichten oder überhaupt Präsentationsformen jenseits traditioneller Textbegriffe zu generieren. Die kollaborative Text- und Datenpublikation wird im digitalen Raum begünstigt, zieht aber auch Probleme bezüglich der Autorinnenschaft und der Differenzierung der Rollen im digitalen Publikationsprozess nach sich (SoSciSo Redaktion: 2017). Abschließend gilt es, die Schlüsselfunktion von Open Access (OA) und freien Lizenzmodellen (z.B. nach Creative Commons) in digitalen Publikationsprozessen zu betonen: Sie schaffen die Voraussetzungen für ungehindertes Forschen und werden damit zu zentralen Bedingungen wissenschaftlichen Publizierens.
            
         
         
            
               Veranstaltungsformat Barcamp
            
            
               Mit dem Format eines halbtägigen Barcamps möchte die DHd-AG »Digitales Publizieren« der interessierten Community die Möglichkeit bieten, die soeben skizzierten Themen und Fragen, aber auch andere Aspekte rund um das digitale Publizieren gemeinsam zu diskutieren und sich dazu auszutauschen (Dogunke 2018). Das Format bedingt, dass das Programm maßgeblich von den Teilnehmerinnen gestaltet wird und sowohl dynamisch als auch interaktiv entwickelt werden kann. Das Barcamp möchte Expertinnen und interessierte Wissenschaftlerinnen aus unterschiedlichen Disziplinen zusammenbringen und wird ausreichend Raum bieten, sich in ausgewählte Bereiche der Thematik zu vertiefen, aber auch grundlegende Fragen zu thematisieren. Ziel ist es, gleichermaßen die Ansprüche einer Informationsveranstaltung mit impulsgebenden Statements zu kombinieren. 
            
            
               Es bestehen zwei Möglichkeiten, Themen für die Veranstaltung zu benennen: Zum einen wird im Vorfeld der Tagung DHd2019 eine Umfrage über den DHd-Blog, Twitter und Mailinglisten stattfinden. Hier entscheidet die Quantität der Nennung einzelner Themen über ihre Annahme. Ähnlich gelagerte Themen werden dabei zusammengefasst bzw. gruppiert. Am Barcamp Interessierte haben dabei auch die Möglichkeit, eine Gestaltungsform für den genannten Vorschlag zu nennen und ihre Rolle zu definieren (s.u.). Spontan können zum anderen aber auch Themen direkt innerhalb des Workshops platziert werden. Die endgültige Tagungsordnung für das Barcamp wird gemeinsam mit dem Plenum zu Beginn des Workshops festgelegt. Die DHd-AG »Digitales Publizieren« möchte die Ergebnisses des Barcamps erstens zur Überarbeitung des Arbeitspapieres »Digitales Publizieren« nutzen und damit einen Beitrag zur Klärung des aktuellen Selbstverständnisses in der Gemeinschaft leisten. Zweitens soll die Veranstaltung der weiteren Vernetzung der Interessierten innerhalb der Community dienen. Drittens soll aber auch das gewählte Format auf seine Eignung geprüft werden, die Kommunikation zwischen der AG und der Community aktiver zu gestalten, woraus sich bei positivem Befund auch weitere Veranstaltungen ergeben könnten.
            
         
         
            Potentielle Themen und Fragen
            
               Um eine Vorstellung von potentiellen Themen und der inhaltlichen Gestaltung der Veranstaltung zu bekommen, seien im Folgenden einige Aspekte und zentrale Fragen zum digitalen Publizieren genannt, welche die Verfasserinnen der Einreichung auf der Grundlage eigener Erfahrung und der aktuellen Forschung im Rahmen der AG Digitales Publizieren identifiziert haben:
            
            
               Aktuelle und zukünftige Publikationsformate: Welche Rolle wird PDF als Publikationsformat in Zukunft haben? Werden Beiträge direkt in XML verfasst werden können?
               
                  Data Publications als Publikationsformat
                  : Wie und in welcher Form können (Forschungs)daten publiziert werden? Welche Formate existieren bereits und gibt es Best Practice Beispiele? Wie können Datenpublikationen als wissenschaftliches Publikationsformat etabliert werden?
               
               
                  (darauf aufbauend): 
                  Was zählt eigentlich als digitale Publikation und welche Abgrenzungen zu anderen Publikationsformen sind notwendig?
                   Welche technischen und inhaltlichen Kriterien müssen beispielsweise Blogbeiträge erfüllen, um als wissenschaftliche Publikation zu gelten? 
               
               
                  Kollaboratives Schreiben
                  : Wie kann die Rolle der beteiligten Personen kenntlich gemacht werden und welche Rolle gibt es außer der der Autorinnen bei einer Publikation?
               
               
                  Infrastrukturen für digitale Publikationen
                  : Welche Repositorien und Publikationsumgebungen existieren und sind für Forscherinnen im deutschsprachigen Raum zugänglich? Welche Standards haben sich etabliert?
               
               
                  Wie kann die Qualität von digitalen Publikationen gemessen werden?
                   Welche Bedeutung könnte der Impactfaktor in Zukunft haben? Wie kann die Zitationshäufigkeit von digitalen Publikationsformen gesteigert werden?
               
               
                  Welche Bedeutung kommt dem traditionellen Intermediären im digitalen Publikationszyklus zu
                  ? Sind Bibliotheken die neuen Verlage? Ist das hybride Publizieren nur eine Übergangserscheinung oder ein langfristiges Erfolgsmodell?
               
               
                  Wie ist der aktuelle Stand bei den Lizenzen und Rechten im Kontext vom digitalen Publizieren?
                   Wie stark hat sich Open Access wirklich durchgesetzt?
               
               Bedarf es genuiner Gutachterkulturen für digitale Publikationen?
               Wie gestalten sich digitale Publikationsworkflows?
               Hat beim digitalen Publizieren die wissenschaftliche Kommunikation einen direkteren Einfluss auf die Publikation?
               
                  Warum hat sich bisher trotz der stetig wachsenden Bedeutung von Forschungsdaten das Modell der
                  enhanced publication
                  noch nicht durchgesetzt und welche Chancen bestehen für dieses Format (Degwitz 2015: 52)?
               
               
                  Welche Rolle kommt im Sinne des Titels der
                   Tagung cross- bzw. intermedialen Inhalten 
                  bei digitalen Publikationen zu
                  ?
               
            
         
         
            Durchführung
            
               Wer ein Thema vorschlägt, hat gleichzeitig die Möglichkeit, auch ein Durchführungsformat zu wählen. Die unterschiedlichen Formate werden mit der Umfrage zusammen vorgeschlagen. Der Grund für diese flexible und Teilnehmerinnen-gesteuerte Auswahl des Barcamps ist es, dass einige der oben genannten Themen sich eher für ein Expertengespräch eignen, während andere eher in einer gemeinsamen Diskussion thematisiert werden könnten oder Gegenstand eines Impulsreferats sein könnten. Es soll daher weder bei den Inhalten noch bei den Formaten fest Vorgaben geben.
            
            
               Die ein Thema vorschlagenden Personen können selber angeben, ob sie a) sich für das Thema grundsätzlich interessieren oder sich b) als ExpertIn für das Thema im Rahmen des Workshops zur Verfügung stellen. Zusätzlich werden die Organisatorinnen im Vorfeld des Workshops Expertinnen zu den einzelnen Themen einladen, bzw. Themen als gemeinsame Diskussionen mit dem Plenum planen und vorbereiten. Die Moderation und Durchführung der Veranstaltung wird von Mitgliedern der AG bedient.  
            
            Folgende Formate von circa jeweils 30 Minuten Dauer sind denkbar:
            
               
                  Expertinnenformat: Eine Expertin bzw. ein Experte hält ein impulsgebendes Referat, danach findet eine moderierte Diskussion statt.
               
               Thementische (abhängig vom Raum): Es gibt unterschiedliche Thementische, an denen Expertinnen Rede und Antwort stehen.
               Diskussionen: Mehrere Expertinnen diskutieren zu einem Thema, danach folgt eine Diskussion mit dem Plenum.
               Gruppenformat: Kleinere Gruppen diskutieren gemeinsam ausgewählte Themen und präsentieren die Ergebnisse danach dem Plenum.
            
            
               Vor allem der letzte Punkt scheint für das Tagungsformat gut geeignet zu sein, da dadurch alle Beteiligten involviert werden. Für die Durchführung dieser unterschiedlichen Formate wäre ein gut unterteilbarer Raum ebenso sinnvoll wie der Einsatz von Moderationsmaterialen (Flipcharts etc.). Eine laufende Dokumentation der Ergebnisse des Barcamps wird während des Workshops über ein Etherpad erfolgen. Des Weiteren sollen zentrale Ergebnisse in die neue Version des Workingspapers “Digitales Publizieren” integriert werden.Abhängig vom Verlauf des Barcamps können weitere Formate, wie z. B. ein Blogbeitrag, möglich sein.
            
         
         
            Organisatorisches
            Das Barcamp wird von der DHd-AG Digitales Publizieren veranstaltet. Die Planung und Durchführung wird organisiert von:
            Katrin Neumann, (Max-Weber-Stiftung), 
            
               neumann@maxweberstiftung.de, Forschungsinteressen:
            Digitales Publizieren, Publikationsplattformen, Wissenschaftliches Bloggen
            Melanie Seltmann, (Universität Wien), 
            
               melanie.seltmann@univie.ac.at, Forschungsinteressen:
            Digitales Publizieren, Natural Language Processing, Citizen Science
            Walter Scholger, (Universität Graz), 
            
               walter.scholger@uni-graz.at, Forschungsinteressen:
            Digitales Publizieren, Digitale Editionen, Open Access und Lizenzen
            Timo Steyer, (Forschungsverbund Marbach Weimar Wolfenbüttel/Herzog August Bibliothek Wolfenbüttel),
            
               steyer@hab.de, Forschungsinteressen:
            Digitales Publizieren, Digitale Editionen, Metadaten und Datemmodellierung
            
               Die Teilnehmerzahl ist auf 40 Personen begrenzt. Der Workshop sollte eine Dauer von einem halben Tag haben. Benötigt werden ein Beamer, Moderationsmaterial und eine Raumgröße, welche die Bildung mehrere Arbeitsgruppen ermöglicht.
            
         
      
      
         
            
               Bibliographie
               
                  Degkwitz, Andreas (2015): “Enhanced Publications Exploit the Potential of Digital Media”, in: Evolving Genres of ETDs for Knowledge Discovery. Proceedings of ETD 2015 18th International Symposium on Electronic Theses and Dissertations 51-59.
                    
               
                  DHd-Arbeitsgruppe (2016): "Digitales Publizieren", in: DHd-Arbeitsgruppe (eds.): Working Paper "Digitales Publizieren"
                        http://diglib.hab.de/ejournals/ed000008/startx.htm [letzter Zugriff: 21.09.2018]
                    
               
                  Dogunke, Swantje / Steyer, Timo / Mayer, Corinna (2018): "Barcamp Data and Demons: von Bestands- und Forschungsdaten zu Services. Treffen sich ein Bibliothekar, eine Archäologin, ein Informatiker, …", in: LIBREAS. Library Ideas 33
                        https://libreas.eu/ausgabe33/dogunke/ [letzter Zugriff: 21.09.2018].
                    
               
                  Fitzpatrick, Kathleen (2011): Planned Obsolescence Publishing, Technology, and the Future of the Academy. New York: New York Univ. Press.
                    
               
                  Herb, Ulrich (2012): "Offenheit und wissenschaftliche Werke: Open Access, Open Review, Open Metrics, Open Science & Open Knowledge”, in: Herb, Ulrich (eds): Open Initiatives: Offenheit in der digitalen Welt und Wissenschaft. Saarbrücken Universaar 11-44.
                    
               
                  Kohle, Hubertus (2017): “Digitales Publizieren” in: Jannidis, Fotis / Kohle, Hubertus / Rehbein, Malte (eds.): Digital Humanities. Eine Einführung. Stuttgart: Metzler Verlag 199-205.
                    
               
                  Maciocci, Giuliano (2017): "Designing Progressive Enhancement Into The Academic Manuscript: Considering a design strategy to accommodate interactive research articles", in: Blogpost auf eLife Sciences
                        https://elifesciences.org/labs/e5737fd5/designing-progressive-enhancement-into-the-academic-manuscript [letzter Zugriff: 21.09.2018].
                    
               
                  Penfold, Naomi (2017): "Reproducible Document Stack – supporting the next-generation research article", in: Blogpost auf eLife Sciences
                        https://elifesciences.org/labs/7dbeb390/reproducible-document-stack-supporting-the-next-generation-research-article [letzter Zugriff: 21.09.2018].
                    
               
                  SoSciSo Redaktion (2017): "Kollaboratives Schreiben mit webbasierten Programmen", in: Blogpost auf Social Science Software 
                        https://www.sosciso.de/de/2017/kollaboratives-schreiben/ [letzter Zugriff: 21.09.2018].>
                    
               
                  W3C (2017): "W3C Data Activity. Building the Web of Data" 
                  https://www.w3.org/2013/data/> [letzter Zugriff: 21.09.2018].
                    
            
         
      
   



      
         Computer und Internet haben die Art und Weise, wie Forscher kommunizieren und zusammenarbeiten, grundlegend verändert. Ab Anfang der 90er Jahre konnten Wissenschaftlerinnen und Wissenschaftler über verschiedene Orte und Zeitzonen hinweg kollaborativ an Text, Bild, Audio, Video und Code arbeiten. Während E-Mail, Newsgroups und Online-Chats eine many-to-many-Kommunikation im virtuellen Raum ermöglichten, kamen die wichtigsten aktuellen Entwicklungen in der wissenschaftlichen Online-Kommunikation durch soziale Medien: mit Microblogging, Blogs, Wikis und Social Network Sites (SNS) wie Facebook, Academia.edu, ResearchGate und anderen. Durch sie wurden die Hindernisse für die Veröffentlichung und Kommunikation im Internet deutlich reduziert. Produktionsprozesse, die bisher professionelles Wissen, Ausrüstung und Kapital erforderten, können nun von einfachen Personen mit Computer- und Internetzugang durchgeführt werden. In der Folge wurde das Ökosystem der wissenschaftlichen Kommunikation breiter, schneller, interaktiver, dynamischer, multimodaler und zunehmend vernetzter (König 2015).
         Als öffentlich geführte wissenschaftliche Notizbücher eignen sich insbesondere Wissenschaftsblogs zur selbstkritischen Reflektion des eigenen Forschungsprozesses wie auch zur Dokumentation desselben. Nicht nur Nachwuchswissenschaftlerinnen und Nachwuchswissenschaftlern bietet Bloggen die Möglichkeit, bereits in einem frühen Stadium auf ihr Projekt aufmerksam zu machen, mit erfahrenen Wissenschaftlerinnen und Wissenschaftlern in Austausch zu treten, sich zu vernetzen, Schreiben zu üben und Gedanken im Schreibprozess zu ordnen. Wissenschaftsblogs haben ein hohes Potential für die schnelle Verbreitung und Diskussion aktueller Forschungsinhalte und nutzen die Möglichkeiten des Web 2.0 für eine direkte und interaktive Publikation, bei der multimediale Inhalte 
                wie Bilder, Grafiken, Animationen und Verlinkungen ohne Mehrkosten eingebunden werden können. Wissenschaftsblogs werden zwar zumeist für die eigene Fachcommunity geschrieben, sie sind jedoch offen einsehbar und werden ebenso von Journalisten und von der breiten Öffentlichkeit wahrgenommen.
            
         Wissenschaftsblogs bieten Einblicke in die Werkstatt von Forschenden und zeigen Forschung im Entstehen (Mounier 2013). Gerade in den Digital Humanities sind Blogs und Twitter die wichtigsten Medien für die Diskussion neuer Forschungsansätze und Methoden (Ullyot 2012). Blogs dokumentieren den Forschungsprozess und damit die Phase vor der abschließenden Projektveröffentlichung. Damit ersetzen sie bisherige Praktiken und Formate der Kommunikation und Publikation zumeist nicht – auch wenn sie es theoretisch könnten –, sondern ergänzen diese und stellen in ihrer Ausprägung etwas Neues dar: ein eigenes Format, das Kennzeichen aus der analogen (mündlich wie schriftlichen) und der digitalen Wissenschaftskommunikation als „missing link“ mischt und um neue Merkmale ergänzt. Wenn das Medium die Botschaft ist (Marshall McLuhan), dann zeigen bloggende Forscherinnen und Forscher, wie sie sich Wissenschaft vorstellen: offen, vernetzt, horizontal, direkt, schnell, vielseitig, multimedial… und mit der akzeptierten Möglichkeit, sich zu irren (König 2015). Forschende schreiben in Blogs über einzelne Aspekte ihres Themas, über Publikationen, die sie gelesen haben, über Vorträge und Veranstaltungen, die sie besucht oder über Begegnungen, die sie inspiriert haben. Blogbeiträge handeln von einem konkreten Ereignis oder Gegenstand oder entwickeln theoretische und methodische Überlegungen. Zumeist zeigt ein Wissenschaftsblog die subjektive Lebenswelt der Forschenden und macht somit ganz generell die Subjektivität der Wissenschaft und des wissenschaftlichen Tuns deutlich. 
         Mit de.hypotheses.org wurde Anfang 2012 eine Plattform für geistes- und sozialwissenschaftliche Blogs geschaffen, in deren Umfeld seither eine stetig wachsende deutschsprachige Community als Teil eines europäischen Netzwerks entstanden ist. Mittlerweile sind dort über 500 deutschsprachige Blogs aus allen geisteswissenschaftlichen Disziplinen vereint. Die Blogplattform trägt zur Sichtbarkeit und zur Vernetzung der Bloggenden bei und ist eine zentrale Anlaufstelle, bei der die Blogs langzeitarchiviert werden, eine ISSN verliehen bekommen und die Blogbeiträge mit Permalinks ausgestattet sind. Für die Startseite des Portals werden von einer Redaktion und vom Community Management die aktuell besten Beiträge ausgewählt und kuratiert, die darüber eine erhöhte Sichtbarkeit erhalten. 
         Die bei de.hypotheses vorhandenen unterschiedlichen Blogtypen belegen die große Vielfalt der geisteswissenschaftlichen Blogosphäre. Es gibt Blogs von Forschergruppen und zu Forschungsprojekten, thematische Gemeinschaftsblogs, Blogs zu Quellen und Methoden, Blogs von Instituten und wissenschaftlichen Einrichtungen wie Archive und Bibliotheken, Seminar- und Tagungsblogs, Blogs, die eine Zeitschrift oder eine Publikation begleiten, Blogs für Lehre und Didaktik, Fotoblogs, Blogs zu einer wissenschaftlichen Debatte etc. (König 2013).
         Der auf einen halben Tag angelegte Workshop knüpft direkt an den medientheoretischen und –praktischen Teil des Tagungsthemas an und richtet sich an DH-Forschende, die bisher noch nicht bloggen und ein eigenes Wissenschaftsblog anlegen möchten – ob als Einzel- oder als Gemeinschaftsblog, ob begleitend zur Lehre oder zu einem Forschungsprojekt – und dafür ein Konzept entwickeln und grundlegende inhaltliche und technische Überlegungen anstellen und praktisch einüben möchten. 
         Im Rahmen des Workshops wird zum einen die theoretische und konzeptionelle Seite des wissenschaftlichen Bloggens als eigenes multimediales Medium besprochen, zum anderen ein praktischer Teil angeboten. Zunächst werden einleitend verschiedene aktuelle Praktiken des Wissenschaftsbloggens, der besondere Schreibstil und die Interaktion mit der Leserschaft thematisiert und Elemente für die Strategiebildung für ein eigenes Wissenschaftsblog erläutert (darunter: was bloggen? wie bloggen? wie viel Zeit investieren? für welches Publikum? alleine oder kollaborativ bloggen? wie Themen für das Wissenschaftsblog finden? (Scherz 2013). Es werden best practice Beispiele aus verschiedenen geisteswissenschaftlichen Disziplinen und aus den DH vorgestellt. Gegenstand der Diskussion sind darüber hinaus rechtliche Fragen (vom Einbinden fremder Inhalte wie Bilder und Videos und dem Lizenzieren eigener Inhalte, über die Bestimmungen der DSGVO bis hin zum „Eigenplagiat“ bei Promovierenden usw.) sowie die Frage nach dem „return of investment“ des Bloggens, das durchaus zeitintensiv sein kann und aufgrund der zumeist mangelnden offiziellen Anerkennung überlegt erfolgen sollte. Thematisiert wird außerdem der Umgang mit Kommentaren im Blog sowie die Frage, was Promovierende über ihre Dissertationen bloggen können und was nicht. 
         In einem Hands-on-Teil – der etwa drei Viertel der Zeit des Workshops einnehmen wird – werden anschließend Schritt für Schritt die einzelnen Aspekte der Blogpraxis vorgestellt und vorgeführt. Die Teilnehmerinnen und Teilnehmer vollziehen die einzelnen Schritte an eigens eingerichteten Wordpress-Schulungsblogs nach, von der Gestaltung und Einrichtung des Blogs, der Formulierung einer guten Überschrift, einer sinnvollen Navigation und Kategorienbildung bis hin zum Einbetten von Videos, und wenden damit das Gelernte sofort an. Sie lernen darüber die Grundlagen moderner CMS-Systeme kennen. Während des Workshops werden parallel zum praktischen Teil Tipps gegeben für die Anfangsphase eines wissenschaftlichen Blogs und für Themen wie Suchmaschinenoptimierung, Menüführung und graphische Gestaltung. 
         Inhalte und Übungen des Praxisteils sind: Erstellen einer Menüleiste, öffentliche Autorennamen einstellen und Profil ausfüllen, Rechteverwaltung bei mehreren Nutzerinnen und Nutzern, Titel und Untertitel ändern, Design-Theme auswählen (welche sind für welche Form des Bloggens geeignet?), Bild in die Kopfzeile einfügen, Nennung des Urhebers und Lizenz, eigenen Artikel erstellen, Überschrift auswählen, Zitat einfügen, Fußnoten, Verlinkung einfügen, Weiterlesen-Button, Bildrechte, Bilder und Lizenzen einfügen (Grundlagen CC-Lizenzen), Kategorien, Schlagwörter zuweisen, Seite anlegen, Menü anlegen, Meta, Text, Bild und Link, RSS-Feed, Verknüpfung zu Twitter, Videos einbinden, Statistiken lesen, Reichweite vergrößern, rechtliche Bestimmungen der DSGVO, Umgang mit Kommentaren.
         Die Anzahl der Teilnehmenden ist auf 25 begrenzt. Der Workshopraum sollte über ein W-Lan und einen Beamer verfügen. Eine weitere technische Ausstattung wird nicht benötigt. Besondere technische Vorkenntnisse sind für die Teilnahme nicht erforderlich. Ein eigenes Laptop oder ein anderes Endgerät muss selbst mitgebracht werden. 
         Workshopleiterinnen: Dr. Mareike König: Sie ist Projektleiterin der deutschsprachigen Plattform für geisteswissenschaftliche Blogs de.hypotheses und leitet dort die Redaktion. Ihre Forschungsinteressen beziehen sich auf Wissenschaftskommunikation im Web 2.0 und hier speziell auf das Wissenschaftsbloggen als neue Form des wissenschaftlichen Schreibens als Herausforderung für unsere Wissenschaftskultur. 
         Kontakt: Dr. Mareike König, DHIP, 8, rue du Parc Royal, 75003 Paris, mkoenig@dhi-paris.fr
         Ulla Menke: Sie ist Community Managerin der Blogplattform de.hypotheses seit 2016 und arbeitet in der Max Weber Stiftung. Als Community Managerin kümmert sie sich um die rund 350 Wissenschaftsblogs, die es auf der Plattform de.hypotheses derzeit gibt und steht den Bloggenden seit 2016 mit Tipps und Hilfe bei Fragen rund um Technik, SEO, Blognavigation und Bloggestaltung zur Seite. 
         Kontakt: Ulla Menke, Max Weber Stiftung, Rheinallee 8, Bad Godesberg, menke@maxweberstiftung.de
      
      
         
            
               Bibliographie
               
                  König, Mareike (2013):
                  Die Entdeckung der Vielfalt: Geschichtsblogs auf der internationalen Plattform hypotheses.org,
                        in: 
                        Haber, Peter / Pfanzelter, Eva (eds.): 
                        Historyblogosphere. Bloggen in den Geschichtswissenschaften,
                        München: Oldenbourg 181–197.
                    
               
                  König, Mareike (2015):
                  Herausforderung für unsere Wissenschaftskultur: Weblogs in den Geisteswissenschaften
                        in: 
                        Schmale, Wolfgang (ed.):
                  Digital Humanities. Praktiken der Digitalisierung, der Dissemination und der Selbstreflexivität,
                        Stuttgart: Steiner 57-74.
                    
               
                  Scherz, Sabine (2013a):
                  Warum sollte ich als Wissenschaftler/in bloggen? 
                        in Redaktionsblog, 21.5.2013, 
                        https://redaktionsblog.hypotheses.org/1209.
                    
               
                  Scherz, Sabine (2013b):
                  Mein erster wissenschaftlicher Blogartikel – was schreibe ich bloß?
                        in Redaktionsblog, 24.5.2013, 
                        https://redaktionsblog.hypotheses.org/1214. 
                    
               
                  Scherz, Sabine (2013c):
                  Wie finde ich Themen für mein Wissenschaftsblog?
                        in Redaktionsblog, 28.5.2013, 
                        https://redaktionsblog.hypotheses.org/1217. 
                    
               
                  Scherz, Sabine (2013d):
                  Texte für das Wissenschaftsblog schreiben, wie?
                        in Redaktionsblog, 5.6.2013, 
                        https://redaktionsblog.hypotheses.org/1220. 
                    
               
                  Mounier, Pierre (2013):
                  Die Werkstatt öffnen: Geschichtsschreibung in Blogs und sozialen Medien,
                        in:
                        Haber, Peter / Pfanzelter, Eva (eds.):
                  Historyblogosphere. Bloggen in den Geschichtswissenschaften,
                        München: Oldenbourg 51-59.
                    
               
                  Ullyot, Michael (2012):
                  On Blogging in the Digital Humanities,
                        in: Ullyot, http://ullyot.ucalgaryblogs.ca/2012/02/24/on-blogging-in-the-digital-humanities/.
                    
            
         
      
   



      
         Die automatische Analyse von Tweets mit politischem Inhalt kann Sozial- und Politikwissenschaftlern Aufschluss über die Prozesse politischer Meinungsbildung geben. Beiträge in den sozialen Medien spiegeln oft Tendenzen in der Zufriedenheit mit politischen Parteien wider und helfen umstrittene oder viel diskutierte Themen zu identifizieren. Auch das Kommunikationsverhalten verschiedener Gruppen lässt sich aus ihrer Interaktion bei Twitter analysieren (z.B. unterschiedliche Dominanz von Echokammereffekten (Colleoni et al., 2014) oder Verbreitung von Gerüchten und Fake News (Ma et al., 2018)).
         Eine wichtige Rolle spielt dabei die Sentimentanalyse, die es erlaubt zu identifzieren, ob ein Akteur Zustimmung oder Ablehnung zu einem bestimmten Inhalt signalisiert. Für Textdaten lässt sich das Sentiment meist recht gut bestimmen. Tweets sind aufgrund ihrer Kürze jedoch zum einen oft schwer recht kryptisch, zum andern enthalten sie häufig weitere Materialien, insbesondere Bilder, die nennenswert zur Aussage beitragen. Z.B. ist der Text „und wieder ein neuer Morgen“ neutral formuliert, gewinnt aber eine negative Bedeutung, wenn er um eine Bild bereichert wird, das eine lange Autoschlange zeigt. Ebenso kann es sein, dass ein relativ neutrales Bild lediglich über den Text ein positives oder negatives Sentiment zugewiesen bekommt.
         Die meisten bisher existierenden Sentimentanalyseverfahren beschränken sich auf die Verarbeitung entweder von Text- oder von Bilddaten. Modelle, die beide Modalitäten
         berücksichtigen, sind noch vergleichsweise selten, können jedoch eine signifikant höhere Genauigkeit bei der Sentiment-Vorhersage erreichen als solche, die dies nicht tun (You et al. 2016). Fast alle multimodalen Verfahren nutzen eine Deep-Learning-Architektur. Solche Verfahren sind herkömmlichen Lernverfahren zwar oft überlegen, sie sind aber aufgrund der Vielzahl der möglichen Architekturen auch relativ schwer zu optimieren. Das Ziel dieser Arbeit ist es, verschiedene multimodale Sentimentanalyseverfahren und -architekturen systematisch zu vergleichen und auf ihre Vor- und Nachteile hin zu untersuchen.
         Das grundsätzliche Schema der Modelle orientiert sich am "Latent Multimodal Mixing" (Bruni et al. 2014); hierbei werden zunächst Text- und Bild-Features extrahiert, als Vektoren kodiert
          und anschließend in einem dritten Schritt auf einen gemeinsamen (multimodalen) Vektorraum abgebildet (Fusion). Aus diesen Vektoren kann dann mit Methoden des maschinellen Lernens das Sentiment berechnet werden. Innerhalb dieses Schemas können beliebige und auch neuartige Kombinationen verschiedener Methoden zur Feature-Extraktion und Fusion verwendet werden. Hierfür gibt es unter anderem folgende Möglichkeiten:
         
            Texte können mit dem Doc2Vec-Verfahren auf einen latenten Vektorraum abgebildet werden. Dieses Verfahren erzielte in der Vergangenheit bereits gute Ergebnisse bei der Sentimentanalyse. (Le et al. 2014)
            Basierend auf einem existierenden Word-Embedding-Modell (z.B. GloVe (Pennington et al. 2014)) können die Word-Embeddings aller Wörter eines Textes auf verschiedene Arten zu einem Text-Embedding aggregiert werden (z.B. gewichteter Mittelwert, elementweises Minimum/Maximum). (De Boom et al. 2016)
            Für die Extraktion visueller Features können bereits existierende Deep Learning-Modelle zur Bild-Klassifikation in leicht modifizierter Form wiederverwendet werden. (Campos et al. 2017)
            Aus dem Farbhistogramm eines Bildes können können statistische Features erster Ordnung berechnet werden.
            Der Fusionsschritt besteht aus einer einfachen Verkettung der Text- und Bild-Vektoren. Zusätzlich kann auch eine affine Projektion auf einen latenten multimodalen Vektorraum gelernt werden. (Chen et al. 2017)
         
         Die Datengrundlage für das Training der Modelle bilden manuell annotierte multimodale Tweets u.a. aus dem Photo Tweet Sentiment Benchmark (Borth et al. 2013), sowie das Columbia MVSO Image Sentiment Dataset (Dalmia et al. 2016). Aufgrund der unterschiedlichen Größe der Datensätze wird ein Transfer Learning-Ansatz verfolgt: Die Modelle werden zunächst auf den MVSO-Daten vortrainiert und anschließend mithilfe der Twitter-Daten feinadjustiert.
         Erste Testergebnisse bestätigen, dass Modelle, die Text- und Bild-Features fusionieren, eine höhere Genauigkeit erreichen können als unimodale Modelle. Allerdings haben die bisher getesteten Modelle derzeit noch Schwierigkeiten damit, negatives Sentiment korrekt zu klassifizieren.
      
      
         
            
               Bibliographie
               
                  You, Quanzeng / Luo, Jiebu / Jin, Hailin / Yang, Jianchao (2016):
                  Cross-modality Consistent Regression for Joint Visual-Textual Sentiment Analysis of Social Multimedia, 
                        in: The Ninth International Conference on Web Search and Data Mining, February 2016, San Francisco, CA, USA.
                    
               
                  Bruni, Elia / Tran, Nam Khanh / Baroni, Marco (2014):
                  Multimodal distributional semantics, 
                        in: Journal of Artificial Intelligence Research 49, 1 (Januar 2014), 1-47
                    
               
                  Colleoni, Elanor/ Rozza, Alessandro / Arvidsson, Adam (2014):
                  Echo Chamber or Public Sphere? Predicting Political Orientation and Measuring Political Homophily in Twitter Using Big Data, 
                        in: Journal of Communicatio 64 (2014) 317–332
                    
               
                  De Boom, Cedric / Van Canneyt, Steven / Demeester, Thomas / Dhoedt, Bart (2016):
                  Representation learning for very short texts using weighted word embedding aggregation, 
                        in: Pattern Recognition Letters 80, C (September 2016), 150-156.
                    
               
                  Campos, Victor / Jou, Brendan / Giró-i-Nieto, Xavier (2017):
                  From Pixels to Sentiment: Fine-uning CNNs for Visual Sentiment Prediction, 
                        in: Image and Vision Computing 65 (September 2017), 15-22
                    
               
                  Chen, Xingyue / Wang, Yunhong / Liu, Qingjie (2017):
                  Visual and Textual Sentiment Analysis Using Deep Fusion Convolutional Neural Networks, 
                        in: IEEE International Conference on Image Processing (ICIP), Beijing, China
                    
               
                  Dalmia, Vaidehi / Liu, Hongyi / Chang, Shih-Fu (2016):
                  Columbia MVSO Image Sentiment Dataset, 
                        in: CoRR, abs/1611.04455
                    
               
                  Borth, Damian / Ji, Rongrong / Chen, Tao / Breuel, Thomas / Chang, Shih-Fu (2013):
                  Large-scale Visual Sentiment Ontology and Detectors Using Adjective Noun Pairs, 
                        in: The 21st ACM International Conference on Multimedia, October 2013, Barcelona, Spain
                    
               
                  Le, Quoc / Mikolov, Tomas (2014):
                  Distributed Representations of Sentences and Documents, 
                        in: The 31st International Conference on Machine Learning, June 2014, Beijing, China
                    
               
                  Ma, Jing / Gao, Wei / Wong, Kam-Fai (2018)
                  Rumor Detection on Twitter with Tree-Structured Recursive Neural Networks, 
                        in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 1980–1989, Melbourne, Australia, July 15 - 20, 2018
                    
               
                  Pennington, Jeffrey / Socher, Richard / Manning, Christopher (2014):
                  GloVe: Global Vectors for Word Representation, 
                        in: Empirical Methods in Natural Language Processing (EMNLP), 1532-1543
                    
            
         
      
   



      
         Die einfache Zugänglichkeit von Wissenschaftsblogs ab der Jahrtausendwende ermöglicht es Forschenden, selbst zu entscheiden wann, wo und was sie veröffentlichen wollen. Diese selbstbestimmte Übernahme eines wissenschaftlichen Publikationsraums ist ein spektakulärer Schritt, ähnlich wie die Erfindung von "Essays" durch Montaigne im 16. Jahrhundert oder die Entstehung der "Gelehrtenrepublik" ab der Mitte des 17. Jahrhunderts (König 2015: 58). Wissenschaftliche Blogs sind Orte, in denen aus laufenden Forschungsprojekten kommuniziert und mit der Fachcommunity diskutiert werden kann. Sie ermöglichen Einblicke in das Labor oder die Werkstatt der Forschenden und zeigen damit "Wissenschaft im Entstehen" (Mounier 2013). Der wissenschaftliche Austausch über Blogartikel, Kommentare und Links ist interaktiv, schnell und direkt, in einer einzigartigen Weise, die in anderen Publikationsformaten wie Mailinglisten oder Zeitschriften nicht möglich ist oder nicht praktiziert wird. Blogs können als das fehlende Bindeglied zwischen mündlicher Kommunikation auf Konferenzen oder in Universitätsseminaren und schriftlicher Kommunikation in traditionellen Artikeln oder Rezensionen angesehen werden. Sie ermöglichen es Forschenden, eine direkte Verbindung zugleich zu ihren Peers und zu ihren Studierenden herzustellen und darüber hinaus mit Journalisten und der interessierten Öffentlichkeit in Kontakt zu treten. 
         Seit 2012 sorgt das Blogportal für die Geistes- und Sozialwissenschaften de.hypotheses.org für eine florierende Blogpraxis im deutschsprachigen Raum. Das Portal ist Teil der europäischen Plattform hypotheses und bietet als zentraler Einstiegsort kostenlose und werbefreie Blogs an für Forschende, die technische Updates, Hosting und Sicherheitsfragen nicht selbst übernehmen können oder wollen und Teil einer Community sein möchten. Community Management und Redaktion bewerben die besten aktuellen Beiträge in den sozialen Medien und auf der Startseite der Plattform. Sie bieten außerdem technischen und graphischen Support und beantragen bei den Nationalbibliotheken die Zuteilung einer ISSN für die Blogs. Die Blogbeiträge sind mit Permalinks versehen und die Inhalte der Plattform werden von BnF und DNB archiviert. Derzeit sind auf der deutschsprachigen Seite rund 350 Wissenschaftsblogs vereint.
         Obwohl es sich um ein relativ neues Phänomen handelt, ist die Forschung zur wissenschaftlichen Nutzung von sozialen Medien in den letzten Jahren stark gewachsen (für einen umfassenden Überblick siehe Sugimoto et al. 2017). Die empirische Forschung zur Nutzung von sozialen Medien erfolgt durch Fragebögen, qualitative Interviews und teilnehmende Beobachtungsstudien. Diese Studien untersuchen Praktiken von innen heraus und fragen Forschende nach ihren Methoden, Vorlieben oder Widerständen in Bezug auf die wissenschaftliche Nutzung von sozialen Medien (siehe z.B. Ponte und Simon (2011), hauptsächlich für Großbritannien; Bader, Fritz und Gloning (2012) und Pscheida et al. (2013) für Deutschland). Andere Forschungsbereiche untersuchen digitale Praktiken und Online-Communities von außen, z.B. über die Analyse von Inhalten und Sprache oder über die Analyse von Netzwerken anhand von Links und Kommentaren. Beide Arten von Studien zeigen zumeist eine Vielzahl von unterschiedlichen Nutzungen, Zwecken und Motiven der sozialen Medien, je nach Plattform, akademischem Rang, Status, Geschlecht und Alter der Forschenden sowie unterschiedlich nach Disziplinen, Ländern und geografischen Regionen. 
         Statistiken und Beobachtungen der geisteswissenschaftlichen Blogs geben einen Einblick in die Motivationen der Bloggenden, in ihre Blogpraktiken wie auch in ihre Kommunikation über Kommentare und Verlinkungen. Abgesehen von den beiden erwähnten bereits gealterteten Umfragen allgemein zu sozialen Medien in der Wissenschaft (Bader, Fritz und Gloning, 2012, sowie Pscheida et al. 2013), gab es anders als im angelsächsischen Raum (Jarreau 2015) im deutschsprachigen Raum noch keine spezifische Befragung geisteswissenschaftlicher Bloggenden im größeren Ausmaß. Diese Lücke wird durch eine Umfrage geschlossen, die im Herbst 2018 bei den rund 350 Blogs von de.hypotheses sowie auch darüber hinaus durchgeführt wird und deren Ergebnisse der Vortrag vorstellen möchte. 
         Die Umfrage zielt in erster Linie darauf, Gründe für das Wissenschaftsbloggen sowie konkrete Praktiken des geisteswissenschaftlichen Bloggens abzufragen und darüber mögliche Änderungen im Publikations- und Kommunikationsverhalten von Geisteswissenschaftlerinnen und -wissenschaftlern empirisch gestützt zu ermitteln. Als empirische Studie mit medientheoretischem Bezug knüpft der Vortrag damit direkt an das Tagungsthema an.
         Die Hauptfragebereiche der Umfrage beziehen sich auf Motivationen für das Bloggen, auf Inhalte, Zeitaufwand, Publikum und formale Gestaltung sowie auf den „Erfolg“ der Blogs in Bezug auf Kommentare, Rückmeldungen und Zugriffsstatistiken. Das Abfragen von Personendaten soll ermöglichen, diese Antworten mit akademischem Rang, Alter und Geschlecht der Bloggenden zurückzukoppeln und darüber etwa gender- und statusspezifische Praktiken ausmachen zu können. Dagegen geht es nicht um die Abfrage der Zufriedenheit der Bloggenden mit der Plattform de.hypotheses selbst. Folgende Themenblöcke werden u.a. angesprochen:
         Die Nutzung wissenschaftlicher Blogs ist je nach Strategie und Zielen der Forschenden sehr unterschiedlich. In der Kategorie "Über das Blog" auf den Wissenschaftsblogs von de.hypotheses bekommt man dazu einen Einblick. Bloggende Forschende nennen als erste Motivation den Wunsch, ihr Forschungsthema zu diskutieren, ihre Online-Reputation zu verbessern, sich in der Wissenschaft zu positionieren und Netzwerke zu pflegen (König 2015: 59). Darüber hinaus wollen die Forschenden das Schreiben üben oder ihren Schreibstil verbessern und sich kreativ ausdrücken. Andere Untersuchungen deuten darauf hin, dass Bloggen Forschenden das Gefühl vermittelt, in ihrer Arbeit mit anderen verbunden zu sein (Mewburn und Thomson 2013: 1107). Blogs können als Dokumentation für Forschungsprojekte dienen, als eine Art digitaler Zettelkasten, der über Schlagwörter und Kategorien strukturiert und öffentlich zugänglich ist. Diese Angaben zur Motivation und Gründe des Bloggens werden in der Umfrage abgefragt, wobei es zugleich auch darum gehen wird, ob diese Ziele subjektiv nach Empfinden der Einzelnen erreicht werden.
         Ein weiterer Fragenblock der Umfrage behandelt die internen Abläufe bei der Veröffentlichung auf Wissenschaftsblogs. Einige Blogs funktionieren ähnlich wie Zeitschriften: Sie haben eine Redaktion, die Autorinnen und Autoren einlädt, Artikel redaktionell bearbeitet und sicherstellt, dass Blogbeiträge in traditionellen Bibliographien und Bibliothekskatalogen katalogisiert werden. Aber auch in Einzelblogs publizieren Autorinnen und Autoren oftmals erst, nachdem die Beiträge von einer anderen Person gegengelesen worden sind. Dies schließt an die Beobachtung an, dass Forschende ihre Blogs als Orte der Selbstpublikation nicht leichtsinnig befüllen, sondern sich viele Gedanken machen, was sie wann, wie und in welcher Form publizieren. Vielen Forschenden fällt es schwer, unfertige oder aufkommende Ideen zu veröffentlichen. Sie haben Angst davor, sich zu irren und wollen vermeintliche Sackgassen nicht öffentlich machen. Die Angst vor Plagiaten hindert sie ebenso daran, über aktuelle Erkenntnisse und aktuelle Projekte zu bloggen. Welche strategischen und konzeptionellen Grundideen geisteswissenschaftliche Bloggende verfolgen soll ebenso wie die Organisation der redaktionellen Zwischenschritte vor der Veröffentlichung durch die Umfrage deutlich werden.
         Es gibt eine große Vielfalt an Inhalten, die in wissenschaftlichen Blogs veröffentlicht werden. Einige Bloggende schreiben grundsätzlich nur über ihr Forschungsthema. Andere diskutieren wissenschaftliche Arbeitspraktiken, geben Karriereberatung oder nutzen ihr Blog zur Begleitung der Lehre. Häufig wird in wissenschaftlichen Blogs die akademische Kultur allgemein kritisiert (Mewburn und Thomson 2013: 1110). Alles in allem lassen Blogs den Forschenden als hybride Person erscheinen und zeigen, dass die akademischen Interessen von Wissenschaftlerinnen und Wissenschaftlern viel breiter sind als die in klassischen Medien veröffentlichte Forschung das widerspiegelt (Mewburn und Thomson 2013: 1114). Darüber hinaus unterstreicht der unterschiedliche und informelle Stil der Blogartikel die Vielfalt des wissenschaftlichen Schreibens und die Vielfalt der Perspektiven. In wissenschaftlichen Blogs ist es möglich, in der ersten Person Singular zu schreiben, engagiert, witzig, kreativ und essayistisch zu schreiben, Smileys oder Strikes zu verwenden, Code, Bilder und Videos einzubetten und damit multimedial zu publizieren (König 2015: 64-65). Die Umfrage soll Aufschluss geben, ob und in welchem Umfang die bloggenden Geisteswissenschaftlerinnen und Geisteswissenschaftler von den stilistischen Freiheiten des Genres profitieren oder ob sie sich freiwillig an traditionellere Formate und Publikationsrhythmen anpassen.
         Einige explorative Studien deuten darauf hin, dass sich Blogs weder bei der Sprachwahl noch bei der Themenwahl an ein Laienpublikum wenden (Mahrt und Puschmann 2014: 4; Mewburn und Thomson 2013: 1113). In der Umfrage wird gezielt bei der deutschsprachigen Community von hypotheses abgefragt, an welches Publikum sich die Forschenden in der Regel wenden, ob der Transfer von Forschungsergebnissen in die Öffentlichkeit zu den Zielen gehört und ob sich die Bloggenden sprachlich auf ein Laienpublikum einstellen oder ob sie auf Medienaufmerksamkeit zielen. Es gibt Rückmeldung von Bloggenden, wonach Blogbeiträge als Vorstufe für Peer Review-Artikel gesehen werden und Forschende aufgrund von Blogbeiträgen aufgefordert worden sind, diese zu vollständigen Artikeln auszubauen. Wie verbreitet dieses Phänomen ist, soll die Umfrage empirisch zeigen.
         Bloggen wird als eine hochgradig interaktive Praxis angesehen (Mahrt und Puschmann 2014: 6), obwohl Kommentare zu geistes- und sozialwissenschaftlichen Blogs knapper geworden sind, u.a. weil die Diskussion von Blogartikeln auf Twitter, Facebook oder andere soziale Medien verschoben wurde (König 2015: 72). Auf dem Blog-Hub SciLogs mit mehrheitlich Bloggenden aus den Naturwissenschaften erhalten Artikel durchschnittlich fünf Kommentare. Blogging-Stars wie der österreichische Astronom Florian Freistätter wiederum erhalten regelmäßig zwischen 50 und 100 Kommentare pro Artikel (Lobin 2017: 226). Die aktuellen Statistiken für die Plattform de.hypotheses liegen zwar vor, bei den Bloggenden wird aber nachgefragt, wie sie mit den Kommentaren umgehen, ob sie selbst welche schreiben, ob die Inhalte der Kommentare überwiegend positiv, neutral oder negativ sind, welche andere Form von Rückmeldungen sie für ihre Blogbeiträge erhalten und wie wichtig ihnen diese für das Einschätzen des eigenen Erfolgs sind.
      
      
         
            
               Bibliographie
               
                  Bader, Anita / Fritz, Gerd / Gloning, Thomas (2012): Digitale Wissenschaftskommunikation 2010-2011. Eine Online-Befragung. Gießen, 
                        
                     http://geb.uni-giessen.de/geb/volltexte/2012/8539/
                  . 
                    
               
                  Jarreau, Page (2015):
                  All the Science That Is Fit to Blog. An Analysis of Science Blogging Practices. 
                        LSU Doctoral Dissertations 1051, 
                        https://digitalcommons.lsu.edu/cgi/viewcontent.cgi?article=2050&context=gradschool_dissertations.
                    
               
                  König, Mareike (2013):
                  Die Entdeckung der Vielfalt: Geschichtsblogs auf der internationalen Plattform hypotheses.org, 
                        in: 
                        Peter Haber / Eva Pfanzelter (eds.):
                  Historyblogosphere. Bloggen in den Geschichtswissenschaften. 
                        München: Oldenbourg 181–197. 
                    
               
                  König, Mareike (2015):
                  Herausforderung für unsere Wissenschaftskultur: Weblogs in den Geisteswissenschaften, 
                        in: 
                        Wolfgang Schmale (ed.):
                  Digital Humanities. Praktiken der Digitalisierung, der Dissemination und der Selbstreflexivität. 
                        Stuttgart: Steiner 57-74.
                    
               
                  Lobin, Henning (2017):
                  Aktuelle und künftige technische Rahmenbedingungen digitaler Medien für die Wissenschaftskommunikation, 
                        in: 
                        Peter Weingart / Holger Wormer / Andreas Wenninger / Reinhard F. Hüttl (eds.):
                  Perspektiven der Wissenschaftskommunikation im digitalen Zeitalter. 
                        Weilerswist: Velbrück 223-258.
                    
               
                  Mahrt, Merja / Cornelius Puschmann (2014):
                  Science Blogging: an exploratory study of motives, styles, and audience reactions, 
                        in: Journal of Science Communication 13/3: A05. 
                        https://jcom.sissa.it/archive/13/03/JCOM_1303_2014_A05
                  (accessed August 31, 2018).
                    
               
                  Mewburn, Inger / Pat Thomson (2013):
                  Why Do Academics Blog? An Analysis of Audiences, Purposes and Challenges, 
                        in: Studies in Higher Education 38/8: 1105–1119.
                    
               
                  Mounier, Pierre (2013):
                  Die Werkstatt öffnen: Geschichtsschreibung in Blogs und sozialen Medien, 
                        in: 
                        Peter Haber / Eva Pfanzelter (eds.):
                  Historyblogosphere. Bloggen in den Geschichtswissenschaften. 
                        München: Oldenbourg 51-59.
                    
               
                  Ponte, Diego / Simon, Judith (2011):
                  Scholarly Communication 2.0: Exploring Researchers' Opinions on Web 2.0 for Scientific Knowledge Creation, 
                        Evaluation and Dissemination, Serials Review 37(3): 149-156.
                    
               
                  Pscheida, Daniela / Albrecht, Steffen / Herbst, Sabrina / Minet, Claudia / Köhler, Thomas (2013):
                  Nutzung von Social Media und onlinebasierten Anwendungen in der Wissenschaft. Erste Ergebnisse des Science 2.0-Survey 2013 des Leibniz-Forschungsverbunds “Science 2.0”, 
                        https://d-nb.info/1069096679/34.
                    
               
                  Sugimoto, Cassidy R. / Sam Work / Vincent Larivière / Stefanie Haustein (2017):
                  Scholarly Use of Social Media and Altmetrics: a Review of the Literature, 
                        in: Journal of the Association for Information Science and Technology, 68/9: 2037–2062.
                    
            
         
      
   



      
         
            Abstract
            Die Bedeutung von Social Media in den digitalen Geisteswissenschaften wächst. Nicht nur als Gegenstand der Analyse (z.B. in Gao et al. 2018 oder Reid 2011) sind Social Media für die Digital Humanities von Interesse, sondern auch zunehmend für die Dissemination von Forschungsergebnissen (vgl. Ross 2012). Vor allem in Blogs und Twitter wurde großes Potential für Diskussionen und die Verbreitung von Ergebnissen erkannt (vgl. Puschmann/Bastos 2015, Terras 2012). Auch in der Rezeptionsforschung der Wissenschaftskommunikation zeigt sich, dass Webmedien besonders relevant sind (vgl. Brossard 2013, 14096–14101) und dass diese darum in besonderem Maße zur 
                    „scientific literacy” beitragen könnten (vgl. Schäfer 2017, 283). Generell bietet (informelle) Wissenschaftskommunikation über Webmedien noch viel ungenutztes Potential (vgl. Schäfer 2017, 279–280, Neuberger 2014, Voigt 2012). Unser Beitrag zeigt, wie eine multimediale und multimodale webbasierte Strategie die Dissemination von Digital-Humanities-Methoden unterstützen und die Wissenschaftskommunikation des Forschungsfeldes stärken kann. Die quantitative Analyse der Erfolge dieser Strategie lässt Rückschlüsse darauf zu, welche Methode wem wie vermittelt werden sollte und bildet daher eine wichtige Basis für die Konzeption von Forschungsprojekten und der universitären Lehre.
                
         
         
            Konzeptioneller Rahmen – Multimedialität, Multimodalität und Codierungssysteme in forTEXT
            forTEXT ist ein Vermittlungsprojekt für digitale Methoden der Textanalyse, das sich vor allem an Forschende richtet, die bisher noch nicht mit digitalen Methoden arbeiten (siehe 
                    
                  https://fortext.net).
                Neben der ‘analogen’ Dissemination in Workshops und universitärer Lehre wird auch ein Schwerpunkt auf die online-Vermittlung gelegt, da an wissenschaftlichen Themen Interessierte diese Kanäle häufig als Informationsquelle nutzen (vgl. Brossard 2013, 14098). Die hier vorgestellte webbasierte Strategie als Teil des Disseminationskonzeptes in forTEXT soll darüber hinaus zur DH-Wissenschaftskommunikation beitragen und so die Sichtbarkeit des Forschungsgebiets erhöhen (zur Bedeutung der Geisteswissenschaften in der Wissenschaftskommunikation vgl. Scheu/Volpers 2017). Für die forTEXT-Disseminationsstrategie sind Multimedialität, Multimodalität und multiple Codierungen zentrale Aspekte, die wir wie folgt definieren:
                
            
               Multimedialität: Aufbereitung und/oder Nutzung unterschiedlicher medialer Kanäle. 
                    „Medium” verstehen wir wie Roesler/Stiegler (2005, 150–152) als Vermittlungssystem innerhalb eines Kommunikationsprozesses, bei dem auch das Medium selbst Teil der Vermittlung ist.
                
            
               Multimodalität: Aufbereitung und/oder Nutzung unterschiedlicher Kommunikationsmodi. Dabei verstehen wir 
                    „Modus” als Bezeichnung für eine semiotische Einheit wie z.B. Design oder Sprache (vgl. Bucher 2007, 53).
                
            
               Multiple kulturelle Codierung: Wir übernehmen hier ein semiotisches Verständnis von 
                    „Code
                    ” als Bezeichnung für ein System relevanter Informationseinheiten (vgl. Eco 1985, 58f.). Kulturelle Codes funktionieren als Bedeutungsnetz aus Referenzen auf ein kollektives Wissenskorpus (vgl. Barthes 1976, 25). Um den Begriff klar vom informationstechnologischen (Binär-)Code zu trennen, sprechen wir von Codierung oder Codierungssystem.
                
            Medien, Modi und Codes wirken auf unterschiedlichen Ebenen des Vermittlungsprozesses. Dabei sind Medien und Modi stark miteinander verbunden. Modi können aber als Varianten in andere Medien übertragen werden. Codes sind inhaltliche Elemente, weshalb sie für die Dissemination von Forschungsergebnissen zentral sind. Sie können sich auf einen Modus in einem Medium beziehen oder modi- und medienübergreifend sein: 
            
               
            
         
         
            Arbeitspraxis – die webbasierte Disseminationsstrategie
            
               Die forTEXT-Webseite als Basis medialer Vermittlung von Digital-Humanities-Inhalten
               
                  
               
               Das zentrale Vermittlungsmedium in forTEXT ist die Projektwebseite. Hier werden in Textbeiträgen sowohl Bilder als auch Videos eingebettet. Die forTEXT-Webseite bildet die Basis für die multimediale Web-Strategie, da hier grundlegende Modi und kulturelle Codierungen umgesetzt wurden, die in den sozialen Medien erweitert werden. Die primär genutzten Modi und ihre kulturellen Codierungen sind:
               
                  Design: Gedecktes Farbschema und serifenlose Schrift stehen für Schlichtheit und Sachlichkeit. Nur im Logo gibt es verspielte Elemente, die an eine Handschrift erinnern und die Verbindung von Tradition und Modernität vermitteln.
                    
               
                  Sprache: Die Beiträge erfüllen die Ansprüche wissenschaftlichen Schreibens. Die Wissenschaftlichkeit wird durch die technische Funktionalität zum Zitieren unterstützt.
                    
               
                  Stimme: Grundsätzlich ist die Webseite mehrstimmig angelegt, da hier verschiedene Autor*innen schreiben. Alle nutzen einen sachlichen Tonfall und die implizite Leserin wird stets mit formellem 
                        „Sie” angesprochen.
                    
               
                  Bildlichkeit: Die eingebetteten Bilder sind zumeist digitale Repräsentationen der eingesetzten Tools und scheinen als solche zunächst gegenstandsneutral. Allerdings sind die Bilder häufig auch Visualisierungen der in forTEXT durchgeführten Fallstudien, d.h. sie zeigen nicht nur grafische, sondern auch textliche Elemente und verweisen auf die Modellierung eines Forschungsgegenstandes, die bei der Erstellung der Grafik stattgefunden haben muss.
                    
               Die forTEXT-Webseite richtet sich in erster Linie an drei Zielgruppen, die sich für die forTEXT-Disseminationsstrategie als besonders relevant erwiesen haben:
               
                  Studierende – Lernende der DH-Methodik
                  Nachwuchswissenschaftler*innen – Umsetzende der DH-Methodik
                  Digitale Geisteswissenschaftler*innen – Lehrende der DH-Methodik
               
            
            
               Social Media in forTEXT
               Ausgehend von den Inhalten der Webseite, deren Modi und den entsprechenden Codierungen werden drei soziale Medien zur Vermittlung genutzt. Anders als die Webseite sollen die Social-Media-Kanäle jeweils primär eine Zielgruppe erreichen: YouTube vor allem Zielgruppe 1, Pinterest Zielgruppe 2 und Twitter Zielgruppe 3.
               YouTube
               Auf YouTube erstellen wir eigene Inhalte, die die Artikel der Webseite aufgreifen, weiterführen und ergänzen. Es gibt derzeit zwei Inhaltstypen; Fallstudien und Tutorials. Beide können als Open-Educational-Ressources genutzt werden. In methodischen Fallstudien wird zum Beispiel mittels NER verglichen, welche Bedeutung die Hauptfiguren in Goethes 
                        Werther und in Plenzdorfs 
                        neuem Werther haben. Wir erklären, wie die NER-Machine-Learning-Prozesse funktionieren und verlinken sowohl zur Webseite als auch zu forTEXT-Tutorials. In den forTEXT-NER-Tutorials wird in drei kleinen Einheiten die Installation, Anwendung und das Training eines eigenen NER-Modells gelehrt.
                    
               
                  
               
               Design und sprachliche Elemente der forTEXT-YouTube-Videos richten sich nach den Vorgaben der Webseite. Abbildungen der eingesetzten Tools werden ergänzt von piktografischen Animationen. Diese sind zwar schlicht, befördern jedoch Unvoreingenommenheit und Autodidaktik. Dem Vorurteil einer geringeren Technikaffinität weiblicher Menschen begegnen wir mit einem weiblichen Voice-over (vgl. Schelhowe 2000). Dadurch werden Schwellenängste abgebaut und der Eindruck vermittelt, dass Nutzer*innen und digitale Tutorin sich gemeinsam autodidaktisch an die Methoden heranwagen.
               Pinterest
               
                  
               
               Die Anpassbarkeit in Hinblick auf Design, Stimme und Bildlichkeit der Kommunikationsmodi ist bei Pinterest am geringsten. Hier werden überwiegend fremde Artikel 
                        „gepinnt”, die lediglich mit einer kurzen Beschreibung angereichert werden. Auch führt die Besonderheit von Pinterest als Chimäre zwischen sozialem Medium und Suchmaschine dazu, dass die sprachlichen Elemente nicht nur für die menschliche Wahrnehmung, sondern insbesondere technologisch eine Rolle spielen. Primär werden hier DH-Forschende angesprochen, die Pinnwände für Tools (z.B. Stanford-NER, Carto, Gephi, CATMA), einzelne Methoden (z.B. Netzwerkanalyse, Stilometrie, Topic Modeling), Diskussionen und viele andere Themen der Digital Humanities finden.
                    
               Twitter
               Bei Twitter sind Layout und Typografie der Tweets nicht veränderbar. Jedoch wird bei jedem Tweet das forTEXT-Logo angezeigt. Im Gegensatz zu Pinterest ist auf Twitter die Ausgestaltung der Stimme bedeutsam. Hier steht die Kommunikation mit der eigenen Forschungscommunity im Vordergrund. Die Beschränktheit der Tweets auf 280 Zeichen führt dazu, dass eher Fachbegriffe als Umschreibungen genutzt werden. Hashtags führen zu Themen, die für die Community bedeutsam sind und folgen einem kulturellen Sprachcode. Hier nimmt forTEXT an kollegialen Insider-Gesprächen Teil und betont die forschungsrelevante Seite des Projektes.
            
         
         
            Quantitative Analyse
            Die webbasierte forTEXT-Disseminationsstrategie wird regelmäßig quantitativ ausgewertet. Zusätzlich zur kontinuierlichen Steigerung von Aufmerksamkeit für das Projekt (quantitativ messbar durch Impressionen, Interaktionen, Betrachtungszeiten), werden auch Analysen durchgeführt, die eher konzeptionelle Aspekte der webbasierten Dissemination von Forschungsmethoden in den Fokus rücken. Auf Basis dieser Analysen entwickeln wir eigene Relevanzmetriken, die neben quantitativen auch qualitative Aspekte berücksichtigen, wie bspw. demografische Daten, die anzeigen, welche Zielgruppen über welche Medien, welche Modi und welche Codes tatsächlich erreicht werden können, aber auch Kommentare, Feedback und Interaktionen mit anderen Nutzer*innen.
            Zum jetzigen Zeitpunkt läuft die forTEXT-Social-Media-Arbeit seit drei Monaten. Auf allen medialen Kanälen zeigt sich bereits eine steigende Aufmerksamkeit, auch wenn die Zahlenwerte nach Medium stark differieren. Twitter erzielt mit durchschnittlich 20.000 Impressionen im Monat quantitativ die größte Reichweite. Auch die Interaktionsrate ist mit bis zu 7,4% relativ hoch – die Zielgruppe 3 kann hier sehr gut erreicht werden. Mit Pinterest konnten in den ersten drei Monaten durchschnittlich 1.737 monatliche Impressionen erreicht werden, wobei die einzelnen Monate mit 780 Betrachtern im ersten Monat und 9.300 Betrachtern im dritten Monat stark schwanken. Hier zeigt sich, dass die mit maschinellem Lernen verknüpfte Suchmaschine Pinterest länger braucht, um Inhalte und Interessierte zusammen zu bringen. Neue Inhalte müssen regelmäßig und vergleichsweise hochfrequent (derzeit fünf tägliche Pins) verlinkt werden, damit die Pinterest-Algorithmen ein Profil einzuordnen lernen und anderen Nutzer*innen empfehlen. Eine Einsicht aus der Analyse der Pinterest-Daten ist, dass hier insbesondere Nutzerinnen erreicht werden können. Die Vermittlung von forTEXT-Inhalten über YouTube läuft derzeit erst etwa einen Monat, sodass die Zahlenwerte (140 Impressionen im September) noch relativ gering sind. Qualitative Rückmeldung zeigt aber, dass die Videos bisher vor allem im Rahmen der DH-Lehre auf Interesse stoßen.
            Bereits zu diesem frühen Zeitpunkt zeigt die Fallstudie des forTEXT-Projektes, welche Aspekte einer multimedialen, multimodalen und multipel codierten Vermittlungsstrategie sich als produktiv erweisen. Die Vorannahme, dass auf Twitter vor allem die eigene Community erreichbar ist, hat sich bestätigt. Hingegen deutet der Gender-Gap auf Pinterest an, dass hier weniger Zielgruppe 2, sondern eher Zielgruppe 1 erreicht werden kann, da vor allem die Zielgruppe der Studierenden geisteswissenschaftlicher Fächer meist überwiegend weiblich ist. Aus Feedback zu den forTEXT-YouTube-Videos konnten wir erfahren, dass diese derzeit vor allem für Lehrende von Interesse sind. Neben den vor allem im Marketing üblichen Relevanzkriterien von Impressionen, Engagement und Interaktion (die auch für die wissenschaftliche Impactmessung fruchtbar gemacht werden können, vgl. Herb/Beucke 2013) ist für die Vermittlung von DH-Methoden daher die tatsächlich erreichte Zielgruppe und deren Nutzungsmotivation relevant. So kann forTEXT am Ende nicht nur selbst Social Media produktiv nutzen, sondern auch aufzeigen, welche Medien für welche Ziele der Vermittlung von DH-Methoden besonders bedeutend sind.
         
      
      
         
            
               Bibliographie
               
                  Barthes, Roland (1976): 
                  S/Z. Frankfurt am Main: Suhrkamp.
                    
               
                  Brossard, Dominique (2013): 
                  New media landscape and the science information consumer, 
                        in: PNAS 110 (3), 14096–14101. 
                        
                     https://www.pnas.org/content/pnas/110/Supplement_3/14096.full.pdf
                  , [Zugriff 21.12.2018]. 
                    
               
                  Bucher, Hans-Jürgen (2007): 
                  Textdesign und Multimodalität. Zur Semantik und Pragmatik medialer Gestaltungsformen, 
                        in: 
                        Roth, Kersten Sven / Spitzmüller, Jürgen (eds.):
                  Textdesign und Textwirkung in der massenmedialen Kommunikation. 
                        Konstanz: UVK, 49–76. 
                    
               
                  Eco, Umberto (1985): 
                  Einführung in die Semiotik. 
                        München: Fink.
                    
               
                  Herb, Ulrich / Beucke, Daniel (2013): 
                  Die Zukunft der Impact-Messung. Social Media, Nutzung und Zitate im World Wide Web, 
                        in: Wissenschaftsmanagement. Zeitschrift für Innovation 19 (4), 22–25. 
                        
                     https://publikationen.sulb.uni-saarland.de/bitstream/20.500.11880/23789/1/Die_Zukunft_der_Impact_Messung_fuer_Reps_fertig.pdf
                   [Zugriff: 21.12.2018].
                    
               
                  Gao, Jin / Nyhan, Julianne / Duke-Williams, Oliver / Mahony, Simon (2018): 
                  Visualising The Digital Humanities Community: A Comparison Study Between Citation Network And Social Network, 
                        in: Digital Humanities 2018. Book of Abstracts. Puentes-Bridges.
                    
               
                  Neuberger, Christian (2014): 
                  Social Media in der Wissenschaftsöffentlichkeit. Forschungsstand und Empfehlungen, 
                        in: 
                        Weingart, Peter / Schulz, Patricia (eds.): 
                  Wissen – Nachricht – Sensation. Zur Kommunikation zwischen Wissenschaft, Medien und Öffentlichkeit. 
                        Weilerswist: Velbrück, 315–368.
                    
               
                  Puschmann, Cornelius / Bastos, Marco (2015):
                  How Digital Are the Digital Humanities? An Analysis of Two Scholarly Blogging Platforms, 
                        in: PLOS ONE 10 (2): e0115035.
                        
                     https://doi.org/10.1371/journal.pone.0115035
                  
                   [Zugriff 2.10.2018].
               
               
                  Reid, Alexander (2011):
                  Social Media Assemblages in Digital Humanities: from Backchannel to Buzz, 
                        in: 
                        Wankel, Charles (ed.): 
                  Teaching Arts and Science with the New Social Media. 
                        West Yorkshire: Emerald Publishing, 321–338.
                    
               
                  Roesler, Alexander / Stiegler, Bernd (eds. 2005): 
                  Grundbegriffe der Medientheorie. 
                        Paderborn: UTB.
                    
               
                  Ross, Claire (2012): 
                  Social media for digital humanities and community engagement, 
                        in: 
                        Warwick, Claire / Terras, Melissa / Nyhann, Julianne (eds.): 
                  Digital Humanities in Practice. 
                        London: Facet Publishing.
                    
               
                  Schäfer, Mike S. (2017): 
                  Wissenschaftskommunikation online, 
                        in: 
                        Bonfadelli, Heinz et al. (eds.): 
                  Forschungsfeld Wissenschaftskommunikation. Wiesbaden: Springer. 
                        
                     https://link.springer.com/content/pdf/10.1007%2F978-3-658-12898-2.pdf
                   [Zugriff 21.12.2018].
                    
               
                  Schelhowe, Heidi (2000): 
                  Informatik, 
                        in: 
                        Braun, Christina von / Stephan, Inge (eds.): 
                  Gender-Studien. Eine Einführung. Stuttgart, Weimar: Metzler, 207–216.
                    
               
                  Scheu, Andreas M. / Volpers, Anna Maria (2017): 
                  Sozial- und Geisteswissenschaften im öffentlichen Diskurs, 
                        in: 
                        Bonfadelli, Heinz et al. (eds.): 
                  Forschungsfeld Wissenschaftskommunikation. Wiesbaden: Springer. 
                        
                     https://link.springer.com/content/pdf/10.1007%2F978-3-658-12898-2.pdf
                   [Zugriff 21.12.2018].
                    
               
                  Terras, Melissa (2012):
                  The Impact of Social Media on the Dissemination of Research: Results of an Experiment, 
                        in: Journal of Digital Humanities 1 (3).
                        
                     http://journalofdigitalhumanities.org/1-3/the-impact-of-social-media-on-the-dissemination-of-research-by-melissa-terras/
                  
                   [Zugriff 2.10.2018].
               
               
                  Voigt, Kristin (2012): 
                  Informelle Wissenschaftskommunikation und Social Media. 
                        Berlin: Frank & Timme.
                    
            
         
      
   



      
         In diesem Poster werden die Möglichkeiten und Grenzen der Öffnung eines Forschungsprojekts für andere Forschende, aber auch für die interessierte Öffentlichkeit dargestellt. Das Open Science-Projekt 
                Handke: in Zungen dient dabei als Beispiel, anhand dessen verschiedene Aspekte der “Öffnung” von digitaler Geisteswissenschaft diskutiert werden. Nach einer kurzen Vorstellung des Projekts widmet sich der Beitrag den darin angewandten Methoden der Offenen Wissenschaft, aber auch den Anforderungen, die diese mit sich bringen sowie der Frage danach, wie und wozu Offene Wissenschaft konsequent umgesetzt werden kann. 
            
         Seit den beginnenden 1980er Jahren haben Fremdsprachen in den Bühnentexten des österreichischen Schriftstellers Peter Handke (*1942) zunehmend an Bedeutung gewonnen. In den frühen, sprachkritischen Stücken der 1960er und 70er Jahre spielen die Sprache, ihre Gemachtheit und die Reflexion darüber die zentrale Rolle – mit einem Umschwung, der sich am “dramatischen Gedicht” Über die Dörfer (1981) festmachen lässt, werden die Bühnenarbeiten Handkes zunehmend “erzählend” (Kastberger/Pektor 2012: 5), gewinnen zunehmend an “Handlung”. Mit dieser “Wende” (Höller 2013), mit der auch der Beginn von Handkes Tätigkeit als Übersetzer einhergeht, halten auch die fremden Sprachen Einzug in die Stücke des Autors. 
         Im vorgestellten Projekt werden sämtliche fremdsprachigen Wörter und Textteile in den beinah 30 Bühnentexten Handkes erhoben und untersucht. Die Leitfragen dabei sind, ob und in welcher Weise bestimmte Sprachen für bestimmte semantische Felder und Themenbereiche eingesetzt werden, welche Sprachen vorherrschen, ob und wie sich die Wichtigkeit einzelner Sprachen im Lauf der Zeit verändert und wie die verschiedenen einfließenden Fremdsprachen miteinander in Beziehung stehen. Für die Analyse dieser Fragen werden die relevanten Textstellen in der relationalen Datenbank 
                Handke: in Zungen gesammelt, wo sie sortier-, durchsuch- und auswertbar gemacht werden. Die Datenbank und das Projekt, in dessen Rahmen sie entsteht, sind der Offenen Wissenschaft verpflichtet und dienen daher als Ausgangspunkt für den geplanten Beitrag.
            
         Die Umsetzung eines Offenen Ansatzes in Forschungsprojekten bringt eine Reihe an Themen mit sich, mit denen sich Forschende der traditionellen Geisteswissenschaften nicht vorrangig beschäftigen müssen, die aber in den Digital Humanities von zentraler Bedeutung sind. Zu diesen gehören etwa die Frage nach offener Lizenzierung von Daten, Code und Forschungsergebnissen wie Aufsätzen und Präsentationen, aber auch jene nach deren (langfristiger) Aufbewahrung und Verfügbarmachung, nach adäquater Dokumentation und nach Kommunikation und Vermittlungsarbeit. Das 
                Handke: in Zungen-Projekt eignet sich für eine Diskussion dieser verschiedenen Aspekte von Open Science deshalb in besonderer Weise, weil es unter anderem dank Unterstützung durch Wikimedia Deutschland im Rahmen des Wikimedia-Fellowship-Programms zu Freiem Wissen und Offener Wissenschaft umgesetzt wurde. Aus diesem Grund hat das Projekt neben der eigentlichen Datenbank-Web-App mehrere online-Präsenzen, die zur Öffnung des Projekts und des darin gesammelten Wissens beitragen: Zwei GitHub-Repositories machen Projekt-Informationen und -Logbuch sowie den Code der Web-App verfügbar, eine Wikiversity-Seite versammelt den Datenmanagementplan sowie alle weiteren relevanten Informationen und Berichte zum Projekt, in einer offenen Zotero-Gruppe sind die Quellenangaben der bearbeiteten Primärtexte verfügbar und auf einem Twitter-Account werden alle Interessierten über Neuigkeiten aus dem Projekt auf dem Laufenden gehalten.
            
         Die zahlreichen Kommunikations- und Distributionskanäle, die von diesem Projekt bespielt werden, werden in diesem Beitrag vorgestellt und ihre jeweils spezifischen Vor- und Nachteile diskutiert. Ebenso werden die Voraussetzungen und Rahmenbedingungen des Projekts (rechtliche Voraussetzungen, Personal- und Zeitressourcen), die seinen Grad an Öffnung beeinflusst haben, zum Thema gemacht. Ebenfalls thematisiert werden die Notwendigkeit und Rolle von Publikumsveranstaltungen in Offenen Forschungsprojekten. Diese Bereiche werden dabei den Aktionsfeldern von Open Science zugeordnet, wie sie das Open Science Network Austria OANA definiert (Open Access, Open Research Data, Open Evaluation, Citizen Science, Open Methodology).
         Es soll dabei vorgeschlagen werden, “Open Science” nicht als eine strikt definierte Methode mit einem fixen Satz an verpflichtenden Elementen der Öffnung zu verstehen. Vielmehr sollte “Open” als eine Skala gesehen werden, auf der Projekte, die offene Methoden anwenden wollen, den für sie jeweils angemessenen Platz finden müssen, der von den oben erwähnten Rahmenbedingungen mitbestimmt wird. Grundsätzlich jedoch, so das abschließende Argument dieses Beitrags, sollte sich die geisteswissenschaftliche Forschung - insbesondere die digitale - konsequent auf ihre eigene Öffnung hin orientieren. Dafür sprechen neben praktischen auch ideologische Argumente. So formulieren es auch Pomerantz und Peek in ihrem Aufsatz 
                Fifty shades of open, in dem die gesellschaftliche Bedeutung von Offener Wissenschaft thematisiert wird: “As the number of open resources of all types increases, the more open resources will be created using them and derived from them, and the more open resources there will be. This snowballing growth of openness is socially beneficial, and, we believe, will make the world a better place.” (Pomerantz/Peek 2016)
            
      
      
         
            
               Bibliographie
               Höller, Hans: Eine ungewöhnliche Klassik nach 1945. Das Werk Peter Handkes. Berlin: Suhrkamp 2013.
               Kastberger, Klaus / Pektor, Katharina: Vorwort, in: Dies. (Hg.): Die Arbeit des Zuschauers. Peter Handke und das Theater. Salzburg/Wien: Jung und Jung 2012.
               Open Science Network Austria (OANA): Über Open Science. 
                        https://www.oana.at/ueber-open-science/
               
               
                  Pomerantz, Jeffrey / Peek, Robin:
                  Fifty shades of open, 
                        in: 
                        First Monday 4/2016. 
                        http://firstmonday.org/ojs/index.php/fm/article/view/6360/5460
               
               Ressourcen zum Projekt:
               Web-App: 
                        https://handkeinzungen.acdh.oeaw.ac.at/
               
               Projekt-Logbuch: 
                        https://github.com/vanyh/handkeinzungen
               
               Github-Repository: 
                        https://github.com/vanyh/handkeinzungen-app
               
               Wikiversity-Seite: 
                        https://de.wikiversity.org/wiki/Wikiversity:Fellow-Programm_Freies_Wissen/Einreichungen/Dramatische_Sprachen:_Fremdsprachen_in_den_B%C3%BChnentexten_von_Peter_Handke
               
               Zotero-Gruppe: 
                        https://www.zotero.org/groups/1840645/peter_handke_stage_texts
               
               Twitter-Account: 
                        https://twitter.com/HandkeinZungen
               
            
         
      
   



      
         
            Traditionelle und digitale Arbeitsweisen
            Die Anwendung computergestützter Verfahren in den Geistes- und Kulturwissenschaften prägt seit geraumer Zeit die Entwicklung unterschiedlicher Fachdisziplinen (vgl. Thaller 2012). Neue Methoden bahnen sich ihren Weg in den Methodenkanon ganz unterschiedlicher Domänen (vgl. Sahle 2015). Wie aber kann man Lehrenden – mit den unterschiedlichen Ansprüchen universitär Dozierender oder Lehrender an Schulen – einen möglichst niedrigschwelligen, aber dennoch wissenschaftlich seriösen Zugang zu dem Repertoire digitaler Methoden der Texterforschung eröffnen, das zum Spektrum der Digital Humanities zählt? Wie kann man sowohl Begeisterung wie kritische Kompetenz im konkreten Umgang mit Verfahren der digitalen Textanalyse so vermitteln, dass die Alltagspraxis des Lehrens und Forschens davon profitiert? Man muss nicht immer gleich einen theoretischen „Paradigmenwechsel" ausrufen, sondern kann das „neue” Feld besser zunächst im „hands-on"-Modus erschließbar machen. Durch einen niedrigschwelligen Disseminationsansatz entsteht die Möglichkeit, dass alte Fragen und neue Methoden sinnvoll aufeinander bezogen werden können (vgl. etwa Horstmann / Kleymann 2019).
            Das im November 2017 an der Universität Hamburg gestartete DFG-Projekt forTEXT (https://fortext.net) entwickelt vor diesem Hintergrund Strategien zur Dissemination digitaler Verfahren für die Arbeit mit Texten (vgl. Horstmann / Jacke / Meister 2018). In den auf der projekteigenen Webseite als Open-Access-Publikationen bereitgestellten zitierfähigen Besprechungen von Routinen, Ressourcen und Tools werden sämtliche Phasen eines literaturwissenschaftlichen Forschungsprojekts abgedeckt. Das Projekt leistet damit die Übersetzungsarbeit zwischen literaturwissenschaftlichen Fragestellungen und technischem Know-how, die für die Vermittlung digital gestützten Arbeitens an traditionellere Geisteswissenschaftlerïnnen notwendig ist.
                
         
         
            Routinen
            In der Rubrik Routinen stellen wir einführende Einträge zu digitalen 
                    Methoden der Textdigitalisierung, -annotation, -analyse, -visualisierung, -präsentation etc. zur Verfügung, in denen neben Definition, Diskussion und technischen Hintergründen stets auch die literaturwissenschaftliche Tradition der jeweiligen Methode betont wird. In 
                    Lerneinheiten zum Selberlernen werden Nutzerïnnen schrittweise an die Umsetzung der vorgestellten Methode in Kombination mit der Anwendung eines konkreten Tools (vgl. Abschnitt 4) und ausgewählter Ressourcen (vgl. Abschnitt 3) herangeführt. Die 
                    Lehrmodule bieten ebenfalls in Verbindung mit konkreten Ressourcen und Tools die Möglichkeit, das bereitgestellte Material in die eigene universitäre Lehrveranstaltung zu integrieren. Es werden zudem Unterrichtsmaterialien für den schulischen Unterricht erarbeitet, die durch eine noch erhöhte Komplexitätsreduktion Routinen der digitalen Literaturerforschung zugänglich machen und dezidiert an fachliche und KMK-Lernziele anknüpfen.
                
         
         
            Ressourcen
            Ausgewählte und etablierte deutschsprachige 
                    Textsammlungen, die sinnvoll mit den besprochenen Routinen der digitalen Literaturwissenschaft kombiniert werden können, stellen wir nicht nur vor, sondern ordnen und bewerten diese entsprechend ihrer thematischen Schwerpunkte. Die einzelnen Einträge folgen dabei einem wiedererkennbaren Schema, sodass insgesamt eine schnelle und bedarfsgerechte Orientierung ermöglicht wird. In der Kategorie Ressourcen bieten wir außerdem Tutorial-
                    Videos, die digitale Methoden anhand ausgewählter Tools Schritt für Schritt als Screencasts erklären und Video-Fallstudien, die literaturwissenschaftliche Fragestellungen beispielhaft mithilfe digitaler Tools bearbeiten und vorstellen. Außerdem enthält die Ressourcen-Kategorie auf literaturwissenschaftlichen Theorien basierende 
                    Tagsets und ein umfangreiches 
                    Glossar mit Erläuterungen zu Standardbegriffen der DH.
                
         
         
            Tools
            Für jede vorgestellte Methode stellen wir mindestens ein Tool vor, das für die praktische Umsetzung dieser Methode eingesetzt werden kann. Die Tools werden bedarfsgerecht hinsichtlich ihrer Funktionalität, Anwendungsfreundlichkeit, Nutzerbetreuung, Datensicherheit, Nutzungsbedingungen und des Grads ihrer Etablierung im wissenschaftlichen Diskurs befragt. Die Tooleinträge folgen – wie auch die einzelnen Beitragsformate in den Kategorien Routinen und Ressourcen – einem wiedererkennbaren Schema, in dem konkrete Fragen aus Nutzerïnnenperspektive gestellt und beantwortet werden.
         
         
            CATMA 6
            Mit der Entwicklung der sechsten Version von CATMA (https://catma.de) hat forTEXT im Oktober 2019 neue Funktionen, eine projektzentrierte Arbeitsstruktur und ein vollständig überarbeitetes, intuitiver nutzbares Interface des webbasierten, kollaborativ nutzbaren Annotations- und Analysetools (derzeit weltweit gut 13.000 Accounts) zur Verfügung gestellt. Das Tool integriert sich durch seine nutzerïnnenfreundliche Zugänglichkeit und die Konzentration auf die Methode der manuellen Annotation sowie der Analyse und Visualisierung von Text- wie Annotationsdaten in das forTEXT-Disseminationmodell und orientiert sich an den Bedarfen textwissenschaftlicher Fachwissenschaften.
                
         
         
            Nicht-digitale und digitale Dissemination
            Das Projekt wird durch umfangreiche Maßnahmen der nicht-digitalen Dissemination seiner Inhalte begleitet. Einerseits bieten die Projektmitarbeiterïnnen bedarfsgerechte Workshops und Vorträge für Forschungsgruppen oder Veranstaltungsreihen an Universitäten und auf Konferenzen an. Darüber hinaus werden schulinterne Workshops durchgeführt, die auf die z. T. sehr unterschiedliche technische Infrastruktur vor Ort eingehen und sich in der inhaltlichen Ausrichtung ebenfalls eng an der spezifischen Bedarfslage der Teilnehmerïnnen orientieren.
            Die umfangreiche Social-Media-Strategie von forTEXT (vgl. Horstmann / Schumacher 2019) ist ein essentieller Teil des gesamten Dissiminationsprogramms: Auf Twitter, Youtube, Facebook und Pinterest treten wir in unterschiedlichen Modi mit diverse Zielgruppen in Kontakt und führen diese in die digitale Arbeit mit Texten ein. So tritt forTEXT nicht nur an neue Nutzerïnnengruppen heran, sondern integriert sich auch selbst im fachwissenschaftlichen/DH-Diskurs.
         
         
            Individualisiertes Empfehlungssystem
            Im Januar 2020 wird ein digitales Empfehlungssystem implementiert, das im Frage-Antwort-Schema die Projekte der Nutzerïnnen so klassifiziert, dass die automatische Generierung individualisierter Empfehlungen von Routinen, Ressourcen und Tools zur Bearbeitung der jeweiligen Fragestellung möglich sein wird. Das Empfehlungssystem wird somit dafür sorgen, dass die einzelnen Bereiche von forTEXT einerseits zusammengefasst, andererseits aber auch bedarfsorientiert und effektiv durch sie navigiert werden kann. Das System macht damit insbesondere Nutzerïnnen ohne vorherige DH-Erfahrung den Einstieg in digitale Methoden zur Unterstützung ihrer Projekte individuell möglich.
         
      
      
         
            
Von den derzeit 13.033 Accounts wurden 3030 nur einmalig benutzt und 1876 waren Guest-Accounts, sodass man von 8127 Nutzerïnnen ausgehen kann (Stand: Dez. 2019).

         
         
            
               Bibliographie
               
                  Horstmann, Jan / Jacke Janina / Meister, Jan Christoph (2018): „Digital vs. Humanities. Didaktische Aufbereitung digitaler Methoden für die klassischen Geisteswissenschaften im Projekt forTEXT“, in: 
                        Kritik der digitalen Vernunft. DHd 2018 Köln. Konferenzabstracts, 386–391. 
                        http://dhd2018.uni-koeln.de/wp-content/uploads/boa-DHd2018-web-ISBN.pdf [Zugriff: 26. August 2019].
                    
               
                  Horstmann, Jan / Schumacher, Mareike (2019): „Social Media, YouTube und Co: Multimediale, multimodale und multicodierte Dissemination von Forschungsmethoden in forTEXT“, in: Sahle, Patrick (ed.): 
                        DHd 2019. Digital Humanities: multimedial & multimodal. Konferenzabstracts, 207–211. DOI: 
                        
                     10.5281/zenodo.2596095
                  .
                    
               
                  Horstmann, Jan / Kleymann, Rabea (2019): „Alte Fragen, neue Methoden – Philologische und digitale Verfahren im Dialog. Ein Beitrag zum Forschungsdiskurs um Entsagung und Ironie bei Goethe“, in: 
                        Zeitschrift für digitale Geisteswissenschaften DOI: 
                        
                     10.17175/2019_007
                  .
                    
               
                  Sahle, Patrick (2015): „Digital Humanities? Gibt’s doch gar nicht!”, in: Baum, Constanze / Stäcker, Thomas (eds.): 
                        Grenzen und Möglichkeiten der Digital Humanities. Sonderband der Zeitschrift für digitale Geisteswissenschaften, 1. DOI:
                         
                  
                     10.17175/sb001_004
                  .
                    
               
                  Thaller, Manfred (2012): „Controversies around the digital humanities: an agenda”, in: 
                        Historical Social Research, 
                        37(3): 7–23.
                    
            
         
      
   



      
         
            Die Digitalisierung verändert die Bedingungen für die Produktion, Distribution und Rezeption und damit auch für die Erforschung von Literatur. In den Digital Humanities stehen dabei bislang insbesondere die neuen Möglichkeiten der digitalen Auswertung (Distant/Scalable Reading) und die Digitalisierung vorhandener Druckbestände im Zentrum der Aufmerksamkeit. Die veränderten medialen Bedingungen führen jedoch nicht nur zu einer Übersetzung von gedruckten Texten in digitale Objekte, sondern bringen selbst produktiv neue literarische Formen und Gattungen hervor, für die computergestützte Elemente konstitutiv sind. Hierzu zählen etwa literarische Hypertexte, Blog-Formate, computergestützte kollektive und kollaborative Projekte, literarische Tweets und Twitter-Bots, Texte und Textgeneratoren, die auf computerlinguistische Methoden setzen, schließlich auch frühere Formen computergestützter Literaturproduktion wie der Poesieautomat von Hans Magnus Enzensberger oder die 
                Stochastischen Texte von Theo Lutz. (Rettberg 2019, Suter 2012, Tomaszek 2011, Lutz 1959). Hinzu kommen im Bereich Literaturforschung und -archive zunehmend digitale Vor- und Nachlässe, die eine Vielzahl von unterschiedlichen Datenträgern und Datenformaten beinhalten.
            
         Das jüngst ins Leben gerufene interdisziplinäre 
                Science Data Center für Literatur (SDC4Lit) hat sich das Ziel gesetzt, die Anforderungen, die Digitale Literatur an ihre Archivierung, Erforschung und Vermittlung stellt, systematisch zu reflektieren und entsprechende Lösungen für einen nachhaltigen Datenlebenszyklus Literatur langfristig umzusetzen.
            
         Für die Archivierung, Analyse und Vermittlung von Digitaler Literatur wird eine Forschungsplattform entwickelt. Da eine solche Plattform nur in der interdisziplinären Zusammenarbeit zu bewerkstelligen ist, sind im Projekt Partner mit unterschiedlichen Expertisen in den einzelnen Teilbereichen vereint, nämlich das Deutsche Literaturarchiv Marbach, das Höchstleistungsrechenzentrum Stuttgart, sowie das Institut für Maschinelle Sprachverarbeitung und die Abteilung Digital Humanities der Universität Stuttgart.
         Die born-digital Bestände des Deutschen Literaturarchivs bestehen zum einen aus digitalen Nachlässen und zum anderen aus archivierten netzliterarischen Werken. Der umfangreichste digitale Nachlass am Deutschen Literatuarchiv ist von Friedrich Kittler und umfasst 1,5 Millionen Dateien. Zur deutschsprachigen Netzliteratur können weitaus weniger Objekte gezählt werden. Netzliteratur ist durch Verlinkungen und Multimedialität geprägt. Das erschwert die Definition von Objektgrenzen und führt zu nichtlinearen Objektstrukturen, die in der Rezeption nichtlineare Handlungen ermöglichen
         Zum einem scheinen sich diese Texte also zur Anwendung computergestützter und computerlinguistischer Methoden besonders anzubieten, da sie genuin in elektronischer Form vorliegen. Zum anderen bringt gerade diese Form für ihre Archivierung und Bereitstellung eine Reihe von besonderen Anforderungen mit sich.
         Digitale Nachlässe sind aufgrund großer Mengen an Daten ohne computergestützte Methoden kaum erschließbar und zugänglich zu machen. Um auf diese wachsende Herausforderung in Archiven und Bibliotheken einzugehen, soll der Einsatz von Methoden der Digital Humanities für die inhaltliche Erschließung textbasierter born-digital Bestände erprobt werden. Wenn digitale Nachlässe bereits obsolete Dateiformate enthalten, sind diese nicht ohne vorherige Formatmigration für aktuelle computergestützte Analysen zugänglich.
         Auch literarische Webseiten sind von den hochfrequenten Erneuerungszyklen digitaler Technik betroffen. Weiterentwicklungen der Betriebssysteme, der Browser, des HTML-Standards und gängiger Webtechnologien können zu fehlerhafter Darstellung oder fehlenden Funktionen einer Webseite führen. Um ein Werk der Netzliteratur dokumentieren zu können, sind daher neue Formen der Modellierung von Texten, die über eine bloß lineare Form hinausgehen, gefragt.
         Diese und weitere Bestände sollen mit modernen digitalen Methoden erschlossen, erforscht und vermittelt werden können. Im Zentrum stehen daher der Aufbau verteilter langzeitverfügbarer Repositories für Digitale Literatur inklusive Forschungsdaten und die Entwicklung der SDC4Lit-Forschungsplattform. Die Repositories werden vom Projekt und seinen Kooperationspartnern regelmäßig erweitert und bilden den zentralen Speicher für das Harvesting von Netzliteratur und weiteren Formen elektronischer Literatur im künftigen Betrieb des SDC. Die Forschungsplattform bietet die Möglichkeit zum computergestützten Arbeiten mit den Beständen der Repositories.
         Bereits entwickelte oder in der Entwicklung befindliche Ansätze zur Archivierung und Bereitstellung von WARC-Archiven (Lin et al. 2017), Textkorpora (Fischer et al. 2019) und Analysefunktionen (Hinrichs et al. 2010) sowie strukturierte Reflexionen eigener Strategien (Kramski, von Bülow 2011) weisen auf eine modulare und integrierte Lösung bei der Bereitstellung von Daten und Services. Die entsprechend geplante modulare Architektur der bereitgestellten Services ermöglicht eine nachhaltige Integration von Repositories und Analysemethoden sowie die Möglichkeit zur späteren bedarfsorientierten Einbindung von Korpora und Analysewerkzeugen.
         Für die Entwicklung des Repositories und der Forschungsplattform ist der Kontakt zu an der Herstellung, Verbreitung, Erforschung und Vermittlung von elektronischer Literatur beteiligten Communities ein entscheidendes Element. Diese Beteiligung wird über einen mehrköpfigen Beirat und Outreach-Maßnahmen wie Workshops, Seminare und die Arbeit mit Fokusgruppen erreicht. Eine wichtige Aufgabe des Projekts ist in diesem Zusammenhang die Modellierung von Formen digitaler Literatur, die zunächst beispielorientiert im Umgang mit einem bereits vorhandenen Corpus digitaler Literatur erfolgt. Daraus entstehen sowohl technische als auch gattungspoetologische Herausforderungen, etwa bei der Begriffsbildung (digitale vs. elektronische Literatur), bei der medienbezogenen Abgrenzung von digitaler und nicht-digitaler und post-digitaler Literatur, und schließlich in Bezug auf gattungspoetologische und literaturgeschichtliche Fragen zur elektronischen Literatur seit den 1950er Jahren mit einem Fokus auf den deutschsprachigen Raum und mit Blick auf internationale Entwicklungen in Literatur und Literaturforschung. (Block, 2004; Gould, 2012; Rettberg, 2019; Seiça, 2015)
         Neben digitalen Objekten und entsprechenden Metadaten wird auch ein Repository der anfallenden Forschungsdaten nachvollziehbar und nachhaltig gespeichert. Zu den Forschungsdaten zählen erstens die bei der Arbeit des SDC anfallenden Forschungsdaten, insbesondere solche, die für das Anbieten von Diensten auf der Plattform notwendig sind, etwa mittels Machine Learning errechnete Datenmodelle für an das Corpus angepasste computerlinguistische Analysewerkzeuge (Eigennamenerkenner, Parser, Topic Models etc.). Zweitens soll das Repository die Möglichkeit bieten, die von Nutzer*innen der Forschungsplattform generierten Forschungsdaten strukturiert zu speichern und für die weitere Forschung zur Verfügung zu stellen, etwa Annotationen oder ergänzte Metadaten zu einzelnen Objekten oder zu Objektklassen.
         Die Sammlung, Bereitstellung, Erforschung und Vermittlung von Literatur im medialen Wandel ist eine Aufgabe, die Forschung und Archive gleichermaßen betrifft. SDC4Lit verfolgt deshalb das Ziel, diese Aufgabe und die entsprechenden Unteraufgaben interdisziplinär zu bearbeiten.
      
      
         
            
      Deutsches Literaturarchiv Marbach: Literatur im Netz, 
               , Zugriff 20.9.2019.
    
         
         
            
               Bibliographie
               
                  
                  Block, Friedrich W.  (2004): 
	p0es1s. Ästhetik digitaler Poesie = The aesthetics of digital poetry. Erscheint anlässlich der Ausstellung "p0es1s. Digitale Poesie" im Kulturforum Potsdamer Platz, Berlin, 13. Februar bis 4. April 2004. Ostfildern: Hatje Cantz.
      
               
                  Gould, Amanda Starling  (2012): „A Bibliographic Overview of Electronic Literature“. In: 
	Electronic Literature Directory  o.V.
      
               
                  Hinrichs, Erhard W., Marie Hinrichs und Thomas	Zastrow  (2010): 
	WebLicht: Web-Based LRT Services for German, Proceedings of the ACL 2010 System Demonstrations, S. 25–29.
      
               
                  Kramski, Heinz Werner, Ulrich von Bülow
	(2011):
	„Es füllt sich der Speicher mit köstlicher Habe“ – Erfahrungen mit digitalen Archivmaterialien im Deutschen Literaturarchiv Marbach, in: Caroline Y. Robertson von Trotha. Robert Hauser (Hg.), Neues Erbe : Aspekte, Perspektiven und Konsequenzen der digitalen Überlieferung, Karlsruhe: KIT Scientific Publishing, S. 141-162.
      
               
                  Rettberg, Scott (2019): Electronic literature. Cambridge, UK: Polity Press.
      
               
                  Seiça, Álvaro (2015): 
	Um Feixe Luminoso: Uma Leitura da Coleção de Literatura Electrônica Portuguesa. Florianópolis: Universidade Federal de Santa Catarina.
      
               
                  Suter, Beat (2012): 
	Von Theo Lutz zur Netzliteratur. Die Entwicklung der deutschsprachigen elektronischen Literatur, , Zugriff am 31.12.2019.
      
               
                  Tomaszek, Patricia (2011): German Net Literature: In the Exile of Invisibility, >, Zugriff am 19.9.2019.

            
         
      
   



      
         
Spätestens mit der Herausbildung des Social Web (auch Web 2.0) seit
knapp 15 Jahren, das nicht nur für die Verteilung von Information,
sondern tatsächlich auch zur Mitgestaltung von Inhalten genutzt werden
kann, hat das Internet die gesellschaftliche Kommunikationskultur
(jedenfalls die derjenigen, die über verlässlichen Zugang verfügen und
diesen nutzen) entscheidend gewandelt. Mit ResearchGate, Academia.edu,
Mendeley und als neue, explizit nicht-kommerzielle Variante 
 HCommons
 entstanden eine Reihe sozialer Medien spezifisch für den
 wissenschaftlichen Bereich, über die Forschungsergebnisse
 ausgetauscht und bewertet werden können und mit denen v.a. der
 Kontakt zu Kolleg|inn|en aufgenommen werden kann (Sugimoto et
 al. 2016). Jenseits dieser spezialisierten sozialen Medien nutzen
 Wissenschaftler|innen auch die allgemeinen Plattformen wie Facebook
 und Twitter, letzteres vor allem, um wissenschaftlichen Diskussionen
 zu folgen, Forschung zu kommentieren und auf eigene
 Veröffentlichungen - von Ergebnissen, jedoch auch von Daten und
 Software - aufmerksam zu machen (vgl. van Noorden 2014). Über die
 allgemein gebräuchlichen sozialen Medien ist es möglich, auch Laien
 zu erreichen, sei es, um die 
eigene Reichweite zu erhöhen oder um neue Nutzerkreise zu gewinnen, die mitunter sogar am Forschungsprozess partizipieren können. Entsprechende Programme wie public engagement
oder Citizen bzw. Crowd
Science sind institutionell erwünscht (vgl. Deutsche Akademie der Technikwissenschaften et al. 2014) und innerhalb der Wissenschaften durchaus verbreitet (vgl. Franzoni & Sauermann 2014).

         
            Die öffentliche Publikation von Forschungsdaten
            
Forschung – nicht zuletzt die in den Geisteswissenschaften – generiert große Mengen an Daten, Information und Wissen, die für (Teil)Öffentlichkeiten interessant und relevant sein können. 
Nun ist die Publikation von Forschungsdaten – zusätzlich zu  den
bisher gebräuchlichen  Publikationsmedien – zwar weithin erwünscht (siehe RFII 2016), zur Zeit allerdings alles andere als weitreichend umgesetzt. 
Dafür können sehr viele unterschiedliche Ursachen ausgemacht werden
(vgl. Kaden 2018). Auf der anderen Seite bieten soziale Medien, hier
vor allem Twitter, die Möglichkeit, granulare  Informationshäppchen fein dosiert in den Timelines von Nutzer|inne|n erscheinen zu lassen und über diesen Weg deren Aufmerksamkeit zu gewinnen. 
Die Nutzung von privatwirtschaftlichen Plattformen, die vorwiegend
monetäre Interessen verfolgen,  für die Wissenschaftskommunikation ist
nicht unproblematisch. Momentan existieren allerdings schlicht keine
nicht-kommerziellen Alternativen Plattformen, über die man auf relativ
simple Weise ein ähnlich großes Publikum erreichen könnte. 

            
  Ein Twitterprojekt, das weitreichende Beachtung fand bis hin zu einem Artikel in der New York Times
, war das Projekt @9nov38 -
heute vor 75 Jahren, in dem fünf Historiker|innen 
die zeitliche Dimension in die Erzählung von Ereignissen der
Reichspogromnacht über Twitter mit einbezogen. Nun ist die manuelle
Erstellung einzelner Tweets sehr aufwendig und für größere Datensätze
eigentlich nicht ohne weiteres zu leisten. Doch im Grunde liegen die
Daten, die für derartige Projekte gesammelt wurden, im Normalfall
bereits in einem strukturierten Format vor, etwa in einer Datenbank
oder als Spreadsheet. Auf dieser Grundlage wurde nach einem Austausch
mit den am @9Nov38-Projekt beteiligten Historiker|innen auf dem Histocamp 2015 der Webservice
autoChirp entwickelt, zunächst im Rahmen eines Projektseminars, seither weiter betreut durch das Institut für Digital Humanities
 (IDH) in Köln (Hermes et al. 2017). autoChirp ist ein Webservice, der nicht auf eine spezifische Anwendung hin entwickelt wurde, sondern eine Plattform bietet, um diversen, u.a. historischen Projekten einen niedrigschwelligen Zugang zu für sie hilfreicher Technologie zu ermöglichen. In dem bewusst einfach gehaltenen Webinterface können strukturierte Daten hochgeladen werden, um sie automatisiert auf spezifizierte Zeitpunkte zu schedulen und zu veröffentlichen. Das erste Projekt, das autoChirp nutzte, war @NRWHistory, bei dem in einem Projektseminar von Düsseldorfer Historiker|innen die Entstehung des Landes NRW um 70 Jahre zeitversetzt nacherzählt wurde 
(siehe ). Kurz darauf wurde über
autoChirp mit @TiwoliChirp ein
weiterer Veröffentlichungskanal für bereits über eine Smartphone-App
veröffentlichten Forschungsergebnisse der Literaturwissenschaft
eingesetzt. 

            
Inzwischen greifen eine ganze Reihe von Projekten, die regelmäßige Tweets publizieren, auf autoChirp zurück. Das mit mehr als 4000 Followern mit Abstand reichweitenstärkste davon ist 
@Die_Reklame, mit
dem Akteure aus dem Projekt  @9Nov38 
bemerkenswerte historische Werbeanzeigen twittern. Von Interesse sind
diese, weil gerade Werbung extrem gegenwartsbezogen ist, was einen
Einblick in die entsprechende Zeit der ursprünglichen Publikation gibt
(vgl. Hoffmann 2018). Ein weiteres Projekt mit historischem Bezug ist
Verbrannte Orte  (@pictureXnet), das die Orte von Bücherverbrennungen im Dritten Reich auf einer Karte sammelt 
(siehe ) und diese an den entsprechenden Jahrestagen der Verbrennungen vertwittert. Die Twitter-Plattform hilft hier dabei, Aufmerksamkeit zu generieren und auch Daten zu den Ereignissen zu sammeln. Einen sehr ähnlichen Ansatz verfolgt das Projekt @gedenkplaetze.

            
  2019 jährte sich zum 250. Mal des Geburtstag von Alexander von Humboldt. In diesem Jahr von besonderem Interesse war daher seine Chronik 
  (siehe ), die von der Berlin-Brandenburgischen Akademie der Wissenschaften (BBAW) herausgegeben wird und inzwischen auch über autoChirp an Twitter angebunden wurde (Hermes 2017). Bemerkenswert hier ist, dass die Chronik unter den Twitter-Account @AvHChrono
  jahrestagsaktuell verfolgt werden kann, was von knapp 200 Leser|innen in Anspruch genommen wird. Diese tagesaktuelle Konsultation der Daten hat auch schon zur Feststellung von Fehlern geführt, die an die BBAW rückgemeldet und anschließend korrigiert wurden. Auch dieses Beispiel zeigt, dass Social Media keine Kommunikation auf der Einbahnstraße sein muss.

         
         
            Die Perspektive der Kunstgeschichte
            Zwei der neuesten autoChirp-nutzenden Projekte kommen aus dem Bereich der Kunstgeschichte, einer bildbasierten Wissenschaft, deren Grundlage historische visuelle Objekte sind. Daher bedeutet die Einführung digitaler Methoden in das Fach vor allem die Entwicklung von Analyseprozessen, die sich auf Bild- und Metadaten beziehen (Klinke 2018). Diese werden nicht nur in der Forschung erzeugt, sondern kommen bisher vor allem aus den Sammlungsinstitutionen (GLAM).
            
               
                  
                   Abbildung 1: Ein Tweet aus dem Fundus des ClevelandFunFacts-Twitterbots
               
            
            Museen sind einer umfangreichen Transformation unterworfen, in der sie ihre Aufgaben unter dem Vorzeichen der Digitalisierung, Social Media und Virtual Reality neu definieren müssen (Kohle 2019). So eröffnet die Publikation der Sammlungsdaten als Open Data neue Möglichkeiten, die kulturellen Artefakte in neue, zeitgenössische Zusammenhänge zu bringen, in denen sie neue Bedeutungszuschreibungen erhalten können. Durch die Verwendung von autoChirp können offene Sammlungsdaten und globale Öffentlichkeit durch das visuelle Medium Twitter zusammengebracht werden. Auch hier erlaubt Twitter nicht nur die Kommunikation in eine Richtung, sondern auch die Partizipation des Publikums durch Kommentare, Retweets und das Einbinden in neue Kontexte.
            
Zwei Beispiele aus dem Jahr 2019 machen dies deutlich: Der Tweetbot @cart_fun_facts
baut auf der Open Data-Strategie des Cleveland Museum of Art auf. Das
1916 gegründete Museum ist eines der umfassendsten Kunstmuseen der
Welt, das am 23. Januar 2019 bekannt gegeben hat, dass es sich ab
sofort als eine Open-Access-Institution betrachtet, die die
Bezeichnung Creative Commons Zero (CC0) für hochauflösende Bilder und
Daten im Zusammenhang mit ihrer Sammlung verwendet (siehe ). 
 Die Öffentlichkeit hat damit jetzt die Möglichkeit, Bilder von mehr als 30.000 gemeinfreien Kunstwerken zu kommerziellen und nichtkommerziellen Zwecke zu teilen, neu zu mischen und wiederzuverwenden. Der von Harald Klinke (LMU München) entwickelte Tweetbot verwendet die in der Datenbank befindlichen “Fun Facts”, die täglich auf Flashcards zusammen mit den Abbildungen der Kunstwerke im Format eines visuellen Memes über den autoChirp-Service getwittert werden 
(siehe Abbildung 1).

            
Ein weiteres Beispiel ist der auf der Digital Art History Summer School 2019 in Malaga (DAHSS) durch Studierende entwickelte Tweetbot 
@thyssenmlgbot. Dieser twittert die Werke des dortigen Museum Carmen Thyssen unter Zuhilfenahme von NLP-Techniken und autoChirp, wodurch die Beschreibungstexte auf relevante Topics untersucht und diese in Hashtags umgewandelt werden. Dieses Projekt hat einerseits gezeigt, wie Studierende mithilfe von digitalen Kompetenzen einer GLAM-Institution helfen können, ihre Werke einer breiteren Öffentlichkeit zu vermitteln. Andererseits, wie diese Vermittlung einen Rückkanal erhalten kann, der es dem Publikum erlaubt, auf die Werke zu reagieren (beispielsweise durch in die Tweets integrierte Frage nach der vermuteten Entstehungszeit des Werks). Auf diese Weise können auch Werke, die üblicherweise nicht in der Ausstellung gezeigt werden, sondern im Depot verbleiben, sichtbar gemacht werden. Ein Online-Tool wie autoChirp ist dafür ein Hilfsmittel, das einen niederschwelligen Zugang zu neuen Formen der digitalen Museumskommunikation ermöglicht und deshalb gerade auch in der Lehre eingesetzt werden kann.

         
         
            autoChirp und autoPost
            
Das IDH betreibt inzwischen neben autoChirp zur automatisierten Veröffentlichung auf Twitter auch autoPost
für analoge Aufträge für Facebook-Seiten. Beide Services basieren auf
Spring, einem quelloffenen Java-Framework für Web-Anwendungen.
Der Quellcode ist unter Open Source-Lizenz (Eclipse
Public Licence) auf GitHub beziehbar (siehe 
 und ),
so dass eigene Services betrieben werden können. Das IDH stellt aber
auch beide Services für alle Interessierten zur Verfügung (siehe 
https://autochirp.spinfo.uni-koeln.de
und ). Bei der Implementation wurde vor allem auf Modularität und Erweiterbarkeit geachtet, um das Programm ohne größeren Aufwand auf weitere Social Media Plattformen, wie z.B. Instagram portieren zu können, sofern diese eine entsprechende API (Application-Programming-Interface) anbieten.

            
               
                  
                   Abbildung 2: Screenshot des autoPost-Services, mit dem große
  Mengen von geplanten Facebook-Posts realisiert  werden können (hier zum Tiwoli-Projekt).
               
            
            
               Bei der Datenpersistenz wurde bei autoPost auf eine schwergewichtigere, aber performantere Datenbank gesetzt, da die Erfahrung mit autoChirp gezeigt hat, dass ein freier Scheduling-Service sehr gut angenommen wird und die Zahl der Datenbankeinträge dementsprechend groß werden kann. Um Nutzer|inne|n von autoChirp die Möglichkeit zu bieten, ihre Inhalte, die in autoChirp schon geplant sind, auch auf Facebook zu veröffentlichen, wurde für autoChirp eine Export-Funktion angelegt. Tweets können gruppenweise als TSV-Datei heruntergeladen und in autoPost als Facebook Posts importiert werden.
                
         
         
            Zwischenfazit zum Nutzerzuspruch
            Während autoChirp schon seit 2016 läuft und für knapp 150 Nutzer|innen-Accounts bereits über 17.500 Tweets veröffentlicht hat (weitere 10.000 Tweets sind terminiert, aktuelle Zahlen erhält man über die 
Statistik-Seitedes
Services), startete autoPost erst im Herbst 2019.
Mit 
                  Syrian Modern History
                und

                  Public History Weekly
                konnten aber bereits zwei wissenschaftlich betreute Accounts mit kombiniert über 36.000 Facebook-Abonnent|inn|en gewonnen werden, die autoPost täglich zur Bewerbung von Archiv-Artikeln nutzen. 

            Die Services autoChirp und autoPost sind Beispiele, an denen sich eine der wichtigen Aufgaben für die Digital Humanities spezifizieren lässt: Die Entwicklung erfolgte, weil Wissenschaftler|innen (nicht nur) aus den Geisteswissenschaften einen Bedarf hatten, ihre Daten auf Social Media Plattformen zu teilen. Dafür benötigten sie Tools, die eine niedrige Einstiegsschwelle haben und ihnen dabei Arbeit abnehmen können, wenn sie Aspekte ihrer Forschung öffentlich sichtbar machen und Studierende sowie die interessierte Öffentlichkeit in den Forschungsprozess (hier zuvorderst: In die Datensammlung) einbinden wollen. Insofern verstehen wir die Entwicklung von autoChirp und autoPost als Hilfsmittel zur Etablierung einer offenen, transparenten und partizipativen Wissenschaft (Open Science). Die Erfahrungen mit den hier vorgestellten Tools zeigt, dass die Methoden sowohl von den Wissenschaftler|inne|n, als auch vom Publikum angenommen werden und mithin das Potenzial haben, den Geisteswissenschaften eine größere Präsenz in der Öffentlichkeit zu ermöglichen und damit eine höhere Relevanz in der Gesellschaft zu erzielen.
         
      
      
         
            
               Bibliographie
               
                  Deutsche Akademie der Technikwissenschaften / Union der Deutschen Akademien der Wissenschaften / Deutsche Akademie der Naturforscher Leopoldina 
  (2014): “Zur 
  Gestaltung der Kommunikation zwischen Wissenschaft, Öffentlichkeit und den Medien. Empfehlungen vor dem Hintergrund aktueller Entwicklungen.” München: acatech – Deutsche Akademie der Technikwissenschaften e.V. / Mainz: Union der Deutschen Akademien der Wissenschaften e.V. / Halle (Saale): Deutsche Akademie der Naturforscher Leopoldina e.V. – Nationale Akademie der Wissenschaften.

               
                  Fischer, Frank / Strötgen, Jannik (2015): 
  „When Does German Literature Take Place? – On the Analysis of Temporal Expressions in Large Corpora”, in: 
  Proceedings of DH2015, Sydney: Alliance of Digital Humanities Organisations.

               
                  Franzoni, Chiara / Sauermann, Henry 
  (2014): “Crowd science: The organization of scientific research in open collaborative projects”, in: 
  Research Policy, Amsterdam: Elsevier Volume 43/1, 1-20.

               
                  Hermes, Jürgen
  (2017): „Neu: Alex von Humboldt auf Twitter!", in 
  TEXperimenTales, 17/08/2017,
  .
  [letzter Zugriff 18.12.2019]

               
                  Hermes, Jürgen / Hoffmann, Moritz / Eide, Øyvind / Geduldig, Alena / Schildkamp, Philip
  (2017): „Twhistory with autoChirp" in: 
  DHd 2017 Bern – Digitale Nachhaltigkeit. Abstractband. Bern: DHd, 277ff.

               
                  Hoffmann, Moritz
  (2018): „Von Funden und Schwellen: Die Reklame“, 
  
  [letzter Zugriff 18.12.2019]

               
                  Kaden, Ben
  (2018): „Warum Forschungsdaten nicht publiziert werden“, in: 
  LIBREAS, Library Ideas  33.

               
                  Klinke, Harald
  (2018): „Daten­analyse in der Digitalen Kunstgeschichte. Neue
  Methoden in Forschung und Lehre und der Einsatz des DHVLab in der
  Lehre”, in: Harald Klinke (Hg.): 
  #DigiCampus. Digitale Forschung und Lehre in den Geisteswissenschaften, München, 2018, S. 19-34.

               
                  Kohle, Hubertus
  (2019): 
  Museen digital. Eine Gedächtnisinstitution sucht den Anschluss an die Zukunft. Heidelberg: Heidelberg University Publishing.

               
                  RfII – Rat für Informationsinfrastrukturen 
  (2016): Leistung aus Vielfalt. Empfehlungen zu Strukturen, Prozessen und Finanzierung des Forschungsdatenmanagements in Deutschland, Göttingen. URL: 
   [letzter Zugriff 18.12.2019]

               
                  Schwarz, Ingo
  (2019): „Zur Alexander von Humboldt-Chronologie”, in Ottmar Ette
  (Hg): 
  edition humboldt digital, hg. v. Berlin-Brandenburgische Akademie der Wissenschaften, Berlin. Version 5 vom 11.09.2019. URL: 
   [letzter Zugriff 18.12.2019]

            
         
      
   



      
         
            Vortragende
            
               David Lassner
               
                  Master Informatik David Lassner, Doktorand an der TU Berlin im Bereich Maschinelles Lernen für Digital Humanities, insbesondere für quantitative Literaturanalyse. 
                  
                  lassner@tu-berlin.de
               
            
            
               Stephanie Brandl
               
                  Dipl. Math. Stephanie Brandl, Technische Universität Berlin. Forschungsschwerpunkte: Maschinelles Lernen, Natural Language Processing. 
                  
                  stephanie.brandl@tu-berlin.de
               
            
            
               Louisa Guy
               
                  Louisa Guy, Doktorandin, Le Mans Université. Forschungsinteressen: Digitale Textanalyse, Anwendung von Methoden der Computerlinguistik auf sozialwissenschaftliche Kontexte. 
                  
                  louisa.guy.etu@univ-lemans.fr
               
            
            
               Anne Baillot
               
                  Prof. Dr. Anne Baillot, Le Mans Université. Forschungsschwerpunkte: Digitale Philologie, Digital Humanities, Translation Studies. 
                  
                  anne.baillot@univ-lemans.fr
               
            
         
         
            Anforderungen
            Maximalanzahl Teilnehmender: 25
             
             Räumliche Anforderungen:
    
                   Beamer
                  Whiteboard/Tafel 
                   Stromversorgung für Laptops der Teilnehmenden
                   Wifi
               
            
             
            
               Anforderungen an die Teilnehmenden:
               Wir erwarten, dass die Teilnehmenden ihre eigenen Laptops mitbringen, die bestenfalls schon die nötige Software vorinstalliert haben. Wir werden kurz vor der Konferenz eine Willkommens-E-Mail mit den Softwareanforderungen verschicken. Die praktischen Sitzungen werden mithilfe von Jupyter Notebooks (Python3, Jupyter) abgehalten. Wir planen zusätzlich als Absicherung einen Online-Zugang zu einem JupyterHub Server mit vorinstallierten Paketen für Teilnehmende, bei denen die Installation Schwierigkeiten macht. Die praktischen Sitzungen sind so konzipiert, dass nur sehr geringe, bis gar keine Programmierkenntnisse notwendig sind. Im Wesentlichen sollen die Teilnehmenden die Parameter und Eingabedaten der vorgegebenen Programme modifizieren, Teilnehmende mit mehr Programmierkenntnissen ermutigen wir natürlich tiefer in die Programme einzusteigen und auch diese zu modifizieren.
            
         
         
            Beschreibung
            Der Workshop besteht aus einem allgemeineren Teil zu Bias im Maschinellen Lernen, in dem grundlegend in die Thematik eingeführt wird, und einem spezifischeren Teil, in dem ML-Biases im Kontext von DH behandelt werden. Beide Teile beinhalten Vortrags- sowie Mitmachsessions. Ziel des Workshops ist es, dass die Teilnehmenden sich des Problems von Bias in Machine Learning Modellen bewusst werden und die grundlegenden Techniken zur Erkennung und zur Unterdrückung von Biases kennenlernen. Es soll außerdem gemeinsam erarbeitet werden, auf welche Weise DH-ForscherInnen mit den Biases umgehen können - denn in vielen Anwendungen sind diese nicht gewünscht: Ein System zur Vorauswahl von Bewerbern sollte Männer nicht bevorzugen, ein Modell zur Gesichtserkennung sollte keinen Unterschied in der Genauigkeit haben, weil sich die Hautfarbe der Personen auf den Bildern ändert (Buolamwini et al. 2018), und ein Modell zur Erkennung von Hate-Speech im Internet sollte nicht kontextfrei bspw. Begriffe wie “homosexuell” als toxisch einstufen.
            
            Gleichzeitig können Biases in ML Modellen erwünscht sein, wenn man beispielsweise die Veränderung von Biases in Sprache analysiert.
            Teilnehmende werden im Vorfeld ermutigt eigene Daten mitzubringen, mit denen sie im zweiten praktischen Teil experimentieren können.
            Das Workshopprogramm wird online unter 
    bias-ml-dh.davidlassner.com öffentlich zur Verfügung gestellt und die Kursmaterialien auf Github unter 
    github.com/millawell/bias-ml-dh veröffentlicht. Dort sollen die Teilnehmenden auch schon im Vorfeld einen Eindruck bekommen, welche ihrer eigenen Daten möglicherweise zum Workshop mitgebracht werden könnten.
    
            
               Zeittafel
               
                  
                     Zeit
                     Titel
                     Vortragende
                  
                  
                     Halbtag 1.1
                     Einleitung, Motivation
                     Anne Baillot, David Lassner
                  
                  
                     Halbtag 1.2
                     Erkennung von Biases in ML
                     David Lassner
                  
                  
                     Kaffepause
                     
                     
                  
                  
                     Halbtag 1.3
                     Verhinderung von Biases in ML
                     Stephanie Brandl
                  
                  
                     Halbtag 1.4
                     Praktische Sitzung 1
                     
                  
                  
                     Halbtag 2.1
                     Autorinnen um 1800
                     Anne Baillot
                  
                  
                     Halbtag 2.2
                     Revolte auf Twitter
                     Louisa Guy
                  
                  
                     Kaffepause
                     
                     
                  
                  
                     Halbtag 2.3
                     Praktische Sitzung 2
                     
                  
                  
                     Halbtag 2.4
                     Abschlussdiskussion
                     
                  
               
            
            
               Erkennung von Biases
               Zu Beginn steht die Begriffsklärung (Datenbias, Modellbias, etc.) und konkrete Beispiele zur Erkundung verschiedener Biases in verschiedenen Datensätzen, sowie Modellarchitekturen. Beispielsweise anhand konkreter Architekturen neuronaler Netze zur Textklassifikation, deren erster Layer ein Embedding-Layer auf Word2Vec-Basis ist (Mikolov et al. 2013).
               Es werden verschiedene Methoden vorgestellt, wie Biases in Modellen und Daten erkannt werden können (Caliskan et al. 2017, May et al. 2019, Garg et al. 2018, Bolukbasi et al. 2016, Swinger et al. 2019).
            
            
               Wie lassen sich Biases verhindern?
               Innerhalb der letzten 3 Jahre wurden zahlreiche Methoden veröffentlicht, die darauf abzielen Biases in Word Embeddings und anderen NLP Anwendungen zu verringern. In diesem Teil wollen wir einen Überblick über die wichtigsten Methoden verschaffen, ihre Stärken und Schwächen aufzeigen und diskutieren.
               Aktuell können diese Methoden in 3 Kategorien eingeteilt werden:
               
                  Manipulation von Datensätzen
                  Datensätze werden so verändert, beispielsweise durch Datenanreicherung, dass Biases im Datensatz nicht mehr zu finden sind und so auch nicht mitgelernt werden. Zum Beispiel schlagen Zhao et al (2018) vor, jeden Satz in einem Datensatz zu kopieren, sodass dieser in mehreren Varianten vorkommt: eine für jedes grammatikalische Geschlecht. So wird eine balancierte Repräsentation zwischen den (binären) Geschlechtern garantiert. Bestehende ML-Methoden die ansonsten biased Ergebnisse erzeugen, können so faire Modelle lernen.
               
               
                  Anpassung der Methode
                  Zhang et al (2018) schlagen vor den Einfluss geschützter demografischer Informationen wie Geschlecht oder Postleitzahl auf das Klassifikationsergebnis mit Adversarial Learning zu verringern. Drei verschiedene Definitionen von „equality“ und Parität werden analysiert und für jeden Definition wird eine entsprechende Strategie vorgestellt um demografische Parität zu sichern.
               
               
                  Zusätzlicher Analyseschritt
                  Bolukbasi et al (2016) zeigen, dass mit Hilfe von Wortlisten ein Unterraum errechnet werden kann, der die geschlechtsbezogene Information in Word Embeddings beinhaltet. Wörter werden mit Hilfe dieser Wortlisten in geschlechtsneutral (z.B. doctor) und geschlechtsspezifisch (z.B. grandmother) eingeteilt. In dem entsprechenden Unterraum werden dann alle Wörter, die grammatikalisch geschlechtsneutral sind, auch neutralisiert, so dass beispielsweise 
    doctor zentriert zwischen den Word Embeddings für “Mann” und “Frau” liegt.
    
                  Allerdings zeigen auch einige dieser Methoden Schwächen und es wurde bereits gezeigt, dass in vielen Fällen Biases weiterhin rekonstruiert werden können (Gonen & Goldberg, 2019). 
               
            
            
               Praktische Sitzung 1
               Im ersten praktischen Teil sollen dann ML Modelle selbst ausprobiert und werden und, anhand von verschiedenen Analysemethoden, Biases explorativ erkundet werden.
               Wir stellen eine fertige ML-Pipeline zur Textklassifizierung zur Verfügung, die mit vortrainierten Word Embeddings arbeitet. Die Klassifizierung soll dahingehend analysiert werden, welche Biases sie enthält. Dann sollen die vortrainierten Word Embeddings mithilfe von Tensorflow Projector erkundet werden und es sollen Richtungen identifiziert werden, die für die Biases in den Ergebnissen verantwortlich sein könnten. Die Teilnehmenden sollen die vortrainierten Word Embeddings auf Grundlage ihrer Erkenntnisse modifizieren und untersuchen, wie sich das Klassifikationsergebnis dadurch ändert.
               Des Weiteren sollen die Biases dieser Pipeline mithilfe von standardisierten Wort-list Tests (SEAT, May et al. 2019 / WEAT, Caliskan et al. 2017) analysiert werden.
               Zuletzt soll den Teilnehmenden auch die Möglichkeit gegeben werden, die Korpuszusammensetzung für das Training der Word Embeddings zu modifizieren und selber trainierte Word Embeddings anstelle der vortrainierten zu verwenden, beispielsweise mithilfe von Sampling, Vereinigung, Mitteln.
            
            
               Erkenntnisgewinn für DH durch Untersuchung von Biases 
               Biases in historischen Textdatensätzen können auf Biases in den Gesellschaften ihrer Entstehung sowie in ihrer Aufbewahrungs- und Tradierungsgeschichte aufdecken. Mit Blick auf die wachsende Wichtigkeit von Cultural Heritage Studies in den Digital Humanities sind diese Art von Biases ein hochaktuelles Forschungsfeld (Garg et al 2018). Der Korpuskonstruktion muss in diesen Fällen allerdings besondere Sorgfalt beigemessen werden, da nur bei einem für die jeweilige Forschungsfrage möglichst ausgewogenen Korpus auch tatsächlich durch die Biases im Korpus auch auf die Biases in der Gesellschaft Rückschlüsse gezogen werden können (Underwood 2019, Bode 2020). Kurz gesagt birgt jeder Schritt in der Geschichte der zu untersuchenden Objekte die Gefahr eines unbewusst und ungewollt induzierten Bias, die der bewussten und gewollten Analyse von Biases im Wege stehen können.
            
            
               Autorinnen um 1800
               Digitale Methoden machen es möglich, das traditionelle Narrativ der Literaturgeschichte zu überdenken und damit Literatur in den Vordergrund zu rücken, die etwa aus Gendergründen im Kanon als zweitrangig überliefert worden war. Zumindest machen sie es theoretisch möglich: Es soll nämlich gezeigt werden, dass digitale Korpora und Methoden die Biases der traditionellen Historiographie auch im literarischen Bereich nur zu leicht reproduzieren und dass die Korpusbildung und der Trainingsprozess einer besonderen Zuspitzung brauchen, um z.B. die Rolle von schreibenden Frauen deutlich machen zu können. Argumentiert wird hier am Beispiel von Autorinnen aus der Zeit um 1800 – der Phase nämlich, wo der (wohl männliche) Autor sich als literarischer, wirtschaftlich tragfähiger Wert etabliert.
            
            
               Tweetanalyse von #aufschrei und #blacklivesmatter
               Auf dem sozialen Netzwerk Twitter führten die Hashtags „aufschrei“ und „blacklivesmatter“ 2013 zu kollektiven Revolten, die online begannen, sich dann aber auch auf den Alltagsdiskurs ausweiteten. Unter #aufschrei berichteten Frauen über ihre Erfahrungen mit Sexismus und unter #blacklivematters ging es um Erlebnisse mit Rassismus. An diesem Beispiel werden Methoden zur Quellenanalyse vorgestellt. Ziel ist es, die Dynamik der digitalen Bewegungen von #aufschrei und #blacklivesmatter anschaulich zu machen.
            
            
               Praktische Sitzung 2 und Abschlussdiskussion
               Im zweiten praktischen Teil sollen die Teilnehmenden ihre eigene Expertise einbringen und in Gruppen individuelle Fragestellungen formulieren, die mithilfe der zuvor kennengelernten Modelle untersucht werden können. Wenn möglich, sollen sofort erste Prototypen entwickelt werden.
               Falls Teilnehmende keine eigenen Korpora bzw. Fragestellungen mitbringen, stellen wir eine ML-Pipeline zur Verfügung, die existierende Systeme zur Erkennung von Hatespeech im Internet auf Tweets mit dem Hashtag #aufschrei bzw. #blacklivesmatter sowie einer Kontrollgruppe aus zufälligen anderen Tweets anwendet. Mithilfe dieser Pipeline sollen Teilnehmende untersuchen, wie Sprache einer neu entstehenden Bewegung, die nicht dem Mainstream entspricht, möglicherweise automatisch als Hatespeech erkannt wird.
            
         
      
      
         
            
      Was tatsächlich bei Amazon passiert ist: 
      https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G
            
            
      Beispiel von https://twitter.com/jessamyn/status/900867154412699649
      bezüglich des www.perspectiveapi.com
      Interfaces, außerdem Davidson et al. (2019)
    
         
         
            
               Bibliographie
               
                  Bode, Katherine (Forthcoming 2020): Why You Can’t Model Away Bias, Modern Language Quarterly 81.1. preprint: katherinebode.files.wordpress.com/2019/08/mlq2019_preprintbode_why.pdf [letzter Zugriff 27. September 2019].
                    
               
                  Bolukbasi, Tolga / Kai-Wei Chang / James Y Zou / Venkatesh Saligrama /  Adam T Kalai (2016): Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Conference of NIPS.
                    
               
                  Buolamwini, Joy / Timnit Gebru (2018): Gender shades: Intersectional accuracy disparities in commercial gender classification. Conference on fairness, accountability and transparency.
                    
               
                  Caliskan, Aylin / Joanna J. Bryson  /  Arvind Narayanan. (2017): Semantics derived automatically from language corpora contain human-like biases. Science 356.
                    
               
                  Davidson, Thomas / Debasmita Bhattacharya  / Ingmar Weber (2019): Racial Bias in Hate Speech and Abusive Language Detection Datasets. arXiv preprint arXiv:1905.12516. 
                    
               
                  Garg, Nikhil /  Londa Schiebinger / Dan Jurafsky  /  James Zou (2018): Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences.
                    
               
                  Gonen, H.  /  Yoav Goldberg (2019): Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them. Conference of the NAACL.
                    
               
                  May, Chandler /  Alex Wang / Shikha Bordia / Samuel R. Bowman  /  Rachel Rudinger (2019): On Measuring Social Biases in Sentence Encoders. Conference of the NAACL.
                    
               
                  Mikolov, T. / Chen, K. /  Corrado, G.  /  Dean, J. (2013): Efficient estimation of word representations in vector space. In ICLR.
                    
               
                  Sap, Maarten / Dallas Card / Saadia Gabriel / Yejin Choi  /  Noah A. Smith (2019): The Risk of Racial Bias in Hate Speech Detection. Conference of the ACL.
                    
               
                  Swinger, Nathaniel / Maria De-Arteaga /  Neil Heffernan IV / Mark Leiserson  /  Adam Kalai (2019): What are the biases in my word embedding?. Conference on Artificial Intelligence, Ethics, and Society (AIES).
                    
               
                  Underwood, Ted (2019). Distant Horizons: Digital Evidence and Literary Change. University of Chicago Press.
                    
               
                  Zhang, B. H. /  Lemoine, B. / Mitchell, M. (2018): Mitigating unwanted biases with adversarial learning. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society.
                    
               
                  Zhao, J. / Wang, T. / Yatskar, M. / Ordonez, V. /  Chang, K. W. (2018): Gender bias in coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876.
                    
            
         
      
   



      
         Obwohl mediävistische Forschung sich bereits seit
	    geraumer Zeit digitaler Methoden bedient und in der
	    Gründungsgeschichte des Humanities Computing, heute als
	    Digital Humanities bekannt, eine herausragende Stellung
	    einnimmt (s. Bleier et al. 2019: 1-12) , spiegelt sich
	    diese real existierende Interdisziplinarität bisher kaum
	    bis gar nicht in den im deutschsprachigen Raum etablierten
	    mediävistischen Studiengängen wider. Dies mag umso mehr
	    verwundern, als die digitale Mediävistik über
	    vergleichsweise klar umgrenzte Fragestellungen,
	    Anwendungsfelder und Vorgehensweisen verfügt; im Gegensatz
	    zu den allgemeiner gehaltenen Digital Humanities, zu denen
	    es einige Lehrstühle und Studiengänge gibt,

		wenngleich lokal wiederum meist mit fachlicher Einschränkung (s. Trier, Leipzig, Stuttgart, Würzburg, Köln, siehe außerdem Sahle 2016) . Themenfelder, denen sich die digitale Mediävistik dezidiert widmet, sind – um nur einige Beispiele zu nennen – computergestützte Analysen von paläographischen Befunden (DigiPal, KPDZ/ CPDA 1-4), historisch-geographische Informationssysteme im mediävistischen Kontext (Mapping Medieval Conflict) und der Einsatz des Maschinellen Sehens zur Mustererkennung in mediävistischen Bildwerken (Computer Vision Lab Heidelberg).
            
         Zu den Herausforderungen einer solch interdisziplinären und im besten Fall auch innovativen Forschung kommen für den wissenschaftlichen Nachwuchs, wie bereits angedeutet, weitere Herausforderungen hinzu: Neben der Frage der Ausbildung stehen hier vor allen Dingen Fragen nach Karriereperspektiven, Anerkennung alternativer Publikationsformen (die Publikation von Forschungsdaten (vgl. Andorfer 2015) , die Debatte um die Vorzüge einer kumulativen Dissertation versus Monographie) und die Einbettung in bestehende fachliche Infrastrukturen im Vordergrund. Das neu gegründete Subcommittee der 
                Digital Medievalist Community, das sich an Early Stage Researcher richtet, hat sich zum Ziel gesetzt, dieser Gruppe eine Plattform zu bieten und den Gesprächsbedarf in einen Dialog mit der größeren Fachgemeinde zu übersetzen.
            
         
            Digital Medievalist ist eine internationale interessenbasierte virtuelle Forschungsgemeinschaft, die mit einem breiten thematischen Zuschnitt über Disziplingrenzen hinweg Wissenschaftler*innen unterschiedlicher Statusgruppen miteinander vernetzt und verschiedene Wege geht, um gemeinschaftlich Spielräume der einzelnen Fachdisziplinen zu erweitern. Die Gemeinschaft wurde bereits 2003 gegründet, seit 2005 ist sie Herausgeberin des gleichnamigen Open Access Journals. Unabhängig von Standorten bietet die 
                Digital Medievalist Community beispielsweise im Rahmen von gemeinsam organisierten Konferenzaktivitäten ein Netzwerk sowohl für etablierte Wissenschaftler*innen als auch für solche am Beginn ihrer wissenschaftlichen Karriere. Digital Medievalist steht allen Interessierten offen, unabhängig bestehender Erfahrungen in den Digital Humanities oder den mediävistischen Disziplinen, von absoluten Neulingen bis hin zu sogenannten Pionieren im Bereich der (digitalen) Mediävistik. 
            
         Spielräume der DM Community sind bisher bereits, 
                in a nutshell:
         
         
            Die 
                    Digital Medievalist Mailingliste : Mehr als 1.300 Abonnenten (Stand September 2019) nutzen diesen Kanal als Diskussionsplattform, um Rat einzuholen und um Informationen jeglicher Art im Bereich der (digitalen) Mediävistik zu teilen.
                
            Das 
                    Digital Medievalist Journal: Verlagsunabhängige (APC freie) Open-Access Fachzeitschrift der Community; die wissenschaftliche Qualität der Artikel wird im Peer-Review-Verfahren gesichert. 
                
            Die 
                    Digital Medievalist Webseite: Die Onlinepräsenz der Community versammelt alle Informationen über die Community: Wie wird man Mitglied? Wie ist die Organisation aufgebaut, wie lautet die Satzung? Darüber hinaus finden sich hier Ankündigungen sowie wie eine stetig aktualisierte Liste von vergangenen und anstehenden Konferenzen, Kolloquien, Workshops und Sommerschulen mit Relevanz für die (digitale) Mediävistik. Im Webblog werden zukünftig neben CfPs und Veranstaltungshinweisen Projekte und Tools aus der digitalen Mediävistik vorgestellt, auf aktuelle Veröffentlichungen verwiesen sowie die Reihe “ What do Digital Medievalists do? ” (Campagnolo 2017) weitergeführt.
                
            
               Digital Medievalist Zotero Bibliographie : Sammlung einschlägiger Literatur zu allen themenbereichen der digitalen Mediävistik.
                
            Die 
                    Digital Medievalist Facebookgruppe mit mehr als 2.500 Mitgliedern sowie ein Twitteraccount @digitalmedieval mit derzeit über 6.000 Followern erweitern den Spielraum der Wissenschaftskommunikation. 
                
         
         Während der Postersession möchten wir die verschiedenen Initiativen der 
                Digital Medievalist Community vorstellen und die Vernetzung innerhalb der deutschsprachigen DH vorantreiben. Hierbei möchten wir vor allen Dingen die geplanten Aktivitäten des neugegründeten Postgraduate Subcommittees skizzieren und einen Peer-to-Peer-Austausch fördern. Das Subcommittee hat es sich zum Ziel gesetzt, einerseits bereits bestehende Aktivitäten wie den Blog auf der Webseite oder die Präsenz der Community auf Twitter zu beleben und andererseits ab 2020 neue eigene Aktivitäten in Angriff zu nehmen; hierzu zählen insbesondere die Organisation von gemeinsamen Panels (so etwa auf dem International Medieval Congress in Leeds) und die Produktion von einem Podcast, in dem Nachwuchsforscher zu Wort kommen sollen. Die Erhöhung der Sichtbarkeit der existenten Infrastrukturen soll außerdem Denkanstöße für eine Diskussion um interdisziplinäres Arbeiten, erforderliche Skills und eine Reform der universitären Curricula im mediävistischen Kontext liefern und damit auch in übergreifender Perspektive beispielhaft zu aktuellen Debatten um die Profilierung geisteswissenschaftlicher Disziplinen zwischen Tradition und gegenwärtigen Anforderungen beitragen.
            
      
      
         
            
	      Für ein ausführliches Verzeichnis der DH Studiengänge
	      siehe https://registries.clarin-dariah.eu/courses/
    
         
         
            
               Bibliographie
               
                  Andorfer, Peter (2015):
  „Forschungsdaten in den (digitalen) Geisteswissenschaften: Versuch
  einer Konkretisierung“, Göttingen: GOEDOC (DARIAH-DE working papers
  14)
  http://nbn-resolving.de/urn:nbn:de:gbv:7-dariah-2015-7-2
               
               
                  Bleier, Roman / Fischer, Franz / Hiltmann, Torsten /
  Viehhauser, Gabriel / Vogeler, Georg (2019):
  „Digitale Mediävistik und der deutschsprachige Raum“ in: Das
  Mittelalter 24. 1 : 1-12 DOI:
  
               
               
                  Campagnolo, Alberto (2017): „What do digital
  medievalists do?“
  https://digitalmedievalist.wordpress.com/2017/08/10/what-do-digital-medievalists-do/
               
               
                  Computer Vision Lab Uni Heidelberg:
  https://hciweb.iwr.uni-heidelberg.de/compvis/
               
               
                  DigiPal (2011-2014): „Digital Resource and
  Database of Palaeography, Manuscript Studies and Diplomatic“
  
               
               
                  Digital Medievalist Webseite: 
  https://digitalmedievalist.wordpress.com/
               
               
                  Digital Medievalist Journal:
  https://journal.digitalmedievalist.org/
               
               
                  Digital Medievalist Mailingliste: 
  https://digitalmedievalist.wordpress.com/mailing-list/
               
               
                  Digital Medievalist on Facebook:
  https://www.facebook.com/groups/49320313760/
               
               
                  Digital Medievalist on Twitter:
  https://twitter.com/digitalmedieval
               
               
                  Digital Medievalist on Zotero:
  https://www.zotero.org/groups/2138266/digitalmedievalist
               
               
                  Kodikologie und Paläographie im digitalen
  Zeitalter 1-4 / Codicology and Palaeography in the Digital Age
  1-4 (2009, 2010, 2015, 2017):
  Herausgegeben vom Institut für Dokumentologie und Editorik,
  Norderstedt: BoD.
  https://www.i-d-e.de/publikationen/schriften/
               
               
                  Mapping Medieval Conflict:
 https://www.i-d-e.de/publikationen/schriften/
               
               
                  Sahle, Patrick (2016) : “Zur
  Professoralisierung der Digital Humanities” in: DHdBlog 23. März
  2016
  
               
               
Alle angegeben Links wurden am 4. Januar 2020 geprüft.
            
         
      
   



      
         
            Einleitung
            
  Täglich werden auf der ganzen Welt Onlineartikel, Blogbeiträge etc. veröffentlicht, zu denen Leserinnen und Leser (i.F. generisches Femininum) Kommentare verfassen. Aufgrund der hohen Anzahl an Partizipierenden gelten Nutzerbeiträge als besonders authentische Echtzeitrückmeldungen und erlauben einen Zugang zu heterogenen Meinungsäußerungen (Busch, 2017). Auch durch Partizipation auf Social Media Plattformen, in Onlineforen und öffentlichen Chats werden Daten generiert, die wertvolle Informationen über Nutzerverhalten und menschliches Denken beinhalten. Dies gilt umso mehr, als diese Plattformen Orte sind, an denen Menschen miteinander in Verbindung treten, in verschiedenen Formen und Dimensionen Gemeinschaft pflegen, Informationen verbreiten und ihre Meinungen austauschen. Dabei generierte Daten zeichnen sich durch ihren interaktiven, kontemporären und personenbezogenen Charakter aus und ermöglichen folglich Rückschlüsse auf Meinungen, Interessen und Stimmungen in der Bevölkerung. Entsprechend sind sie von besonderem Interesse für private Unternehmen oder öffentliche Institutionen (Schoen, 2002; Holtz-Bacha, 2019: 276). Zunehmend wird auch im akademischen Kontext auf Nutzerdaten zurückgegriffen (u.a. Mohammad, 2016; Aker et al., 2016). 
    
         
         
            Forschungsgegenstand
            

Gegenstand des vorzustellenden Projektes ist eine Pilotstudie, in der zwei Masterthesisprojekte in Beziehung zueinander gesetzt wurden. Bei dem ersten Thesisprojekt (Guhr, 2019) handelt es sich um eine im Bereich der Digital Humanities durchgeführte computergestützte Analyse von Leserkommentaren in französischen Onlinemedien. Das zweite Thesisprojekt aus dem Bereich des IT-/Datenschutzrechts betrachtet ethische und rechtliche Erwägungen bei der Analyse von nutzergenerierten Inhalten auf Social Media Plattformen und die Frage, wie deren Berücksichtigung in wissenschaftlichen Datenanalyseprojekten unterstützt werden kann. Infolge der Auseinandersetzung mit dem jeweils anderen Masterthesisprojekt entstand ein interdisziplinäres Streitgespräch zwischen den Autorinnen. 
    
            
      Die Masterthesis (Guhr, 2019) umfasst u.a. verschiedene gemischt qualitativ-quantitative Analysen von Onlinezeitungsartikeln und zugehörigen Leserkommentaren zur französischen Präsidentschaftswahl 2017. Die Daten sind über den Onlineauftritt einer großen französischen Tageszeitung öffentlich zugänglich. 40 ausgewählte Onlineartikel mit den dazugehörigen 3.127 Leserkommentaren wurden zu einem Korpus zusammengestellt. Die extrahierten Leserkommentardaten beinhalteten zusätzlich zu den nutzergenerierten Beitragstexten auch Datum und Uhrzeit der Beitragserstellung sowie die Nicknames und teilweise bürgerliche Namen der Nutzerinnen. Mithilfe von Distant Reading Methoden wurden im Korpus behandelte Themen identifiziert. Anschließend wurde eine automatisierte Sentimentanalyse der Kommentare durchgeführt, um Informationen über die emotionale Einstellung der Nutzerinnen zu Wahlkampfthemen und zum/zur Präsidentschaftskandidat/in herausstellen zu können.
    
            
      Der zweiten Masterthesis (Brokering, 2019) lag die Frage zugrunde, wie Datenanalyseprojekte im akademischen Kontext rechtskonform und ethischer gestaltet werden können. Am Beispiel von Forschung mit Social Media Daten wurde herausgestellt, wie durch Analysen von nutzergenerierten Inhalten Interessen und Rechte der Nutzerinnen berührt werden. Für die hierdurch aufgeworfenen, neuen ethischen und besonders datenschutzrechtlichen Fragestellungen fehlt es bestehenden inhaltlichen und institutionellen Ansätzen der Forschungsethik noch an befriedigenden Antworten, die eine ethische Praxis von Social Media Datenanalysen gewährleisten. Daraufhin wurde evaluiert, inwiefern das IT-rechtliche Konzept des
      Regulation by Design
      eine effektivere Implementierung ethischer und rechtlicher Erwägungen in Social Media Datenanalysen unterstützen kann. 
      Regulation by Design
      zielt auf eine proaktive Berücksichtigung regulatorischer Erwägungen bereits im Zeitpunkt des Designs, d.h. der Planung und Entwicklung, von Produkten und Aktivitäten wie auch Forschung. Es findet seine bekannteste Ausprägung im datenschutzrechtlichen Prinzip des 
      Privacy by Design.
    
         
         
            Interdisziplinäres Streitgespräch
            

Im Dialog der Autorinnen trafen die Perspektiven der praxisorientierten und der juristischen Forschung aufeinander. Aus letzterer wurde Kritik am Umgang mit persönlichen Informationen geäußert und für eine höhere Sensibilität gegenüber den Interessen der Nutzerinnen und insbesondere datenschutzrechtlichen Erwägungen plädiert. Sobald Social Media Daten eine Identifizierbarkeit der postenden Personen auch nur ermöglichen, z.B. weil sie die Nutzernamen oder auch die IP-Adresse der Nutzerin enthalten, handelt es sich um persönliche Daten und damit finden datenschutzrechtliche Vorgaben wie die europäische DSGVO Anwendung. Diese erfordert typischerweise die Information der betroffenen Nutzerin über die konkrete Verwertung ihrer Daten und die Einwilligung in diese. Die Wirksamkeit einer mittels der AGB des jeweiligen Social Media Anbieters erteilten Einwilligung ist als zweifelhaft zu bewerten, da sie nicht projektspezifisch ist. Angesichts des Umstands, dass die verwendeten Nutzerdaten zu Forschungszwecken umgewidmet werden und originär im Rahmen der privaten Nutzung von Social Media Diensten entstanden sind, ist in Erwägung zu ziehen, ob über das datenschutzrechtlich erforderliche Mindestmaß hinausgehende Maßnahmen zum Schutz der Interessen der Nutzerinnen ethisch geboten sind. Auch eine Anonymisierung der Nutzerdaten, z.B. durch Entfernen des Nutzernamens kann das Re-Identifikationsrisiko angesichts fortschrittlicher De-Anonymisierungstechniken nur reduzieren. Hier sind weitergehende u.a. auch im Verhältnis zum Grad an Sensibilität der betroffenen Nutzerinhalte angemessene Anonymisierungsmaßnahmen in Erwägung zu ziehen. Gleichzeitig ist zu berücksichtigen, dass Nutzerinnen an ihren originell und kreativ gestalteten Beiträgen auch urheberrechtliche Interessen und Rechte haben, sodass andererseits eine Erkennbarkeit der Autorin durch die Forschenden sicherzustellen sein kann. Die praxisorientierte Forscherin wies demgegenüber auf die schwierige Umsetzbarkeit aufwendiger Maßnahmen zum Schutz der Nutzerinnen angesichts begrenzter finanzieller, technischer und zeitlicher Spielräume in der Forschungspraxis hin sowie auf die Gefahr, dass Forschungsdaten durch Datenschutzmaßnahmen an Wert/Aussagekraft verlieren würden. Als Beispiele nannte sie den Wert von Nutzernamen als potenzielle Informationsquelle hinsichtlich Gender und Nationalität sowie für die kumulative Betrachtung verschiedener Beiträge einer Person. Auch die Erhebung von Datum und Uhrzeit der Beitragserstellung ermögliche eine chronologische Ordnung von Beiträgen. Dabei kritisierte die Juristin, dass bereits aus derartigen Informationen umfangreiche Aktivitätsprofile über einzelne Nutzerinnen erstellt werden könnten, die ggf. in Verbindung mit Nutzungsdaten derselben Nutzerinnen auf weiteren Social Media Plattformen Rückschlüsse auf Tagesabläufe, Vorlieben und Social Media Verhalten einzelner Nutzerinnen erlauben. Im daraus resultierenden Streitgespräch wurde erkennbar, wie schwierig es ist, die jeweiligen Positionen in einer der anderen Forschenden verständlichen Weise zu kommunizieren. 
    
            
      Weiteres Vorgehen im Projekt war es, die rechtlich-ethischen Herausforderungen gemeinsam zu definieren, wobei die Perspektiven beider Forschungsrichtungen Beachtung finden sollten. Auf dieser gemeinsamen Grundlage und unter Berücksichtigung verschiedener Ansätze des 
      Regulation by Design-Konzeptes wurden Methoden und Herangehensweisen diskutiert, die eine effektive Berücksichtigung der Herausforderungen in der Forschungspraxis erreichen sollen.
    
            
Das Projekt verfolgt damit das Ziel, den Dialog zwischen datenbasierter Forschung und IT-Recht anzuregen und insbesondere das Bewusstsein für Nutzerinteressen und Datenschutzerwägungen unter Forschenden zu erhöhen. Es soll reflektiert werden, wie die Kommunikation zwischen IT-Rechtlerinnen und Datenforschenden unter Berücksichtigung der unterschiedlichen Perspektiven, Interessen und Limitierungen verbessert werden kann. Die gemeinsamen Definitionen der Herausforderungen und die diskutierten Lösungsvorschläge sollen Datenforschenden ermöglichen, ihre Forschungsarbeit ohne größeren Mehraufwand bereits im Stadium der Vorbereitung und Durchführung von Datenanalysen rechtskonform und ethisch sensibel zu gestalten. 
    
         
      
      
         
            
               Bibliographie
               
                  Aker, Ahmet / Paramita, Monica / Kurtic, Emina / Funk, Adam / Barker, Emma
                   (2016): „Automatic label generation for news comment clusters“, in:
                  Proceedings of the 9th International Natural Language Generation Conference
                  , 
                  Edinburgh, UK:  61–69
	https://pdfs.semanticscholar.org/4da7/ac02c56d43312425a854d63e71f89dd288ec.pdf [letzter Zugriff 26. Juni 2019].
      
               
                  Brokering, Annalena
                   (2019): 
                  Drawing from approaches in regulatory theory for the regulation of new technologies and design theory, how can ethical considerations be effectively incorporated into data science activities?.
	Masterthesis, The University of Edinburgh, Edinburgh Law School.
      
               
                  Buchanan, Elizabeth / Zimmer, Michael
                   (2016): „Internet Research Ethics“, in: 
                  Stanford Encyclopedia of Philosophy
                   https://plato.stanford.edu/entries/ethics-internet-research/ [letzter Zugriff 19. August 2019].
               
               
                  Busch, Andreas
	(2017): Informationsinflation: Herausforderungen an die politische Willensbildung in der digitalen Gesellschaft, in: Gapski, Harald / Oberle, Monika / Staufer, Walter (eds.): 
	Medienkompetenz. Herausforderungen für Politik, politische Bildung und Medienbildung, Schriftenreihe der Bundeszentrale für Politische Bildung.
	Bonn: Bundeszentrale für Politische Bildung 53-62
	http://www.bpb.de/lernen/digitale-bildung/medienpaedagogik/medienkompetenz-schriftenreihe/257594/informationsinflation
	[letzter Zugriff 11. Juni 2019].
      
               
                  Dashtipour, Kia / Poria, Soujanya / Hussain, Amir / Cambria, Erik / Hawalah, Ahmad Y. A. / Gelbukh, Alexander / Zhou, Qiang
                   (2016): „Multilingual Sentiment Analysis: State of the Art and Independent Comparison of Techniques“, in:
                  Cognitive Computation
	(2016) 8: 757-771 https://link.springer.com/article/10.1007/s12559-016-9415-7 [letzter Zugriff 26. Juni 2019].
      
               
                  Golder, Susan P. / Ahmed, Shahd / Norman, Gill / Booth, Andrew
                   (2017): „Attitudes Toward the Ethics of Research Using Social Media: A Systematic Review“, in: 
                  Journal of Medical Internet Research 19 http://eprints.whiterose.ac.uk/117721/ [letzter Zugriff 26. September 2019].
      
               
                  Guhr, Svenja
                   (2019): 
                  Computergestützte Analyse von französischen Onlinemedien zur Präsidentschaftswahl 2017, Masterthesis, Georg-August-Universität Göttingen.
      
               
                  Holtz-Bacha, Christina
                   (2019): „Demoskopie - Medien - Politik. Umfragen im Bundestagswahlkampf 2017“, in: Holtz-Bacha, Christina: 
                  Die (Massen-)Medien im Wahlkampf. Die Bundestagswahl 2017.
	Wiesbaden: Springer Fachmedien Wiesbaden GmbH 263-280.
      
               
                  Locatelli, Elisabetta
                   (2018): „Ethics of Social Media Research: State of the Debate and Future Challenges“, in: Hunsinger, Jeremy / Klastrup, Lisbeth / Allen, Matthew M. (eds.): 
                  Second International Handbook of Internet Research.
	Dordrecht: Springer 1-22.
      
               
                  McKee, Heidi / Porter, James E.
                   (2008): „The Ethics of Digital Writing Research: A Rhetorical Approach“, 
                  College Composition and Communication
	59: 711 http://wrconf08.writing.ucsb.edu/Pdf_Articles/McKee_Article.pdf [letzter Zugriff 26. September 2019].
      
               
                  Mohammad, Saif
                   (2016): „A Practical Guide to Sentiment Annotation: Challenges and Solutions“, in: 
                  Proceedings of the NAACL 2016 Workshop on Computational Approaches to Subjectivity, Sentiment, and Social Media (WASSA) 
	174-179 http://www.aclweb.org/anthology/W16-0429 [letzter Zugriff 16. Mai 2019].
      
               
                  Moreno, Megan A. / Goniu, Natalie / Moreno, Peter S. / Diekema, Douglas
                   (2013): „Ethics of Social Media Research: Common Concerns and Practical Considerations“, in:
                  Cyberpsychology, Behavior, and Social
	Networking 16: 708-713 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3942703/ [letzter Zugriff 16. Mai 2019].
      
               
                  Perez Vallejos, Elvira / Koene, Ansgar / Carter, Christopher J. / Hunt, Daniel / Woodard, Christopher / Urquhart, Lachlan / Bergin, Aislinn / Statche, Ramona
                   (2019): „Accessing Online Data for Youth Mental Health Research: Meeting the Ethical Challenges“, in:
                  Philosophy & Technology
	32: 87-110 https://link.springer.com/article/10.1007/s13347-017-0286-y [letzter Zugriff 26. September 2019].
      
               
                  Schoen, Harald
                   (2002): „Wirkung von Wahlprognosen auf Wahlen“ in: Berg, Thomas (ed.) (2002): 
                  Moderner Wahlkampf
                  . 
                  Blick hinter die Kulissen.
                   Opladen: Leske und Budrich 171-191.
               
               
                  Williams, Matthew L. / Burnap, Pete / Sloan, Luke
                   (2017): „Towards an Ethical Framework for Publishing Twitter Data in Social Research: Taking into Account Users’ Views, Online Context and Algorithmic Estimation“, in:
                  Sociology
	51: 1149-1168 https://journals.sagepub.com/doi/pdf/10.1177/0038038517708140 [letzter Zugriff 26. September 2019].
      
            
         
      
   



      
         Die vorliegende Arbeit versucht, im Rahmen einer empirisch fundierten Diskursanalyse von Texten sozialer Medien eine Brücke zwischen qualitativ-hermeneutischer Kulturwissenschaft (hier: Literatur- und Politikwissenschaft) und quantitativ-komputationeller digitaler Geisteswissenschaft zu bauen und beide Methodenlinien synergetisch miteinander zu verschränken. In diesem erweiterten Abstract beschreiben wir einen neuen Datensatz von Twitter-Beiträgen deutscher Parlamentarier des 19. Deutschen Bundestags als Datengrundlage der Diskursanalyse und erste Teilergebnisse, die aus der Analyse dieses Datensatzes resultieren. Ein Fixpunkt dieses Vorgehens ist das historisch markierte Epochenkonstrukt der Romantik in seiner literarischen und sozialen Ausformung (Lebensform, Wertekanon usw.) und seine (Wieder-)Aufnahme bzw. Adaption im aktuellen parteipolitischen Diskurs in Deutschland. 
         Ausgangspunkt unserer Arbeiten waren Beobachtungen, die einen Bezug zwischen rechtspopulistischen Parteien und Symbolen der deutschen Romantik nahelegten. Während der AfD-Politiker Björn Höcke von seinem Parteikollegen beispielsweise als „romantischer Nationalist“ bezeichnet wurde, trug sein Parteigenosse Andreas Wild bei einem Auftritt im Bundestag eine blaue Kornblume an seinem Revers. Diese Blume, ein zentrales Symbol der Romantik, wurde in den 1930er Jahren sogar zu einem Erkennungszeichen der illegalen Nationalsozialisten in Österreich. Die semantische Doppelbesetzung der blauen Kornblume eröffnet folglich rechtspopulistischen Politikern einen diskursiven Spielraum, sich einerseits implizit an den Nationalsozialismus anzulehnen, andererseits diese Identifikation in der Öffentlichkeit nicht eindeutig zum Ausdruck bringen zu müssen. 
         Um diese Einzelbeobachtungen systematischer einordnen und die Hypothese von der auffälligen Verwendung von Konzepten der Romantik-Epoche im Diskursverhalten einer rechtspopulistischen Partei einer strengeren Prüfung unterziehen zu können, entwickelten wir ein Korpus von Twitter-Beiträgen aller Abgeordneten des (aktuellen) 19. Bundestags (es kann damit als Ergänzung der Redenkorpora des Bundestags von Barbaresi (2018) bzw. Blätte & Blessing (2018) betrachtet werden, die aber auch frühere Legislaturperioden umfassen). Dieses Korpus sollte Grundlage für eine computerlinguistische Diskursanalyse zur Prüfung der Hypothese sein (einen ähnlichen Studienansatz zur Überprüfung sprachlich markierter Stereotypen zwischen politischen Parteien beschreiben Sylwester & Purver (2015)).
         
            Korpus: Für unsere Untersuchung haben wir
DeBAC (
            Deutscher Bundestags
            Abgeordnete-
            Corpus), das nach unserem Kenntnisstand erste Twitter-Korpus deutscher Bundestags-abgeordneter für die laufende 19. Legislaturperiode, aufgebaut. Es umfasst zum Zeitpunkt der Abfassung dieses Abstracts (Januar 2020) 887.008 Tweets von 478 Parlamentariern über einen Zeitraum vom 21.11.2008 bis 2.1.2020; dieses Korpus wird fortlaufend aktualisiert. Es umfasst 
                alle im Bundestag vertretenen Parteien sowie parteilose Abgeordnete.
            
         Da dieser Datensatz natürlich nicht nur für Fragestellungen im Romantik-Kontext, sondern für die deutschsprachige politische Diskursanalyse generell wertvoll sein kann, stellen wir es der Fachöffentlichkeit zur Verfügung (
                https://github.com/JULIELab/DeBAC). Aus rechtlichen Gründen distribuieren wir dabei nur die Tweet-IDs und dazugehörigen Meta-Daten (u.a. Autor, Erstellungszeitpunkt und Parteizugehörigkeit), während die Rohtexte über ein ebenfalls mitgeliefertes Skript heruntergeladen werden können. 
            
         
            Analytik: Im ersten Anlauf suchten wir nach
Stichwörtern, die Romantik-Konzepte indizieren. Hierzu wurde eine
explorative Umfrage unter mehreren Literaturwissenschaftlern (allesamt
Mitglieder des Graduiertenkollegs „Modell Romantik“ an der
Friedrich-Schiller-Universität Jena)
durchgeführt, um gebräuchliche lexikalische Signale für diese Epoche
zu bestimmen. Dabei stellte sich heraus, dass nicht nur direkte
Lexikalisierungen wie
„
            Romantik“,
„
            Romantiker“,
„
            romantisch“
romantikrelevant sind, sondern auch solche wie
„
            Gemeinschaft“,
„
            Wesen“,
„
            Glauben“,
„
            Heimat“ (man denke an Friedrich Schlegels Über den Republikanismus, Novalis' 
                Glauben und Liebe usw.). Das Suchergebnis wurde sowohl quantitativ analysiert als auch qualitativ interpretiert. Die folgende Tabelle zeigt die Häufigkeiten von Tweets mit diesen Stichwörtern und ihre Zuordnung zu Parteien:
            
         
            Tabelle 1: Häufigkeit der Stichwörter mit Romantikbezug, gruppiert nach Parteien im Bundestag. Tweets der insgesamt vier fraktionslosen Abgeordneten (mit sehr niedrigen Belegzahlen) sind zur Übersichtlichkeit nicht aufgeführt
            
               Suchwort (Regulärer Ausdruck)
               
                  CDU/CSU
               
               SPD
               AfD
               FDP
               LINKE
               GRÜNE
               S
            
            
               
                  /[Rr]omantik/
               
               
                  29
               
               14
               7
               11
               20
               15
               96
            
            
               
                  /[Rr]omantisch/
               
               9
               10
               7
               13
               2
               3
               44
            
            
               
                  /[Rr]omantisier/
               
               1
               2
               2
               5
               0
               4
               14
            
            
               
                  /[Gg]lauben/
               
               
                  375
               
               298
               350
               252
               198
               277
               1750
            
            
               
                  /[Gg]emeinschaft/
               
               424
               399
               104
               234
               
                  260
               
               343
               1764
            
            
               
                  /[Ww]esen/
               
               925
               844
               504
               700
               688
               835
               4496
            
            
               
                  /[Hh]eimat/
               
               
                  1504
               
               941
               562
               312
               314
               639
               4272
            
            
               Insgesamt
               3267
               2508
               1536
               1527
               1478
               2116
               12436
            
         
         Die Tabelle zeigt, dass die direkten Lexikalisierungen „
                Romantik“, „
                Romantiker“ und „
                romantisch“ vergleichsweise selten vorkommen und wenn, dann verweisen sie meist auf eine Lesart im Sinne von „
                realitätsfern“, z.B.:
            
         #Grüne und #Linke wollen, dass #Karlsruhe die Patenschaft für ein Seenotrettungsschiff einer Nichtregierungsorganisation (NGO) im Mittelmeer übernimmt. Eine romantische, realitätsferne Weltsicht.
                (https://twitter.com/MarcBernhardAfD/status/1062048613923201026)
            
         Dagegen kommen indirektere Lexeme wie „
                Gemeinschaft“ und „
                Heimat“ weitaus häufiger vor und werden im Sinne eines abgrenzenden und ausschließenden Charakters eingesetzt, z.B.: 
            
         
            Feste, Feiern, Schwimmbäder: Der Verlust öffentlicher Orte und von Gemeinschaftserlebnissen. Nicht alle haben private Pools. 
                https://t.co/jZsxnmFjCP(https://twitter.com/Renner_AfD/status/1155441711105134592)
            
         #Bayern gibt Unsummen für illegale Migranten aus. Geld, das vielen älteren Menschen fehlt, die Jahrzehnte für unsere Heimat und unsere Gesellschaft hart gearbeitet haben. Schützen Sie unser Sozialsystem gegen Armutseinwanderung und geben wir den Rentnern mehr. #AfD zur #LtwBayern 
                https://t.co/0imAQg3oCj(https://twitter.com/ProfMaier/status/1044102746411073536)
            
         Diese überwiegend qualitative inhaltsanalytische Vorgehensweise haben wir anschließend durch eine einfache quantitative Untersuchung im Rahmen einer automatischen Emotionsanalyse ergänzt (s.a. entsprechende Vorarbeiten von Hellrich et al. (2019) bzw. Buechel et al. (2017). Hierzu haben wir sämtliche Tweets unseres Korpus mithilfe des Software-Werkzeugs JEmAS (Buechel & Hahn 2016) analysiert und ihnen so einen emotionalen Stimmungswert anhand der darin vorkommenden Lexeme zugewiesen. 
         Dieses Verfahren liefert für relativ häufige Wörter intuitiv
plausible Ergebnisse. Das Lexem
„
            Heimat“, das in insgesamt 4.325
Tweets vorkommt, wird etwa von CDU und CSU am
positivsten verwendet und von Der Linken am wenigsten
(aber immer noch) positiv. Demgegenüber mussten wir
feststellen, dass für unsere Ausgangsforschungsfrage
zentrale Begriffe (
„
            Romantik“,
„
            romantisch“,
„
            romantisieren“) in unserem derzeitigen Korpus zu selten vorkommen, um damit auf Grundlage von reinen Worthäufigkeiten zuverlässige Daten erheben zu können. Eine sinnvolle Erweiterung unserer bisherigen Arbeiten besteht daher in der Anwendung fortgeschrittenerer komputationaler Modelle zur Emotionserkennung, die etwa auf Deep Learning (Nay 2016) oder Topic Modeling (Nguyen et al., 2015) beruhen. Unsere Studie ist damit dem weiteren Kontext der Meinungsklima- und Emotionsanalytik im Umfeld parlamentarischer politischer Akteure zuzuordnen (vgl. a. Abercrombie & Batista-Navarro 2018, Green & Larasati 2018, Blätte 2018, van der Zwaan et al. 2016, Rheault et al. 2016, Nguyen et al. 2015, Zirn 2014, Lietz et al. 2014), ein aktueller Schwerpunkt im zur Zeit stark expandierenden Bereich 
                Computational Social Science.
            
         
            Danksagung. Tinghui Duan ist Doktorand des Graduiertenkollegs „Modell Romantik“, das von der DFG unter Fördernummer GRK 2041 gefördert wird; Sven Buechel ist Mitarbeiter eines unter der Förderlinie „Big Data in der makrooökonomischen Analyse“ (Fachlos 2; GZ 23305/003#002) geförderten Projekts des Bundesministeriums für Wirtschaft; Udo Hahn ist PI in beiden Projekten. Die Autoren bedanken sich bei den zwei anonymen Gutachtern für Ihre kritische Anmerkungen und bei Christof Schöch für seine verständnisvolle Kommunikation.
            
      
      
         
            
               http://modellromantik.uni-jena.de/
            
         
         
            
               Bibliographie
               
                  Abercrombie, Gavin / Batista-Navarro, Riza T. (2018): "Identifying opinion-topics and polarity of parliamentary debate motions", in: 
                        WASSA 2018 – Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis @ EMNLP 2018 280-285.
                    
               
                  Barbaresi, Adrien (2018): "A corpus of German political speeches from the 21st century", in: 
                        LREC 2018 – Proceedings of the 11
                  th
                  International Conference on Language Resources and Evaluation 792-797.
                    
               
                  Blätte, Andreas (2018): "Zum Verwechseln ähnlich? Eine Klassifikationsanalyse parlamentarischen Diskursverhaltens auf Basis des PolMine-Plenarprotokollkorpus", in: 
                        Computational Social Science. Die Analyse von Big Data, Nomos 139-162.
                    
               
                  Blätte, Andreas / Blessing, André (2018): "The GermaParl corpus of parliamentary protocols", in: 
                        LREC 2018 – Proceedings of the 11
                  th
                  International Conference on Language Resources and Evaluation 810-816.
                    
               
                  Buechel, Sven / Hahn, Udo (2016): "Emotion analysis as a regression problem: dimensional models and their implications on emotion representation and metrical evaluation", in: 
                        ECAI 2016 – Proceedings of the 22
                  nd
                   European Conference on Artificial Intelligence 1114-1122.
                    
               
                  Buechel, Sven / Hellrich, Johannes / Hahn, Udo (2017): "The course of emotion in three centuries of German text: a methodological framework", in: 
                        dh 2017 – Digital Humanities 2017: Conference Abstracts of the 2017 Conference of the Alliance of Digital Humanities Organizations (ADHO).
                    
               
                  Green, Nathan / Larasati, Septina (2018): "The first 100 days: a corpus of political agendas on Twitter", in: 
                        LREC 2018 – Proceedings of the 11
                  th
                  International Conference on Language Resources and Evaluation 2785-2789.
                    
               
                  Hellrich, Johannes / Buechel, Sven / Hahn, Udo (2019): "Modeling word emotion in historical language: quantity beats supposed stability in seed word selection", in: 
                        LaTeCH-CLfL 2019 – Proceedings of the 3
                  rd
                  Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature @ NAACL-HLT 2019 1-11.
                    
               
                  Lietz, Haiko / Wagner, Claudia / Bleier, Arnim / Strohmaier, Markus (2014): "When politicians talk: assessing online conversational practices of political parties on Twitter", in: 
                        ICWSM 2014 – Proceedings of the 8
                  th
                  International AAAI Conference on Weblogs and Social Media 285-294.
                    
               
                  Nay, John J. (2016): "gov2vec: learning distributed representations of institutions and their legal text", in: 
                        NLP + CSS 2016 – Proceedings of the [1
                  st
                  ] Workshop on Natural Language Processing and Computational Social Science @ EMNLP 2016 49-54.
                    
               
                  Nguyen, Viet-An / Boyd-Graber, Jordan / Resnik, Philip / Miler, Kristina (2015): "Tea Party in the House: a hierarchical ideal point topic model and its application to Republican legislators in the 112
                        th Congress", in: 
                        ACL-IJCNLP 2015 – Proceedings of the 53
                  rd
                  Annual Meeting of the Association for Computational Linguistics & 7
                  th
                  International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing 1438-1448.
                    
               
                  Rheault, Ludovic / Beelen, Kaspar / Cochrane, Christopher / Hirst, Graeme (2016): "Measuring emotion in parliamentary debates with automated textual analysis", in: 
                        PLoS ONE, 11, e0168843.
                    
               
                  Sylwester, Karolina / Purver, Matthew (2015): "Twitter language use reflects psychological differences between Democrats and Republicans", in: 
                        PLoS ONE, 10, e0137422.
                    
               
                  van der Zwaan, Janneke M. / Marx, Maarten / Kamps, Jaap (2016): "Validating cross-perspective topic modeling for extracting political parties' positions from parliamentary proceedings", in: 
                        ECAI 2016 – Proceedings of the 22
                  nd
                  European Conference on Artificial Intelligence 28-36.
                    
               
                  Zirn, Cäcilia (2014): "Analyzing positions and topics in political discussions of the German Bundestag", in: 
                        Proceedings of the Student Research Workshop @ ACL 2014 26-33.
                    
            
         
      
   



      
         Die Digital Humanities (DH) existieren als Forschungsfeld (wenn auch nicht unter diesem Label) bereits seit den 1940er Jahren, so man Roberto Busas Projekt des Index Thomisticus als Grundstein ansieht. Wechselt man von der wissenschaftshistorischen zu einer wissenschaftspolitischen Perspektive und betrachtet Faktoren wie z. B. Forschungsförderungen und Projekte, sind die DH erst seit gut 10–20 Jahren Teil der (deutschen) Wissenschaftslandschaft. Ein Ringen um Akzeptanz ist teilweise bis heute zu beobachten. Gleichwohl lässt sich sagen, dass die DH mit Kuhn gesprochen durchaus mittlerweile den Status einer Normalwissenschaft erlangt haben und sich im Produktivbetrieb befinden.
         Typischerweise lässt sich in der wissenschaftlich-disziplinären Ontogenese nach der Etablierung einer Disziplin bzw. eines Forschungsfeldes der Übergang in eine neue Phase verzeichnen. Einerseits, weil sich durch den Produktivbetrieb und das vermehrte Einbringen vergleichbarer wissenschaftlicher Erkenntnisse Fragen nach dem epistemischen Status, der Validität, der Verwertbarkeit und der weiterführenden Fragenentwicklung stellen, andererseits, weil der wissenschaftspolitische Legitimationsdruck abnimmt und dadurch Ressourcen frei werden und sich neue Handlungsspielräume eröffnen. In dieser Phase der Reife und Entwicklung von Selbst-bewusstsein befinden sich die DH derzeit.
         
Vereinzelte Beiträge zur theoretischen Reflexion der DH als solche, ihrer Objekte und Methoden, Diskussionen auf Twitter und Blogs, sowie Konferenzthemen (exemplarisch: der Titel der DHd-Konferenz 2018 “Kritik der digitalen Vernunft”) stellen eindeutige Marker für diesen Befund dar. Es ist allerdings auch zu konstatieren, dass sich an der von Thiel in der FAZ 2012 vorgebrachten Kritik an der Theorielosigkeit der DH (, zuletzt abgerufen am 27.09.2019) bislang wenig verändert hat. Denn obwohl die Relevanz der Theoriebildung für die DH schon verschiedentlich betont wurde und immer wieder auch Theoriebeiträge vorgelegt werden, sind derlei Überlegungen bislang eher nebenbei und wenig zentralisiert in einzelnen, unabhängigen Projekten oder projektlosen Einzelarbeiten angestellt worden. Ähnlich institutionalisierte Diskurse wie z. B. im angelsächsischen Raum die “Debates in the Digital Humanities“ sucht man vergeblich. Im deutschsprachigen Raum entstanden so in der (bisweilen naiven) Akzentuierung der Entwicklung digitaler Werkzeuge klaffende Lücken in der Theoretisierung der Aktivitäten und Gegenstände, welche hinter der vordergründigen 
Methoden-Orientierung nicht sofort ins Auge springen. Da die Werkzeuge und Methoden ja einfach scheinbar “funktionieren”, werden Fragen nach ihrem epistemologischen Status verhängnisvollerweise allzu leicht in die zweite Reihe gestellt.
         
         Das Gebot der Stunde ist also die systematische wissenschaftlich-disziplinäre Selbstreflexion, Theoriebildung und epistemologische Positionierung der DH. Nachdrücklich sollte daher ein tiefgreifendes, akademisches Aufspüren jener Besonderheiten des Digitalen gefordert werden, die unter dem Signum des fundamentalen und allumfassenden Wandels einen Wissenschaftsbereich wie die DH nun schon seit einigen Jahrzehnten glaubwürdig rechtfertigen. Jene Suche sollte sich über die spezifischen Methoden der digitalen Transformation spannen. Was offenkundig fehlt sind z. B. informationstheoretische, kulturwissenschaftliche und philosophische Grundlagen und ein theoretisches Fundament, welches mit Hilfe einer flächendeckenden, systematischen Untersuchung die isolierten und verstreuten Ansätze sinnbringend und letztlich auch den einzelnen digitalen GeisteswissenschaftlerInnen Souveränität stiftend miteinander verknüpfen könnte. Insbesondere die Geisteswissenschaften, die sich durch ihre epistemische Sensibilität auszeichnen, besitzen das theoretische und methodische Rüstzeug um die unreflektierte Anwendung digitaler Werkzeuge und den naiven Glauben in digital konstruierte wissenschaftliche Erkenntnisse vermeiden bzw. überwinden zu können.
         Der Workshop “Spielplätze der Theoriebildung in den Digital Humanities” möchte an genau dieser Stelle ansetzen. Es soll ein Impuls gesetzt werden, der sich auf mehreren Ebenen erstreckt: Mit der dezidierten Thematisierung der Theoretisierung der DH wird die Community für die Relevanz des Themas sensibilisiert und gleichermaßen wird der Status Quo bestimmt, inwiefern Interesse und Kapazitäten von Seiten der einzelnen ForscherInnen für dieses Thema bereits vorhanden sind. Dies dient auch als Grundlage für etwaige Verstetigungsansätze dieser Forschungsrichtung auf mittelfristiger Perspektive hin, z. B. in Form einer eigenen Zeitschrift oder einer AG in der DHd. Inhaltlich wird eine Kartographierung der Objekte, Perspektiven und Methoden als Teil einer kritischen Refraktion der Digital Humanities unternommen, sowie Ansätze zur wissenschaftlichen Selbstdeutung der DH (als Disziplin, Feld oder Hilfswissenschaft) gesammelt. Hierzu loten die Teilnehmenden gemeinsam die Spielräume wissenschaftstheoretischer Grundlagen und Arbeitsfelder aus und schaffen damit eine Basis für systematische Deutungen.
         Der Workshop hat die Struktur eines World Cafés: Nach einer kurzen Begrüßung und Vorstellung des Programms im Plenum rotieren die Teilnehmende zwischen einer Reihe unterschiedlicher Themenfelder bzw. Thementische. So erhalten sie die Möglichkeit, sich innerhalb stetig aktualisierter Gruppenkonstellationen in Diskussionen über Perspektiven, Thesen und Themen der DH einzubringen. Insbesondere kontroverse sachliche Diskussionen sollen provoziert werden, um eine möglichst differenzierte und breite Grundlage für ein Theorienfundament zu schaffen. Die Diversität der beteiligten wissenschaftlichen Disziplinen auf dem Gebiet der DH, die möglicherweise in der Vergangenheit einer holistischen Theoriebildung der DH im Wege stand, wird dabei als Trumpfkarte gespielt. Für die Einigung auf eine gemeinsame Sprache und die Integration der Perspektiven werden im Rahmen des Workshops erste Ansätze konturiert und protokolliert. Moderierende an den Thementischen leiten die Gespräche, geben Denkimpulse, dokumentieren die Ergebnisse analog und digital und stellen diese abschließend dem gesamten Plenum vor. Eine zusammenfassende Reflexion der Ergebnisse und die Entwicklung eines thematischen Ausblicks runden den Workshop ab. Für die Moderation stehen zunächst die Einreichenden zur Verfügung. Sie werden ergänzt von verschiedenen einschlägigen KollegInnen, die an der Entwicklung des Workshops beteiligt waren (u. a. Patrick Sahle, Enes Türkoğlu und Rabea Kleymann). Nach der Bewilligung und Veröffentlichung des Workshops können sich außerdem noch weitere Interessenten für die Moderation einzelner Thementische melden und werden dann von den Organisatoren ausgewählt. Das Format des World Cafés ist für die Zielsetzungen des Workshops optimal, da die materialen Beiträge von Seiten der Teilnehmenden kommen, zudem kann ihre Heterogenität nicht nur aufgefangen, sondern produktiv genutzt werden. Die Unterteilung in Thementische gibt nur eine lockere Strukturierung vor und dient auch der Feststellung von Interessensprioritäten der Community. Weiterhin findet “am Rande” eine wechselseitige Identifikation und Vernetzung der Teilnehmenden statt.
         Die Themeninseln sollen folgende Schwerpunkte haben:
         
            
               Objekte der DH: Aus geisteswissenschaftlicher Sicht stellen sich die Gegenstände der Informatik alles andere als selbstverständlich dar: Daten sind nicht “neutral”, sondern bereits Interpretationen und Produkte von Forschungsprozessen und -methoden. Dasselbe gilt für Datenmodelle und letztlich auch für Algorithmen, deren Einfluss auf die Transformation von Daten für den geisteswissenschaftlichen Forschungsprozess selbstverständlich mitreflektiert werden muss. Aus informatischer Sicht sollte außerdem die Frage nach digitalen bzw. digitalisierten Objekten neu gestellt werden, welche durch Verfahren der technischen Reproduzierbarkeit notwendigerweise einen neuen ontologischen Status aufweisen.
                
            
               Methoden der DH: Die Forschungsgegenstände werden maßgeblich durch die Forschungsmethoden geprägt. Man könnte auch sagen, dass sie durch Forschungsmethoden erst als Gegenstände hervorgebracht werden. Eine Reflexion der Methoden ist daher unerlässlich und stellt sich nicht nur aus wissenschaftssoziologischer und wissenschaftspolitischer Perspektive im Hinblick auf Forschungsgelder, sondern auch aufgrund des neuen Zuganges, den die DH zu Forschungsgegenständen ermöglichen, z. B. in Form einer digitalen Hermeneutik und des Distant Readings.
                
            
               Werkzeuge der DH: Forschungsmethoden und Werkzeuge stehen in einem dialektischen Verhältnis zueinander. Software wird geformt von den Daten und den Datentransformationsaufgaben, die Daten hingegen werden strukturiert nach der verarbeitenden Software. Es ergeben sich Sachzwänge, deren Ausläufer sich bis hinein in das Research Software Engineering, die Prototypenkonzeption und Usability-Testing bemerkbar machen.
                
            
               Medialität und Digitalität der DH: Die Digitalität ist kein Phänomen der Geisteswissenschaften, sondern muss in einem größeren gesellschaftlichen Rahmen gedacht werden – die Digitalität der Kultur ist der Kontext einer Digitalisierung von Kultur. Logiken der Algorithmizität, Hyperreferentialität und technischer Performativität werden in die Forschung eingeschrieben und müssen bei einer Theorie der DH mitberücksichtigt werden (Beispiele: Informationstheorie geisteswissenschaftlicher Forschungsdaten, Transmedialisierung, Materialität des Digitalen).
                
            
               Wissenschaftstheorie der DH: Dies ist der bis dato wohl am prominentesten diskutierte Punkt, der sich auf das Verhältnis der DH zu den “klassischen” Geisteswissenschaften und der Informatik bezieht, sowie auf den Status der DH als eigenständige Disziplin, als Feld oder als Hilfswissenschaft. Es stellt sich die Frage, ob die DH eine eigene Wissenschaftstheorie brauchen oder befriedigend über etablierte Wissenschaftstheorien (z. B. von Fleck, Kuhn, Popper) beschrieben werden können. Der Theorienpluralismus und neue epistemische Forschungsdarstellungen sind hier ebenso zu diskutieren, wie das Problem der Inkommensurabilität, dass sich mit dem Semantic Web für Forschungsdaten neu stellt.
                
            
               DH und Öffentlichkeit: An das wissenschaftlich-disziplinäre Selbstverständnis der DH als Forschungsfeld schließen sich auch die Untersuchung des Verhältnisses zwischen DH und Öffentlichkeit an. Dies umfasst Fragen nach der Positionierung der DH im öffentlichen Diskurs rund um (geisteswissenschaftliche) Forschung, Fragen der Forschungsethik und Forschungsförderung, Open Access und Bürgerbeteiligung (“citizen science”).
                
         
         Der Workshop bietet damit Raum für verschiedene “Spielplätze” im Bereich der Theoriebildung der Digital Humanities: Spielräume des Theoretischen durchsetzen dann die Spielräume der Forschungspraxis und machen diese wissenschaftstheoretisch greifbar. Es lässt sich argumentieren, dass die DH gewappnet dafür sind, ein neues Kapitel ihrer jungen Wissenschaftsgeschichte zu schreiben. In diesem Sinne besteht die Hoffnung, dass der Workshop “Spielplätze der Theoriebildung in den Digital Humanities” durch die Zusammenführung interessierter WissenschaftlerInnen zur Initialzündung wird, nach der ForscherInnen gemeinsam und engagiert die Fundamentbildung der DH vorantreiben.
         Interessierte ForscherInnen haben auch nach dem Workshop die Möglichkeit in Kontakt zu bleiben, nicht nur, weil die Ergebnisse des Workshops digital zur Verfügung gestellt werden, sondern auch weil die Organisatoren die Möglichkeit für weitere Kollaboration anbieten wollen. Glückt das Vorhaben des Workshops als Inkubator, ist eine Verstetigung und Institutionalisierung des Forschungsinteresses geplant, um einerseits eine offene Plattform des Austausches und der Diskussion zu bieten und andererseits um Forschungsergebnisse wieder in die Community (und auch die Öffentlichkeit) zurückzuspielen.
         
            Einreichende:
         
         
            Jonathan D. Geiger, M. A.
                    
                  Akademie der Wissenschaften und der Literatur | Mainz, Digitale Akademie
                  Geschwister-Scholl-Str. 2 in 55131 Mainz
                  Forschungsinteressen: (Sozial)Epistemologie, Wissenssoziologie, Philosophie der Digitalität, Digital Humanities, Theorie von Informatik, Informations- und Dokumentationswissenschaft
               
            
            Jasmin Pfeiffer, M. A.
                    
                  Universität des Saarlandes, Lehrstuhl für Neuere deutsche Literaturwissenschaft | Medienwissenschaft
                  Campus, Gebäude A22, Raum 0.20, 66123 Saarbrücken
                  Forschungsinteressen: Theorie und Analyse des Computerspiels, Fiktionstheorien, Virtuelle Realitäten, Medialität und Materialität, Digitalität, Theorie der Algorithmen 
               
            
         
         
            Teilnehmende: max. 40
         
            Anforderungen an die Raumausstattung:
         
         
            Beamer: Ja
            Tafel/Whiteboard: Nein
            Flipchart: Nein (aber Flipchart-Papierbögen, die dann an die Pinnwände geheftet werden können)
            Moderationskoffer: 6 Stück
            Pinnwand: 6 Stück
            Steckdosenleisten: 3 Stück
            weitere Anmerkungen:
                    
                  Ein Laptop für den Beamer im Plenum wäre gut.
                  Ideal wäre die Möglichkeit neben dem Raum für das Plenum auch Zugang zu 1–3 weiteren (kleineren) Räumlichkeiten zu haben bzw. überhaupt Raum zum Ausweichen zu haben, sodass sich die Arbeitsgruppen etwas verteilen können.
                  Ein Bonus (wenn auch keine Notwendigkeit) wäre ein kleiner Stehtisch für jede Arbeitsgruppe, also vor jede Pinnwand (insgesamt 6 Stück).
               
            
         
      
      
         
            
               Bibliographie
               
                  
                  Bauer, J. (2011): ‘Who are you Calling Untheoretical?’, 
	Journal of Digital Humanities, 1(1). Available at:
	.
      
               
                  Brügger, N.  (2016): ‘Digital Humanities in the 21st Century: Digital Material as a Driving Force’, 
	digital humanities quarterly, 10(2). Available at:
	.
      
               
                  Capurro, R.  (1978): 
	Information: ein Beitrag zur etymologischen und ideengeschichtlichen Begründung des Informationsbegriffs. München: Saur.
      
               
                  Capurro, R.  (2017): 
	Homo Digitalis: Beiträge zur Ontologie, Anthropologie und Ethik der digitalen Technik. Wiesbaden: Springer.
      
               
                  Castells, M.  (2003): 
	Das Informationszeitalter: Wirtschaft, Gesellschaft, Kultur. Wiesbaden: Springer.
      
               
                  Cecire, N.  (2011): ‘Theory and the Virtues of Digital Humanites’, 
	Journal of Digital Humanities, 1(1). Available at:
	.
      
               
                  Ciula, A. / Eide, Ø.  (2017): ‘Modelling in digital humanities. Signs in context’, 
	Digital Scholarship in the Humanities, 32(1), pp. 33–46. DOI:
	.
      
               
                  Dahlström, M.  (2011): ‘Critical Editing And Critical Digitisation’, 
	Text Comparison and Digital Creativity, (The Production of Presence and Meaning in Digital Text Scholarship). DOI:
	.
      
               
                  Deck, K.-G.  (2018): ‘Digital Humanities – Eine Herausforderung an die Informatik und an die Geisteswissenschaften’, 
	Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3. DOI: 
	10.17175/sb003_002.
      
               
                  Flanders, J.  /  Jannidis, F.  (2019): 
	The shape of data in the digital humanities. Modeling texts and text-based resources.
	London, New York: Routhledge (Digital research in the arts and humanities).
      
               
                  Floridi, L.  (2013): 
	The philosophy of information. Oxford: Oxford Univ. Press.
      
               
                  Frabetti, F.  (2015): 
	Software theory: a cultural and philosophical study. London; New York: Rowman & Littlefield International (Media philosophy).
      
               
                  Friedewald, M.  /  Leimbach, T.  (2011): ‘Computersoftware als digitales Erbe: Probleme aus Sicht der Technikgeschichte’, in 
	Neues Erbe. Aspekte, Perspektiven und Konsequenzen der digitalen Überlieferung. KIT Scientific Publishing.
      
               
                  Gius, E.  /  Jacke, J.  (2017): ‘The Hermeneutic Profit of Annotation. On Preventing and Fostering Disagreement in Literary Analysis’, in 
	International Journal of Humanities and Arts Computing 11(2), 233–254.
      
               
                  Gnadt, T.  et al. (2017): ‘Faktoren und Kriterien für den Impact von DH-Tools und Infrastrukturen’. DARIAH-DE, Niedersächsische Staats- und Universitätsbibliothek. Available at:
	.
      
               
                  Hall, G. (2012):  “Blog Post: Has Critical Theory Run Out of Time for Data-Driven Scholarship?, 
	Debates in the Digital Humanities. Available at:
	
	(Zugriff: 19. August 2019).
      
               
                  Heßbrüggen-Walter, S.  (2018): ‘Philosophie als digitale Geisteswissenschaft’, 
	Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3. DOI: 
	10.17175/sb003_006.
      
               
                  Hui, Y.  (2016): 
	On the existence of digital objects. London: University of Minnesota Press.
      
               
                  Kaden, B.  (2016): ‘Zur Epistemologie digitaler Methoden in den Geisteswissenschaften’, 
	Berliner Beiträge zu Digital Humanities.
      
               
                  Koch, G. (ed.)  (2017): 
	Digitalisierung. Theorien und Konzepte für die empirische Kulturforschung. Köln: Herbert von Halem Verlag.
      
               
                  Matzner, T.  (2016): ‘Beyond data as representation. The performativity of Big Data in surveillance.’, 
	Surveillance & Society, 14(2), pp. 197–204.
      
               
                  McCarty, W.  (2014): ‘Getting there from here. Remembering the future of digital humanities: Roberto Busa Award lecture 2013’, 
	Literary and Linguistic Computing, Volume 29(Issue 3), pp. 283–306. DOI: 
	10.1093/llc/fqu022.
      
               
                  Mohabbat Kar, R.  /  Parycek, P.  (2018): ‘Berechnen, ermöglichen, verhindern: Algorithmen als Ordnungs- und Steuerungsinstrumente in der digitalen Gesellschaft’, in 
	(Un)berechenbar? Algorithmen und Automatisierung in Staat und Gesellschaft. Berlin: Fraunhofer-Institut für Offene Kommunikationssysteme FOKUS, Kompetenzzentrum Öffentliche IT (ÖFIT), pp. 7–39. Available at: 
	https://nbn-resolving.org/urn:nbn:de:0168-ssoar-57562-7.
      
               
                  Nassehi, A.  (2019): 
	Muster. Theorie der digitalen Gesellschaft. München: Beck Verlag.
      
               
                  Nerbonne, J.  (2015): ‘Die Informatik als Geisteswissenschaft’, 
	Sonderband der Zeitschrift für digitale Geisteswissenschaften, 1(1). DOI: 
	10.17175/sb001_003.
      
               
                  Porter, D.  (no date): 
	The Uncanny Valley and the Ghost in the Machine: a discussion of analogies for thinking about digitized medieval manuscripts, 
	Dot Porter Digital. Available at:
	.
      
               
                  Reiche, R. et al. (2014): ‘Verfahren der Digital Humanities in den Geistes- und Kulturwissenschaften’. (DARIAH-DE Working Papers), (4).
      
               
                  Sahle, P.  (2015): ‘Digital Humanities? Gibt’s doch gar nicht!’, 
	Grenzen und Möglichkeiten der Digital Humanities. DOI: 
	10.17175/sb001_004.
      
               
                  Scheinfeldt, T.  (2012): 
	“Blog Post: Where’s the Beef? Does Digital Humanities Have to Answer Questions?,
	Debates in the Digital Humanities. Available at: 
	
	(Zugriff: 19 August 2019).
      
               
                  Schröter, J.  /  Böhnke, A. (eds)  (2004): 
	Analog/Digital – Opposition oder Kontinuum? Zur Theorie und Geschichte einer Unterscheidung. Bielefeld: transcript Verlag.
      
               
                  Stalder, F.  (2016): 
	Kultur der Digitalität. Berlin: Suhrkamp.
      
               
                  Türkoglu, E.  (2019): ‘Vom Digitalisat zum Kontextualisat – einige Gedanken zu digitalen Objekten’, in. 
	DHd 2019 Digital Humanities: multimedial & multimodal. Konferenzabstracts, Frankfurt am Main. DOI: 
	10.5281/zenodo.2600812.
      
               
                  Wettlaufer, J.  (2016): ‘Neue Erkenntnisse durch digitalisierte Geschichtswissenschaft(en)? Zur hermeneutischen Reichweite aktueller digitaler Methoden in informationszentrierten Fächern’, 
	Zeitschrift für digitale Geisteswissenschaften. DOI: 
	10.17175/2016_011.
      
            
         
      
   

