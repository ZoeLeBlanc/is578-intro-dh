

      
         Annotation natürlicher Sprachdaten aus sozialen Medien zur
        Erforschung zeitgenössischer Szenen, zur Sprach- und Trendanalyse und zur
        Weiterentwicklung von Sprachtechnologien gewinnt mit der zunehmenden Verfügbarkeit
        großer Datenbestände weiter an Bedeutung (Farzindar / Inkpen 2015). Zeitgenössische
        Kommunikation in sozialen Medien verfügt über inhaltliche und strukturelle
        Besonderheiten und ist von umgangssprachlicher Ausdrucksform geprägt. Beiträge, die
        im Kontext internetbasierter Diskussionskulturen in Foren entstehen, stellen eine
        wichtige Forschungsquelle dar. Diese nutzergenerierten Texte, in Form von semi- oder
        unstrukturierten Kommentaren, repräsentieren Meinungen und Bewertungen einer
        Gemeinschaft zu einem Thema, Produkt oder Werk und beziehen sich in der Regel auf
        inhaltliche, technische oder ästhetische Aspekte. Die Autoren verwenden dabei
        Sprachmittel wie Metaphern, Analogien, Ambiguität, Humor und Ironie sowie
        metalinguistische bildhafte Mittel wie Emoticons oder andere graphische Zeichen
        (Reyes et al. 2012).
         Vor diesem Hintergrund adressiert dieses Projekt Herausforderungen, die bei der linguistischen und statistischen Verarbeitung von realen web-basierten Daten entstehen. Es wird ein Ansatz semi-automatischer Annotation zur Extraktion von Begriffen für die ontologiebasierte Beschreibung von computergenerierten audiovisuellen Kunstwerken einer digitalen Kunstszene präsentiert. Forschungsgegenstand ist die Diskussionskultur der Demoszene, einer spezialisierten Computerkunstszene. Bisher sind die zahlreichen Beiträge der Gemeinschaft, die sich auf ästhetische und technische Aspekte der Kunstwerke beziehen, nicht erschlossen. Bei diesen Beiträgen handelt es sich um informelle, emotionale, kurze und unstrukturierte Kommentartexte. Das verwendete Vokabular ist mehrsprachig und beinhaltet fachspezifische Terminologien, exklusive Neologismen und einen eigenen szenespezifischen orthographischen Stil. Diese Beiträge bieten detaillierte Einblicke in die Charakteristika der Werke, weshalb ihre Erschließung deren Verständnis fördert und eine gezielte Recherche einzelner Werke ermöglicht. Das Projekt befasst sich mit der Fragestellung, in wieweit sich aktuelle Verfahren der natürlichen Sprachverarbeitung (NLP), die auf grammatikalisch korrekte Schriftformen optimiert und auf Zeitungskorpora trainiert sind, anwenden lassen. Somit leistet das präsentierte Projekt einen Beitrag im Bereich der Entwicklung von Ansätzen zur Aufbereitung großer textbasierter Datenbestände sowie der Erforschung des Sprachgebrauchs zeitgenössischer digitaler Kunstszenen, aber auch hinsichtlich Nutzung semantischer Technologien.
         Die Anwendung von NLP-Verfahren für textbasierte Kommunikation
          in soziale Medien bedarf einiger Anpassungen an die sprachlichen Besonderheiten
          (Maynard 2012). Die Nutzung standardisierter Techniken ist bisher nur wenig
          erfolgversprechend (Gimpel 2011; Finin 2010). Bestehende Frameworks, wie das Natural
          Language Toolkit (NLTK, vgl. Bird et al. 2015), bieten die Möglichkeit der
          Implementierung eines individuellen NLP-Prozesses, bei dem verschiedene
          Verarbeitungsschritte modular integriert und miteinander kombiniert werden können.
          Für das vorliegende Projekt wurde eine Pipeline konzipiert und implementiert, die
          die Generierung von Annotationsebenen, begonnen mit der Tokenisierung und
          Part-of-Speech Tagging bis hin zur Extraktion von relevanten werkbeschreibenden
          Begriffen umfasst. Zur Evaluation des entwickelten Ansatzes wird ein regelbasiertes
          überwachtes Experiment mit einer definierten Teilmenge von 1255 Kommentaren
          durchgeführt. Es lässt sich feststellen, dass Emoticons und Partikeln falsch
          verarbeitet werden. Darüber hinaus werden auch Nomen, Verben und Adjektive,
          insbesondere Gerundien häufig falsch annotiert. Das Experiment zeigt, dass die
          konzipierte Pipeline für das vorliegende Kommentarkorpus iterativ optimiert werden
          muss. Der generierte Index werkbeschreibender Terminologie wird ferner für die
          Erweiterung einer domainspezifischen Ontologie zur Unterstützung semantischer
          Annotation verwendet. Hierfür wird ein Ansatz für das Lernen von Ontologien aus
          Texten verfolgt, wobei die ermittelten Begriffe als Kandidaten für Instanzen
          beschrieben werden. Als Referenzontologie wird eine auf CIDOC CRM-basierte Adaption
          verwendet (Hastik et al. 2013).
         Dieses Projekt präsentiert einen innovativen Ansatz, um mit NLTK Kommentartexte aus Onlineforen der Demoszene zu annotieren. Das Standard-Tagset muss jedoch angepasst werden. Die Erweiterung der CIDOC CRM-basierten Ontologie auf Basis des generierten Indexes ermöglicht die semantische Beschreibung der Werke.
      
      
         
            
               Bibliographie
               
                  Bird, Steven / Klein, Ewan / Loper, Edward (2015):
              Natural Language Processing with Python. NLTK
              Book http://www.nltk.org/book/
              [letzter Zugriff 15. Februar 2016].
               
                  Farzindar, Atefeh / Inkpen, Diana (2015): Natural Language Processing for Social Media. San
              Francisco: Morgan & Claypool.
               
                  Finin, Tim / Murnane, Will / Karandikar, Anand / Keller,
                Nicholas / Martineau, Justin (2010): "Annotating Named Entities in
                Twitter Data with Crowdsourcing", in: Proceedings of the
                NAACL HLT 80–88. 
               
                  Gimpel, Kevin / Schneider, Nathan / O'Connor, Brendan /
                  Dipanjan, Das / Mills, Daniel / Eisenstein, Jacob / Heilman, Michael /
                  Yogatama, Dani / Flanigan, Jeffrey / Smith, Noah A. (2011):
                  "Part-of-Speech Tagging for Twitter: Annotation, Features, and Experiments",
                  in: Proceedings of the 49th Annual Meeting of the
                  Association for Computational Linguistics 42-47. 
               
                  Hastik, Canan / Steinmetz, Arnd / Thull, Bernhard
                  (2013): "Ontology based Framework for Real-Time Audiovisual Art", in: IFLA World Library and Information Congress. 79th
                  IFLA General Conference and Assembly: Audiovisual and Multimedia with
                  Cataloguing http://library.ifla.org/87/1/124-hastik-en.pdf [letzter Zugriff
                  15. Februar 2016]. 
               
                  Maynard, Diana / Bontcheva, Kalina / Rout, Dominic
                  (2012): "Challenges in Developing Opinion Mining Tools for Social Media",
                  in: Proceedings of @NLP can u tag #usergeneratedcontent?!
                  Workshop at International Conference on Language Resources and
                  Evaluation (LREC 2012) 8.
               
                  Reyes, Antonio / Rosso, Paolo / Buscaldi, Davide
                  (2012): "From Humor Recognition to Irony Detection: The Figurative Language
                  of Social Media", in: Data Knowledge Engineering.
                  Applications of Natural Language to Information Systems 74: 1-12. 
            
         
      
   



      
         Dieses Poster soll den DARIAH-DKPro-Wrapper vorstellen, der aus einer Kooperation zwischen dem Lehrstuhl für Computerphilologie der Universität Würzburg und dem Ubiquituous Knowledge Processing Lab der TU Darmstadt im Rahmen von DARIAH-DE entstanden ist. 
         DKPro integriert zahlreiche (unabhängig entstandene) Softwarekomponenten zum Natural Language Processing (NLP) und ermöglicht so dem Nutzer die Anwendung typischer NLP-Aufgaben wie Tokenisierung, Part-of-Speech-Tagging, Named Entity Recognition oder Dependency Parsing mit State-of-the-Art Werkzeugen. Es basiert auf dem Framework UIMA. Für Nutzer, die nicht aus dem Umfeld der Informatik oder Computerlinguistik kommen, ist die Schwelle zur Verwendung allerdings recht hoch: das komplexe Framework muss in Java angesprochen werden.
         Um diese Hürde zu senken und einer größeren Zahl auch von weniger technisch
        versierten Nutzern die Verwendung zu ermöglichen, wurde der DARIAH-DKPro-Wrapper
        entwickelt. Dieser ermöglicht es, eine Pipeline mit mehreren Komponenten über die
        Kommandozeile auszuführen und damit auch längere Textdokumente und Textsammlungen zu
        verarbeiten. Zudem können eine ganze Reihe von Einstellungen bequem und individuell
        über Konfigurationsdateien vorgenommen werden: über die Auswahl der Sprache bis hin
        zur Aktivierung und Deaktivierung einzelner Komponenten und der Auswahl bestimmter
        Komponenten oder Modelle. Auf diese Weise kann jeder Nutzer vorgefertigte Pipelines
        verwenden oder eine auf seine Bedürfnisse zugeschnittene Pipeline individuell
        zusammenstellen. Der Wrapper ist stets aktuell über GitHub (https://github.com/DARIAH-DE/DARIAH-DKPro-Wrapper) verfügbar, ebenso wie
        die dazugehörige Dokumentation des DARIAH-DKPro-Wrapper v0.4.3 (2016). 
         Um die anschließende Weiterverarbeitung derart prozessierter Dokumente ebenfalls zu vereinfachen, wurde ein entsprechendes Ausgabeformat entwickelt. Dieses lehnt sich an das CoNLL2009-Format
        an und stellt die Ergebnisse der Pipeline in tabellarischer Form dar. Dabei befindet sich in jeder Zeile ein Token, während die dazugehörigen Informationen wie Lemma, POS-Tag und ähnliches je in einer Spalte stehen. Dadurch werden alle durch Komponenten der Pipeline ermittelten Informationen in einer Datei zusammengefasst. Dieses Format hat den Vorteil, dass es für menschliche Nutzer übersichtlich und gut lesbar ist. Zudem ist es als Tabstopp-getrennte Datei auch für gängige Skriptsprachen wie Python oder R, sowie Tabellenkalkulationsprogramme wie Excel leicht zugänglich.
      
         Um die Verwendung des Wrappers und die Weiterarbeit mit dem Ausgabeformat zusätzlich zur Dokumentation anschaulich zu beschreiben, wurden außerdem eine Reihe von Tutorials zu Beispielanwendungen aus Bereichen der digitalen Literaturwissenschaft, wie zum Beispiel der Stilometrie oder dem Topic Modeling, verfasst. Die Dokumentation sowie die Tutorials sind ebenfalls auf GitHub zu finden.
         Das Poster wird all diese Punkte in übersichtlicher Form zusammenführen und potentiellen Nutzern präsentieren. Dabei werden die Funktionsweise der Pipeline, die Arbeit mit den Konfigurationsdateien, der Aufbau und die Verwendung des Ausgabeformats sowie Anwendungsbeispiele im Mittelpunkt stehen.
      
      
         
            
               Bibliographie
               
                  Dokumentation: DARIAH-DKPro-Wrapper v0.4.3 (2016): User guide DARIAH-DKPro-Wrapper v0.4.3 DARIAH2 -
            Cluster 5, Use Case 1 Team. Universität Würzburg, TU Darmstadt - DARIAH-DE
            https://rawgit.com/DARIAH-DE/DARIAH-DKPro-Wrapper/master/doc/user-guide.html
              [letzter Zugriff 08. Januar 2016].
               
                  CoNLL-2009 Format (2008-*): CoNLL-2009 Shared Task. Syntactic and Semantic Dependencies in
              Multiple Languages. Institute of Formal and Applied Linguistics, Charles
              University in Prague, Czech Republic, Faculty of Mathematics and Physics
              https://ufal.mff.cuni.cz/conll2009-st/task-description.html
                [letzter Zugriff 08. Januar 2016]. 
            
         
      
   



      
         Zu den wichtigsten Arbeitsinstrumenten der Digital Humanities gehören die algorithmische Verarbeitung von Daten, die statistische Modellierungen von Zusammenhängen in Daten und visuelle Analysemethoden, um Daten verstehen zu können. Diese Instrumente folgen bestimmten Routinen wissenschaftlicher Praxis: Sie verwenden erprobte Algorithmen und halten sich an Standards der Datenmodellierung. Gleichzeitig konstituieren sie aber auch die wissenschaftliche Praxis mit – sie sind, um mit Ludwick Fleck zu sprechen, Mittel zur Profilierung eines Denkstils innerhalb eines wissenschaftlichen Denkkollektivs. Dazu gehören nicht nur die erwähnten Arbeitsinstrumente, sondern auch sprachliche Mittel: Fachbegriffe, Metaphern, Stile.
         Um die Verknüpfung von Arbeitsinstrumenten und wissenschaftlichen Routinen genauer zu verstehen, müssen die Praktiken und Kulturen, in denen diese Instrumente erstellt werden, reflektiert werden. Welchen Einfluss hat die Wahl einer bestimmten Programmiersprache zur Implementierung eines Algorithmus auf die Digital-Humanities-Praxis? Beispielsweise die Verwendung des „postmodernen“ Perl (Wall 1999) statt Python? Welchen Einfluss hat die Programmiersprache auf die Art der algorithmisch erstellen Visualisierung? Beispielsweise die Verwendung von D3.js, P5.js, R oder der Software Excel (Bubenhofer 2015)? Welchen wissenschaftlichen Paradigmen entspringen populäre statistische Modellierungen? Beispielsweise Topic Modelling oder Support Vector Machines? Und mit welchen sprachlichen Mitteln werden die angewandten Instrumente in den wissenschaftlichen Diskurs eingebracht und legitimiert? 
         Wissenschaftsgeschichtliche Ansätze von Ludwik Fleck (Fleck 1983, 2011) oder auch
        Thomas S. Kuhn (Kuhn 1996) lassen sich fruchtbar verknüpfen mit Überlegungen der
        Software Studies (Fuller 2003; Mackenzie 2006; Manovich 2013; Cox / McLean 2012),
        die den kulturellen Kontext und die soziale Praxis als Einflussfaktoren von
        Software-Erstellung und -Nutzung betonen. Ebenso existiert eine Diskussion um die
        Rolle von Algorithmen, statistischen Modellierungen oder generell Software-„Tools“
        in Forschungsprozessen der Digital Humanities (Berry 2014; Bubenhofer / Scharloth
        2015; Kath et al. 2015; Rieder / Röhle 2012). Die reiche Praxis der
        Informationsvisualisierung und Visual Analytics für Fragen der Digital Humanities
        führt ebenso nicht nur zu methodischen, sondern auch methodologischen und
        theoretischen Diskussionen (Chen et al. 2008; Keim et al. 2010). Auch die Rolle von
        Denkstilen in Wissenschaftsdiskursen und ihre Manifestation auf sprachlicher Ebene
        wird in neuerer Zeit intensiver reflektiert (Czachur 2013; Fix 2011; Schiewe
        1996).
         Der Vortrag möchte vor diesem Hintergrund Code, Modelle, Visualisierungen und Sprache als Mittel und Instrument im Kontext wissenschaftlicher Routinen in den Digital Humanities reflektieren. Inwiefern drücken sich in den gewählten Programmiersprachen, Algorithmen, Visualisierungstypen, statistischen Modellen und sprachlich gefassten Interpretationen unterschiedliche Denkstile der Digital Humanities aus? Wo liegen die Chancen, aber auch die Gefahren, diese Denkstile zu reproduzieren? Welche Auswirkungen haben die Wahl und der reflektierte oder nicht reflektierte Umgang mit den Instrumenten auf wissenschaftliche Innovation in den Digital Humanities?
         Dazu wird zunächst der Einsatz und die Typen von Visualisierungen in den textorientierten Digital Humanities analysiert und dann die technischen aber auch kulturellen Entstehungsbedingungen der algorithmischen Visualisierungen untersucht.
      
      
         
            
               Bibliographie
               
                  Berry, David M. (2014): Critical
            Theory and the Digital. London, Oxford, New York, New Delhi,
            Sydney: Bloomsbury.
               
                  Bubenhofer, Noah (2015): "Coding Cultures: Über den
            Zusammenhang von Programmiersprachen und Denkstilen", in: Sprechtakel. Linguistische Notizen https://www.bubenhofer.com/sprechtakel/2015/08/08/coding-cultures-ueber-den-zusammenhang-von-programmiersprachen-und-denkstilen/
            [letzter Zugriff 17. August 2015].
               
                  Bubenhofer, Noah / Scharloth, Joachim (2015):
            "Maschinelle Textanalyse im Zeichen von Big Data und Data-driven Turn –
            Überblick und Desiderate", in: Zeitschrift für
            Germanistische Linguistik 43, 1: 1–26.
               
                  Chen, Chun-houh / Härdle, Wolfgang / Unwin, Antony
            (eds.) (2008): Handbook of data visualization (=
            Springer handbooks of computational statistics). Heidelberg:
            Springer.
               
                  Cox, Geoff / McLean, Alex (2012): Speaking Code. Coding as Aesthetic and Political Expression.
            Cambridge, Mass.
               
                  Czachur, Waldemar (2013): "Ludwik Flecks
            Denkstilansatz als Inspiration für die Diskurslinguistik", in: Zeitschrift des Verbandes Polnischer Germanisten 2:
            141–150.
               
                  Fix, Ulla (2011): Denkstile und
            Sprache. Die Funktion von „Sinn-Sehen“ und „Sinn-Bildern“ für die
            „Entwicklung einer wissenschaftlichen Tatsache“ http://home.uni-leipzig.de/fix/Fleck.pdf [letzter Zugriff 04.
            März 2014].
               
                  Fleck, Ludwik / Werner, Sylwia / Zittel, Claus (eds.)
            (2011): Denkstile und Tatsachen: Gesammelte Schriften
            und Zeugnisse. Berlin: Suhrkamp.
               
                  Fleck, Ludwik / Schäfer, Lothar / Schnelle, Thomas
            (eds.) (1983): Erfahrung und Tatsache: gesammelte
            Aufsätze. Frankfurt am Main: Suhrkamp.
               
                  Fuller, Matthew (2003): Behind the
            blip: essays on the culture of software. New York:
            Autonomedia.
               
                  Kath, Roxana / Schaal, Gary S. / Dumm, Sebastian
            (2015): "New Visual Hermeneutics", in: Zeitschrift für
            germanistische Linguistik 43, 1: 27–51.
               
                  Keim, Daniel A. / Kohlhammer, Jörn / Ellis, Geoffrey et
              al. (2010): Mastering the information age -
              solving problems with visual analytics. Goslar: Eurographics
              Association.
               
                  Kuhn, Thomas S. (1996): Structure
              of Scientific Revolutions. University of Chicago Press.
               
                  Mackenzie, Adrian (2006): Cutting
              Code: Software And Sociality (Digital Formations). Bern / Berlin /
              Frankfurt am Main / New York / Paris / Wien: Peter Lang.
               
                  Manovich, Lev (2013): Software
              Takes Command. New York / London: INT edition.
               
                  Rieder, Bernhard / Röhle, Theo (2012): "Digital
              Methods: Five Challenges", in: Berry, David M. (eds.): Understanding Digital Humanities. Basingstoke: Palgrave
              67–84.
               
                  Schiewe, Jürgen (1996): Sprachenwechsel - Funktionswandel - Austausch der Denkstile. Die
              Universität Freiburg zwischen Latein und Deutsch. Tübingen: Niemeyer.
               
                  Wall, Larry (1999): Perl, the first
              postmodern computer language
                  http://www.wall.org/~larry/pm.html [letzter Zugriff 19. August
                2015].
            
         
      
   



      
         
            Einleitung
            Dieser Beitrag stellt eine neuartige Methode zur optischen Zeichenerkennung (
          Optical Character Recognition, OCR) speziell für Textvorlagen des 17. Jahrhunderts vor. Anstatt ein neues OCR-Verfahren zu entwickeln, werden zwei etablierte Open-Source-Lösungen genutzt. Die Ausgaben der Programme werden computergestützt kombiniert, um so eine möglichst genaues Textergebnis zu erhalten. Die Besonderheiten und die Güte der Methode wird anhand der Texterfassung von Gelegenheitsgedichten von Simon Dach illustriert.
        
            
               OCR
               OCR bezeichnet die Gesamtheit von Verfahren, die in der Lage sind, aus
            Rastergrafiken Schriftzeichen zu erkennen. Der Begriff wird sowohl für die
            eigentliche Mustererkennung als auch für den gesamten Prozess der
            Bildverarbeitung verwendet. Letzterer gliedert sich normalerweise in drei
            Schritte: 1. Bildoptimierung: Diese besteht aus der
            Bitonalisierung der Digitalisate, ihrer Begradigung (sog. Deskewing) und aus der Entfernung von Artefakten (sog. Despeckling). Außerdem können beim Scannen
            entstandene Wellen in einzelnen Zeilen automatisch begradigt werden (sog.
            Dewarping). 2.
            Strukturerkennung ( Optical Layout
            Recognition, OLR): Die einzelnen Seiten werden u. a. in Spalten,
            Absätze und Zeilen gegliedert. 3. Mustererkennung
          (OCR): Für diese Aufgabe gibt es verschiedene Lösungsvorschläge sowohl
          im kommerziellen wie auch im Open-Source-Bereich. Besonders verbreitet sind
          die Software FineReader der Firma ABBYY sowie BITAlpha aus dem Hause Tomasi, die u. a. von
          Bibliotheken eingesetzt werden. Die bekanntesten Open-Source-Lösungen sind
          das ursprünglich von Hewlett-Packard entwickelte und heute von Google
          betreute Tesseract (GitHub 2016a) und das
          ursprünglich am DFKI Kaiserslautern entwickelte OCRopus (GitHub 2016b). 
               Grundsätzlich lassen sich bei OCR zwei unterschiedliche Erkennungsansätze unterscheiden: zeichenorientierte Verfahren wie Tesseract vergleichen das Bild eines Zeichens Pixel für Pixel mit einer Datenbasis (dem sog. Modell) und geben das ähnlichste Zeichen zurück. Sequenzorientierte (segmentierungsfreie) Verfahren wie OCRopus legen ein Raster fester Größe über eine Zeile und bestimmen anhand der Folgen der einzelnen Spalten, repräsentiert als Bitvektoren (0 entspricht weiß, 1 schwarz) die wahrscheinlichste Zeichensequenz. 
            
            
               Gelegenheitsgedichte
               Unsere Studie beschäftigt sich mit OCR am Beispiel von Gelegenheitsgedichten
            des 17. Jahrhunderts, denen durch die von Segebrecht (1977) initiierte
            literaturwissenschaftliche Neubewertung eine zunehmende kulturgeschichtliche
            Bedeutung zukommt (vgl. Klöker 2010: 39). Der Zugriff auf diese Drucke wurde
            durch das VD17 (HAB 2007-2016)1 und durch das Handbuch des personalen
            Gelegenheitsschrifttums in europäischen Bibliotheken und Archiven
            (Garber 2001-2013) erleichtert. Dennoch kann ein digitales Korpus für diese
            Textsorte heute nur als Desiderat wahrgenommen werden. Für Werke von Simon
            Dach ist die Ausgangslage scheinbar besser: Mit der digitalisierten
            vierbändigen Ausgabe von Ziesemer (Ziesemer 1936-1938) steht ein großer Teil
            der heute bekannten Gedichte zur Verfügung (vgl. auch Dach o. J.; TextGrid
            2015). 2 Jedoch trübt sich dieser Eindruck beim textkritischen Blick. 3
               
               111 Funeralschriften Simon Dachs wurden im Verlauf des DFG-Pilotprojektes zum
            OCR-Einsatz bei der Digitalisierung der
              Funeralschriften der Staatsbibliothek zu Berlin (2009-2011)
              (Federbusch / Polzin 2013) digitalisiert und per OCR erfasst. Die in der
              vorliegenden Studie genutzten Drucke zeichnen sich dahingehend aus, dass
              eine einheitliche Schrifttype sowie ein einfaches Layout vorliegen. Im
              Unterschied zu Texten des 18. und 19. Jahrhunderts war für diese Drucke noch
              ein relativ hoher manueller Aufwand erforderlich. Die Schrifttypen weisen
              daher eine vergleichsweise hohe Varianz bzgl. ihrer Form auf. Die 111
              Trauergedichte weisen eine Textgenauigkeit von bis zu 95% auf. Der
              Schwerpunkt der folgenden Studie liegt auf der Entwicklung und Prüfung von
              Methoden, die perspektivisch eine korrektere Übertragung der Textquellen aus
              dem 17. Jahrhundert liefern soll. 
            
         
         
            Arbeitsablauf
            
               
               
                  Abb. 1: Modell eines vollständigen Erfassungsworkflows
                (diese Studie betrifft die eingefärbten Stationen). 
            
            Abbildung 1 gibt einen Überblick über den Arbeitsablauf der hier vorgestellten Methode. Im Unterschied zu existierenden Workflows unterteilt unser Vorschlag die Bildoptimierung in zwei Phasen: 1.
                global: Das komplette Digitalisat wird beschnitten, binarisiert, begradigt und von Artefakten befreit. Danach findet die Optische Layouterkennung (OLR) statt. 2.
                lokal: Die identifizierten Textzonen werden aus dem Bild der Seite ausgeschnitten und nochmals begradigt. Dadurch wird die häufig zu beobachtende Trapezform der Digitalisate, die durch Scannen von Büchern ohne Auftrennen des Buchrückens entsteht, behandelt. Die Bilder für die einzelnen Zonen werden anschließend in Zeilen zerschnitten und den OCR-Engines übergeben.
              
            Unser Vorgehen bei der OCR orientiert sich an der manuellen Texterfassung per Double Keying: Dabei werden Texte von zwei unabhängigen
                Erfassern transkribiert. Im Vergleich der beiden Textversionen werden die
                Unterschiede ermittelt und die korrekte Version ausgewählt. Um den
                Genauigkeitsgewinn durch die Mehrfacherfassung zu erhöhen, wurden zwei
                paradigmatisch verschiedene OCR-Verfahren, Tesseract und OCRopus, mit
                unterschiedlichen Stärken und Schwächen eingesetzt. Beide Open-Source-Programme
                erlauben ein Training auf die vorwendeten Typen und die Anwendung spezifischer
                OCR-Modelle. Dies ist wie Springmann et al. (2015) zeigen ein wesentlicher
                Vorteil gegenüber den meisten Closed-Source-Lösungen, da die mitgelieferten
                OCR-Modelle insbesondere für frühe Druckerzeugnisse bzw. gebrochene Schriften
                sehr schlechte Ergebnisse bzgl. der Textgenauigkeit liefern. Die automatische
                Vereinigung der beiden Textversionen findet im Wesentlichen auf Basis einer
                Textdifferenzberechnung mit Hilfe von diff (Hunt /
                McIlroy 1976) statt, wobei im Falle von Unterschieden verschiedene
                Bewertungsheuristiken zur Bestimmung der korrekten
                Textversion eingesetzt werden. Das skizzierte Vorgehen erlaubt auch die
                Kombination von mehr als zwei Textversionen sowie den anschließenden Einsatz von
                OCR-Nachkorrekturverfahren (vgl. z. B. Vobl et al. 2014). 
         
         
            Evaluation
            Die Güte der hier vorgestellten Methode wird anhand der Volltexterfassung von
                  Funeralschriften Simon Dachs (vgl. 1.2) evaluiert. Dabei konzentriert sich die
                  Evaluation auf drei Punkte: 
            
               Welchen Einfluss hat die Wahl der Binarisierungsmethode auf die Textgenauigkeit?
               Wie groß ist der Unterschied zwischen einem Standardmodell und einem speziell für die zu erfassenden Texte trainierten Modell bzgl. der Textgenauigkeit?
               Kann die Vereinigung zweier durch OCR erzeugter Texte die Textgenauigkeit erhöhen?
            
            Ein typisches Beispiel für die Untersuchungsgrundlage sowie die entsprechenden OCR-Ausgaben gibt Abbildung 2.
            
               
               
                  Abb. 2: Vergleich der OCR-Ergebnisse. 
            
         
         
            Material
            
               Ground Truth
               Voraussetzung für die Evaluation und das Modelltraining ist fehlerfreier
                        Volltext ( Ground Truth). Um für die Studie
                        entsprechende Daten zu gewinnen, wurde eine manuelle Korrektur aller 111
                        Texte vorgenommen. Die Korrektur schloss nicht nur die Text-, sondern auch
                        die datenstrukturelle Ebene ein. Der Aufwand belief sich auf 150 Stunden. Im
                        Ergebnis liegen alle Texte im DTA-Basisformat vor und sind über die
                        Qualitätssicherungsplattform DTAQ zugänglich. 
            
            
               Materialauswahl
               Für das Training der spezifischen OCR-Modelle wurden 30 Seiten Ground-Truth zufällig ausgewählt. Für die Evaluation der Modelle wurden 25 andere zufällig ausgewählte Seiten verwendet.
            
            
               Referenzlexikon
               Zur Vereinigung beider OCR-Versionen wurde ein Referenzlexikon gültiger historischer Schreibungen des 17. Jahrhunderts herangezogen. Dazu wurden Wortformen (
                          n=217067) aus DTA-Texten dieses Zeitraums extrahiert.
                        
            
         
         
            Durchführung
            
               Vorverarbeitung
               Für Beschneidung und Begradigung wurde das Programm Scantailor (GitHub 2016 a) eingesetzt. Für die Binarisierung,
                          Artefaktbereinigung und Zeilenglättung wurde sowohl Scantailor als auch das
                          in OCRopus enthaltene Werkzeug nlbin verwendet. 
            
            
               OLR
               Die einzelnen Textzonen (Abschnitte und Kustoden) wurden mit Hilfe von Leptonica (Bloomberg 2001-2015) lokalisiert und
                            manuell nachkorrigiert. Für die Untergliederung der Zonen in Zeilen wurde
                            ebenfalls Leptonica eingesetzt. 
            
            
               OCR
               Die Zeichenerkennung erfolgte sowohl mit OCRopus als auch mit Tesseract. Die erste Versuchsreihe basierte auf mitgelieferten Modellen. Für die zweite Versuchsreihe wurden die OCR-Programme mit Ground-Truth-Daten trainiert. Für das Training der OCRopus-Modelle wurde OCRopus eingesetzt. Dabei wurde für das Training aus Gründen der Modellvergleichbarkeit eine feste Anzahl von Iterationsschritten (
                              n=30000) festgelegt. Die Tesseract-Modelle wurden mit Hilfe von
                              VietOCR erstellt.
                            
            
            
               Textvereinigung
               Die Textvereinigung wurde in
                              Python mit Hilfe des Moduls
                              difflib implementiert. Neben dem Referenzlexikon standen zur Konfliktauflösung auch die von den OCR-Programmen zurückgelieferten Konfidenzen auf Zeichenebene zur Verfügung. Waren sich die beiden Engines bzgl. eines Wortes bzw. einer Textsequenz uneins, wurde zunächst dem Wort Vorrang gegeben, dass sich im Referenzlexikon befindet. Konnte dort keine der beiden Versionen gefunden werden, wurde die Entscheidung auf Basis der Konfidenzwerte getroffen.
                            
            
            
               Qualitätsmessung
               Die Bestimmung der Textqualität erfolgte durch Messung des Anteils falsch erkannter Zeichen (Fehlerrate in Prozent) im Vergleich zum fehlerfreien Volltext.
            
         
         
            Ergebnisse und Diskussion
            Tabelle 1 gibt einen Überblick über die Ergebnisse der Evaluation bzgl. der
                            Fehlerrate auf Zeichenebene unter Berücksichtigung der Vorverarbeitung des
                            Trainings- und Testmaterials, der Modellklasse (standard vs. spezifisch) und der
                            eingesetzten OCR-Software (OCRopus, Tesseract). Das beste (grün) und das schlechteste Ergebnis (rot) sind hervorgehoben. Da wir keinen
                            Einfluss auf die Vorverarbeitung der Trainingsmaterialien der mitgelieferten
                            Modelle haben, ist die Matrix in dieser Hinsicht unvollständig. 
            
               
               
                  Tab. 1: Darstellung der Ergebnisse auf Einzel-OCR-Ebene im Bezug auf
                                Vorverarbeitungsmethode für Trainings- und Testmaterial, Modelltyp und verwendete
                                OCR-Software. 
            
            Die geringste erreichte Fehlerrate (3,89 %) liegt etwa im Bereich der
                                Textgenauigkeit der 111 Gedichte aus der Pilotstudie von Federbusch (Federbusch
                                / Polzin 2013). Die Fehlerrate von Tesseract ist jeweils höher als die von
                                OCRopus. Der sequenzorientierte Ansatz hat klare Vorteile bei der Erkennung von
                                Schriftzeichen, die die typischen Charakteristika früher Drucke aufweisen. 5
            
            Desweiteren zeigt sich, dass die Vorverarbeitung mit nlbin für Tesseract sowohl auf Trainings- als auch auf Testebene jeweils schlechtere Ergebnisse bringt. Für OCRopus sind die Ergebnisse bzgl. der Vorverarbeitung differenzierter: Die beste Kombination liefert eine Vorverarbeitung des Trainingsmaterials mit nlbin bei einer nachfolgenden Vorverarbeitung des Testmaterials mit Scantailor. Unterschiede im Ergebnis der Vorverarbeitung beider Programme illustriert Abbildung 3.
            
               
                Abb. 3: Bild einer Textzeile nach der Vorverarbeitung mit nlbin (oben) und
                                  Scantailor (unten). 
            
            Die von Scantailor durchgeführte Bildvorverarbeitung ist deutlich normativer und für einen zeichenorientierten Ansatz wie Tesseract besser geeignet. Das Training sequenzorientierter Ansätze leidet unter dieser Vergröberung.
            Es zeigt sich erneut, dass spezifisch trainierte Modelle eine massive Textgenauigkeitsverbesserung mit sich bringen können (vgl. auch Springmann et al. 2015).
            
               Textvereinigung
               Betrachtet man die Beispielausgaben in Abbildung 2, so wird der
                                    Qualitätsunterschied zwischen beiden OCR-Programmen ersichtlich. An
                                    einzelnen Stellen jedoch (z. B. Großbuchstaben am Anfang der Zeile im
                                    letzten Abschnitt) hat Tesseract Erkennungsvorteile.
               Ausgehend von diesem Befund wurde der jeweils genaueste Text von OCRopus und Tesseract miteinander vereinigt. Es hat sich gezeigt, dass die Konfidenzen, die die Programme für jedes Zeichen zurückliefern, kein verlässliches Kriterium sind, um Konflikte aufzulösen. Die Fehlerrate nimmt zu. Die Strategie, Wörter bzw. Sequenzen zu bevorzugen, die sich im Referenzlexikon befinden, hat dagegen eine messbare Verbesserung mit sich gebracht. Die Anzahl der falsch erkannten Zeichen konnte um 14 % reduziert werden (Fehlerrate 3,34 %). Es ist zu vermuten, dass der Effekt größer wäre, wenn zwei OCR-Ergebnisse mit vergleichbarer Qualität vorlägen. Dies bleibt jedoch zum jetzigen Zeitpunkt für Drucke des 17. Jahrhunderts ein Desiderat.
            
         
      
      
         
            Verzeichnis der im deutschen Sprachraum erschienenen Drucke des 17. Jahrhunderts.
            Vgl auch Dach (o. J.) in http://www.zeno.org/Literatur/M/Dach,+Simon/Gedichte sowie TextGrid (2015).
            „Ziesemers Dach-Ausgabe ist textlich zu wenig genau, um auch für die dort abgedruckten, fast ausnahmslos deutschsprachigen, Gedichte den Rückgriff auf die kasualen Einzeldrucke und andere zeitgenössische Ausgaben entbehren zu können. Jede Stichprobe erweist für jedes einzelne Gedicht Transkriptionsfehler und unerklärte Texteingriffe.“ (Walter 2008: 466)
            Für Frakturdrucke des 19. Jahrhunderts ist ein solch starker Unterschied zwischen den Tesseract und OCRopus nicht nachgewiesen.
         
         
            
               Bibliographie
               
                  Bloomberg, Dan (2001-2015): Leptonica http://www.leptonica.com/
                                    [letzter Zugriff: 15. Oktober 2015].
               
                  Dach, Simon (o. J.): Gedichte
                  
                                      http://www.zeno.org/Literatur/M/Dach,+Simon/Gedichte [letzter
                                      Zugriff 15. Oktober 2015]. 
               
                  Federbusch, Maria / Polzin, Christian (2013): Volltext via OCR - Möglichkeiten und Grenzen.
                                      Testszenarien zu den Funeralschriften der Staatsbibliothek zu Berlin -
                                      Preußischer Kulturbesitz. Berlin Staatsbibliothek zu Berlin http://staatsbibliothek-berlin.de/fileadmin/user_upload/zentrale_Seiten/historische_drucke/pdf/SBB_OCR_STUDIE_WEBVERSION_Final.pdf
                                      [letzter Zugriff 15. Oktober 2015].
               
                  Garber, Klaus (2001-2013): Handbuch
                                      des personalen Gelegenheitsschrifttums in europäischen Bibliotheken und
                                      Archiven. 13 Bände. Hildesheim / Zürich / New York: Olms /
                                      Weidmann. 
               
                  GitHub Inc. (2016a): ScanTailor
                  http://scantailor.org/ [letzter
                                      Zugriff 15. Oktober 2015].
               
                  GitHub Inc. (2016b): OCRopus
                  https://github.com/tmbdev/ocropy [letzter Zugriff 15. Oktober
                                        2015]. 
               
                  GitHub Inc. (2016c): Tesseract
                  https://github.com/tesseract-ocr [letzter Zugriff 15. Oktober
                                          2015].
               
                  HAB = Herzog August Bibliothek Wolfenbüttel
                                          (2007-2016): VD17. Das Verzeichnis der im deutschen
                                          Sprachraum erschienenen Druck des 17. Jahrhunderts http://www.vd17.de/index.php?category_id=1&article_id=1&clang=0.
               
                  Hunt, James W. / McIlroy, M. Douglas (1976): "An
                                          Algorithm for Differential File Comparison" in: Computing
                                          Science Technical Report (Bell Laboratories) 41 http://www.cs.dartmouth.edu/~doug/diff.pdf
               
               
                  Klöker, Martin (2010): "Das Testfeld der Poesie.
                                          Empirische Betrachtungen aus dem Osnabrücker Projekt zur 'Erfassung und
                                          Erschließung von personalen Gelegenheitsgedichten'", in: Keller, Andreas /
                                          Lösel, Elke / Wels, Ulrike / Wels, Volkhard (eds.): Theorie und Praxis der Kasualdichtung in der Frühen Neuzeit (=
                                          Chloe. Beihefte zu Daphne 43). Amsterdam / New York: Rodopi 39-84. 
               
                  Python Software Fundation (1990-2016): difflib - Helpers for Computing Deltas 
                  https://docs.python.org/2/library/difflib.html [letzter Zugriff
                                          15. Oktober 2015].
               
                  Segebrecht, Wulf (1977): Das
                                          Gelegenheitsgedicht. Ein Beitrag zur Geschichte und Poetik der
                                          deutschen Lyrik. Suttgart: Metzler.
               
                  Springmann, Uwe / Lüdeling, Anke / Schremmer, Felix
                                          (2015): "Zur OCR frühneuzeitlicher Drucke am Beispiel des RIDGES-Korpus von
                                          Kräutertexten (Poster)", in: Tagung der DHd (Digitale
                                          Geisteswissenschaften im deutschsprachigen Raum), Graz https://www.linguistik.hu-berlin.de/de/institut/professuren/korpuslinguistik/mitarbeiter-innen/anke/pdf/SpringmannLuedelingSchremmer2015.pdf
                                          [letzter Zugriff 15. Oktober 2015].
               
                  TextGrid (2015): Die digitale
                                          Bibliothek bei TextGrid
                  https://textgrid.de/digitale-bibliothek [letzter Zugriff 15.
                                            Oktober 2015] 
               
                  VietOCR
                  http://vietocr.sourceforge.net/ [letzter Zugriff: 15. Oktober
                                              2015].
               
                  Vobl, Thorsten / Gotscharek, Annette / Reffle, Uli /
                                                Ringlstetter, Christoph / Schulz, Klaus U. (2014): "PoCoTo - an
                                                open source system for efficient interactive postcorrection of OCRed
                                                historical texts" in: Proceedings of the First
                                                International Conference on Digital Access to Textual Cultural Heritage
                                                (DATeCH '14): 57-61 http://dl.acm.org/citation.cfm?id=2595197 [letzter Zugriff 15.
                                                Oktober 2015].
               
                  Walter, Axel E.(2008): "Dach digital? Vorschläge zu
                                                einer Bibliographie und Edition des Gesamtwerks von Simon Dach nebst einigen
                                                erläuterten Beispielen vernachlässigter bzw. unbekannter Gedichte", in:
                                                Walter, Axel E. (ed.) in: Simon Dach (1605–1659).
                                                Werk und Nachwirken. Tübingen: Niemeyer: 465-522.
               
                  Ziesemer, Walter (ed.) (1936-1938): Simon Dach: Gedichte. Vier Bände. Halle an der Saale:
                                                Niemeyer.
               
            
         
      
   



      
         
            Einleitung: Digital Humanities und Filmanalyse
            Während sich die „Vermessung der Kultur“ (Lauer 2013) in den textorientierten
          Geisteswissenschaften in den letzten Jahren rasant entwickelt hat (vgl. etwa
          Konzepte wie Culturomics, Distant
          Reading, etc.), so befindet sich die „Vermessung ästhetischer
          Erscheinungen“ (Flückinger 2011) für den Bereich der Filmwissenschaft und
          Filmanalyse noch in den Anfängen. Flückinger (2011: 44) spricht in diesem
          Zusammenhang gar von einem Spannungsfeld zwischen Empirie und Ästhetik, welches
          sich zwangsläufig ergeben muss, wenn man „die eigentümliche Unschärfe, die allen
          künstlerischen Werken eignet, in messbare Einheiten zerlegen will“. Dabei lassen
          sich quantitative Ansätze in der Filmanalyse mindestens bis in das Jahr 1912
          1 zurückverfolgen und auch aktuelle Lehrbücher zur Filmanalyse beschreiben
          gleichermaßen weiche (qualitative) und harte (quantitative) Kategorien und Methoden (Korte 2004: 15). Bei
          quantitativen Ansätzen steht vor allem die Analyse von Dauer und
          Auftretenshäufigkeit einzelner Einstellungen in einem Film im Mittelpunkt (vgl.
          Salt 2006, Kap. „The Numbers Speak“). So stellt etwa die online verfügbare
          Datenbank 
                  Cinemetrics
                (Cinemetrics o. J.) entsprechende Informationen
          zur Länge und Verteilung einzelner Einstellungen für mehrere tausend Filme
          bereit und ermöglicht so vergleichende Analysen von Filmen aus unterschiedlichen
          Genres und Epochen. 
            Während die Segmentierung der Filme in der Cinemetrics-Datenbank von der Community manuell vorgenommen wird, gibt es
            auch Beispiele für Forschungsarbeiten, bei denen die quantifizierbaren Parameter
            automatisch erhoben werden. Hoyt, Ponot und Roy (2014) präsentieren etwa einen
            Prototyp namens ScripThreads, der in der Lage ist, Filme
            der American Film Scripts Online-Datenbank zu parsen und
            die Handlungsentwicklung eines Films anhand der Szenen und Figuren zu
            visualisieren. Ein Beispiel für die vergleichende Analyse von Filmmetadaten
            findet sich im 
                  Cinegraph-Projekt  von Chris Weaver
            (2014). Hier können Filme anhand unterschiedlicher Metadaten (z. B. Filmname,
            Veröffentlichungsdatum, Bewertung, Genre, Oscars, Darsteller, etc.) miteinander
            verglichen und in einer interaktiven Darstellung zueinander in Beziehung gesetzt
            werden. 
            Daneben finden sich im Netz eine ganze Reihe experimenteller Tools, die nicht
              immer einen wissenschaftlichen Anspruch haben, aber gut illustrieren, welche
              weiteren Aspekte von Filmen automatisch analysierbar sind: Beispielhaft sei etwa
              das Python-Tool 
                  VideoGrep
                (Lavigne 2014) genannt, welches das
              Durchsuchen von Filmdialogen nach bestimmten Schlüsselwörtern ermöglicht, um auf
              Basis der Treffer dann einen automatischen Zusammenschnitt („supercut“) all der
              Szenen, in denen das gesuchte Wort vorkommt, zu erstellen. Die Anwendung 
                  Pretentious-O-Meter
                (Beard 2015) analysiert automatisch, wie
              groß die Bewertungslücke zwischen Nutzerbewertungen und professionellen
              Filmkritiken eines Films ist und visualisiert dies in einem Kontinuum, welches
              von „mass-market“ bis „very pretentious“ reicht. Weitere Ansätze der
              automatischen Filmanalyse finden sich für die Farbverwendung in Filmen: Frederic
              Brodbeck (2011) visualisiert in seinem 
                  Cinemetrics-Projekt Filme als kreisförmig angeordnete
              Timelines, in denen u. a. die jeweils dominanten Farben zu sehen sind. Ein
              weiteres Projekt visualisiert Filme als zusammengestauchte Einzelframes, um so
              farbige 
                  MovieBarcodes
                (MovieBarcodes o. J.) zu erstellen. 
            Auch auf der DHd 2015 wurde das Thema der Quantifizierung filmischer Strukturen über Filmbild, Filmschnitt und Filmstil bereits auf methodischer Ebene thematisiert (Heftberger 2015) und Howanitz (2015) präsentierte eine erste
                Distant Watching-Studie für das „Fern-Sehen“ memetischer YouTube-Videos, deren „Schnittkurven“ er auf Frame-Ebene analysiert. In diesem Beitrag knüpfen wir thematisch an die genannten DHd-Vorträge an und diskutieren grundlegende Möglichkeiten der computergestützten Filmanalyse, die über die Quantifizierung von Einstellungen und Szenen hinausgehen. Dabei sollen weitere automatisch quantifizierbare Parameter zur Diskussion gestellt werden, um so neue Perspektiven und Zugänge zur computergestützten Filmanalyse aufzuzeigen und das Thema noch stärker in den Digital Humanities zu verankern. Um die Grenzen und Möglichkeiten dieser Ansätze besser illustrieren zu können, wurde eine Reihe von Prototypen erstellt, die nachfolgend kurz vorgestellt werden.
              
         
         
            Prototypen für die computergestütze Filmanalyse
            In diesem Abschnitt werden drei unterschiedliche Prototypen beschrieben, die
                jeweils auf unterschiedliche quantifizierbare Aspekte von Filmen abzielen und
                damit die Untersuchung ganz unterschiedlicher Fragestellungen erlauben. Die
                Tools greifen allesamt auf im Web frei verfügbare Informationen zu Filmen
                zurück: So stehen etwa über die Plattformen 
                  OpenSubtitles
                oder die 
                  Internet Script Movie Database
               
                maschinenlesbare Dialoge von Filmen und Serien in großem Umfang zur Verfügung.
                Zusätzlich können detaillierte Metadaten sowie auch nutzergenerierte Bewertungen
                und Kommentare zu Filmen über Plattformen wie 
                  IMDb
                (Internet Movie Database)
                abgerufen werden. Darüber hinaus soll als weiterer quantifizierbarer Parameter,
                der direkt aus den Filmen extrahiert werden kann, die Farbverwendung 2 in die Analysen mit einbezogen werden. Alle nachfolgend beschriebenen
                Prototypen wurden jeweils mit Standard-Webtechnologien (HTML / CSS / JavaScript)
                und bestehenden Python-Bibliotheken umgesetzt. 
            
               
                  SubVis – Analyse der Filmsprache
                  
               Das SubVis-Tool analysiert über OpenSubtitles verfügbare Dialoge von beliebigen, zunächst
                  allerdings nur englischsprachigen Filmen anhand typischer linguistischer
                  Parameter wie Wortfrequenzen oder POS-Tagging und visualisiert die
                  Ergebnisse in einem interaktiven Web-Interface. Zusätzlich kann die
                  Auftretenshäufigkeit einzelner Zeichen oder längerer Sprachsequenzen (=
                  jeweils ein eingeblendeten Untertitel) für beliebig definierbare
                  Analyseintervalle (z. B. jeweils für 5 Minuten-Sequenzen) in einer Timeline
                  dargestellt werden, um bspw. auf einen Blick zu sehen, an welchen Stellen im
                  Film besonders viel oder wenig gesprochen wird (vgl. Abbildung 1). 
               
                  
                  
                     Abb. 1: Beispielhafte Visualisierung der Zeichen-
                    und Sprachsequenzhäufigkeiten für jeweils fünfminütige Teilabschnitte
                    des Films „Anchorman: The Legend of Ron Burgundy“.
               
               
                  Beispielhafte Fragestellungen, die mit dem Tool untersucht werden können:
                  
               
                  Gibt es für die Filme unterschiedlicher Regisseure jeweils typische Schlüsselwörter?
                  Kann man für Filme aus unterschiedlichen Genres beobachten, dass an
                      bestimmten Stellen (z. B. Anfang oder Schluss) besonders viel oder wenig
                      gesprochen wird? 
                  Wird in Filmen aus den 1980er Jahren insgesamt mehr gesprochen als in Filmen der 1990er Jahre?
               
            
            
               
                  Series Analysis Tool (SAT) – Analyse von TV-Serien anhand von Nutzerbewertungen, Figuren und Sprache
                    
               Das Series Analysis Tool ( SAT)
                    ermöglicht die Analyse von Serien und einzelnen Episoden. Dabei werden
                    verschiedene Parameter in einer Timeline-Darstellung visualisiert. Ein
                    wesentlicher Analyseaspekt ist dabei die Bewertung einzelner Episoden durch
                    die IMDb-Community, sodass auf einen Blick erkennbar ist, ob eine Serie im
                    Laufe der Zeit besser oder schlechter bewertet wird, oder ob es einzelne
                    Episoden gibt, die auffallend positiv oder negativ bewertet wurden.
                    Zusätzlich liest das Tool das Figureninventar für jede Episode aus und
                    erlaubt es, die Darstellung nach bestimmten Figuren zu filtern. So kann
                    schnell erkannt werden, ob das Auftreten bestimmter Figuren ggf. Einfluss
                    auf die Bewertung einzelner Episoden hat. Weiterhin wurde die Sprache der
                    Serien hinsichtlich Sentiment- und Emotionswörtern analysiert (vgl.
                    Abbildung 2). Als Datengrundlage dient ein bestehendes Korpus (Tiedemann
                    2012), in dem alle auf 
                     OpenSubtitles
                   in englischer
                    Sprache verfügbaren Untertitel von TV-Serien und Filmen bis zum Jahr 2013
                    enthalten sind. Dabei kam für die Sentiment Analyse das AFINN-Lexikon (Nielsen 2011) und für die Identifikation acht
                    grundlegender Emotionen (Angst, Wut, Freude, etc.) das NRC
                    Emotion Lexicon (Mohammad / Turney 2010) zum Einsatz. Sowohl die
                    Sentiment-Scores (positiv / negativ) als auch die Emotionsmarker können für
                    jede Episode in die Visualisierung mit einbezogen werden, um so potenzielle
                    Korrelationen zu den Nutzerbewertungen aufzuzeigen. 
               
                  
                  
                     Abb. 2: Beispielhafte Visualisierung der Serie
                      „Breaking Bad“, mit paralleler Darstellung der Benutzerbewertungen sowie
                      der Sentiment-Analyse der Dialoge für jede einzelne Episode.
               
               
                  Beispielhafte Fragestellungen, die mit dem Tool untersucht werden können:
                    
               
                  Gibt es generelle Trends bei der Bewertung von Serien mit zunehmender Zahl von Staffeln?
                  Wirken sich Sentiment- und Emotionsmarker der Dialoge positiv oder negativ auf die Bewertung einer Episode aus?
                  Wirkt sich das Auftreten bestimmter Nebenfiguren positiv oder negativ auf die Bewertung einer Episode aus?
               
            
            
               
                  MovieColors – Analyse von Filmen anhand von Farbe und Sprache
                    
               Der Prototyp
                      MovieColors erlaubt die computergestützte Analyse von Filmen anhand der Parameter Farbe und Sprache. Dabei wird zunächst der Film in einzelne Frames zerlegt. Mithilfe eines Clustering-Algorithmus werden dann die jeweils dominanten Farben extrahiert. Anhand dieser Farbinformation können charakteristische Farbprofile – ähnlich wie im eingangs erwähnten
                      MovieBarcodes-Projekt – für den gesamten Film erstellt werden. Zusätzlich wird die Sprache des Films über dessen Untertitel anhand von Wortfrequenzen und grundlegenden Sentiment-Werten (positiv / negativ) analysiert.
                    
               Die Visualisierungskomponente des Tools erlaubt es, Farbinformation und
                      Sprachanalyse in einer parallelen Ansicht darzustellen, um so potenzielle
                      Korrelationen zwischen dem Sentiment der Sprache und besonders markanten
                      Schlüsselwörtern sowie auch der Farbverwendung identifizieren zu können
                      (vgl. Abbildung 3). Zusätzlich kann jeder Frame einzeln angezeigt werden,
                      zusammen mit dem entsprechenden Untertitel sowie einer Analyse der
                      dominanten Farben im jeweiligen Bild.
               
                  
                  
                     Abb. 3: Analyse des Films „König der Löwen“, mit
                        Darstellung des Farbprofils (oben), der Sentiment-Analyse (Mitte) sowie
                        der häufigsten Wörter (unten) entlang der Zeitachse des Films.
               
               
                  Beispielhafte Fragestellungen, die mit dem Tool untersucht werden können:
                      
               
                  Gibt es charakteristische Farbprofile für Filme aus verschiedenen Genres oder Epochen?
                  Korrelieren bestimmte Farben mit positiven oder negativen Sentiment-Scores, also etwa dunkle Farben bei negativer Sprache?
                  Korrelieren bestimmte Farben mit Schlüsselwörtern, also etwa schwarz und lila immer dann, wenn der Bösewicht des Films auftritt?
               
            
         
         
            Ausblick
            Die in diesem Beitrag vorgestellten Prototypen beschreiben erste Versuche, Filme
                      computergestützt anhand unterschiedlicher, automatisch quantifizierbarer
                      Parameter zu analysieren. Im Austausch mit Kollegen aus der Medienwissenschaft
                      werden die Tools in den nächsten Monaten praktisch erprobt und je nach
                      Fragestellung iterativ angepasst und gegebenenfalls um weitere Funktionen
                      ergänzt. Sobald die Prototypen weiter ausgearbeitet sind, sollen sie auch der
                      Community über den DH-Regensburg-Blog zugänglich gemacht werden. Gleichzeitig sind
                      weitere Prototypen angedacht, bei denen als zusätzliche Analyseparameter
                      Gesichtserkennung (vgl. Arandjelovic / Zisserman 2005) sowie auch die Auswertung
                      der Audiospur (vgl. Zulko 2014) umgesetzt werden sollen. 
         
         
            Danksagungen
            Alle hier beschriebenen Prototypen wurden im Rahmen des Projektseminars „Digital Humanities“, im Masterstudiengang Medieninformatik an der Universität Regensburg, angefertigt. Besonderer Dank für die engagierte Umsetzung der Tools gebührt Hanns Meißner und Michael Stahl (
                        SubVis), Robert Jackermeier, Florian Ludwig und Alexander Uitz (
                        SAT) sowie Michael Kao (
                        MovieColors).
                      
         
      
      
         
            Vgl. den Vortrag von Tsivian (2014) auf der 1. Cinemetrics Conference, Chicago (Neubauer Collegium 2014).
            Zur historischen Verwendung von Farbe im Film vgl. auch die Online-Datenbank "Timeline of Historical Film Colors" von Flückinger (2011-2013).
         
         
            
               Bibliographie
               
                  Arandjelovic, Ognjen / Zisserman, Andrew (2005):
                          "Automatic face recognition for film character retrieval in feature-length
                          films", in: Proceedings of the Computer Vision and Pattern
                          Recognition Conference (IEEE) 860-867. 
               
                  Beard, Niall (2015): Pretentious-O-Meter http://pretentious-o-meter.co.uk/ [letzter Zugriff 04. Februar
                          2016].
               
                  Brodbeck, Frederic (2011): Cinemetrics. Bachelor graduation project at the Royal Academy of
                          Arts (KABK), Den Haag  http://cinemetrics.fredericbrodbeck.de/ [letzter Zugriff 04.
                          Februar 2016].
               
                  Cinemetrics (o. J.): http://www.cinemetrics.lv/
                          [letzter Zugriff 04.Februar 2016]
               
                  Flückiger, Barbara (2011): "Die Vermessung ästhetischer
                            Erscheinungen", in: Zeitschrift für
                            Medienwissenschaft 5, 2: 44-60. 
               
                  Flückinger, Barbara (2011-2013): Timeline of Historical Film Colors
                  http://zauberklang.ch/filmcolors/ [08. Januar 2016].
               
                  Heftberger, Adelheid (2015): "Filmbild , Filmschnitt ,
                                Filmstil – die Quantifizierung und Visualisierung von filmischen
                                Strukturen", in: Book of Abstracts, DHd 2015. 
               
                  Howanitz, Gernot (2015): „Distant Waching: Ein
                                  quantitativer Zugang zu YouTube-Videos“, in: Book of
                                  Abstracts, DHd 2015. 
               
                  Hoyt, Eric / Ponot, Kevin / Roy, Carrie (2014):
                                    „Visualizing and Analyzing the Hollywood Screenplay with ScripThreads“, in:
                                    Digital Humanities Quarterly 8, 4. 
               
                  IMDb (o. J.): Internet Movie
                                    Database. http://www.imdb.com/ [letzter Zugriff 04. Februar 2016]. 
               
                  IMSDb (o. J.): Internet Script
                                    Movie Database. http://www.imsdb.com/ [letzter Zugriff 04. Februar 2016]. 
               
                  Korte, Helmut (2004): Einführung in
                                      die Systematische Filmanalyse. Berlin: Erich Schmid Verlag. 
               
                  Lauer, Gerhard (2013): "Die digitale Vermessung der
                                        Kultur", in: Geiselberger, Heinrich / Moorstedt, Tobias (eds.): Big Data – Das neue Versprechen der Allwissenheit.
                                        Berlin: Suhrkamp 99-116. 
               
                  Lavigne, Sam (2014): Videogrep. Automatic Supercuts with Python http://lav.io/2014/06/videogrep-automatic-supercuts-with-python/
                                        [letzter Zugriff 04. Februar 2016].
               
                  Mohammad, Saif M. / Turney, Peter D. (2010): "Emotions
                                          evoked by common words and phrases: Using Mechanical Turk to create an
                                          emotion lexicon", in: Proceedings of the NAACL HLT 2010
                                          Workshop on Computational Approaches to Analysis and Generation of
                                          Emotion in Text 26-34. 
               
                  MovieBarcode (o. J.): http://moviebarcode.tumblr.com/ [letzter Zugriff 04. Februar
                                          2016].
               
                  Neubauer Collegium (2014): UChicago
                                          Cinemetrics Conference
                  https://www.youtube.com/watch?v=6ZXj67bygEc [letzter Zugriff 04.
                                            Februar 2016].
               
                  Nielsen, Finn Å. (2011): "A new ANEW: evaluation of a
                                            word list for sentiment analysis in microblogs", in: Proceedings of the ESWC2011 Workshop on „Making Sense of Microposts:
                                            Big things come in small packages“ 93-98. 
               
                  OpenSubtitles (o. J.) www.opensubtitles.org/ [letzter
                                            Zugriff 04. Februar 2016]. 
               
                  Salt, Barry (2006): Moving into
                                              Pictures. London: Starwood. 
               
                  Tiedemann, Jörg (2012): "Parallel Data, Tools and
                                                Interfaces in OPUS", in: Proceedings of the 8th
                                                International Conference on Language Resources and Evaluation
                                                2214-2218.
               
                  Weaver, Chris (2014): Cinegraph
                  http://www.cs.ou.edu/~weaver/improvise/examples/cinegraph/index.html
                                                  [letzter Zugriff 04. Februar 2016].
               
                  Zulko (4.7.2014): "Automatic Soccer Highlights
                                                    Compilations With Python" (Blogpost), in: __del__
                                                    (self) Eaten by the Python http://zulko.github.io/blog/2014/07/04/automatic-soccer-highlights-compilations-with-python/
                                                    [letzter Zugriff 08. Januar 2016].
            
         
      
   



      
         
            Einleitung
            Der Beitrag möchte zeigen, wie die Berücksichtigung detaillierter,
          gattungsbezogener Metadaten auf produktive Weise mit dem Verfahren des Topic
          Modeling verbunden werden kann, um bisher nicht bekannte thematische Strukturen
          im Textverlauf in einer Sammlung spanischer und hispanoamerikanischer Romane zu
          entdecken. Ausgangshypothese ist, dass die Wichtigkeit bestimmter Topics nicht
          nur im Textverlauf variiert, sondern dies auch in verschiedenen Untergattungen
          auf unterschiedliche Weise tut. Eine Pilotstudie wurde im März 2015 beim
          Workshop zu Computational Narratology bei der DHd-Tagung in Graz vorgestellt. Im
          Rahmen der interdisziplinären Würzburger eHumanities-Nachwuchsgruppe
          "Computergestützte literarische Gattungsstilistik (CLiGS)
          wurde dieser Fragestellung nun mit weiter entwickelten Methodik sowie einer neu
          erstellten Sammlung spanischsprachiger Romane aus Spanien und Hispanoamerika
          nachgegangen. 
         
         
            Stand der Forschung und Fragestellung
            Die Frage nach dem Text- oder Handlungsverlauf in narrativen literarischen Texten hat jüngst zunehmende Aufmerksamkeit in der digitalen Literaturwissenschaft erhalten. Matthew Jockers kam durch Sentiment Analysis im Verlauf zahlreicher Romane zu dem (kontrovers diskutierten) Ergebnis, es gäbe sechs oder sieben grundlegende Plotstrukturen (Jockers 2015). Ben Schmidt hat unter anderem den Verlauf von Topic-Wahrscheinlichkeiten in der "screen time" amerikanischer Fernsehserien verfolgt (Schmidt 2014). Der vorliegende Beitrag verbindet die Frage nach dem Textverlauf mit der nach den Untergattungen, seine zentrale Fragestellung lautet: Können wir nach Untergattung unterschiedliche Verlaufsmuster für bestimmte Topics über den Textverlauf hinweg feststellen?
         
         
            Daten
            Die Textsammlung enthält 150 spanische und hispanoamerikanische Romantexte aus
            der Zeit von 1880 bis 1930 (für den spanischen Roman: Altisent 2008; de Nora
            1963, für den hispanoamerikanischen Roman: Gallo 1981; Williams 2009). Die Texte
            sind in TEI aufbereitet und mit detaillierten Metadaten versehen worden. Es
            wurden vier weit gefasste Untergattungen gewählt, um die Romane miteinander
            vergleichen zu können: novela sentimental, novela histórica, novela
            político-social und novela de tendencia
            subjetiva. Die Auswahl der Texte ist auch von der Verfügbarkeit als
            digitaler Volltext beeinflusst und daher nicht unbedingt repräsentativ.
            Abbildung 1 zeigt die Verteilung der Romane nach ausgewählten Metadaten. 
            
               
               
                  Abb. 1: Verteilung der Romane nach Metadaten 
            
         
         
            Methode
            Topic Modeling ist eine unüberwachte, nicht-deterministische Methode aus dem
              Bereich des Natural Language Processing, die auf Annahmen
              aus der distributionellen Semantik basiert und verborgene semantische Strukturen
              in großen Textsammlungen aufdeckt (einführend Blei 2011, grundlegend Blei 2003).
              Gruppen semantisch verwandter Wörter werden insbesondere aufgrund ihres häufigen
              gemeinsamen Auftretens in den untersuchten Dokumenten entdeckt. Ein Topic ist
              eine Wahrscheinlichkeitsverteilung von Wörtern; ein Dokument wird als
              Wahrscheinlichkeitsverteilung von Topics beschrieben. Topic Modeling ist eine in
              den DH äußerst beliebte Methode (Anwendungsbeispiele: Blevins 2010; Rhody 2012;
              Jockers 2013; Schöch 2015). 
            Hier wurde Topic Modeling als Teil eines umfassenden, weitgehend automatischen Arbeitsablaufes als Serie von Python-Skripten implementiert: Präprozessieren der Texte (Segmentierung, Binning, Lemmatisierung, POS-Tagging), das eigentliche Topic Modeling (mit Mallet, siehe McCallum 2002), Aufbereitung des Mallet-Outputs, zahlreiche Visualisierungen als Perspektiven auf die Ergebnisse. Die wichtigsten Parameter: Berücksichtigung ausschließlich der Substantive, Weglassung der 70 häufigsten Substantive, Romansegmente von ca. 600 Wörtern (unter Berücksichtigung von Absatzgrenzen), Anzahl von 70 Topics. Die Python-Skripte sind frei verfügbar und ausführlich dokumentiert, Begleitmaterialien (Skripte, Parameterdatei, Metadaten, Abbildungen) sind unter
                https://github.com/cligs/projects/tree/master/2016/dhd einsehbar.
              
         
         
            Ergebnisse und Diskussion
            Es werden zunächst die Topics selbst dargestellt, dann Unterschiede in den Topic-Verteilungen nach Untergattungen, über den Textverlauf hinweg und schließlich über den Textverlauf in Abhängigkeit der Untergattung.
            
               Topics
               Die Mehrheit der erhobenen Topics beinhaltet konkrete typische Themen und
                  Motive des spanischsprachigen Romans der Epoche. Man erkennt eine klare
                  semantische Beziehung der Wörter: ein konkreter Bereich menschlicher
                  Tätigkeiten, wie in Topic 19 (maestro-colegi o-escuela, dt.
                  "Lehrer-Schule-Schule") oder Topic 23 (sangre-golpe-arma, dt.
                  "Blut-Schlag-Waffe"); oder abstrakte Begriffe und Gefühle, wie bei Topic 69
                  (conciencia-honor-crimen, dt. "Gewissen-Ehre-Verbrechen"). Weniger kohärent
                  ist Topic 45 (marido-rato-chico, dt. "Ehegatte-Weile-Junge"). Die folgenden
                  Wordclouds (Abbildung 2) veranschaulichen die erwähnten Topics.
               
                  
                  
                     Abb. 2: Wordclouds für ausgewählte Topics.
               
            
            
               Untergattungen und Topics
               Die folgende Heatmap (Abbildung 3) zeigt die Verteilung der
                    durchschnittlichen Topic-Wahrscheinlichkeiten in den vier Untergattungen für
                    diejenigen 20 Topics, deren Werte zwischen den Untergattungen besonders
                    stark schwanken (nach Standardabweichung). Besonders distinktive Topics
                    existieren für die novela de tendencia subjetiva
                    (Topic 11: mirada-huerto-silencio, dt. "Blick-Garten-Stille") und die novela sentimental (Topic 45). Wenig überraschend
                    auch, dass die novela histórica als distinktiven
                    Topic unter anderem Topic 57 hat (rey-caballero-príncipe, dt.
                    "König-Ritter-Prinz"). Für die novela
                    histórico-social, für die aufgrund der großen Zahl von Beispielen
                    eine größere Bandbreite an Topic-Verteilungen zu erwarten ist, gibt es
                    keinen vergleichbar stark distinktiven Topic. Dennoch sind die
                    Untergattungen ein wichtiger Faktor für die Verteilung der Topics in der
                    Sammlung und die thematische Komponente spielt für die Definition der
                    Untergattungen tatsächlich eine wesentliche Rolle. 
               
                  
                  
                     Abb. 3: Verteilung von Topic-Scores nach Untergattungen.
               
            
            
               Topics im Textverlauf
               Die Ausprägung der Topics variiert nicht nur hinsichtlich der Untergattungen,
                      sondern auch über den Textverlauf hinweg. So gibt es einige Topics, deren
                      Vorkommen am Anfang der Romane besonders wahrscheinlich ist (Abbildung 4a).
                      Dazu zählen Topic 10 (vino-plato-pan, dt. "Wein-Teller-Brot"), Topic 17
                      (sombrero-ropa-bota, dt. "Hut-Kleidung-Stiefel") und Topic 19, welche auf
                      die Beschreibung von Ambiente, Situation und Personen hindeuten. Gegen Ende
                      der Romane sind andere Topics wahrscheinlicher (Abbildung 4b), z. B. Topic 2
                      (pecado-caridad-conciencia, dt. "Sünde-Wohltätigkeit-Gewissen"), Topic 23
                      und Topic 69, also abstraktere Themen oder solche, die sich auf
                      Wertvorstellungen beziehen. Dies deutet darauf hin, dass in den Romanen am
                      Ende Bilanz gezogen wird, die Handlung einen drastischen Ausgang nimmt oder
                      das im Textverlauf Behandelte in gesellschaftliche oder religiöse Diskurse
                      eingebunden wird.
               
                  
                  
                     Abb. 4a: Verteilung von Topics im Textverlauf (fallend).
               
               
                  
                  
                     Abb. 4b: Verteilung von Topics im Textverlauf (steigend).
               
            
            
               Textverlauf abhängig von den Untergattungen
               Für einige der genannten Topics, die in bestimmten Bereichen des Textverlaufs
                        wahrscheinlicher sind, kann die Tendenz über alle Untergattungen hinweg
                        bestätigt werden (bspw. bei Topic 10 und 17, siehe oben). Es gibt aber auch
                        Themen, bei denen sich durch die Betrachtung des Verlaufs in den einzelnen
                        Untergattungen ein differenzierteres Bild ergibt. Die Wahrscheinlichkeit von
                        Topic 23 beispielsweise nimmt nur für die novela
                        político-social zum Ende hin zu (Abbidung 5a): 
               
                  
                  
                     Abb. 5a: Topic 23 nach Textverlauf und Untergattung.
               
               Das kann so interpretiert werden, dass die novela
                          político-social im Gegensatz zu den anderen Untergattungen dazu
                          tendiert, am Ende des Textes mit einer gewalttätigen Szene und einem Umbruch
                          zu schließen. Topic 19 ist nicht in allen Untergattungen zu Beginn des
                          Textverlaufes stark ausgeprägt, sondern nur bei der novela
                          de tendencia subjetiva. Dies erklärt sich, weil bei diesen Romanen
                          das Schulthema als Teil einer autofiktionalen Erzählung zu Beginn erscheint
                          (Abbildung 5b): 
               
                  
                  
                     Abb. 5b: Topic 19 nach Textverlauf und Untergattung 
               
               Allgemein gilt, dass die Untergattungen sich in ihrer Topicverteilung im Textverlauf auch dann deutlich unterscheiden können, wenn dies für alle Untergattungen zusammengenommen nicht der Fall ist und so leicht übersehen werden könnte.
               Für die Berechnung wurden die Romansegmente von 600 Wörtern bezüglich des Textverlaufs auf 15 Romanabschnitte (Bins) verteilt, um die unterschiedliche Romanlänge zu berücksichtigen. Diese Bins wurden hinsichtlich der Untergattung gruppiert und jeweils das arithmetische Mittel bestimmt. Die in den Plots eingezeichneten Kurven entsprechen der linearen Interpolation dieser gemittelten Werte. Zusätzlich wurde der Standardfehler vertikal um den jeweiligen Kurvenpunkt eingezeichnet, der deutlich macht, wie sehr die jeweiligen dem Mittelwert zugrunde liegenden Werte streuen, also wie gut der Mittelwert die Gesamtheit der Segmentwerte repräsentiert.
            
         
         
            Die Ergebnisse im literaturgeschichtlichen Kontext
            Insgesamt zeigen sich verschiedene Zusammenhänge: Zwischen bestimmten Topics und einzelnen Roman-Untergattungen, zwischen Topics und dem Textverlauf, und dies zum Teil dann auch wieder in Abhängigkeit von den Untergattungen. Aus literaturgeschichtlicher Perspektive betrachtet erweisen sich die in die Untersuchung einbezogenen Metadaten für eine Einordnung der Topic-Resultate als nützlich. Topics sind für die Romangattungen im vorliegenden Korpus ein wichtiger Faktor, ähnlich wie dies für Gattungen wie die klassische Komödie und Tragödie bereits gezeigt werden konnte (Schöch 2015).
            Ein detaillierterer Blick zeigt beispielsweise Folgendes: Topic 11, welches typisch für die
                          novela de tendencia subjetiva ist, ist vor allem in den 1910er- und 1920er-Jahren wichtig sowie für bestimmte Autoren. Interessanterweise ist dieses bei spanischen und hispanoamerikanischen Modernisten vorkommende Thema auch bei der früher wirkenden Schriftstellerin Juana Manuela Gorriti schon wichtig, die offenbar thematische Präferenzen späterer Autoren vorweggenommen hat. Außerdem kommt Topic 11 bei Larreta in einem (modernistischen) historischen Roman vor, obwohl es ansonsten vor allem für die Romane subjektiver Tendenz typisch ist. Es ist anzunehmen, dass für dieses spezielle Thema eher die literarische Strömung bestimmend ist als die Untergattung. Der Topic enthält einige für die modernistische Strömung typische Wörter, etwa zu Sinneseindrücken (azul, dt. "blau", olor, dt. "Geruch") und Zurückgezogenheit (huerto, silencio, campo, soledad, dt. "Garten, Ruhe, Land, Einsamkeit").
                        
         
         
            Fazit und Ausblick
            Die Nutzung von Topic Modeling als Methode kann für die digitale Literaturwissenschaft verbessert werden, wenn spezifisch literaturwissenschaftliche Metadaten in die Betrachtungen einbezogen werden und die Textstruktur - hier als Sequenz von Textverlaufseinheiten - berücksichtigt wird. Verschiedene Visualisierungsstrategien erweisen sich als entscheidende "Interfaces" zu den Daten (im Sinne von Doueihi 2012), die Muster sichtbar machen und den Blick lenken. Die Ergebnisse des Topic Modelings können differenzierter und aus verschiedenen Perspektiven betrachtet und mit literaturhistorischem Wissen in Verbindung gebracht werden. Die Ergebnisse ergänzen und erweitern etablierte hermeneutische Lektürestrategien, insofern sie einen synthetisierenden Blick auf sehr umfangreiche Textsammlungen erlauben.
            Nächste Schritte betreffen insbesondere die weitere Auseinandersetzung mit der
                          Signifikanz von Unterschieden in den Topic-Wahrscheinlichkeiten im Textverlauf,
                          deren Berechnung u. a. durch die mangelnde Normalverteilung der Werte nicht
                          trivial ist. Zusätzlich zu den Untergattungen sollen auch Kategorien wie das
                          Setting modelliert werden. Zudem sollen die Textverlaufs-Daten für die
                          automatische Klassifikation von Romanen nach Untergattungen genutzt werden.
                          Schließlich wird bereits an der Erweiterung der Textsammlung gearbeitet,
                          insbesondere mit Blick auf den Umfang und ein ausgeglicheneres Verhältnis der
                          Untergattungen.
         
      
      
         
            
               Bibliography
               
                  Altisent, Marta E. (2008): A
                            Companion to the Twentieth-Century Spanish Novel. Woodbridge:
                            Tamesis. 
               
                  Blei, David M. (2011): “Introduction to Probabilistic
                            Topic Models,” in:  Communication of the ACM. 
               
                  Blei, David M. / Ng, Andrew Y. / Jordan, Michael I.
                            (2003): “Latent Dirichlet Allocation,” in:  Journal of
                            Machine Learning Research 3: 993–1022. 
               
                  Blevins, Cameron (2010): “Topic Modeling Martha
                              Ballard’s Diary,” in:  Historying
                  http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/
                                [letzter Zugriff 16. Februar 2016]. 
               
                  Doueihi, Milad (2012):  Pour un
                                humanisme numérique (2011). Paris: Seuil. 
               
                  Gallo, Marta (1981):  La Novela
                                Hispanoamericana En El Siglo XIX. Madrid: La Muralla. 
               
                  García de Nora, Eugenio (1963):  La
                                Novela Española Contemporánea. Madrid: Gredos. 
               
                  Jockers, Matthew L. (2013):  Macroanalysis - Digital Methods and Literary History. Champaign,
                                IL: University of Illinois Press. 
               
                  Jockers, Matthew L. (2015): “Revealing Sentiment and
                                Plot Arcs with the Syuzhet Package” in:  Matthew. L.
                                Jockers
                  http://www.matthewjockers.net/2015/02/02/syuzhet/ [letzter
                                  Zugriff 09. Februar 2016]. 
               
                  McCallum, Andrew K. (2002): MALLET:
                                  A Machine Learning for Language Toolkit
                  http://mallet.cs.umass.edu
                                  [letzter Zugriff 09. Februar 2016]. 
               
                  Nachwuchsgruppe CLiGS (o.J.): Computergestützte literarische Gattungsstilistik
                  http://cligs.hypotheses.org/ [letzter Zugriff 16. Februar
                                    2016].
               
                  Rhody, Lisa M. (2012): “Topic Modeling and Figurative
                                    Language,” in:  Journal of Digital Humanities 2  http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody
                                    [letzter Zugriff 09. Februar 2016]. 
               
                  Schmidt, Benjamin M. (2014): “Typical TV Episodes:
                                    Visualizing Topics in Screen Time,” in:  Sapping
                                    Attention
                  http://sappingattention.blogspot.de/2014/12/typical-tv-episodes-visualizing-topics.html
                                      [letzter Zugriff 09. Februar 2016]. 
               
                  Schöch, Christof (2015): “Topic Modeling Genre: An
                                      Exploration of French Classical and Enlightenment Drama [submitted]”, in: 
                                      Digital Humanities Quarterly. 
               
                  Williams, Raymond L. (2009):  The
                                      Twentieth-Century Spanish American Novel. Austin, Texas: University
                                      of Texas Press. 
            
         
      
   



      
         
            Ansatz 
            Neben dem ›klassischen‹ strukturalistischen Paradigma, das sich wesentlich an
          Theoremen der Linguistik orientiert (u. a. Lotman 1972; Titzmann 1977), gibt es
          in der Literaturwissenschaft bereits seit Jahrzehnten Ansätze zu einer
          Strukturanalyse, die sich auf die empirische Soziologie – insbesondere auf die
          Social Network Analysis – bezieht und Struktur
          entsprechend nicht über basale semantische Relationen (etwa als Opposition oder
          Äquivalenz) definiert, sondern über soziale Interaktionen (Marcus 1973; Stiller
          et al. 2003; de Nooy 2006; Stiller / Hudson 2005; Elson et al. 2010; Agarwal et
          al. 2012). Im Kontext der Digital Humanities haben diese Ansätze zu einer
          literaturwissenschaftlichen Netzwerkanalyse (Trilcke 2013) in den letzten Jahren
          eine neue Dynamik gewonnen (Moretti 2011; Rydberg-Cox 2011; Park et al. 2013).
          Aus literaturwissenschaftlicher Sicht versprechen diese Analyseverfahren dabei
          auf umfangreichen Korpora basierende, von quantitativen Daten gestützte
          Erkenntnisse über die Literaturgeschichte wie auch über die generischen
          Eigenarten literarischer Texte. Im Projekt dlina. Digital
          Literary Network Analysis haben wir einen Workflow zur Extraktion,
          Analyse und Visualisierung von Netzwerkdaten aus dramatischen Texten mit
          rudimentärer TEI-Auszeichnung entwickelt (Fischer et al. 2015). Der hier
          projektierte Vortrag wird Ergebnisse der netzwerkanalytischen Auswertung dieser
          Daten präsentieren und vor dem Hintergrund etablierter fachwissenschaftlicher
          Fragestellungen diskutieren. 
         
         
            Datenerhebung und -analyse
            Unser derzeitiges Korpus umfasst 465 deutschsprachige Dramen (Zeitraum 1730 bis
            1930), die aus dem Textgrid
            Repository extrahiert wurden. Die für die Netzwerkanalyse relevanten
            Strukturdaten dieser Dramen (Segmentierung, Figurenidentifikation) wurden in
            einem regelbasierten Prozess händisch ediert, um OCR- und TEI-Tagging-Fehler zu
            beheben sowie solchen ›Eigenarten‹ der literarischen Texte zu begegnen, die die
            Analyseergebnisse verfälschen würden (u. a. unterschiedliche Bezeichnungen
            identischer Figuren; Bezeichnung von Figurengruppen mit unbestimmten Numeralien
            wie ›beide‹ oder ›alle‹; etc.). Die edierten Strukturdaten liegen in einem
            eigens entwickelten Datenformat, dem dlina-Format, in Form von XML-Dateien vor.
            Die Visualisierung der Netzwerke und die Berechnung netzwerkanalytischer Werte
            erfolgt – mittels Python- und D3-Skripten – automatisiert auf Basis der in den
            dlina-Dateien gespeicherten Strukturdaten. Neben Graphen und basalen Werten, die
            die Netzwerke global beschreiben (Network Size, Density, Average Degree, Average
            Path Length), werden dabei auch Zentralitätswerte für sämtliche Figuren eines
            Dramas erhoben (u. a. Degree, Average Distance, Closeness Centrality,
            Betweenness Centrality). Die Implemtierung weiterer Berechnungsrountinen (u. a.
            Clustering Coefficient, logarithmierte Degree Distribution-Tabellen) ist für den
            Winter 2015/16 vorgesehen. Sämtliche Daten und Visualisierungen werden frei
            verfügbar im Netz publiziert (https://github.com/dlina und https://dlina.github.io/linas/). 
         
         
            Literaturwissenschaftliche Auswertung 1: Dramengeschichte 
            Die diachrone Erstreckung unseres Dramenkorpus über ca. 200 Jahre deutscher
              Literaturgeschichte macht es möglich, größere Entwicklungen im Bereich der
              strukturellen Komposition von dramatischen Texten zu beobachten (erste
              Überlegungen dazu haben wir in einem Blogpost skizziert: https://dlina.github.io/200-Years-of-Literary-Network-Data/). Neben
              Werten, die sich auf die Gesamtnetzwerke der einzelnen Dramen beziehen (u. a.
              Network Size, Density, Average Degree; s. exemplarisch zur Average Path Length,
              Abbildung 1), werden dabei auch figurenbezogene Werte, v.a. Zentralitätsmaße,
              einbezogen, die Aufschluss etwa über die Streuung des Personals eines Dramas
              bzw. dessen Zusammensetzung aus ›zentralen‹ und weniger ›zentralen‹ Figuren
              gibt. Auf Grundlage dieser Werte sollen im Vortrag einige globale Thesen der
              Literaturgeschichte diskutiert werden. So werden wir erstens diskutieren, inwieweit sich anhand der netzwerkanalytischen
              Werte eine Ausdifferenzierung der strukturellen Komposition von dramatischen
              Texten am Ende des 18. Jahrhunderts beobachten lässt: Eine solche
              Ausdifferenzierung wäre angesichts des Nebeinanders von ›geschlossenen‹, in der
              Tradition der Französischen Klassik stehenden Dramen und ›offenen‹ Dramen, die
              sich u. a. an der Drramatik Shakespeares orientieren, zu erwarten. Zweitens werden wir einige geläufige
              literaturwissenschaftliche Periodisierungshypothesen testen (u. a. aus dem
              Strukturalismus und der Sozialgeschichte); gefragt werden soll hier, inwieweit
              die Entwicklung der netzwerkanalytischen Werte mit den von der Forschung
              vorgeschlagenen Periodisierungen korreliert. 
            
               
               
                  Abb. 1: Average Path Length (Mean; nach Dekaden) 
            
         
         
            Literaturwissenschaftliche Auswertung 2: Dramentypen
            Die von uns bisher erhobenen Werte zeigen, dass Dramen in dem untersuchten
                  Zeitraum auf sehr unterschiedliche Weise strukturiert wurden. In der
                  ›traditionellen‹ Literaturwissenschaft wurden für solche unterschiedlichen
                  ›Bauformen‹ diverse Typologien entwickelt, in der Germanistik am bekanntesten
                  ist Volker Klotz’ Unterscheidung in eine ›offene‹ und eine ›geschlossen‹
                  Dramenform (Klotz 1960). Diesen typologischen Impuls wollen wir aufgreifen und
                  einen Vorschlag unterbreiten, wie sich mittels netzwerkanalytischer Daten
                  bestimmte Typen der strukturellen Komposition von Dramen unterscheiden (und dann
                  wiederum historisch verorten) lassen. Unser Vorschlag greift dabei Überlegungen
                  aus der Forschung zu sog. Small-world-Netzwerken auf. Diese Forschungen setzen
                  bei der Beobachtung an, dass die Werte von empirisch erhobenen Netzwerken nicht
                  selten signifikant von entsprechenden Random-Netzwerken (also z. B. nach dem
                  Erdős-Rényi-Modell erstellten Graphen) abweichen. Abweichungen sind dabei
                  insbesondere beim Clustering Coefficient, bei der Averge Path Length sowie bei
                  der Degree Distribution zu beobachten (Albert / Barabási 2002). Für den hier
                  projektierten Vortrag werden wir diese Werte – sowie die Werte für die
                  entsprechenden Random-Netzwerke – für unser Gesamtkorpus erheben (sowie einen
                  Workflow für die automatisierte Erhebung entwickeln) und diskutieren. Erste
                  Testläufe deuten dabei darauf hin, dass sich auf diese Weise tatsächlich
                  unterschiedliche Typen der strukturellen Komposition von Dramen beschreiben
                  lassen könnten. So zeigen sich z. B. auffällige Unterschiede bei der Degree
                  Distribution (s. exemplarisch die Tabellen für vier Dramen in Abbildung 2); und
                  mit Blick auf den Clustering Coefficient zeigt sich, dass im Vergleich zu
                  Random-Netzwerken signifikant höhere Werte, wie sie bei Small-world-Netzwerken
                  zu erwarten sind, zwar in mehreren Fällen vorkommen, jedoch keineswegs für alle
                  Dramennetzwerke charakteristisch sind (siehe exemplarisch die Werte in Abbildung
                  3). Im Vortrag werden wir diese Werte für alle Dramen unseres Korpus
                  präsentieren; wir werden diskutieren, inwieweit sich hier – aufbauend auf dem
                  Small-world-Konzept – netzwerkanalytisch basierte Typen der strukturellen
                  Komposition von Dramen unterscheiden lassen und wir werden literarhistorisch
                  fundiert erörtern, welche Eigenschaften der Dramen für die unterschiedlichen
                  Werte verantwortlich sind.
            
               
            
            
               
            
            
               
            
            
               
               
                  Abb. 2.1 bis 2.4: Node Degree Distribution für »Der
                    sterbende Cato« (1731), »Emilia Galotti« (1772), »Götz von Berlichingen« (1773)
                    und »Die Räuber« (1781) 
            
            
               
               
                  Abb. 3: Vergleich des Clustering Coefficent des
                      Dramen-Netzwerks mit dem eines jeweils entsprechenden Random-Netzwerks 
            
         
      
      
         
            
               Bibliographie
               
                  Albert, Réka / Barabási, Albert-László (2002):
                      "Statistical mechanics of complex networks", in: Reviews
                      of Modern Physics 74: 47–97. 
               
                  Agarwal, Apoorv / Corvalan, Augusto / Jensen, Jacob /
                        Rambow, Owen (2012): "Social Network Analysis of Alice in Wonderland" in: Proceedings of the
                        Workshop on Computational Linguistics for Literature. Montréal
                        88–96. 
               
                  de Nooy, Wouter (2006): "Stories, Scripts, Roles, and
                        Networks" in: Structure and Dynamics 1, 2 http://escholarship.org/uc/item/8508h946#page-1 [letzter Zugriff
                        12. Oktober 2015]. 
               
                  Elson, David K. / Dames, Nicholas / McKeown, Kathleen R.
                        (2010): "Extracting Social Networks from Literary Fiction", in: Proceedings of the 48th Annual Meeting of the Association
                        for Computational Linguistics. Uppsala 138–147. 
               
                  Fischer, Frank / Kampkaspar, Dario / Göbel, Mathias /
                          Trilcke, Peer (2015): "Digital Network Analysis of Dramatic Texts",
                          in: DH 2015, Sydney, 2. Juli 2015
                  https://dlina.github.io/Our-Talk-at-DH2015/ [Skript] und https://dlina.github.io/presentations/2015-sydney/sydney.html#/
                            [Slides] [letzter Zugriff 12. Oktober 2015]. 
               
                  Klotz, Volker (1960): Geschlossene
                            und offene Form im Drama. München: Hanser. 
               
                  Lotman, Jurij M. (1972): Die
                            Struktur literarischer Texte. München: Wilhelm Fink. 
               
                  Marcus, Solomon (1973): Mathematische Poetik. Frankfurt am Main: Editura Academiei. 
               
                  Moretti, Franco (2011): "Network Theory, Plot
                            Analysis" in: Stanford Literary Lab Pamphlets 2 http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf [letzter
                            Zugriff 12. Oktober 2015]. 
               
                  Park, Gyeong-Mi / Kim, Sung-Hwan / Cho, Hwan-Gue
                              (2013): "Structural Analysis on Social Network Constructed from Characters
                              in Literature Texts", in: Journal of Computers 8, 9:
                              2442-2447 http://ojs.academypublisher.com/index.php/jcp/article/view/jcp080924422447/7672
                              [letzter Zugriff 12. Oktober 2015]. 
               
                  Rydberg-Cox, Jeff (2011): "Social Networks and the
                              Language of Greek Tragedy", in: Journal of the Chicago
                              Colloquium on Digital Humanities and Computer Science 1, 3 https://letterpress.uchicago.edu/index.php/jdhcs/article/view/86/91
                              [letzter Zugriff 12. Oktober 2015]. 
               
                  Stiller, James / Nettle, Daniel / Dunbar, Robin I. M.
                              (2003): "The Small World of Shakespeare's Plays", in: Human Nature 14: 397–408. 
               
                  Stiller, James / Hudson, Matthew (2005): "Weak Links
                              and Scene Cliques Within the Small World of Shakespeare", in: Journal of Cultural and Evolutionary Psychology 3:
                              57–73. 
               
                  TextGrid: TextGrid Repository
                  https://textgridrep.de [letzter
                              Zugriff 10. Februar 2016].
               
                  Titzmann, Michael (1977): Strukturale Textanalyse. Theorie und Praxis der Interpretation.
                              München: Wilhelm Fink. 
               
                  Trilcke, Peer (2013): "Social Network Analysis (SNA)
                              als Methode einer textempirischen Literaturwissenschaft", in: Ajouri, Philip
                              / Mellmann, Katja / Rauen, Christoph (eds.): Empirie in
                              der Literaturwissenschaft. Münster: mentis 201–247. 
            
         
      
   



      
         Das Panel wird durch vier Kurzvorträge in Probleme der Nutzung digitaler Werkzeuge für nicht-indoeuropäische Sprachen einführen. Die Vorträge basieren auf Erfahrungen aus langfristig angelegten Projekten und haben jeweils individuelle Lösungen für die spezifischen Anforderungen gefunden, die hauptsächlich durch die Sprache formuliert werden.
         Allen Projekten gemein sind Probleme der Nutzung von digitalen Werkzeugen, die insbesondere bei der Verwendung von Quellen historischer oder wenig erforschter Sprachen auftreten. 
         Aktuelle Content Management Systeme und Annotationstools wurden selten im Hinblick auf Anforderungen aus Orchideenfächern entwickelt. Dies betrifft beispielsweise einige Sprachen mit nichtkonkatenativer Morphologie oder komplexen Schriftsysteme. Daher müssen für erwähnte Sprachen entweder existierende Anwendungen angepasst oder erschaffen werden.
         Bei der Adaption können während der Modellierung wichtige Eigenschaften nicht berücksichtigt werden oder bleiben nur als Kommentar erhalten, was eine weitere maschinelle Bearbeitung erschwert. Bezüglich der Datenkodierung ergibt sich das Problem der Ineffizienz. So wurden morphologische Tagsets primär für die indo-europäische Sprachfamilie entwickelt. Für eine tiefe linguistische Annotation müssen aber diese Standards beispielweise für einige semitische Sprachen angepasst werden. 
         Nicht selten ist die Alternative die Eigenentwicklung projektbezogener Lösungen, die aber aufgrund der Anforderungen mit eigenen Datenformaten arbeiten, und so nicht mehr den geltenden Standards folgen und den Austausch erschweren. Hinzu kommt der immense Zeit- und Ressourcenaufwand bei der Implementierung.
         Allerdings sind gerade im deutschsprachigen Raum viele langfristige Projekte auf digitale Tools angewiesen.
         Durch eine Vernetzung solcher Projekte können gemeinsame Anforderungen an, und Begrenzungen von aktuellen Lösungen besprochen und Initiativen zur Entwicklung digitaler Tools und Ressourcen koordiniert werden. Daher ist das Ziel dieses Panels eine erste Zusammenführung langfristig ausgerichteter Projekte im deutschen Sprachraum, die mit historischen nicht-indo-europäischen Sprachen im digitalen Kontext arbeiten. Dabei sollen die Probleme der Nachhaltigkeit entwickelter Werkzeuge und Ressourcen, sowie der bearbeiteten Daten besprochen werden. Anschließend werden die vielfältigen Herangehensweisen mit einem Fokus auf drei große Punkte diskutiert:
         
            Nachhaltigkeit von Repositorien
         
         
            Welche Frameworks werden für welche Datentypen benötigt?
            Wie können Informationen über unpräzise Daten gespeichert werden? 
            Wie gehen verfügbare Systeme mit Multilingualität um?
         
         
            Nachhaltigkeit von (Annotations-)Werkzeugen
         
         
            Analyse historischer Daten impliziert die Annotation von Textmaterialien in Sprachen, die aus verschiedenen Gründen zu Problemen führen.
            Welche Annotationstools können genutzt werden? Mit welchen Limitierungen?
            Was bedeutet es, ein neues Tool zu entwerfen?
            Häufige Anforderungen durch strukturell komplexe Sprachen: Multilevel-Annotation, Textkorrektur während der Annotationsphase, Multilevel-Segmentierung
         
         
            Nachhaltigkeit des annotierten Materials (Standards)
         
         
            Während der Standard TEI-XML als Schnittstellenformat sehr nützlich ist, ergeben sich dennoch Probleme wie:
                    
                  für interne Verarbeitung kann dessen Verwendung hinderlich sein. Daher müssen projekt-spezifische Lösungen mit standardisiertem Export entwickelt werden.
                  Können diese Daten von Dritten in TEI-XML verarbeitet werden?
                  Welche anderen Formate können genutzt werden (z.B. JSON)?
                  Sind existierende Tagset-Formate ausreichend spezifiziert, um auch nicht-europäische Sprachen taggen zu können?
               
            
         
         
            Herausforderungen in der Nutzung vorhandener Tools für arabische Daten
            Alicia González, Tillmann Feige
            Universität Hamburg
            ERC- Projekt COBHUNI (
                    )
                
            Email: 
                    alicia.gonzalez@uni-hamburg.de; 
                    tillmann.feige@uni-hamburg.de
            
            Wir beschreiben den Ansatz, einen Korpus der neben modernem auch klassisches Arabisch (siehe Romanov, 2016) enthält, mit computerlinguistischen und semantischen Verfahren analysierbar zu machen. Wir setzen auf bereits vorhandene Software für die Hauptpunkte Annotation und Analyse. Dazu wurde ein Pflichtenheft erstellt, dass mit vorhandenen Tools abgeglichen wurde.
            Da wir mit arabischen Daten arbeiten, ist eine große Herausforderung die Schrift. Es ist eine linksläufige verbundene Schrift, die durch Konsonanten und lange Vokale repräsentiert wird. Kurze Vokale sind Diakritika, die optional gesetzt werden und gerade bei Referenzen auf religiöse Quellen im Textkorpus vorkommen. Dabei ist vollständige UTF-8 Unterstützung und die saubere Darstellung der Schrift unabdingbar. Dies reduziert die Auswahl erheblich. Hinzu kommt, dass wir auf flexible Import- und Exportmöglichkeiten angewiesen sind. Ähnliche Probleme führen Peralta und Verkinderen auf (Peralta / Verkinderen 2016). Durch unsere Herangehensweise gibt es weitere Einschränkungen wie Mehrebenen-, Multitoken- aber auch Subtoken-Annotation.
            Die Auswahl für die semantische Annotation fiel auf WebAnno, dass durch sein spezielles Datenmodell die erforderliche Datenaufbereitung und Kontrolle gestattet.
            Als Visualisierungstool haben wir ANNIS ausgewählt, dass ebenfalls Arabisch unterstützt, einen konfigurierbaren Converter mitbringt und Mehrebenenkorpora erlaubt, so dass auch hier die Hauptkriterien erfüllt wurden. Zusätzlich lassen sich potentielle Probleme in der Darstellung durch eine anpassbare HTML-Visualisierung umgehen. Durch Zusammenarbeit mit den Entwicklern beider Programme wurde die Unterstützung für Arabisch stetig ausgebaut.
            Im Beitrag werden wir die einzelnen Punkte erläutern und darstellen, warum wir uns für die angeführten Programme und gegen eine Eigenentwicklung entschieden haben, sowie welche Implikationen diese Entscheidung für die Nachhaltigkeit des Projekts, der Daten und der genutzten Tools hat.
         
         
            Tiefe Mehrebenen-Annotation für semitische Sprachen: der Fall von Ge'ez
            
               Cristina Vertan
            
            Universität Hamburg
            ERC-Projekt TraCES (
                    )
                
            Email: 
                    cristina.vertan@uni-hamburg.de
            
            Das südsemitische Gәʿәz ist die Sprache des Königreichs Aksum in der heutigen nordäthiopischen Provinz Tigray, von wo aus die im 4. Jahrhundert beginnende Christianisierung Äthiopiens ihren Anfang nahm. Die in der Folge entstehende reiche Literatur ist in großem Umfang geprägt von Übersetzungen, was durch grammatische Interferenzphänomene reflektiert wird. Das Altäthiopische hat aus einer südsemitischen Schrift ein eigenes Silbenalphabet entwickelt, das bis heute in mehreren modernen Sprachen Äthiopiens und Eritreas Verwendung findet. Innerhalb der semitischen Sprachen fällt es durch die verwendete Rechtsläufigkeit auf; außerdem werden die Vokale vollständig geschrieben. Beides unterscheidet das Gәʿәz von verwandten Sprachen wie Altsüdarabisch, Arabisch, Hebräisch und Syro-Aramäisch Mit den genannten eng verwandten semitischen Sprachen teilt das Altäthiopische die nichtkonkatenative Morphologie. Durch das äthiopische Silbenalphabet sind Morphemgrenzen in der Schrift nicht darstellbar, so dass beispielsweise ein einzelner Vokal als Bestandteil einer Silbe eine eigenständige Wortart darstellt und tokenisiert werden muss.
            Die Komplexität des Annotationstools wird sehr vielfältige linguistische Anfragen und detaillierte Analysen der Sprache ermöglichen, aber auch eine vollautomatische Annotation verhindern. Ein alle morphologischen Merkmale abdeckendes Vektorraum-Modell (das für maschinelle Lernverfahren benutzt werden muss) wäre zu groß. Vorstellbar ist lediglich eine flache automatische Annotation (z. B. der Wortarten); jedoch wird auch für eine solche zunächst eine relativ große Menge an Trainingsdaten benötigt. Daher ist die Entwicklung eines Werkzeugs für die manuelle Annotation ein obligatorischer Schritt.
            Die Besonderheit der entwickelten Lösung (Vertan/Ellwardt/Hummel 2016) sind:
            
               automatische Transkription
               manuelle Korrektur der Transkription während des Annotationsprozesses
               semi-automatische Verfahren: automatische Verläufe werden farbig markiert und sind automatisch zur manuellen Korrektur hinterlegt
               Mehrebenenannotation: Linguistik, Edition, Textstruktur
               Anpassungen an unterschiedliche Schriftsysteme und Transkriptionsregeln
            
         
         
            Nutzungs- und Nachhaltigkeitsstategien im Projekt "Textdatenbank und Wörterbuch des Klassischen Maya"
            Christian M. Prager
            NRW Akademie der Wissenschaften und der Künste
            
               
            
            Email: 
                    
            
            Die Mayaschrift ist das einzig lesbare Schriftsystem der vorspanischen Amerikas. Die über 10.000 Texte sind in einer logographisch-syllabischen Hieroglyphenschrift verfasst und von den rund 800 Zeichen sind erst 60% sicher entziffert. Die Texte enthalten taggenaue Kalenderangaben, die es uns ermöglichen die rund 2000jährige Sprach- und Schriftgeschichte genau zu dokumentieren. Das Projekt (Prager 2015) wird sämtliche Inschriften einschließlich Metadaten in einer Datenbank einzupflegen und darauf basierend ein digitales Wörterbuch des Klassischen Maya zu kompilieren. Herausforderung dabei ist, dass die Schrift noch nicht vollständig entziffert ist und bei der Modellierung zu berücksichtigen ist. Unser Projekt verfolgt den Ansatz, wonach die Bedeutung von Wörtern ihre Verwendung ist - Texte nehmen Bezug auf den Textträger und den Verwendungskontext und nur die exakte Dokumentation sogenannter nicht-textueller Informationen erlaubt es, textuelle und nicht-textuelle Informationsbereiche zueinander in Beziehung zu setzen und bei der Entzifferung von Zeichen und Textstellen zu berücksichtigen. Zum Zweck der Nachhaltigkeit und Nachnutzung greift das Projekt bei der Beschreibung der Artefakte und der relevanten objektgeschichtlichen Ereignisse auf CIDOC CRM zurück, das eine erweiterbare Ontologie für Begriffe und Informationen im Bereich des kulturellen Erbes anbietet. Das entstandene Anwendungsprofil wird durch Elemente aus weiteren Standards und Schemata angereichert und wird damit auch für vergleichbare Projekt nachnutzbar. Die Schemata und erstellten Metadaten werden in einer Linked (Open) Data-Struktur (LOD) abgebildet. Durch die Repräsentation im XML-Format, sowie die Nutzung von HTTP-URIs wird eine einfache Austauschbarkeit und Zitierbarkeit der Daten ermöglicht. Durch diese Umsetzung können Objektmetadaten getrennt vom erfassten Text gespeichert werden und durch die Verwendung der HTTP-URI verlinkt werden. Die Nachnutzung bereits bestehender und fachlich anerkannter Terme trägt darüberhinaus auch zu einer hohen Interoperabilität mit anderen Datenbeständen und Informationssystemen bei. Das ausgestaltete Schema hat eine ontologisch-vernetzte Struktur, die komplexe Beziehungen und Zusammenhänge abbildet.
         
         
            Interdisziplinäre Digitale Zusammenarbeit für seltene Sprachen und Kulturen
         
         
            - Eine Fallstudie über jiddische Texte aus der frühen Neuzeit -
            
               Walther v. Hahn (Universität Hamburg), Berndt Strobach (Wolffenbüttel)
            
            Email: 
                    vhahn@informatik.uni-hamburg.
               de, berndt.strobach@freenet.de>
                
            In den Geisteswissenschaften werden häufig die fachlichen Interpretationen und die sprachlichen Erklärungen von verschiedenen Gruppen mit unterschiedlicher Kompetenz bearbeitet. Gute Beispiele sind Studien zu Texten aus semitischen Sprachen, wobei, speziell bei historischen Dokumenten die historische oder geistes- und sozialgeschichtliche Würdigung von Forschern verfasst werden muss, die des Hebräischen, Arabischen, Aramäischen etc. nicht mächtig sind, die sprachwissenschaftlichen Forscher dagegen bei der Interpretation gelegentlich weniger engagiert bleiben. Extremfälle wie Studien über das Sephardische in Spanien (Ladino, Djudezmo) machen etwa solide Kenntnisse zumindest des Spanischen, Hebräischen, Türkischen, Griechischen und Italienischen zur Voraussetzung für seriöse hermeneutische Forschungsergebnisse.Wir berichten über Studien zu jiddischen Texten aus dem Wolffenbüttel des 18. Jahrhunderts, in denen die Rolle der "Hofjuden" und ihres kultur- und sozialgeschichtlichen Hintergrundes diskutiert wird. 
            Die Herausforderung einer interdisziplinären Zusammenarbeit zwischen Historikern, Sprachwissenschaftlern und Informatikern besteht darin,
            1. die Lesbarkeit der Originalquellen für alle Gruppenmitglieder sicher zu stellen (Invertierte Transkriptionen, Vokalisierung, Visualisierung), sowie 
            2. in der Gruppe eine gemeinsame Behandlung von Vagheit, Unsicherheit und Unbekanntem zu definieren, so dass die Unklarheiten in den einzelnen Forschungsstufen erhalten und im Endergebis sichtbar bleiben (Vagheits-Annotationen und vage Inferenzen). Heute werden derartige Unsicherheiten meist bereits in den Annotationen unterschlagen (von Hahn, 2016).
         
      
      
         
            
               Bibliographie
               
                  Hahn, Walther von (2016):
                        „Humanities meet Computer Science – Digital Humanities between Expectations and Reality“,
                        zu erscheinen in: von Hahn, Walter / Papadima, Liviu / Vertan, Cristina (eds.):
                        Humanities2020, New Trends in Education and Research. 
                        Bukarest: University of Bucharest Publishing House.
                    
               
                  Peralta, José Haro / Verkinderen, Peter (2016): 
                        „‚Find for me!‘: Building a Context-Based Search Tool Using Python“,
                        in: Muhanna, Elias (ed.):
                        The Digital Humanities and Islamic & Middle East Studies. 
                        Berlin: Walter de Gruyter GmbH 199–231. 
                    
               
                  Prager, Christian M. (2015): 
                        „Das Textdatenbank- und Wörterbuchprojekt des Klassischen Maya: Möglichkeiten und Herausforderungen digitaler Epigraphik“,
                        in: Neuroth, Heike / Rapp, Andrea / Söring, Sibylle (eds.):
                        TextGrid: Von der Community - für die Community: Eine Virtuelle Forschungsumgebung für die Geisteswissenschaften.
                        Glückstadt: Werner Holsbusch 105–124 
                        https://www.academia.edu/17957108/Das_Textdatenbank-_und_W%C3%B6rterbuchprojekt_des_Klassischen_Maya_M%C3%B6glichkeiten_und_Herausforderungen_ digitaler_Epigraphik.
                    
               
                  Romanov, Maxim (2016): 
                        Creating Frequency-Based Readers for Classical Arabic
                  http://maximromanov.github.io/2016/05-30.html [letzter Zugriff 1. Dezember 2016].
                    
               
                  Vertan, Cristina / Ellwardt, Andreas / Hummerl, Susanne (2016): 
                        „Ein Mehrebenen-Tagging-Modell für die Annotation altäthiopischer Texte“, 
                        in:
                        DHd 2016: Modellierung - Vernetzung - Visualisierung
                  http://www.dhd2016.de/abstracts/vorträge-061.html.
                    
            
         
      
   



      
         Texte und ihre automatische Analyse stehen im Zentrum vieler Untersuchungen in den Digital Humanities, etwa zur Erforschung sprachlicher und kultureller Wandlungsprozesse (siehe etwa Michel u.a. (2011)) oder im Bereich der Stilometrie (siehe etwa Jannidis (2014)). Die automatische Analyse von Texten beinhaltet typischerweise eine Reihe zunehmend komplexer werdender Schritte, angefangen bei der Segmentierung von Sätzen und Wörtern (Leerzeichen sind kein hinreichendes Kriterium, vgl. 
                „New York“) über die syntaktische und semantische Analyse bis hin zu diskursstrukturellen und pragmatischen Analysen. Die für diese einzelnen Schritte nötigen sprachtechnologischen Komponenten sind oft, zumindest innerhalb einer Anwendungsdomäne, wiederverwendbar. Folglich gibt es mittlerweile eine Fülle von Software-Repositorien, die entsprechende computerlinguistische Komponenten sammeln, und Frameworks, die ihre Integration in sogenannte Pipelines, also funktionsbezogene sequenzielle Kombinationen von einzelnen Komponenten, erleichtern. Die dadurch ermöglichte Wiederverwendung von Komponenten ist im Sinne nachhaltiger Forschung, da diese so nicht mehrfach entwickelt werden müssen und der Software-Austausch zwischen Gruppen unterstützt wird.
            
         
            Uima
            (Unstructured Information Management Architecture)
            
               1
            
             ist ein solches Framework, das sowohl im akademischen Kontext (in Deutschland u.a. DKPro
                
               2
            
             (de Castilho & Gurevych, 2014) und 
                JCoRe
            
               3
             (Hahn u.a., 2016)) als auch in industriellen Anwendungen (etwa bei IBMs 
                Jeopardy Champion 
                Watson (Ferrucci u.a., 2010)) breite Verwendung findet (einen Vergleich unterschiedlicher Frameworks stellen Bank und Schierle (2012) an). 
                Uima ist 
                open source unter der 
                Apache-Lizenz verfügbar und unterstützt mehrere Programmiersprachen, wobei 
                Java in der Praxis eine dominierende Rolle zukommt. 
            
         Wir nutzen mit 
                JCoRe seit fast einem Jahrzehnt 
                Uima für computerlinguistische Problemstellungen in verschiedenen Domänen bzw. Sprachen und stellen die dabei entwickelten Komponenten öffentlich zur Verfügung. Aktuell arbeiten wir daran, unser ursprünglich für bio-medizinische Fragestellungen und englischsprachige Fachtexte entwickeltes Repositorium auf den DH-Bereich, primär für das Deutsche, zu erweitern. 
                JCoRe stellt nicht nur sprachtechnologische Komponenten zur Verfügung, sondern auch die dafür nötigen Modelle für verschiedene Domänen — denn vor allem die Erstellung dieser Modelle ist ein enorm zeit- und rechenintensiver Prozess, der zudem ein hohes Maß an computerlinguistischer Expertise verlangt. Um die Einstiegshürden für die Benutzung solcher Ressourcen zu senken, bieten wir Anleitungen und Beispiele zur deklarativen Erstellung von Textanalyse-Pipelines mit 
                Uima und haben zudem eine interaktive Anwendung entwickelt (Hahn u.a., 2016).
            
         Eine Vielzahl von existierenden Sprachanalyse-Komponenten und Repositorien kann über 
                Uima eingebunden werden, darunter auch einige, die nicht originär für das Framework entwickelt wurden, wie etwa das über 
                DKPro verfügbare Stanford 
                CoreNLP
            
               4
             (Manning u.a., 2014) oder 
                OpenNLP
            
               5
            . Während 
                Uima für den produktiven Einsatz entwickelt wurde, steht beim alternativen 
                Natural Language Toolkit (NLTK)
                
               6
             der Einsatz in der Lehre im Zentrum (Bird u.a., 2009). 
                Uima ist eher mit dem 
                General Architecture for Text Engineering (GATE) Framework (Cunningham u.a., 2011) vergleichbar, das aber ein „geschlossenes“ NLP-System repräsentiert, das exklusiv von den Entwicklern von 
                Gate verwaltet wird. Generell sind integrierte Frameworks vorteilhaft gegenüber Pipelines aus einzelnen Werkzeugen, die mittels Textdateien/-strömen kommunizieren, da nicht bei jedem Schritt zwischen verschiedenen Formaten konvertiert werden muss. Insbesondere werden die bei selbstständigen Werkzeugen verbreiteten 
                in-line-Annotationen (wie etwa 
                „das_Artikel Haus_Nomen“) vermieden, die sich oft als unübersichtlich und fehleranfällig erweisen.
            
         
            Uima und die anderen bisher genannten Frameworks sind primär für den Einsatz auf lokaler Rechner-Infrastruktur gedacht und somit nur bedingt mit Systemen wie 
                WebLicht
            
               7
             (Hinrichs u.a., 2010) vergleichbar, die als Webservice verschiedene dezentral verteilte Komponenten zusammenführen. Dadurch wird zwar der Einstieg in die Nutzung sprachtechnologischer Systeme erleichtert, jedoch sind derartige Systeme nicht für die Verarbeitung großer Datenmengen geeignet und es entsteht eine eher intransparente Abhängigkeit von fremder Infrastruktur. 
                Uima ist somit kein Konkurrent für 
                WebLicht, sondern ermöglicht es vielmehr, Komponenten zu entwickeln, die bei Bedarf auch (durch in 
                DKPro enthaltene Konverter) in 
                WebLicht eingebunden werden können.
            
         Im Kern ist 
                Uima für die sequentielle Anreicherung mit Metadaten ausgelegt. Die möglichen Annotationen werden frei über ein objektorientiertes Typensystem definiert (siehe etwa Hahn u.a., 2007). In 
                Uima wird zwischen Komponenten unterschieden, die Annotationen vornehmen 
                (Analysis Engines), und solchen, die Texte in das interne CAS 
                (Common Analysis System) Format konvertieren 
                (Collection Reader); letztere können dabei auch bereits im Ursprungstext kodierte Metadaten verarbeiten. Die ersten Komponenten, die im Rahmen der Erweiterung 
                JCoRes
                 um DH-Komponenten entstanden und öffentlich zugänglich gemacht wurden, sind ein solcher 
                Collection Reader, der die neuerdings vom 
                Deutschen Textarchiv
            
               8
             (Geyken, 2013) zur Verfügung gestellten Dateien mit TCF-
                
               9
             und 
                Dublin Core-Annotationen
                10 verarbeiten kann, sowie eine entsprechende Erweiterung unseres Typensystems. In der unmittelbaren Zukunft geplante Erweiterungen betreffen 
                Analysis Engines für Text- bzw. Wortsegmentierung und Wortartenerkennung (POS-Tagging) in historischen (literarischen) Texten.
            
         Wir möchten durch unseren Beitrag insbesondere diejenigen, die primär computerlinguistische 
                Anwendungen für Fragestellungen der Digital Humanities realisieren wollen (und damit meist keine computerlinguistischen 
                Entwicklungsinteressen verfolgen), anregen, sich aus dem breiten Fundus existierender Komponenten zu bedienen und diese durch den Einsatz des 
                Uima-Frameworks zu verbinden. Die dadurch implizit eingeführte Modularität erleichtert zudem die Durchführung von Funktionstests, die Anpassung an neue Domänen und darüber hinaus den Austausch mit anderen Forschenden 
                — allesamt Anforderungen an eine nachhaltige Software-Infrastruktur.
            
      
      
         
            
               https://uima.apache.org
            
            
               https://
               
                  DKPro
               
               .github.io
            
            
               http://julielab.github.io
            
            
               http://stanfordnlp.github.io/Core
               
                  NLP
               
            
            
               https://open
               
                  NLP
               
               .apache.org
            
            
               http://www.nltk.org
            
            
               https://weblicht.sfs.uni-tuebingen.de
            
            
               
            
            
               
            
            
               http://dublincore.org
            
         
         
            
               Bibliographie
               
                  Bank, Mathias / Schierle, Martin (2012):
                        „A survey of text mining architectures and the Uima standard“,
                        in: 
                        Proceedings of LREC 2012 3479–3486.
                    
               
                  Bird, Steven / Klein, Ewan / Loper, Edward (2009): 
                        Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit.
                        Sebastopol, CA: O'Reilly.
                    
               
                  de Castilho, Eckart R. / Gurevych, Iryna (2014):
                        „A broad-coverage collection of portable NLP components for building shareable analysis pipelines“,
                        in: 
                        OIAF4HLT 2014 – Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT @ COLING 2014 1–11.
                    
               
                  Cunningham, Hamish / Maynard, Diana / Bontcheva, Kalina (2011): 
                        Text Processing with GATE.
                        Murphys, CA: Gateway Press.
                    
               
                  Ferrucci, David A. / Brown, Eric / Chu-Carroll, Jennifer / Fan, James / Gondek, David C. / Kalyanpur, Aditya A. / Lally, Adam / Murdock, J. William / Nyberg 3rd, Eric H. / Prager, John M. / Schlaefer, Nico / Welty, Christopher A. (2010):
                        „Building Watson: An overview of the DeepQA project“,
                        in: 
                        AI Magazine 31 (3): 59–79.
                    
               
                  Geyken, Alexander (2013):
                        „Wege zu einem historischen Referenzkorpus des Deutschen: das Projekt Deutsches Textarchiv“,
                        in: 
                        Perspektiven einer corpusbasierten historischen Linguistik und Philologie 221–234.
                    
               
                  Hahn, Udo / Buyko, Ekaterina / Tomanek, Katrin / Piao, Scott / McNaught, John / Tsuruoka, Yoshimasa / Ananiadou, Sophia (2007):
                        „An annotation type system for a data-driven NLP pipeline“,
                        in:
                        LAW 2007 – Proceedings of the Linguistic Annotation Workshop @ ACL 2007 33–40.
                    
               
                  Hahn, Udo / Matthies, Franz / Faessler, Erik / Hellrich, Johannes (2016): 
                        „Uima-based JCoRe 2.0 goes GitHub and Maven Central: State-of-the-art software resource engineering and distribution of NLP pipelines“,
                        in: 
                        LREC 2016 – Proceedings of the 10th International Conference on Language Resources and Evaluation 2502–2509.
                    
               
                  Hinrichs, Erhard W. / Hinrichs, Marie / Zastrow, Thomas (2010):
                        „WebLicht: Web-based LRT services for German“,
                        in: 
                        Proceedings of ACL-2010: System Demonstrations 25–29.
                    
               
                  Jannidis, Fotis (2014):
                        „Der Autor ganz nah: Autorstil in Stilistik und Stilometrie“,
                        in: Schaffrick, Matthias / Willand, Marcus (eds.): 
                        Theorien und Praktiken der Autorschaft.
                        Berlin: de Gruyter 169–195.
                    
               
                  Manning, Christopher D. / Surdeanu, Mihai / Bauer, John / Finkel, Jenny Rose / Bethard, Steven J. / McClosky, David (2014): "The Stanford CoreNLP Natural Language Processing Toolkit", in: 
                        Proceedings of ACL-2014: System Demonstrations 55–60.
                    
               
                  Michel, Jean-Baptiste / Shen, Yuan K. / Aiden, Aviva P. / Veres, Adrian / Gray, Matthew K. / The Google Books Team / Pickett, Joseph P. / Hoiberg, Dale / Clancy, Dan / Norvig, Peter / Orwant, Jon / Pinker, Steven / Nowak, Martin A. / Aiden, Erez L. (2011):
                        „Quantitative analysis of culture using millions of digitized books“,
                        in: 
                        Science 331 (6014): 176–182.
                    
            
         
      
   



      
         Graphische Romane vereinen als hybride Gattung Aspekte von Literatur und bildender Kunst (McCloud, 1993). Wie interagieren Bild und Text beim Lesen graphischer Literatur und ermöglichen das Verstehen des Gesamtwerkes? Worauf fokussiert die Aufmerksamkeit des Lesers? Als Methode zur Beantwortung dieser Fragen ist die Blickbewegungsmessung besonders geeignet. Blickbewegungen haben sich in einer Vielzahl an Studien als valides, nichtreaktives Maß für die Verarbeitung und das Verstehen von Text und Bild erwiesen, in dem sich zudem auch unbewusste Verarbeitungsprozesse niederschlagen (Findlay & Gilchrist, 2003; Wade & Tatler, 2005). 
         In früheren Arbeiten (Laubrock, Hohenstein & Thoß, 2016; Dunst, Hartel, Hohenstein Laubrock, 2016) haben wir mit Eyetracking-Analysen gezeigt, dass beim Lesen grafischer Literatur der größte Teil der Aufmerksamkeit dem Text in Sprechblasen und Beschriftungen (Captions) zugewandt wird und nur ein relativ kleiner Teil den originär visuellen Gestaltungselementen alloziert wird. Wird der visuelle Inhalt gar nicht beachtet, oder kann er möglicherweise bereits im peripheren Sehen während der Fixationen auf dem Text verarbeitet werden? Wir hatten bereits berichtet, dass Comics-Experten den Bildanteil stärker beachten und darauf verstehensrelevante Information extrahieren. In einer neuen Serie von Studien untersuchen wir mittels blickkontingenter Präsentation, ob (a) den Bildanteilen mehr Aufmerksamkeit zugewandt wird, wenn die Vorschau verhindert wird, indem das Bild erst eingeblendet wird, wenn der Blick sich auf ein Panel bewegt und (b) die Aufmerksamkeit andere grafische Elemente auswählt, wenn zwar der visuelle Teil der Panels sichtbar ist, der Text aber erst nach Fokussierung eines Panels eingeblendet wird.
         Das visuelle Material wurde auf zweierlei Weise annotiert. Einerseits annotierten Menschen Personen und einzelne Objekte innerhalb der Panels. Andererseits versuchen wir eine objektiven Beschreibung des visuellen Materials mithilfe von Deskriptoren aus dem maschinellen Sehen (Computer Vision), z.B. mittels Farbhistogrammen, lokalem Fourier-Spektrum oder SIFT-Deskriptoren (Lowe, 1999). Der Vorteil dieser Beschreibung ist neben der Objektivität die skriptgesteuerte Anwendbarkeit auf große Datenmengen, etwa digitalisierte Korpora grafischer Literatur. Vergleichbare Arbeiten aus der Schnittstelle von Kunstgeschichte und Informatik ermöglichen beispielsweise eine automatisierte Klassifikation von Kunstrichtungen (Saleh & Elgammal, 2015) und zeigen das Potenzial eines solchen Ansatzes als Stilometrie visueller Merkmale.
         Für die Zuordnung der Blickbewegungsdaten auf das Stimulusmaterial nutzen wir die im Projekt entwickelte Graphic Novel Markup Language (GNML), eine Erweiterung der Comic Book Markup Language (CBML; Walsh, 2012). Das Material wurde mit unserem Editor annotiert, für Weiterverarbeitung und statistische Analyse der Daten nutzten wir ein in Entwicklung befindliches R-Paket. Die objektive Beschreibung des visuellen Materials mit Deskriptoren aus dem maschinellen Sehen wurde unter Nutzung von OpenCV (Bradski, 2000) und VLFEAT (Vedaldi & Fulkerson, 2008) teils in Python und teils in Matlab implementiert, da für R für diesen Anwendungsbereich keine hinreichend entwickelte Funktionsbibliothek existiert. 
      
      
         
            
               Bibliographie
               
                  Bradski, Gary (2000):
                        „The OpenCV library“,
                        in:
                        Dr. Dobb’s Journal of Software Tools 25 (11): 120–125. 
                    
               
                  Dunst, Alexander / Hartel, Rita / Hohenstein, Sven / Laubrock, Jochen (2016):
                        „Corpus Analyses of Multimodal Narrative: The Example of Graphic Novels“,
                        in:
                        DH2016: Conference Abstracts 178–180.
                    
               
                  Findlay, John M. / Gilchrist, Ian D. (2003):
                        Active Vision. The Psychology of Looking and Seeing.
                        Oxford: Oxford University Press.
                    
               
                  Laubrock, Jochen / Hohenstein, Sven / Thoß, Aalexander (2016):
                        „Moving around the city of glass“,
                        in: 
                        DHd 2016: Modellierung - Vernetzung - Visualisierung 186. 
                    
               
                  Lowe, David G. (1999):
                        „Object recognition from local scale-invariant features“,
                        in:
                        Proceedings of the International Conference on Computer Vision (ICCV'99) 1150–1157.
                    
               
                  McCloud, Scott (1993):
                        Understanding comics: the invisible art.
                        Northampton, MA: Tundra.
                    
               
                  Saleh, Babak / Elgammal, Ahmed M. (2015):
                        „Large-scale classification of fine-art paintings: Learning the right metric on the right feature“,
                        in:
                        CoRR abs/1505.00855, 1–21 
                        http://arxiv.org/pdf/1505.00855v1.pdf.
                    
               
                  Vedaldi, Andrea / Fulkerson, Brian (2008):
                        VLFeat: An open and portable library of computer vision algorithms. [Computer Software: http://www.vlfeat.org/ ]
               
                  Wade, Nicholas J. / Tatler, Benjamin W. (2005): 
                        The Moving Tablet of the Eye: Origins of modern eye movement research. 
                        Oxford: Oxford University Press.
                    
               
                  Walsh, John (2012):
                        „Comic Book Markup Language: An Introduction and Rationale“,
                        in:
                        DHQ: Digital Humanities Quarterly 6 (1).
                    
            
         
      
   



      
         
         
            Einführung
            Das ÖBL (Österreichisches Biographisches Lexikon 1815-1950) ist ein umfassendes Werk, das derzeit rund 18,000 Biographien von wichtigen historischen Persönlichkeiten aus der österreichisch-ungarischen Monarchie und der Ersten und Zweiten Republik Österreichs enthält. Während an dem Lexikon noch gearbeitet wird, erscheint es in gedruckter Form, und seit 2009 ist es auch online verfügbar.
                    APIS - Mapping historical networks: Building the new Austrian Prosopographical | Biographical Information System - ist ein interdisziplinäres Digital Humanities Projekt, das WissenschaftlerInnen aus unterschiedlichen Themenbereichen (Biografien, Geschichte, Geographie, Sozialwissenschaften, Informationstechnologie) verbesserten Zugriff (Suchabfragen, API etc.) auf die ÖBL-Daten erlauben wird. Dadurch wird es möglich sein innovative, interdisziplinäre Forschung auf der Grundlage dieser einzigartigen Ressource durchzuführen. Als erstes Beispiel für eine solche angewandte wissenschaftliche Forschung und als wichtiger Test der Brauchbarkeit und Eignung der entwickelten Lösung, wird bereits im APIS Projekt eine soziodemografische Analyse, die die Formen und Muster der Migration von gesellschaftlichen Eliten untersucht, umgesetzt. 
                    In unserer Präsentation konzentrieren wir uns auf die zugrunde liegende technische Lösung, vor allem auf die dynamischen Aspekte - Workflow – und die Ergebnisse der verschiedenen angewandten Verfahren, um den aktuellen Stand der Umsetzung zu beschreiben.
                    
            
         
         
            Ansatz
            ÖBL Daten stehen momentan in einem Ad-hoc-XML-Format zur Verfügung. Diese XMLs enthalten einige Fakten (Geburts- und Todesdaten, Orte, Berufsangaben usw.) in strukturierter Form, der Großteil der Information versteckt sich jedoch in dem unstrukturierten Haupttext der Biographie. Das Hauptziel des Projektes ist Informationen automatisch aus dem freien Text zu extrahieren, und sie in strukturierter Form zur Verfügung zu stellen. Um dieses Ziel zu erreichen, wird ein zweifacher Hybrid-Ansatz verfolgt, der einerseits automatische und manuelle Textverarbeitung kombiniert und andererseits erlaubt die erhobenen Daten in verschiedenen Formaten zu serialisieren. Letzteres beinhaltet nicht nur die Bereitstellung in verschiedenen Formaten (z.B. RDF/JSON), sondern auch die Verwendung verschiedener Ontologien (z.B. CIDOC-CRM (Doerr 2003: 75-92), NDB (Historische Kommission bei der Bayerischen Akademie der Wissenschaften 1953)). Die extrahierten Entitäten sind mit mehreren semantischen Referenz Ressourcen wie zum Beispiel GND (Pfeifer 2012: 80-91), GeoNames
                    1 oder DBpedia (Bizer 2009: 154-165) abgeglichen und mit URIs aus diesen versehen (Entity Linking). Dieser kombinierte Ansatz wurde gewählt, um die höchstmögliche Genauigkeit der Annotationen zu gewährleisten, und den manuellen Aufwand so gering wie möglich zu halten. Obwohl es bewährte Techniken und Methoden für die Verarbeitung natürlicher Sprache gibt, wird manuelle Arbeit (Korrektur) der Forscher, die mit den jeweiligen Wissenschaftsgebieten vertraut sind, nach wie vor erforderlich sein.
                
         
         
            Datenmodell
            Das Datenmodell besteht aus fünf Entitäten (Personen, Institutionen, Orte, Werke und Ereignisse) und einer Meta-Entität (Verweis auf den ursprünglichen Artikel). Es gibt Beziehungen zwischen allen Entitäten (z.B. Person - Institution, Person - Ereignis) und Beziehungen sind auch zwischen den gleichen Objekttypen möglich (z.B. Person -> Vater_von -> Person). Die Beziehungen können auch temporalisiert (Start- und Enddatum) und typisiert werden (Typen können je nach Bedarf angegeben werden). Das erlaubt uns praktisch alle möglichen Szenarien zu modellieren. 
                    Der ursprüngliche Plan war, die Daten nach bestehenden, gut definierten Ontologien zu modellieren. In der Evaluierungsphase wurde uns aber klar, dass sehr viele verschiedene Ontologien existieren. Einige sind wie CIDOC-CRM Event basiert, andere verbinden Entitäten direkt. Wir haben uns deshalb entschlossen ein eigenes (internes) Datenmodell zu erstellen und so den technischen Aufwand für die Verarbeitung, Darstellung und Speicherung der Daten möglichst gering zu halten. Gleichzeitig werden wir aber dieses interne Datenmodell mit Hilfe schon existierender Ontologien (NDB, CIDOC-CRM etc.) in verschiedenen Formen serialisieren und der Öffentlichkeit zur Verfügung stellen. Das stellt die möglichst einfache, nachhaltige Nutzung unserer Daten sicher.
                
         
         
            Extraktion
            Um strukturierte semantische Informationen aus den Biographien zu extrahieren, und die dadurch identifizierten Objekte zu Ressourcen wie GND, GeoNames zu verknüpfen verwenden wir automatische Tools. Die Ergebnisse werden von Experten verifiziert und ausgebessert um die Qualität der Daten zu gewährleisten, und um unser System durch manuelle Korrektur zu verbessern. Während die NLP-Tools eine schnelle Verarbeitung ermöglichen sind die Ergebnisse nicht zu 100% korrekt. Um die Genauigkeit zu verbessern, setzen wir mehrere Systeme, Quellen und Analysen ein. Für die automatische Extraktion haben wir mehrere Tools getestet und bewertet, wie z.B. Stanford NER (Finkel 2005: 363-370), GATE (Cunningham 2011), OpenNLP
                    2, Stanbol (Bachmann-Gmur 2013), basierend auf folgende Kriterien: 1) welche Sprachen unterstützt das System 2) Möglichkeit der Anpassung, 3) Entity Linking Fähigkeiten, 4) Output Format und 5) die Verfügbarkeit und Qualität der API. Apache Stanbol hat sich als das am besten geeignete Werkzeug für unsere Zwecke gezeigt. Stanbol ermöglicht die Verknüpfung von Entitäten wie Personen, Institutionen zu Referenzressourcen (Normdateien, Ontologien). Wir haben die Biographien mit GND und GeoNames abgeglichen, und planen weitere LOD
                    3 Ressourcen hinzuzufügen. Durch die Verknüpfung von oben benannten Entitäten zu den semantischen Ressourcen können wir viele zusätzliche Informationen (z.B. Alternative Namen, Titel von Werken usw.) zu unseren Daten hinzufügen, und so Inhalte mit fehlenden Informationen bereichern.
                    
            
         
         
            Anwendung
            Um den manuellen Arbeitsaufwand (Korrektur der Daten etc.) zu minimieren haben wir eine effiziente und einfache Weboberfläche geschaffen, die es den ForscherInnen erlaubt mit den Daten zu interagieren. Im Sinne einer nachhaltigen Nutzung und einfacher weiteren Betreuung des so entstandenen Tools haben wir uns entschlossen auf erprobte Web-Technologien zu setzen (Django
                    4/MySQL). Die Web-Anwendung ist in Django, einem Python-basierten Web-Entwicklungs-Framework, implementiert. Django ist nicht nur ein ausgereiftes und verbreitetes Tool (Websites wie Disqus, Pinterest und die Washington Times nutzen es), sondern bietet auch die Möglichkeit die volle Bandbreite der verschiedenen Python Bibliotheken nativ im Code zu verwenden (NLTK
                    5, scikit-learn
                    6, NumPy
                    7 etc.) .
                    Die Web-Anwendung stellt die Daten der einzelnen Biographien strukturiert in drei Teilen dar: primäre minimale Informationen, Haupttext mit markierten Anmerkungen und die Listen von Orten, Institutionen und Personen, die mit dem Biographierten in Zusammenhang stehen. Die Anwendung bietet auch Funktionen für die Navigation: dropdown Listen sowie einfache Volltextsuche.
                    Eine weitere wichtige Funktion der Anwendung ist die Möglichkeit, den Text manuell mit Annotationen zu versehen. Dieses Feature erlaubt sowohl die Korrektur von automatischen Annotationen, als auch das Hinzufügen von neuen Annotationen. Die Kuratoren können die Entitäten mit der Maus auswählen oder im Kontextmenü identifizieren. 
                
            Derzeit liegt der Schwerpunkt auf der Darstellung von Orten. Dementsprechend wird die Anwendung mit eingebetteten Karten ausgestattet, an denen identifizierte geographische Orte visualisiert werden können. In der nächsten Phase des Projekts wird eine interaktive Visualisierung entwickelt, um das Verständnis der Daten und die Navigation im Datenbestand zu erleichtern.
         
         
            Arbeitsablauf
            Das System unterstützt zwei Workflows: im ersten Schritt schickt die Anwendung (das Extrakt-Modul) die Biographien im Batch-Modus zu einem Extraktionsservice (lokale Stanbol Instanz), welches die Abfragen an externe Services und/oder lokale Indizes weiterleitet und die gematchten Entitäten in einer Liste in JSON-LD Format zurückgibt. Diese Entitäten werden von dem Extrakt-Modul analysiert und in der Datenbank abgespeichert. Danach werden sie in der Web-Anwendung dargestellt und können von den ForscherInnen überprüft und korrigiert werden.
                    Im zweiten Schritt wird der Workflow vom Benutzer gestartet: Der menschliche Annotator markiert einen String und identifiziert ihn als Ort, die Anwendung schickt den ausgewählten String zur Stanbol Instanz, die die verfügbaren Ressourcen abfragt und mögliche Kandidaten zurückgibt. Diese Treffer werden dem/der ForscherIn in Form eines Autocomplete Feldes angezeigt.
                    
            
         
         
            Schlussfolgerung
            Während wir uns in unserem Abstrakt auf die technische Umsetzung konzentriert haben, ist es wichtig im Auge zu behalten, dass das System nur eine Voraussetzung ist die eigentliche Forschungsfragen beantworten zu können. Alle im Projekt generierten Daten sowie die entwickelte Forschungsumgebung wird der Öffentlichkeit zugänglich gemacht (eine erste Version der Forschungsumgebung wird Ende September in unserem Github Account zugänglich gemacht). Wie schon weiter oben angesprochen versuchen wir die Nachhaltigkeit unserer Lösung auf mehrfache Weise zu erreichen. Zum einen verwenden wir gut etablierte Web-Technologien und ermöglichen somit vielen Entwicklern weltweit unseren Code zu warten und/oder weiter zu entwickeln. Zum anderen verbinden wir unsere Daten mit der LOD-Cloud und serialisieren sie mit Hilfe verschiedener weit verbreiteter Ontologien in den gängigsten Formaten und stellen so sicher, dass andere Projekte unsere Daten mit äußerst kleinem Aufwand direkt in ihre Projekte einbetten können.
         
      
      
         
            
               http://www.geonames.org/
            
             https://opennlp.apache.org/
             http://linkeddata.org/
             https://www.djangoproject.com/
             http://www.nltk.org/
             http://scikit-learn.org/stable/
             http://www.numpy.org/
         
         
            
               Bibliographie
               
                        APIS: Mapping historical networks: Building the new Austrian Prosopographical | Biographical Information System (APIS) 
                        http://www.oeaw.ac.at/acdh/en/apis
               
               
                  Bachmann-Gmur, Reto (2013): 
                        Instant Apache Stanbol (1st ed.).
                        Packt Publishing. ISBN 1783281235.
                    
               
                  Bizer, Christian / Lehmann, Jens / Kobilarov, Georgi / Auer, Soren / Becker, Christian / Cyganiak, Richard / Hellmann, Sebastian (2009): 
                        „DBpedia - A crystallization point for the Web of Data“, 
                        in: 
                        Journal of Web Semantics 7 (3): 154–165.
                    
               
                  Cunningham, Hamish / Maynard, Diana / Bontcheva, Kalina (2011): 
                        Text Processing with GATE (Version 6). 
                        University of Sheffield Department of Computer Science. ISBN 0956599311.
                    
               
                  Doerr, Martin (2003): 
                        „The CIDOC CRM – An Ontological Approach to Semantic Interoperability of Metadata“, 
                        in: 
                        AI Magazine 24 (3): 75–92.
                    
               
                  Finkel, Jenny Rose / Grenager, Trond / Manning, Christopher (2005): 
                        „Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling“,
                        in: 
                        Proceedings of ACL-2005 363–370.
                    
               
                  Historische Kommission bei der Bayerischen Akademie der Wissenschaften (seit 1953): 
                        Neue deutsche Biographie, 
                        Berlin: Duncker & Humblot. ISBN 3-428-00181-8
                    
               
                  ÖBL - Österreichisches Biographisches Lexikon/Austrian Biographical Lexicon (1815-1950) Online-Edition und Österreischiches Biographisches Lexikon ab 1815 (2. Überarbeitete Auflage - online). 
                        Verlag der Österreichischen Akademie der Wissenschaften. Wien.
                        http://www.biographien.ac.at/oebl [letzter Zugriff 26. August 2016]
                    
               
                  Pfeifer, Barbara (2012): 
                        „Vom Projekt zum Einsatz. Die gemeinsame Normdatei (GND)“,
                        in: Brintzinger, Klaus-Rainer (ed.): 
                        Bibliotheken: Tore zur Welt des Wissens. 101. Deutscher Bibliothekartag in Hamburg 2012, Olms, Hildesheim u.a. 2013: 80–91.
                    
            
         
      
   



      
         
            Topic Modeling ist eine Methode zur semantischen Erschließung größerer Textsammlungen, die in den letzten Jahren zunehmend in den Fokus der Aufmerksamkeit digital arbeitender Literaturwissenschaftler gerückt ist. Die Methode nutzt probabilistische Verfahren um aus einer Textsammlung eine Reihe von Verteilungen über die Wahrscheinlichkeiten einzelner Wörter zu erzeugen. Diese werden dann als distinkte semantische Gruppen, sogenannte ‘Topics’, aufgefasst, also als Gruppen inhaltlich zusammenhängender Wörter, die in den einzelnen Texten jeweils mehr oder weniger stark präsent sind (Blei 2012, Steyvers und Griffiths 2006).
            
         Ursprünglich entwickelt, um in größeren Sammlungen kürzerer Fachartikel schnell jene zu identifizieren, die für bestimmte Themen relevant sein könnten, kann diese Methode darüber hinaus für eine Reihe von Problem im Bereich der digitalen Literaturwissenschaft interessante neue Lösungsansätze bieten. Dazu gehört die automatische Identifikation von Romanen, die ähnliche Themen behandeln (wenngleich eine direkte Gleichsetzung probabilistischer ‘Topics’ mit literarischen ‘Themen’ durchaus problematisch ist), ebenso wie die Zuordnung zu bestimmten Genres anhand inhaltlicher Aspekte, oder die quantifizierende Betrachtung der zu- und abnehmenden Bedeutung einzelner Themenfelder über den Verlauf eines einzelnen Romans (vgl. Blevins 2012, Jockers 2011, 
                Rhody 2012, Schöch in Vorbereitung).
            
         Mit den Programmen ‘Mallet’ (vgl. McCallum 2002) und ‘Gensim’ (vgl. Rehurek 2010) stehen zur Zeit zwei State-of-the-Art Implementierungen von Topic Modeling-Algorithmen zur Verfügung. Um die Methode produktiv einzusetzen, sind aber neben der Erzeugung des Modells weitere Arbeitsschritte notwendig (Abb. 1). Im ‘Preprocessing’ gilt es zunächst, die Textsammlungen in eine Form zu bringen, in der sie vom Modellierungsprogramm verarbeitet werden können. Darüber hinaus werden die Texte normalerweise durch das Herausfiltern häufiger Funktionswörter auf die potentiell inhaltsrelevanten Wörter reduziert, was in der Regel den vorhergehenden Einsatz von NLP-Tools (Natural Language Processing) erfordert. Sind die ‘Topics’ dann erst einmal errechnet worden, kann sich eine Visualisierung der Ergebnisse anschließen, oder ihre statistische Evaluierung anhand interner oder externer Kriterien, ein Aspekt dem beim Einsatz von Topic Modeling-Verfahren im DH-Kontext bisher eher zu wenig Beachtung geschenkt wurde.
         Ziel unseres Projektes ist es, den Einstieg in aktuelle Topic Modeling-Verfahren für digital arbeitende Literaturwissenschaftler wesentlich zu vereinfachen, indem wir möglichst viele der notwendigen Arbeitsschritte in einer einheitlichen, umfangreichen und gut dokumentierten Programmbibliothek für die unter digital-quantitativ arbeitenden Geisteswissenschaftlern stark verbreitete Programmiersprache Python anbieten. Hierbei sollen Nutzerinnen und Nutzer bei allen Arbeitsschritten auf vorhandene, in einem ausführlichen Tutorial dokumentierte Funktionen zurückgreifen und so weit wie möglich wie mit einem Kommandozeilentool arbeiten können, ohne selbst programmieren zu müssen. Die Anforderungen an die Programmierkenntnisse der Forschenden, die diese Verfahren einsetzen möchten, werden damit minimiert und die Methode wird so einem größeren Nutzerkreis zugänglich gemacht.
         Für das NLP-Preprocessing steht mit dem DARIAH-DKPro-Wrapper (DDW) ein komfortables Einheitswerkzeug zur Verfügung, das ein großes Spektrum an NLP-Aufgaben abdeckt und linguistische Annotationen in einem Python-Pandas-kompatiblen Ausgabeformat erzeugt. Ein Ziel unserer Bibliothek ist die direkte Anbindung des DDW-Outputs an existierende Implementierungen verschiedener etablierter Varianten von Topic Modeling-Algorithmen. 
         Für die Untersuchung der resultierenden Modelle möchten wir verschiedene Evaluierungsverfahren anbieten, sowohl interne Verfahren wie z.B. das Perplexity-Maß, als auch externe Vefahren, wie z.B. die Weglänge zwischen zwei Begriffen in einem Wörterbuch. Hieran schließen sich verschiedene Optionen zur Visualisierung der Ergebnisse an.
         Im Fokus der Entwicklung steht die Gestaltung schlüssig aufeinander aufbauender Programmbefehle, die einer einheitlichen Syntax folgen und deren Funktion sich schnell erschließen lässt. Sie sollen sich ohne längere Einarbeitung nutzen und zu einer Pipeline zusammenfügen lassen, die die spezifischen Arbeitsschritte eines bestimmten Topic Modeling-Projektes umsetzt. Hierbei können Nutzerinnen und Nutzer auf detaillierte Anleitungen aus einem umfangreichen Tutorial zurückgreifen, in dem alle Funktionen, alle Outputs, und potentielle Kombinationen detailliert dokumentiert und anhand von Beispielen erläutert werden.
         
            Die Entwicklung der Programmbibliothek kann auf Erfahrungen mit einer vorhandenen, Python-basierten Implementierung eines entsprechenden Workflows aufbauen, die allerdings eher “proof of concept”-Character hat (Topic Modeling Workflow “tmw”, vgl. Schöch 2015 und 
            
            ).
         
         
            
               
            
            Abbildung 1: Workflow eines Topic Modeling-Projektes
            
      
      
         
            
               Bibliographie
               
                  Blei, David M. (2012):
                        „Probabilistic Topic Models“,
                        in:
                        Communication of the ACM 55 (4): 77–84
                        10.1145/2133806.2133826.
                    
               
                  Blevins, Cameron (2010):
                        „Topic Modeling Martha Ballard’s Diary“,
                        in:
                        Historying                        . 
                        http://historying.org/2010/04/01/topic-modeling-martha-ballards-diary/.
                    
               
                  Jockers, Matthew L. (2013): 
                        Macroanalysis - Digital Methods and Literary History. 
                        Champaign, IL: University of Illinois Press.
                    
               
                  McCallum, Andrew K.  (2002): 
                        MALLET: A Machine Learning for Language Toolkit
                  http://mallet.cs.umass.edu.
                    
               
                  Rehurek, Radim / Sojka, Petr (2010):
                        „Software framework for topic modelling with large corpora“,
                        in:
                        Proceedings of LREC 2010.
                    
               
                  Rhody, Lisa M. (2012):
                        „Topic Modeling and Figurative Language“,
                        in:
                        Journal of Digital Humanities 2 (1) 
                        http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody/.
                    
               
                  Richardson, Stephen D. / Braden-Harder, Lisa (1988):
                        „The Experience of Developing a Large-Scale Natural Language Text Processing System: CRITIQUE“,
                        in: 
                        Proceedings of the Second Conference on Applied Natural Language Processing 195–202.
                    
               
                  Schöch, Christof (in Vorbereitung):
                        „Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama“,
                        in:
                        DHQ: Digital Humanities Quarterly
                  http://digitalhumanities.org/dhq.
                        Preprint: 
                        https://zenodo.org/record/48356.
                    
               
                  Steyvers, Mark / Griffiths, Tom  (2006): 
                        „Probabilistic Topic Models“, 
                        in: Landauer, T. / McNamara, D. / Dennis, S. / Kintsch, W.:
                        Latent Semantic Analysis: A Road to Meaning.
                        Laurence Erlbaum.
                    
            
         
      
   



      
         Mit „Ulysses: A Critical and Synoptic Edition“ erschien 1984 eine der ersten Forschungseditionen, die auf Basis der systematischen Verwendung von Kollationierungssoftware digital erzeugt wurde. Das Münchner Team um Hans Walter Gabler verwendete hierzu TUSTEP sowohl zur Validierung der Transkripte einzelner Zeugen als auch zur Erschließung der zeugenübergreifenden Synopse. Für die gedruckte Edition wurden die halbautomatisch erzeugten Kollationsergebnisse mit einem eigens entwickelten System komplexer Diakritika ausgezeichnet, die es dem geübten Leser ermöglichen sollten, die Textentstehung über stellenweise mehr als zwanzig inter- und intradokumentarische Textstufen hinweg in einer synoptisch integrierten Textfassung nachzuvollziehen. Während die Konzeption und Umsetzung dieser Arbeit bis heute als bahnbrechend im Bereich der Computerphilologie zu bezeichnen ist, konnte das Potenzial der resultierenden Druckausgabe für die Joyce-Forschung nicht annähernd ausgeschöpft werden. Zu komplex war das Markup, dem es gelingen sollte, zu verknüpfen, was zuvor getrennt war und zu hoch war der Aufwand, sich in diese Systematik einzuarbeiten.
         Im Digitalen hingegen führten die Daten jene Odyssee fort, die die Druckedition beenden sollte. Auf der Suche nach einem Markup-Standard, der es vermag, die Inhalte der Druckedition digital zu repräsentieren, wurden die TUSTEP Ergebnisse zunächst von Tobias Rischer im Rahmen seiner Diplomarbeit (1997) in SGML/TEI transformiert und anschließend in mehreren Überarbeitungen über TEI P4 bis hin zur aktuellen Version der TEI P5v3 (2016) migriert. Dieser Beitrag vollzieht die Evolution dieser “Legacy Data” nach, bis hin zu ihrer jüngsten Station - der noch andauernden Bemühung einer Migration nach TEI P5v3, welche im Rahmen des DFG- und NEH-geförderten Kooperationsprojektes “Diachronic Markup and Presentation Practices for Text Editions in Digital Research Environments” am Lehrstuhl für Digital Humanities der Universität Passau durchgeführt wird. 
         Erstmals seit der zweiten, überarbeiteten Ausgabe der synoptisch-kritischen Gabler Edition 1986 gelang es, aus den TEI-Daten die synoptische Visualisierung der Druckedition zu rekonstruieren und somit eine Konsistenzprüfung gegen die ursprünglichen Daten zu ermöglichen. Erst durch diese visuelle Rückführung offenbarten sich migrationsbedingte Fehler und Provisorien, welche zuvor, wenn überhaupt, nur in Fußnoten und privaten Aufzeichnungen vergangener Beteiligter dokumentiert wurden. Neben dem allgemeinen Versuch, die vollzogenen Änderungen aus den Aufzeichnungen und Migrationsergebnissen früherer Projekte zu rekonstruieren, hat es sich das Passauer Team zur Aufgabe gemacht, Strategien zur Entdeckung, Typisierung und Korrektur derartiger „Migrationsverluste“ zu entwickeln. Ein wesentlicher Bestandteil dieser Arbeit ist die Abschätzung der Leistungsfähigkeit und Wirtschaftlichkeit von automatisierten Batch-Konvertierungen mittels XSLT und Python im Vergleich zur manuellen Intervention und Korrektur der Kodierung. 
         Neben der Identifikation und Korrektur von „Migrationsfehlern“, steht die Rekonstruktion der textgenetischen Perspektive, durch welche sich die Druckedition auszeichnete, im Vordergrund. Während Gabler die textuelle Entwicklung, welche er mittels der Kollation chronologisch aufeinander folgender Textzeugen erschlossen hatte, im Druck synoptisch darstellen konnte, beinhalteten die TEI Guidelines bis zur Version P5v2 kein Modell zur Auszeichnung textgenetischer Prozesse. Es fehlte schlicht die Möglichkeit zur formalisierten Dokumentation einer stufenweisen, zeugenübergreifenden Chronologie der Textentwicklung. In der Druckedition wurde jeder auktorialen Textänderung 
                genau eine Textstufe aus der heuristisch erschlossenen Chronologie zugeordnet. Diese lineare Textentwicklung über intra- und interdokumentarische Textstufen, in Gablers Terminologie auch Overlay und Level genannt, musste im Digitalen in eine Auszeichnung überführt werden, welche die Genese in den Hintergrund rückt und zu jeder auktorialen Modifikation anstelle einer Textstufe eine Liste sämtlicher Zeugen verzeichnet, auf welcher die spezifische Änderung Bestand hat. Diese Art der dokumentenorientierten Kodierung von Textgenese entspricht zwar bis heute der gängigen Auszeichnungspraxis historisch-kritischer Editionsprojekte, repräsentierte aber zu keinem Zeitpunkt die textgenetische Intension der 84er 
                Ulysses Edition. Erst mit der Integration eines textgenetischen Modells in die TEI Guidelines, kann die ursprüngliche Intension erstmals auch in TEI kodiert werden. Hierzu bedarf es einer weiteren Episode der Datenmigration auf der Odyssee zum richtigen Standard. 
            
      
      
         
            
               Bibliographie
               
                  Brüning, Gerrit / Henzel, Katrin / Pravida, Dietmar (2014): 
                        „Multiple Encoding in Genetic Editions: The Case of Faust“,
                        in: 
                        Journal of the Text Encoding Initiative 4. Available from: jtei.revues.org.
                    
               
                  Burnard, Lou / O’Brien O’Keeffe, Katherine / Unsworth, John (2006): 
                        Electronic Textual Editing.
                        New York: Modern Language Association of America.
                    
               
                  Burnard, Lou / Jannidis, Fotis / Pierazzo, Elena / Midell, Gregor / Rehbein, Malte (2010): 
                        „An Encoding Model for Genetic Editions“,
                        in:
                        TEI: Text Encoding Initiative. 
                        Retrieved from www.tei-c.org/ Activities/Council/Working/tcw19.html/. 
                    
               
                  Joyce, James / Gabler, Hans Walter (eds.) (1984): 
                        Ulysses: A Critical and Synoptic Edition.
                        New York: Garland.
                    
               
                  Joyce, James (1922): 
                        Ulysses. 
                        Paris: Shakespeare and Company.
                    
               
                  Fordham, Finn (2010): 
                        I do, I undo, I redo: The Textual Genesis of Modernist Selves in Hopkins, Yeats, Conrad, Forster, Joyce, and Woolf. 
                        Oxford / New York: Oxford University Press.
                    
               
                  Rischer, Tobias (1997): 
                        Eine TEI/SGML-Edition der textkritischen Ausgabe von James Joyces Ulysses. 
                        Diplomarbeit, LMU München.
                    
               
                  TEI Consortium (eds.) (2016): 
                        TEI P5: Guidelines for Electronic Text Encoding and Interchange. P5v3.
                        Available from: http://www.tei-c.org/Guidelines/P5/.
                    
            
         
      
   



      
         
         
            Ausgangslage
            Der Einfluss der unter Digital Humanities (DH) zusammengefassten digitalen Theorien und Methoden auf die geisteswissenschaftlichen Disziplinen wächst stetig. Digitale Projekte erleben in den Geisteswissenschaften einen rasanten Aufschwung (Koller 2016: 43). Damit einher geht der Bedarf an Absolventen geisteswissenschaftlicher Fächer, die bereits während ihres Studiums Kompetenzen im Bereich der Digital Humanities erwerben konnten. Bereits 2013 forderten Vertreter/innen im „Manifest für die DH“ eine Etablierung „digitale[r] Trainingsprogramme in den Geisteswissenschaften“ (DH-Manifest: 2013), angepasst an die unterschiedlichen Bedürfnisse der Fachbereiche und die jeweiligen Karrierestufen. Auch der DHd misst der Ausgestaltung der IT-Ausbildung von Studierenden eine gesteigerte Bedeutung zu. Die Arbeitsgruppe zur Erarbeitung eines „Referenzcurriculums Digital Humanities“
                    2 beschäftigt sich mit der Suche nach einer 
                    bestpraxis, von der Anwender und Institutionen gleichermaßen profitieren (Sahle 2013; Thaller 2015: 3).
                
            Zahlreiche Universitätsstandorte haben auf die neuen Anforderungen mit der Einrichtung unterschiedlich ausgestalteter DH-Studiengänge reagiert (Bartsch/Borek/Rapp 2016: 173; DH Course Registry). Trotz dieser neugeschaffenen Angebote besteht ein zusätzlicher Bedarf an informationstechnologischer Ausbildung in der Breite (Ehrlicher 2016: 625). Zunehmend wird auch in „klassischen“ geisteswissenschaftlichen Berufsfeldern Sicherheit im Umgang mit Software und digitalen Technologien vorausgesetzt. Dieses Grundverständnis digitaler Methoden kann nicht mehr ausschließlich im Selbststudium angeeignet werden (Spiro 2013: 332; Sahle 2016: 79).
         
         
            Projektziele und Rahmenbedingungen
            Hier setzt das Projekt „Digitaler Campus Bayern – Digitale Datenanalyse in den Geisteswissenschaften“ an, welches von der IT-Gruppe Geisteswissenschaften (ITG) der Ludwig-Maximilians-Universität München (LMU) seit Beginn dieses Jahres durchgeführt wird. Grundgedanke ist eine IT-Grundausbildung („
                    IT for all“), welche die Studierenden problemorientiert in die Anwendung digitaler Methoden einführt. Ausgehend von fachwissenschaftlichen Fragestellungen werden Lehrveranstaltungen mit IT-Inhalten in Kooperation mit verschiedenen geschichtswissenschaftlichen Disziplinen und der Kunstgeschichte konzipiert. Dabei soll eine möglichst umfassend angelegte Grundlagenvermittlung in Erfassung, Modellierung, Analyse und anschließender Visualisierung von Daten erfolgen (Lücke/Riepl 2016: 77). Das Verständnis digitaler Methoden steht ebenso im Vordergrund wie eine fachliche Reflexion ihrer Potentiale (Rehbein 2016: 17).
                
            Die Situation der DH an der LMU gestaltete sich bis Projektbeginn (Januar 2016
                    3) ambivalent. In den vorgenannten Studiengängen wurden regelmäßig Überblicksveranstaltungen zur Einführung in die Informatik für Historiker bzw. Kunsthistoriker angeboten. Eine praktische Umsetzung des theoretischen Wissens konnte im Rahmen dieser Veranstaltungen jedoch nicht geleistet werden. Demgegenüber werden durch die ITG, die auf langjährige und umfangreiche Erfahrungen im Bereich des digitalen Projektmanagements
                    4 verweisen kann, optimale Voraussetzungen für eine fortan praxisnahe IT-Ausbildung geschaffen.
                
            Als Mitglied im Münchner Arbeitskreis für digitale Geisteswissenschaften (dhmuc)
                    5 kooperiert die IT-Gruppe zudem fach- und institutionsübergreifend mit zahlreichen kulturellen Einrichtungen. An der Schnittstelle zur universitären Lehre ist es möglich, die „
                    IT for all“-Ausbildung geisteswissenschaftlicher Studierender auf die Anforderungen und Wünsche der potentiellen Arbeitgeberseite im (digitalen) Kultur-, Wissenschafts- und Informationssektor auszurichten.
                
         
         
            Interaktive Lehr- und Lernumgebung 
                    DHVLab
            
            Für die praktische Umsetzung kommt eine interaktive Lehr- und Lernumgebung, das 
                    Digital Humanities Virtual Laboratory – kurz 
                    DHVLab – zum Einsatz
                    6. Die im Entstehen begriffene Plattform umfasst mehrere Komponenten, die im Folgenden vorgestellt werden sollen:
                
            
               Virtuelle Rechenumgebung
               Die virtuelle Rechenumgebung ist das „Herzstück“ der Ausbildungsplattform. Auf dem virtuellen Desktop werden in Abstimmung mit dem/der Kursleiter/in Software und Tools installiert. Dadurch wird die sukzessive Installation durch die Teilnehmer/innen obsolet, wodurch Probleme aufgrund unterschiedlicher Betriebssysteme und Versionierungen vermieden werden. Bei Anmeldung im 
                        DHVLab erhält jede/r Teilnehmer/in eine eigene SQL-Datenbank. Gleichzeitig werden strukturierteDatensammlungen vorgehalten. Diese sind für die Kursteilnehmer/innen zugänglich und für eigene oder im Kurs behandelte Fragestellungen verwendbar. Im Laufe der Lehrveranstaltung können neue Forschungsfragen ausgearbeitet und ein grundsätzliches Verständnis für den sinnvollen Einsatz von Tools und Software
                        7 in den Geisteswissenschaften entwickelt werden.
                    
            
            
               Ausbildungsmaterialien
               Im vergangenen Semester wurde das System testweise in vorgenannten Einführungsveranstaltungen eingesetzt. Die bei der Evaluation gesammelten Erfahrungen fließen unmittelbar in die Erstellung bzw. Erweiterung der Ausbildungsmaterialien. Anhand praxisnaher Manuale wird IT-Grundlagenwissen, in einzelne Lehreinheiten gegliedert, anschaulich dargestellt und erklärt. Die Erstellung von Lehrvideos und Übungsaufgaben ist vorgesehen. Aus diesem Portfolio können Dozentinnen und Dozenten Module entsprechend ihrer fachwissenschaftlichen Schwerpunktsetzung und der Voraussetzungen der Teilnehmer/innen auswählen. Die Seminarplanung und -durchführung erfolgt stets in enger Abstimmung mit den Projektmitarbeitern.
            
            
               Publikationsumgebung
               Für die Vor- und Nachbereitung der einzelnen Sitzungen steht ein WordPress-Blog zur Verfügung. Dort können die Kursleiter/innen Materialen einstellen, die Studierenden ihren Erkenntnisfortschritt und Analyseergebnisse dokumentieren. Dabei erlernen sie gleichzeitig 
                        in praxi das wissenschaftliche Bloggen als innovative Form des Publizierens. Eine abschließende Publikation der studentischen Seminararbeiten ist auf dieser Plattform möglich.
                    
            
            
               Datenrepositorium
               In einem gesonderten Bereich der Datenbankumgebung werden die von den Studierenden im Rahmen einer Lehrveranstaltung erarbeiteten Datenbestände modelliert und nachhaltig abgelegt. Langfristiges Ziel ist der Aufbau eines Forschungsdatenrepositoriums. Nachfolgende Kurse mit ähnlichen Seminarthemen können auf diese Datensammlungen zugreifen, für die eigene Forschungsarbeit verwenden und dadurch sukzessive erweitern. Unterstützung erfährt die ITG durch die Universitätsbibliothek der LMU als Kooperationspartnerin auf dem Gebiet der nachhaltigen und nachnutzbaren elektronischen Publikation von Forschungsdaten.
            
            
               Entwicklung eigener Analyse- und Softwarekomponenten
               Mit dem 
                        DHVLab Analytics Center wurde eine Webanwendung entwickelt, die dazu dient, konkrete geisteswissenschaftliche Fragestellungen mithilfe quantitativer statistischer Methoden zu beantworten, sowie im Stile eines explorativen Werkzeuges neue Forschungsansätze zu eröffnen. Das 
                        Analytics Center kombiniert einführende deskriptive Analysen mit komplexeren Methoden der multivariaten Statistik. Neben diesem Analysetool entsteht derzeit eine Editionsumgebung, die speziell auf die Anforderungen von Studierenden und Promovierenden ausgerichtet wird. Diese wird erstmals im Sommersemester 2017 in einer Übung zur Edition mittelalterlicher Urkunden zum Einsatz kommen. Die Entwicklung weiterer Instrumente ist geplant.
                    
            
         
         
            Der Einsatz der Plattform in der Lehre
            Nach der technischen Realisierung der Plattform und dem Aufbau grundlegender Ausbildungsmaterialien im ersten Projekthalbjahr kommt das System im Wintersemester 2016/2017 erstmals in eigens konzipierten Lehrveranstaltungen zur Anwendung. In der Kunstgeschichte soll das 
                    Analytics Center in einem Seminar zur Beschäftigung mit informatischen und mathematischen Verfahrensweisen anregen. Parallel dazu erfolgt eine Einführung in die Statistiksoftware 
                    RStudio. Ein geschichtswissenschaftliches Hauptseminar beschreitet den Weg von der Originalquelle über die strukturierte Aufnahme und Modellierung von Forschungsdaten sowie die Einführung in die Arbeit mit relationalen Datenbanken hin zur Georeferenzierung. Die in den Seminaren gewonnenen Erfahrungen und Erkenntnisse fließen unmittelbar in die Verbesserung und Ausweitung des bestehenden Lehrmaterials ein (u.a. Erstellung von Anwendungsszenarien). Neben den genannten Kursen wird die Plattform bereits in zahlreichen Lehrveranstaltungen als technische Grundlage verwendet
                    8.
                
         
         
            Konzeption eines fachspezifischen DH-Curriculums
            Die sukzessive wachsende Plattform und die aktuell angebotenen Kurse dienen als Grundlage für eine Institutionalisierung der IT-Grundausbildung in Form eines fachspezifischen DH-Curriculums. Das Konzept für das geplante freiwillige Zusatz-Zertifikat wird derzeit in der Projektgruppe erarbeitet und baut auf Erfahrungen vergleichbarer Angebote im deutschsprachigen Raum auf
                    9. Angedacht ist eine Kombination aus Veranstaltungen, die explizit IT-Grundlagenwissen vermitteln, und praxisorientierten Kursen, in denen die erlernten IT-Inhalte auf fachwissenschaftliche Gegenstände angewendet werden. Wichtig erscheint eine ausgewogene Verschränkung von 
                    eLearning-Angeboten und Präsenzveranstaltungen, da insbesondere letzteren durch den intensiven Austausch der Studierenden mit DH-Spezialisten ein großer Beitrag zum Lernerfolg beigemessen wird
                    10.
                
         
         
            Grundlage einer nachhaltigen IT-Didaktik
            Neben der Langzeitarchivierung der Forschungsdaten wird auch die Nachhaltigkeit der informationstechnologischen Infrastruktur (Serveranlage mit redundant ausgelegten File-, Datenbank- und Web-Servern sowie ausreichenden Storages) durch die IT-Gruppe Geisteswissenschaften dauerhaft gewährleistet. Die Architektur des 
                    DHVLab ist flexibel und skalierbar gestaltet, sodass sie weiter ausgebaut werden kann (bei Bedarf ist ein Hosting der Server am Leibniz-Rechenzentrum in Garching bei München möglich). Für eine nachhaltige IT-Didaktik spielt neben der langfristig gesicherten technischen Infrastruktur insbesondere die inhaltliche Kontinuität eine entscheidende Rolle. Die im Rahmen des Projektes erarbeiteten Lehreinheiten werden dauerhaft zur Verfügung gestellt. Thematisch sind sie so zu gliedern und fachlich anzupassen, dass eine spezifische Auswahl für eine Lehrveranstaltung und damit eine Integration in ein geisteswissenschaftliches Einzelfach möglich ist. Die IT-Gruppe stellt auch nach Ende der Projektlaufzeit die unterstützende Begleitung der Lehrveranstaltungen sicher. Der Vortrag möchte zur Diskussion anregen, inwiefern sich die Anpassung der Materialien an die sich rasch wandelnden Anforderungen im Bereich der Digital Humanities möglichst effizient gestalten lässt. IT-Didaktik scheint nur dann einen Anspruch auf Nachhaltigkeit zu besitzen, wenn sie sich in einem steten Anpassungsprozess befindet.
                
            Ganz im Sinne des „Digitalen Campus Bayern“ ist das Münchener Pilotprojekt auf eine Ausweitung auf andere Studienstandorte ausgerichtet. Die Plattform wird beispielsweise ab 2017 in einem im Aufbau befindlichen Kooperationsprogramm zur DH-Ausbildung der Universitäten Erlangen, München und Regensburg zum Einsatz kommen. Alle Module des 
                    DHVLab können kollaborativ von anderen Hochschulen genutzt werden, um umfassende Sammlungen von Tutorials, Aufgaben, Softwarebeschreibungen, Anwendungsszenarien sowie Sammlungen fachwissenschaftlicher Objekt- und Metadaten aufzubauen und gemeinsam zu pflegen.
                
         
      
      
         
            Vgl. 
                            http://www.dh-curricula.org/index.php?id=1 [letzter Zugriff: 30. November 2016].
                        
             Die Projektlaufzeit beträgt zwei Jahre. Das Vorhaben ist Teil eines Förderprogramms, welches das Bayerische Wissenschaftsministerium aufgelegt hat. Vgl. 
                            https://www.km.bayern.de/pressemitteilung/9340/.html [letzter Zugriff: 30. November 2016].
                        
             Vgl. die Übersicht unter 
                            www.itg.lmu.de/projekte [letzter Zugriff: 30. November 2016].
                        
             Vgl. 
                            http://dhmuc.hypotheses.org/uber [letzter Zugriff: 30. November 2016].
                        
             Für die Dokumentation der technischen Infrastruktur vgl. 
                            http://dhvlab.gwi.uni-muenchen.de/index.php/Category:
               Architektur [letzter Zugriff: 30. November 2016].
                        
             Derzeit stehen in der virtuellen Umgebung u.a. folgende Software und Programme zur Verfügung: LibreOffice-Paket, OCRFeeder und Ocrad (Texterkennung), Python (PyCharm), RStudio (Statistik), Gephi (Visualisierung), epcEdit (XML-Editor), AntConc und TreeTagger (Korpuslinguistik).
             Vgl. die Zusammenstellung auf der Projektseite: 
                            http://dhvlab.gwi.uni-muenchen.de/index.php/Das_DHVLab_im_Einsatz [letzter Zugriff: 30. November 2016].
                        
             Vgl. insbesondere die Angebote in Köln (
                            http://www.itzertifikat.uni-koeln.de/), Passau (
                            http://www.phil.uni-passau.de/zertifikat-dh/) und Stuttgart („Das digitale Archiv“, 
                            http://www.uni-stuttgart.de/dda), letztgenanntes als Vorläufer eines DH-Masterstudienganges [letzter Zugriff: 30. November 2016].
                        
             Vor diesem Hintergrund erscheinen grundständige 
                            eLearning-Angebote wie „The Programming Historian“ (
                            http://programminghistorian.org/) für einen autodidaktischen Einstieg begrüßenswert. Die Initiatoren des DHVLab sind jedoch der Auffassung, dass eine umfassende Präsenzausbildung nicht ersetzt werden kann.
                        
         
         
            
               Bibliographie
               
                  Bartsch, Sabine / Borek, Luise / Rapp, Andrea (2016): 
                        „Aus der Mitte der Fächer, in die Mitte der Fächer: Studiengänge und Curricula – Digital Humanities in der universitären Lehre“,
                        in: 
                        Bibliothek – Forschung und Praxis 40 (2): 172–178 10.1515/bfp-2016-0030.
                    
               
                  DARIAH-EU: 
                        Digital Humanities Registry – Courses
                  https://dh-registry.de.dariah.eu/ [letzter Zugriff 30. November 2016].
                    
               
                  DHI Paris (Teamaccount) (2013): 
                        „Wissenschaftlicher Nachwuchs in den Digital Humanities: Ein Manifest“,
                        in: 
                        Digital Humanities am DHIP, 23. August 2013 
                        http://dhdhi.hypotheses.org/1995 [letzter Zugriff 30. November 2016].
                    
               
                  Ehrlicher, Hanno (2016): 
                        „Fingerübungen in Digitalien. Erfahrungsbericht eines teilnehmenden Beobachters der Digital Humanities aus Anlass eines Lehrexperiments“, 
                        in: 
                        Romanische Studien 4: 623–636 
                        http://www.romanischestudien.de/index.php/rst/article/view/88 [letzter Zugriff 30. November 2016].
                    
               
                  Koller, Guido (2016): 
                        Geschichte digital: Historische Welten neu vermessen. 
                        Stuttgart: Kohlhammer.
                    
               
                  Lücke, Stephan / Riepl, Christian (2016): 
                        „Auf dem Weg zu einem Curriculum in den Digital Humanities“,
                        in: 
                        Akademie Aktuell 57 (1): 74–77 
                        http://badw.de/fileadmin/pub/akademieAktuell/2016/56/0116_17_Riepl_V04.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Rehbein, Malte (2016): 
                        Geschichtsforschung im digitalen Raum. Über die Notwendigkeit der Digital Humanities als historische Grundwissenschaft. (Preprint) 
                        http://www.phil.uni-passau.de/fileadmin/dokumente/lehrstuehle/rehbein/Dokumente/GeschichtsforschungImDigitalenRaum_preprint.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Sahle, Patrick (2013): 
                        DH studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities (= DARIAH-DE Working Papers 1). 
                        Göttingen: GOEDOC 
                        http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2013-1.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Sahle, Patrick (2016): 
                        „Digital Humanities als Beruf. Wie wird man ein „Digital Humanist“, und was macht man dann eigentlich?“, 
                        in: 
                        Akademie Aktuell 57 (1): 78–83 
                        http://badw.de/fileadmin/pub/akademieAktuell/2016/56/0116_18_Sahle_V04.pdf [letzter Zugriff 30. November 2016].
                    
               
                  Spiro, Lisa (2012): 
                        „Openingup Digital Humanities Education“, 
                        in: Hirsch, Brett D. (ed.):
                        Digital Humanities Pedagogy: Practices, Principlesand Politics 331–363 
                        http://www.openbookpublishers.com/product/161/ [letzter Zugriff 30. November 2016].
                    
               
                  Thaller, Manfred (2015): 
                        „Panel: Digital Humanities als Beruf – Fortschritte auf dem Weg zu einem Curriculum“, 
                        in: 
                        Digital Humanities als Beruf: Fortschritte auf dem Weg zu einem Curriculum, vorgelegt auf der Jahrestagung 2015 3–5 
                        https://www.digitalhumanities.tu-darmstadt.de/fileadmin/dhdarmstadt/materials/Digital_Humanities_als_Beruf_-_Stand_2015.pdf [letzter Zugriff 30. November 2016].
                    
            
         
      
   



      
         
            Einführung und Forschungslage
            Wenn Peter Paul Rubens als »Tarantino des Barock« beschrieben wird (im 
                    Tagesspiegel, 2014) oder Alice Schwarzer als der »Erich Honecker des Feminismus« (in 
                    Cicero, 2014), dann handelt es sich um eine Vossianische Antonomasie. Diese Trope ist nach dem niederländischen Humanisten und Rhetoriklehrer Vossius benannt (und wird im Folgenden als ›Vossanto‹ abgekürzt, in Anlehnung an den Vorschlag von Fischer/Wälzholz 2014). Generell spricht man von Antonomasie, wenn eine bestimmte Eigenschaft einer Person für diese selbst steht (z. B. »der Leimener« für Boris Becker). Beim Spezialfall der Vossanto wird einer Person über die Nennung einer anderen (bekannteren, populäreren, berüchtigteren) Person als Referenzgröße eine bestimmte Eigenschaft zugeschrieben. Dabei sorgt ein »untypologisches, aktualisierendes Signal« (Lausberg 1960) für den Bedeutungstransfer (in den oben genannten Beispielen wären dies der Barock und der Feminismus). Anders ausgedrückt: Die Vossanto stellt über einen ›modifier‹ einen Zusammenhang zwischen ›source‹ und ›target‹ her (Bergien 2013). Entitäten können sowohl als ›source‹ als auch als ›target‹ auftreten, wie ebd. am Beispiel Obama demonstriert: bis 2011 trat er in Vossantos vor allem als ›target‹ auf, danach diente er immer mehr als ›source‹. Die ›source‹-Referenz wird im Fachdiskurs im Anschluss an Lakoff 1987 auch als ›paragon‹ bezeichnet (»a specific example that comes close to embodying the qualities of the ideal«, ebd.).
                
            Der Begriff »Vossianische Antonomasie« wird international kaum verwendet, stattdessen wird etwa zwischen »Antonomasia1« und »Antonomasia2« unterschieden: »metonymic« vs. »metaphorical antonomasia« (Holmqvist/Płuciennik 2010). Innerhalb dieses Klassifikationsschemas wäre unsere Vossanto ein Spezialfall von »Antonomasia2«, nämlich wenn es um »comparisons with paragons from other spheres of culture« geht: »Lyotard is a pope of postmodernism, Bush is no Demosthenes; and we can buy the Cadillac of vacuum cleaners.« (ebd.)
            Dieses Stilmittel, dessen reger Gebrauch seit der Antike belegt ist, ist heute medial ubiquitär anzutreffen. Oft findet es sich schon in Überschriften, da es zugleich informativ und rätselhaft sein kann und zudem oft unterhaltsame Qualitäten bietet. Eine eigene größere Sammlung an Musterexemplaren (
                    ) gab den Ausschlag, dieses Phänomen systematisch zu erforschen, mit historischer Perspektive und auf Grundlage größerer englischer und deutscher Zeitungskorpora. Ziel dieser Arbeit ist eine erste methodisch-explorative Analyse des Phänomens Vossanto in der Tageszeitung 
                    New York Times (1987–2007) und der Wochenzeitung 
                    Die Zeit (1995–2011). Die Korpora wurden aufgrund ihrer Verfügbarkeit, Bedeutung und ihres Umfangs gewählt. Die Extraktion der Vossantos erfolgte jeweils korpusspezifisch, um den verschiedenen Formaten und Sprachen Rechnung zu tragen.
                
         
         
            Englischsprachiges Korpus
            Aus den XML-Daten des 
                    New York Times-Korpus (Sandhaus 2008) wurde für jeden der 1.854.726 Artikel der Volltext extrahiert. Anschließend wurde der Text mit Hilfe des NLTK (Bird/Loper/Klein 2009) in Sätze zerlegt und die Wörter jedes Satzes mit dem Part-of-Speech-Tagger des NLTK annotiert. Zusätzlich wurden Named Entities (Personen, Orte, Organisationen) mit dem NLTK-eigenen Named-Entity-Extraktor annotiert. Die so annotierten Sätze wurden mit einer Liste von Vossanto-typischen Mustern (in Form von regulären Ausdrücken) abgeglichen. Eine vereinfachte Darstellung eines solchen Musters ist beispielsweise:
                
            \((PERSON|ORGANIZATION|GPE) *\) (is|has) (often|sometimes)? (been)? (called)? the \(PERSON|ORGANIZATION|GPE) *\) (of|among|from) \((PERSON|ORGANIZATION|GPE) *\)
            Die zu findenden drei Elemente einer Vossanto sind darin durch * gekennzeichnet. Passte ein Satz auf eines der Muster, so wurden diese drei Teile extrahiert und tabellarisch ausgegeben. Anschließend wurden die extrahierten 10.744 Kandidaten manuell überprüft. Nicht-Vossantos und Vossantos mit Städten und Firmen wurden entfernt (Treffer der Art »Algarve, the Riviera of Portugal« oder »Pepsi is the Nike of soft drinks« sind eine eigene Untersuchung wert) und der Fokus auf Vossantos gelegt, in denen Individuen (Personen, Tiere, fiktive Figuren) als ›source‹ oder ›target‹ dienen. 246 Vossantos blieben dabei übrig (Übersicht in unserem Arbeitsrepo, siehe Bibliografie), die sich wie folgt über das Korpus verteilen:
            
               
                  
                  vossantos_nyt.png
               
            
            Am häufigsten als ›source‹ verwendet wurden folgende Namen:
            
               
                  Anzahl
                  source
               
               
                  6
                  Michael Jordan
               
               
                  5
                  Michelangelo
               
               
                  4
                  Babe Ruth
               
               
                  3
                  Zelig
               
               
                  3
                  Rodney Dangerfield
               
               
                  3
                  Neil Young
               
               
                  3
                  Elvis
               
               
                  3
                  Don Quixote
               
            
            Als Beispiel für Treffer seien diejenigen für Michael Jordan genannt:
            
               »Romario is the 
                        Michael Jordan of soccer and Bebeto is the Magic Johnson of soccer« (1994)
                    
               »Bonfire, the 
                        Michael Jordan of dressage horses« (1998)
                    
               »Brian Foster, the 
                        Michael Jordan of BMX racing« (1998)
                    
               »The stunt biker Dave Mirra, the 
                        Michael Jordan of the dirt set« (2000)
                    
               »Cynthia Cooper is the 
                        Michael Jordan, the Larry Bird, the Magic Johnson of this league« (2000)
                    
               »McNabb has been called the 
                        Michael Jordan of the National Football League« (2001)
                    
            
            Trotz der zeitlichen Einschränkung des Korpus lassen sich bereits einige vielversprechende Beobachtungen anstellen und Thesen bilden: 1. Produktive Referenzgrößen einer Vossanto sind sowohl reale als auch fiktionale Figuren (Bsp. für letztere aus der obigen Liste: Woody Allens »Zelig«, Cervantes’ »Don Quixote«). 2. Öffentliche Personen oder bekannte fiktionale Charaktere haben bestimmte Eigenschaften, die sie für die Verwendung als Referenzgröße einer Vossanto prädestinieren oder nicht (es bleibt etwa zu erforschen, warum gerade Michael Jordan und Michelangelo sich so gut eignen und nicht andere Sportler bzw. Künstler). 3. Es gibt historisch stabile Referenzgrößen, deren Bekanntheit vorausgesetzt werden kann (z. B. Michelangelo), und es gibt ephemere Referenzgrößen, die ab irgendeinem Zeitpunkt nicht mehr als Bezugspunkt taugen (für das benutzte zeitgenössische Korpus eher noch nicht relevant).
         
         
            Deutschsprachiges Korpus
            Das deutsche Datenset besteht aus einer Sammlung des Archivs der Wochenzeitung 
                    Die Zeit und enthält die Artikel aus den Jahren 1995 bis 2011. Insgesamt umfasst das Korpus 126.702 Dokumente.
                
            Zunächst wurden die Volltexte (inklusive Überschriften) aller Dokumente extrahiert. Diese wurden dann mit Hilfe des Part-of-Speech-Taggers und Named-Entity-Recognition-Tools des Stanford CoreNLP Package verarbeitet. Für die Analyse deutschsprachiger Texte enthält Stanford CoreNLP speziell für das Deutsche trainierte Modelle (Faruqui und Pado 2010). Somit können alle Texte auf drei Ebenen untersucht werden: auf der Wortebene, der Part-of-Speech-Ebene sowie der Named-Entity-Ebene. Mithilfe von regulären Ausdrücken, die auf den verschiedenen Ebenen angewandt werden können, wurde dann nach Vossanto-Mustern gesucht. Im Gegensatz zur Verarbeitung des englischsprachigen Korpus wurde jedoch noch nicht versucht, auch das ›target‹ einer Vossanto zu extrahieren. Stattdessen wurden Muster entworfen, die das ›source‹-Objekt sowie das »aktualisierende Signal« matchen. Ausschlaggebend für diese Herangehensweise waren die in einem Testdurchlauf beobachtete hohe Anzahl an Vossantos ohne unmittelbaren Verweis auf das ›target‹ sowie eine große Vielfalt an möglichen Formulierungen, die auf die Relation zum ›target‹ hinweisen können. Mithilfe relativ strikter Regeln konnte die Anzahl an falschen Extraktionen im Rahmen gehalten werden. Ein vereinfachtes Beispiel für eine Extraktionsregel lautet etwa: »eine Art PERSON (der|des) (ADJECTIVE)? NOUN«.
            Die Produktivität der beiden häufigsten Referenznamen des NYT-Korpus bestätigt sich im verwendeten deutschen Korpus, etwa wenn vom »Michael Jordan der analytischen Philosophie« die Rede ist (
                    Die Zeit 44/1999) oder vom »bulgarischen Michelangelo« (
                    Die Zeit 14/2001). Ansonsten scheint es sprachen- bzw. kulturspezifische Präferenzen zu geben. Die häufigsten ›sources‹ sind:
                
            
               
                  Anzahl
                  source
               
               
                  9
                  Robin Hood
               
               
                  6
                  Bill Gates
               
               
                  4
                  Franz Beckenbauer
               
               
                  3
                  Daniel Düsentrieb
               
               
                  3
                  Heinz Rühmann
               
               
                  3
                  James Dean
               
               
                  3
                  Jesus Christus
               
               
                  3
                  Norbert Blüm
               
               
                  3
                  Willy Brandt
               
            
            Ähnlich wie im NYT-Korpus ist erkennbar, wie stark typisierend mythische bzw. fiktive Figuren sind (Robin Hood, Daniel Düsentrieb). Daneben zeigt sich, dass »Bill Gates«, der im NYT-Korpus nur zweimal als ›source‹ einer Vossanto vorkommt, im 
                    Zeit-Korpus sechs Mal als Referenz vertreten ist:
                
            
               »eine Art Bill Gates des Stolperns« (1998)
               »Der Bill Gates von Aurich« (2001)
               »der Bill Gates von Ostfriesland« (2001)
               »der Bill Gates von Aurich« (2002)
               »der britische Bill Gates« (2008)
               »der Bill Gates von Estland« (2010)
            
            Die wiederholte Verwendung des »Bill Gates von Aurich« zeigt, wie stark ein ›target‹ mit einer ›source‹ verwachsen kann. (Paradebeispiel hierfür ist im Übrigen Vittorio Hösle, »der Boris Becker der Philosophie«, eine Bezeichnung, die es bis in den Wikipedia-Artikel zu Hösle geschafft hat.) Am Beispiel Bill Gates’ lässt sich wie zuvor am Beispiel Obama demonstrieren, dass ein Name sowohl als ›target‹ als auch als ›source‹ vorkommen kann. Bevor Bill Gates selbst als Referenz verwendet wird, wird er in einem Artikel von 1995 noch durch eine andere Person beschrieben: »Bill Gates ist der Henry Ford des Computerzeitalters«.
            Insgesamt wurden aus 1.456 Vossanto-Kandidaten 225 manuell als Vossantos markiert, die sich wie folgt über die im Korpus vorhandenen Jahre verteilen:
            
               
                  
                  vossantos_zeit.png
               
            
            Zu den fälschlich extrahierten Named Entities gehören »der Berliner Klaus Wowereit«, »der deutsche Michel« oder »der Anton aus Tirol«, stehende Wendungen, die grammatisch unseren definierten Vossanto-Mustern entsprechen.
         
         
            Erkenntnisse und Ausblick
            Die Vossanto ist als Stilmittel nur scheinbar einfach strukturiert, das Erstellen von Extraktionsregeln daher alles andere als trivial. Die vorliegenden Skripte weisen bekannte Lücken auf, die Qualität hängt v. a. von der Verlässlichkeit der benutzten NER-Tools und der Präzision der definierten Muster ab. Fehlende Goldannotationen für dieses Phänomen erschweren zudem eine Evaluierung. Die vorliegende Arbeit hat daher explorativen Charakter, die Optimierung von Precision und Recall lag noch nicht in deren Fokus, ist aber das nächste Ziel dieses Projekts.
            Trotz der genannten Einschränkungen konnten durch diesen korpusbasierten Ansatz neue Erkenntnisse zur Vielgestaltigkeit des Phänomens ›Vossianische Antonomasie‹ gewonnen werden. So lassen sich zahlreiche Spezialfälle unterscheiden und systematisch untersuchen (vgl. auch Fischer/Wälzholz 2014), beispielhaft genannt seien:
            
               Tiere als ›target‹ (»
                        Sea Hero is the Bobo Holloman of racing«, NYT, 1993; »
                        Bonfire, the Michael Jordan of dressage horses«, NYT, 1998),
                    
               Feminisierungen (Adele Schopenhauer, »eine Art 
                        Donna Quichotta des Weimarer Musenvereins«, 
                        Die Zeit 18/2002; »Tracey [Emin], die 
                        Donna Giovanna der britischen Gegenwartskunst«, 
                        Die Zeit 9/2006; »Kati Witt ist jetzt eine 
                        Franziska Beckenbauer der Münchner Olympiabewerbung.«, 
                        Die Zeit 39/2010),
                    
               nicht individualisierbare ›sources‹: »the [God, King, Queen, Satan, Emperor, Oracle, Shogun, Czar, Sultan, Buddha] of«,
               mythologische und fiktive Figuren als ›sources‹: »the [Santa Claus, Midas, Godzilla, Pied Piper, Energizer Bunny, Jupiter, Icarus] of«,
               Personifizierungen, also der Einsatz individueller Personen/Figuren als ›source‹ für Firmen, Vereine, Bands oder Orte als ›target‹ (»
                        Sturm, Ruger is the 
                        Benedict Arnold of the gun industry«, NYT, 1989; »
                        Aerosmith, the 
                        Dorian Gray of rock bands«, NYT, 1993; »the 
                        Hudson has been the 
                        John Barrymore of rivers, noble in profile but a sorry wreck«, NYT, 1996; »the 
                        National Collegiate Athletic Association, the 
                        Kenneth Starr of sports«, NYT, 1998).
                    
            
            Zu letzteren Beispielen gehört nun endlich auch der titelgebende »Helmut Kohl unter den Brotaufstrichen« (
                    der Freitag 35/2011).
                
            Auch zur Distribution der Vossantos innerhalb der beiden Zeitungskorpora ließen sich belastbare Ergebnisse gewinnen. Demnach sind Vossantos besonders im Kultur- und Sport-Ressort beliebt (Vorkommen in der Sektion »Arts« der NYT: 78; in der Sektion »Sports«: 57; auf dem nächsten Rang mit großem Abstand »New York and Region«: 28 – im »Feuilleton + Literatur«-Ressort der Zeit: 76, »Politik«: 54, nächstrangig ist weit entfernt »Wirtschaft« mit 23 Vorkommen; »Sport« hat hier keine Treffer, denn die gedruckte 
                    Zeit hat kein dediziertes Sport-Ressort).
                
         
      
      
         
            
               Bibliographie
               
                  Bergien, Angelika (2013):
                        „Names as frames in current-day media discourse“, 
                        in: Felecan, Oliviu (ed.): 
                        Name and Naming. Proceedings of the second international conference on onomastics. Cluj-Napoca: Editura Mega 2013: 19–27.
                    
               
                  Bird, Steven / Loper, Edward / Klein, Ewan (2009): 
                        Natural Language Processing with Python. 
                        O’Reilly Media Inc.
                    
               
                  Faruqui, Manaal / Pado, Sebastian (2010):
                        „Training and Evaluating a German Named Entity Recognizer with Semantic Generalization“, 
                        in: 
                        Proceedings of Konvens 2010.
                    
               
                  Fischer, Frank / Wälzholz, Joseph (2014): 
                        „Jeder kann Napoleon sein: Vossianische Antonomasie: Eine Stilkunde“, 
                        in:
                        Frankfurter Allgemeine Sonntagszeitung 51 (21. Dezember 2014): 34 
                        .
                    
               
                  Holmqvist Kenneth / Płuciennik Jarosław (2010): 
                        „Princess antonomasia and the truth: Two types of metonymic relations“,
                        in: Burkhardt, Armin / Nerlich, Brigitte (eds.): 
                        Tropical Truth(s): The Epistemology of Metaphor and Other Tropes. 
                        Berlin/New York: De Gruyter 373–381 
                        10.1515/9783110230215.
                    
               
                  Lakoff, George (1987): 
                        Women, Fire, and Dangerous Things: What Categories Reveal about the Mind. 
                        Chicago: The University of Chicago Press.
                    
               
                  Lausberg, Heinrich (1960): 
                        Handbuch der literarischen Rhetorik. Eine Grundlegung der Literaturwissenschaft 2. 
                        München: Hueber.
                    
               
                  Sandhaus, Evan (2008): 
                        The New York Times Annotated Corpus LDC2008T19. DVD. 
                        Philadelphia: Linguistic Data Consortium.
                    
               
                  Arbeitsrepositorium: 
                        
               
               
                  Folien zum Vortrag: 
                        
               
            
         
      
   



      
         
            Forschungsstand
            Die Anwendung von Methoden der Netzwerkanalyse auf literarische Texte hat sich in den letzten Jahren zu einem eigenständigen Forschungsfeld der 
                    Digital Literary Studies entwickelt. Im Vordergrund stehen dabei häufig computerlinguistische Fragen, insbesondere solche nach der automatisierten Extraktion von Netzwerkdaten (z.B. qua 
                    named entity recognition, 
                    co-reference resolution) und deren Evaluation (u. a. Elson et al. 2010; Park et al. 2013; Agrarwal et al. 2013; Rochat 2014; Fischer et al. 2015; Waumans et al. 2015; Jannidis et al. 2016). 
                
            Darüber hinaus wird ausgelotet, inwiefern sich mittels visueller und/oder statistischer Auswertung der Netzwerkdaten genuin literaturwissenschaftliche Erkenntnisse gewinnen bzw. neue Wege der literaturwissenschaftlichen Analyse entwickeln lassen: Neben Ansätzen zur quantitativen Beschreibung und Hierarchisierung des Figurenpersonals (Jannidis et al. 2016) werden hier, im Rahmen korpusbasierter Analysen, Optionen der literaturhistorischen Periodisierung auf Basis von quantitativen Strukturdaten diskutiert (Trilcke et al. 2015) sowie Typen der ästhetischen Modellierung sozialer Formationen in und durch literarische Texte differenziert (Stiller et al. 2003; Stiller & Hudson 2005; Trilcke et al. 2016).
         
         
            Forschungsdesiderat: Plotanalyse
            Nahezu keine Rolle spielte bisher jedoch ein durchaus hehres Erkenntnisversprechen, das – bereits in der prä-automatisierten Zeit formuliert (de Nooy 2006) – auch den Fluchtpunkt des einschlägigen ›Pamphlets‹ von Franco Moretti steht: dass nämlich die Netzwerkanalyse als ein Instrumentarium der quantitativen »plot analysis« (Moretti 2011) fungieren könne. 
            Tatsächlich lässt sich dieses Erkenntnisversprechen mit den derzeit verfolgten Ansätzen im Bereich der literaturwissenschaftlichen Netzwerkanalyse kaum aufgreifen, geschweige denn einlösen (so auch Prado et al. 2016). Denn die sequentielle Dimension literarischer Texte, mithin ihre Temporalität, bleibt hier in der Regel ausgeblendet: Erfasst, visualisiert und analysiert werden statische Netzwerke. Plot ist jedoch wesentlich ein Konzept, das die Temporalität narrativer (wie auch dramatischer) 
                    1 Texte theoretisch fassen soll: »the repeated attempts to redefine parameters of plot reflect both the centrality and the complexity of the temporal dimension of narrative« (Dannenberg 2005: 435). Plot lässt sich begreifen als Konzept zur Beschreibung der »progressive structuration« (Kukkonen 2013, §4) literarischer Texte. 
                
            Versuche, die Netzwerkanalyse in Richtung einer quantitativen Plotanalyse weiterzuentwickeln, stehen also zunächst vor der Aufgabe, bei ihrer Modellierung des Untersuchungsgegenstandes die Zeitdimension zu berücksichtigen. Der Text ist entsprechend nicht lediglich als ein statisches Netzwerk zu modellieren, sondern als eine sich über die Zeit verändernde Folge von Netzwerkzuständen. Erst anhand dieser Netzwerkdynamiken lassen sich die Erkenntnispotenziale, die netzwerkanalytische Zugänge für die quantitative Plotanalyse bergen, überhaupt diskutieren. 
         
         
            Forschungsvorhaben 
            Der projektierte Vortrag wird – in Anschluss an Prado et al. 2016 – aus theoretischer und methodischer Perspektive sowie anhand exemplarischer Fallstudien eine Erweiterung der bisherigen, auf die Analyse 
                    statischer Strukturen fokussierten Forschung zu literarischen Netwerken um die Analyse 
                    progressiver  Strukturierungen vorschlagen. Übergreifendes Ziel ist es, zu prüfen, ob (und mit welchen Einschränkung) sich auf diesem Wege ein Beitrag zur Operationalisierung des literaturwissenschaftlichen Plot-Konzepts erarbeiten lässt. Dabei soll es nicht darum gehen, das semantische reiche und vielseitige Plot-Konzept der ›traditionellen‹ Literaturwissenschaft durch ein quantitatives und insofern notgedrungen reduktionistisches Konzept zu ersetzen. Vielmehr soll zunächst der wesentlich bescheidenere Nachweis erbracht werden, dass sich bestimmte Aspekte dessen, was gemeinhin im Rahmen des Plot-Konzepts diskutiert wird, durchaus mittels der computerbasierten Analyse von Netzwerkdynamik beobachten lassen, etwa ereignishafte Konfliktverläufe (so schon Moretti 2011), Formen der sozialen Integration und Desintegration von Figuren oder basale Techniken der Handlungsführung, z.B. die Komposition von Haupt- und Nebenhandlung(en). 
                
            Entsprechend der zweigleisigen Auswertungsroutinen, die auf netzwerkanalytische Daten angewendet werden, wird der Vortrag zwei Szenarien der netzwerkbasierten Analyse der progressiven Strukturierung literarischer Texte diskutieren: zum einen (3.1) sind Möglichkeiten und Erkenntnispotenziale der 
                    Visualisierung dynamischer Netzwerke, zum anderen (3.2) Möglichkeiten und Erkenntnispotenziale der Berechnung
                     netzwerkanalytischer Maße für dynamische Netzwerke auszuloten.
                
            
               Visualisierung von Netzwerkgraphen
               Während die Visualisierung dynamischer Netzwerke in anderen Domänen bereits seit längerem gang und gäbe ist (vgl. exemplarisch Pohl et al. 2008; Frederico et al. 2011), wurde erst vor Kurzem der Versuch unternommen, entsprechende Visualisierungsverfahren auch auf literarische Netzwerke anzuwenden (Xanthos et al. 2016). Während Xanthos et al.  u.a. auf didaktische Anwendungsszenarien hinweisen, wird ein literaturwissenschaftliches Erkenntnispotenzial lediglich angedeutet; eine Diskussion dessen, was durch eine solche Visualisierung nicht nur 
                        sichtbar, sondern auch 
                        erkennbar wird, bleibt aus. 
                    
               Hingegen zeigen erste, im Vortrag zu vertiefende Zwischenergebnisse unserer Analysen, dass die dynamische Visualisierung insbesondere dann erkenntnisrelevant wird, wenn es darum geht, multiplexe Netzwerke zu modellieren, d. h. Netzwerke, die unterschiedliche Interaktionstypen zugleich erfassen. So zeigt eine statische Visualisierung von Lessings bürgerlichem Trauerspiel 
                        Emilia Galotti die Familie Galotti als eine geschlossene Triade (siehe Abb. 1): Die Kanten symbolisieren hier szenische Kopräsenzen (Interaktionstyp 1), wobei jene Kanten, die 
                        zugleich Verwandtschaftsverhältnisse darstellen, rot erscheinen (Interaktionstyp 2). 
                    
               
                  
               
               Abb. 1: Statisches Netzwerk zu Lessing: 
                        Emilia Galotti (rote Knoten: Familienmitglieder; rote Kanten: Familienmitglieder sind szenisch kopräsent)
                    
               Zerlegt man das statische Dramennetzwerk (Abb. 1) nun nach Akten und dynamisiert es damit, so zeigt sich, dass die Familie Galotti zu keinem Zeitpunkt des Dramas gemeinsam auf der Bühne steht (vgl. Abb. 2). 
               
                  
               
               Abb. 2: Dynamisches Netzwerk zu Lessings 
                        Emilia Galotti, zerlegt nach Akten
                    
               Anschaulich und 
                        erkennbar wird auf diese Weise eine Position der traditionellen Forschung, nach der Lessing in 
                        Emilia Galotti nicht nur die äußere Bedrohung der ›bürgerlichen‹ Kleinfamilie, sondern auch deren innere Problematik inszeniert hat (siehe z. B. Alt 1994: 268). Die Analyse der dynamischen Strukturierung zeigt hier die soziale Desintegration der familäre Triade, die als formal beschreibbarer Teilaspekt des zentralen dramatischen Konflikts verstanden werden kann. 
                    
               Dass dynamische Visualisierungen in diesem Sinne aus literaturwissenschaftlicher Sicht v.a. für die Analyse multiplexer Netzwerke produktiv gemacht werden können, werden wir im Vortrag anhand weiterer Beispiele aus dem dlina-Korpus (philologisch kuratierte Netzwerkdaten zu 465 deutschsprachige Dramen aus der Zeit 1730–1930, siehe 
                        https://dlina.github.io/Introducing-DLINA-Corpus-15-07-Codename-Sydney/) zeigen. Darüber hinaus werden wir zum Zweck eines intergenerischen Vergleichs exemplarisch dynamische Visualisierung von Romannetzwerken diskutieren. Zu reflektieren sind hier insbesondere Fragen der Sequenzierung: Während Dramen mit ihrer Einteilung in Akte und Szenen eine naheliegende Segmentierung vorgeben, liefert die romantypische Einteilung in Kapitel keine vergleichbar überzeugenden Ergebnisse. 
                    
            
            
               Berechnung netzwerkanalytische Maße 
               Mehr noch als die Visualisierung statischer Netzwerke stellt diejenige dynamischer im Grunde keine Option eines korpusbasierten 
                        distant reading dar. Sie ermöglicht zwar die anschauliche Modellierung einzelner Netzwerke, kann aber nur begrenzt Erkenntnisse über eine große Anzahl von Netzwerken liefern: Methoden, mit denen sich die auf algorithmischen Layouts basierenden Netzwerkgraphen kontrolliert miteinander vergleichen lassen, fehlen weitgehend; zudem kostet die Rezeption von dynamischen Visualisierungen – etwa der von Xanthos et al. 2016 präsentierten Prototypen – schlicht Zeit, wir haben es hier also eher mit 
                        fast reading, denn mit 
                        distant reading zu tun. Die Berechnung netzwerkanalytischer Maße und deren statistische Weiterverarbeitung bietet hingegen Möglichkeiten, aus einer dezidierten 
                        distant reading-Perspektive sowohl allgemeine Charakteristika der Netzwerke eines Korpus zu beschreiben als auch, vergleichend, spezifische formale Typen von Netzwerken innerhalb des Korpus zu identifizieren (entsprechend unserer Überlegungen zum 
                        Small World-Phänomen in statischen Netzwerken, siehe Trilcke et al. 2016). 
                    
               Von Carley (2003: 135–136) wurden dabei mehrere rudimentäre globale Maße (i. e. 
                        size, 
                        density, 
                        homogeneity in the distribution of ties, 
                        rate of changes in nodes, 
                        rate of changes in ties) für die Analyse dynamischer Netzwerke vorgeschlagen. Darüber hinaus haben Prado et al. 2016 für die Anwendung von akteursorientierten Maßen, v.a. Zentralitätsindices, bei der Rekonstruktion von Plot
                        -Verläufen plädiert. Im Vortrag werden wir einzelne dieser Maße – u. a. 
                        size pro Akte und Szenen; 
                        density pro Akte und Szene; die 
                        change-
                        rates; sowie einfache Zentralitätsmaße – für das dlina-Korpus berechnen; die dafür nötigen Daten liegen bereits, philologisch kuratiert, in den dlina-Zwischenformat-Dateien vor (zum Zwischenformat: 
                        https://dlina.github.io/Introducing-Our-Zwischenformat/ – die Daten sind offen, siehe unser Github-Repositorium: 
                        https://github.com/dlina); eine entsprechende Erweiterung des in Python geschriebenen Auswertungstools 
                        dramavis (Kittel / Fischer 2016) wird derzeit entwickelt. Die erhobenen Daten werden wir schließlich mit Rekurs auf ausgewählte literaturwissenschaftliche Konzepte für die Beschreibung spezifischer Plot-Phänomene diskutieren, insbesondere in Hinblick auf Expositionstypen (Pfister 1977: 124–136), auf die ›klassische‹ Aktstruktur der Tragödie sowie auf das Kompositionsprinzip von Haupt- und Nebenhandlung (Pfister 1977: 286–289).
                    
            
         
         
            Resümee
            Der Vortrag liefert einen Beitrag zur Methodenentwicklung und -reflektion im Bereich der 
                    Digital Literary Studies. Auf literaturtheoretisch-methodologischer Ebene diskutiert er Möglichkeiten einer netzwerkanalytischen Operationalisierung des literaturwissenschaftlichen Plot
                    -Konzepts, wobei der literarische Text zu diesem Zweck nicht, wie bisher die Regel, als statische Struktur, sondern als ›progressive Strukturierung‹ modelliert wird. Als empirische Grundlage der Methodendiskussion fungieren Analysen von Dramen und Romanen, in denen exemplarisch die Potenziale und die Grenzen des Ansatzes verdeutlich werden. 
                
         
      
      
         
             Unter systematischen Gesichtspunkten können die Unterschiede zwischen narrativen und dramatischen Texten in Hinblick auf das Plot-Konzept zunächst vernachlässigt werden (vgl. Korthals 2003); entsprechend wurden sowohl ›epische‹ als auch ›dramatische‹ Texte bis ins 19. Jahrhundert hinein verschiedentlich unter dem Oberbegriff ›pragmatische Gattung‹ vereint. 
         
         
            
               Bibliographie
               
                  Agarwal, Apoorv / Corvalan, Augusto / Jensen, Jacob / Rambow, Owen (2012): 
                        „Social Network Analysis of Alice in Wonderland“, 
                        in: 
                        Proceedings of the Workshop on Computational Linguistics for Literature. 
                        Montréal 88–96 
                        http://www.aclweb.org/anthology/W12-2513 [letzter Zugriff 25. August 2016].
                    
               
                  Alt, Peter-André (1994): 
                        Die Tragödie der Aufklärung. Eine Einführung. 
                        Tübingen / Basel: Francke.
                    
               
                  Carley, Kathleen M. (2003): 
                        „Dynamic Network Analysis“, 
                        in: Breiger, Ronald / Carley, Kathleen M. / Pattison, Philipp (eds.): 
                        Dynamic Social Network Modeling and Analysis. Workshop Summary and Papers. 
                        Washington D.C.: 133–145 
                        http://www.nap.edu/read/10735/chapter/9.
                    
               
                  Dannenberg, Hilary (2005): 
                        „Plot“, 
                        in: Herman, David / Jahn, Manfred / Ryan, Marie-Laure (eds.): 
                        The Routledge Encyclopedia of Narrative Theory. 
                        London: Routledge 435–439.
                    
               
                  Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario / Trilcke, Peer (2015): 
                        „Digital Network Analysis of Dramatic Texts“, 
                        in: 
                        DH2015: Global Digital Humanities
                  http://dh2015.org/abstracts/xml/FISCHER_Frank_Digital_Network_Analysis_of_ Dramati/FISCHER_Frank_Digital_Network_ Analysis_of_Dramatic_Text.html [letzter Zugriff 25. August 2016].
                    
               
                  de Nooy, Wouter (2006): 
                        „Stories, Scripts, Roles, and Networks“, 
                        in: 
                        Structure and Dynamics 1.2 
                        http://escholarship.org/uc/item/8508h946#page-1 [letzter Zugriff 25. August 2016].
                    
               
                  Elson, David K. / Dames, Nicholas / McKeown, Kathleen R. (2010): 
                        „Extracting Social Networks from Literary Fiction“, 
                        in: 
                        Proceedings of ACL-2010. 
                        Uppsala: 138–147 
                        http://dl.acm.org/ft_gateway.cfm?id=1858696&type=pdf&CFID=659731302 &CFTOKEN=83466756 [letzter Zugriff 25. August 2016].
                    
               
                  Federico, Paolo / Aigner, Wolfgang / Miksch, Silvia / Windhager, Florian / Zenk, Lukas (2011): 
                        „A Visual Analytics Approach to Dynamic Social Networks“, 
                        in: 
                        Proceedings of the 11th International Conference on Knowledge Management and Knowledge Technologies (i-KNOW).
                        Graz 
                        http://publik.tuwien.ac.at/files/PubDat_198995.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Jannidis, Fotis / Reger, Isabella / Krug, Markus / Weimer, Lukas / Macharowsky, Luisa / Puppe, Frank (2016): 
                        „Comparison of Methods for the Identification of Main Characters in German Novels“, 
                        in: 
                        DH2016: Conference Abstracts 578–582 
                        http://dh2016.adho.org/abstracts/297 [letzter Zugriff 25. August 2016].
                    
               
                  Kittel, Christopher / Fischer, Frank (2016): 
                        dramavis (v0.2.1). GitHub 
                        https://github.com/lehkost/dramavis [letzter Zugriff 25. August 2016].
                    
               
                  Korthals, Holger (2003): 
                        Zwischen Drama und Erzählung. Ein Beitrag zur Theorie geschehensdarstellender Literatur. 
                        Berlin: Erich Schmidt
                    
               
                  Kukkonen, Karin (2013): 
                        „Plot“, 
                        in: Hühn, Peter et al. (eds.): 
                        The Living Handbook of Narratology. Hamburg 
                        http://www.lhn.uni-hamburg.de/article/plot [letzter Zugriff 25. August 2016].
                    
               
                  Moretti, Franco (2011): 
                        Network Theory, Plot Analysis (= Stanford Literary Lab Pamphlets, No. 2). 1.5.2011. 
                        http://litlab.stanford.edu/LiteraryLabPamphlet2.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Park, Gyeong-Mi / Kim, Sung-Hwan / Cho, Hwan-Gue (2013): 
                        „Structural Analysis on Social Network Constructed from Characters in Literature Texts“, 
                        in: 
                        Journal of Computers 8.9: 2442–2447 
                        http://ojs.academypublisher.com/index.php/jcp/article/view/jcp080924422447/7672 [letzter Zugriff 25. August 2016].
                    
               
                  Pfister, Manfred (1977): 
                        Das Drama. Theorie und Analyse.
                        München: Fink.
                    
               
                  Pohl, Mathias / Reitz, Florian / Birke, Peter (2008): 
                        „As Time Goes by. Integrated Visualization and Analysis of Dynamic Networks“, 
                        in: 
                        AVI 2008 – Proceedings of the Working Conference on Advanced Visual Interfaces. 
                        Neapel 372–375 
                        http://doi.acm.org/10.1145/1385569.1385636 [letzter Zugriff 25. August 2016].
                    
               
                  Prado, Sandra D. / Dahmen, Silvio R. / Bazzan, Ana L.C. / Carron, Padraig Mac / Kenna, Ralph (2016): 
                        „Temporal Network Analysis of Literary Texts“, 24.2.2016 
                        https://arxiv.org/pdf/1602.07275 [letzter Zugriff 25. August 2016].
                    
               
                  Rochat, Yannick (2014): 
                        Character Networks and Centrality. 
                        Thèse de Doctorat. Lausanne 
                        https://infoscience.epfl.ch/record/203889/files/yrochat_thesis_infoscience.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Stiller, Jaames / Nettle, Daniel / Dunbar, Robin I. M. (2003): 
                        „The Small World of Shakespeare's Plays“, 
                        in: 
                        Human Nature 14: 397–408 
                        https://www.staff.ncl.ac.uk/daniel.nettle/shakespeare.pdf [letzter Zugriff 25. August 2016].
                    
               
                  Stiller, James / Hudson, Mathew (2005): 
                        „Weak Links and Scene Cliques Within the Small World of Shakespeare“, 
                        in: 
                        Journal of Cultural and Evolutionary Psychology 3: 57–73.
                    
               
                  Trilcke, Peer / Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario (2015): 
                        „200 Years of Literary Network Data“ [Blogposts], 
                        https://dlina.github.io/200-Years-of-Literary-Network-Data/ [letzter Zugriff 25. August 2016].
                    
               
                  Trilcke, Peer / Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario / Kittel, Christopher (2016): 
                        „Theatre Plays as ›Small Worlds‹? Network Data on the History and Typology of German Drama, 1730-1930“, 
                        in: 
                        DH2016: Conference Abstracts 417–419 
                        http://dh2016.adho.org/abstracts/407 [letzter Zugriff 25. August 2016].
                    
               
                  Waumans, Michaël C. / Nicodème, Thibaut / Bersini, Hugues (2015): 
                        „Topology Analysis of Social Networks Extracted from Literature“, 
                        in: 
                        Plos One 3. Juni 2015 
                        10.1371/journal.pone.0126470.
                    
               
                  Xanthos, Aris / Pante, Isaac / Rochat, Yannick / Grandjean, Martin (2016): 
                        „Visualising the Dynamics of Character Networks“, 
                        in: 
                        DH2016: Conference Abstracts 417–419 
                        http://dh2016.adho.org/abstracts/407 [letzter Zugriff 25. August 2016].
                    
            
         
      
   



      
         Die Forschungstätigkeiten Georeferencing und Entity Linking sind wichtiger Bestandteil vieler DH-Projekte. Webservices/APIs und Tools versuchen diese Tätigkeiten zu vereinfachen und zu beschleunigen. Eines der bekannteren Tools, wenigstens im deutschsprachigen Raum ist dabei vermutlich der ‘DARIAH-DE Datasheet Editor’. Dieser zeichnet sich durch seine einfache Benutzung aus, sei es was den Datenimport (ausfüllen einer Tabelle oder Hochladen einer CSV-Tabelle) betrifft oder die anschließende Disambiguierung/Verifizierung der vom ‘Getty Thesaurus of Geographic Names’ zurückgelieferten Treffer über ein Graphical User Interface. 
         Das Projekt TEIHencer greift diese Vorzüge des ‘DARIAH-DE Datasheet Editors’ auf und versucht diese einerseits mit der ‘TEI-Welt’ zu verknüpfen sowie mit GeoNames und der GND zwei alternative Normdaten Ressourcen einzubinden.
         Konkret handelt es sich bei TEIHencer um ein Plug-In zu dem Python/Django basierten prosopographisch-geographischen Informationssystem APIS. Mit Hilfe des TEIHencers ist es möglich, XML/TEI kodierte Texte in denen Lokalitäten, Orte ausgezeichnet sind, über eine Webformular in APIS zu importieren. Während des Imports werden die Orts-Entitäten entsprechend eines vom Benutzer wählbaren X-Path Ausdruckes geparst, gegen GeoNames und GND abgeglichen und im Falle von Übereinstimmung angereichert und in einer relationalen Datenbank gespeichert. Die gespeicherten Entitäten können anschließend über das APIS-Web-Interface im Falle mehrerer Treffer disambiguiert werden. Dies erfolgt über eine Kartendarstellung, in welcher die verschiedenen Treffer zu einer Entität aufscheinen. Darüber hinaus können über das APIS-Web-Interface noch weitere Informationen zu den Entitäten ergänzt (z.B. alternative Schreibweisen, Datierungen) sowie die einzelnen Entitäten miteinander in typisierte Beziehungen gesetzt werden (z.B. Ort A ist Nachfolger von Ort B.; Ort A ist Teil von Ort B).
         Die mit Hilfe von TEIHencer angereicherten Daten können dann wieder als XML/TEI Dokument (kodiert als  Element) exportiert bzw. über HTTP GET request abgerufen und so etwa in andere Applikationen eingebunden werden.
         Im Zuge der Posterpräsentation soll der TEIHencer der einschlägigen DH-Comunity vorgestellt werden und zwar an dem konkreten Fallbeispiel der “Andreas Okopenko: Tagebücher aus dem Nachlass (Hybridedition)”. Dabei handelt es sich um ein digitales Editionsprojekt, das eine Auswahl der Tagebücher Andreas Okopenkos im Zeitraum von 1949 bis 1955 inhaltlich erschließen und einem breiteren Publikum zugänglich machen möchte. Einer der Schwerpunkte des Projekts liegt hierbei auf der inhaltlichen Erschließung des örtlichen Wirkungs- und Schaffensraums des Nachkriegsavantgardisten, indem nicht nur erwähnte Orte (), sondern nach Maßgabe auch Werke und Organisationen ( und ) mit geographischen Normdaten verknüpft werden, um so ein umfassenderes Bild von Okopenkos kulturellem Kontext vermitteln zu können.
         Neben der eigentlich Applikation und des konkreten Use-Cases wird am Poster auch das Konferenzthema “Kritik der Digitalen Vernunft” bzw. das Subthema “Kritik digitaler Angebote, Projekte und Werkzeuge” in Form der Frage nach der Nachhaltigkeit des vorgestellten Tools reflektiert. Eine solche glauben wir nämlich insofern gewährleisten zu können, als das Tool a) in ein konkretes Projekt (Okopenko) eingebettet ist, b) einen weit verbreiteten Standard (TEI) unterstützt, c) auf bestehende Eigenentwicklungen (APIS) aufbaut und d) Teile des Codes als selbstständige Module (TEI-Modul als python-package) konzipiert sind, die auch jenseits der konkreten Applikation Anwendung finden können. Darüber hinaus, e) ist der gesamte Code auf GitHub publiziert [3].  
      
      
         
            
               
                    http://www.getty.edu/research/tools/vocabularies/tgn/index.html
                
            
            
               
                    https://geobrowser.de.dariah.eu/edit/index.html
                
            
            
               
                    https://github.com/acdh-oeaw/teihencer
                
            
            
               
                    https://github.com/acdh-oeaw/apis-core
                
            
            
               
                    https://www.onb.ac.at/bibliothek/sammlungen/literatur/forschung/projekte/andreas-okopenko-tagebuecher-aus-dem-nachlass-hybridedition/
                
            
         
      
   



      
         
            Einführung
            Das laufende DFG-Projekt „Redewiedergabe“ stellt einen Anwendungsfall quantitativer Sprach- und Literaturwissenschaft dar und beschäftigt sich mit dem Phänomen „Redewiedergabe“ auf der Grundlage großer Datenmengen. Zu diesem Zweck wird zum einen ein Korpus manuell mit Redewiedergabeformen annotiert, zum anderen werden Verfahren zur automatischen Erkennung des Phänomens entwickelt. Ziel ist es, Forschungsfragen nach der Entwicklung von Redewiedergabe vor allem im 19. Jahrhundert zu beantworten. 
            Das Poster präsentiert einen Überblick über das Gesamtprojekt sowie erste Projektergebnisse.
         
         
            Stand der Forschung
            Sowohl aus linguistischer als auch aus literaturwissenschaftlicher Perspektive ist Redewiedergabe ein interessantes Phänomen. Die Art und Weise, wie die Figurenstimme in die Erzählung eingebunden ist, steht in engem Zusammenhang mit Erzählweise und -haltung, sowie der Konstruktion der erzählten Welt. Folglich wird dem Phänomen in der Erzählforschung viel Aufmerk­samkeit geschenkt und es liegen zahlreiche systematische Analysen vor (vgl. z.B. Genette 1998; Martínez / Scheffel 2007). Zu Phänomenen wie der erlebten Rede, dem Bewusstseinsstrom usw. gibt es eine umfangreiche Spezialforschung (Überblick bei McHale 2014). Aus linguistischer Perspektive ist Redewiedergabe vor allem in Bezug auf den Funktionswandel des Konjunktivs im Zusammenhang mit seinem Auftreten in indirekter Rede untersucht worden (vgl. z.B. Übersicht in Ágel 2000). In geringem Umfang sind auch Redewiedergabeverben und ihr Verhältnis zur wiedergegebenen Rede in das Blickfeld der Forschung gerückt (eine kurze Synopse bei Fritz 2005).
            Ein Vorbild für die ausführliche, manuelle Annotation von Redewiedergabe ist v.a. Semino / Short 2004. Implementierungen der automatischen Erkennung stammen vor allem aus dem Bereich der Computerlinguistik und werden oft als Vorverarbeitungsschritt für andere Anwendungen durchgeführt (z.B. Wissensextraktion, Sprechererkennung oder dem Aufbau von sozialen Netzwerken literarischer Figuren, vgl. z.B. Krestel / Bergler / Witte 2008; Elson / Dames / McKeown 2010; Iosif / Mishra 2014). Eine literaturwissenschaftlich motivierte Anwendung ist die Untersuchung von Schöch et al. 2016 zur Erkennung von direkter Wiedergabe in französischen Romantexten. Die wichtigste Vorarbeit für das vorgestellte Projekt ist die Studie Brunner 2015, auf deren Ergebnissen es aufbaut. In dieser Studie wurde ein Korpus von 13 Erzähltexten manuell annotiert und Prototypen für die automatische Erkennung (sowohl regelbasiert als auch mit Hilfe von maschinellem Lernen) wurden entwickelt und ausgewertet.
         
         
            Datengrundlage, Methodik und Ziele
            Das Untersuchungskorpus umfasst die Jahre 1840-1920 und enthält sowohl fiktionale als auch nicht-fiktionale Texte. Der nicht-fiktionale Teil setzt sich zusammen aus Texten des „Mannheimer Korpus Historischer Zeitungen und Zeitschriften“ und der Zeitschrift „Die Grenzboten“ (digitalisiert durch die Staats- und Universitätsbibliothek Bremen), der fiktionale Teil aus Erzählungen der Sammlung der Digitalen Bibliothek (textgrid). So sind sowohl Beobachtungen von Entwicklungen über die Zeit hinweg als auch Vergleiche zwischen Textsorten möglich.
            Auszüge aus den Texten werden manuell annotiert. Das in Brunner 2015 vorgestellte und an Kategoriensystemen der Literaturwissenschaft orientierte Annotationssystem wurde für das Projekt erweitert und präzisiert. Es unterscheidet zwischen Wiedergabe von gesprochener Sprache, von Schrift und von Gedanken sowie den Typen direkte Wiedergabe (
                    Er sagte: "Ich bin hungrig."), indirekte Wiedergabe (
                    Er sagte, er sei hungrig.), erzählte Wiedergabe (
                    Er sprach über das Mittagessen.) und freie indirekte Wiedergabe ('erlebte Rede') (
                    Wo sollte er jetzt nur etwas zu essen bekommen?). Attribute spezifizieren die Annotation (z.B. Verschachtelungstiefe) und markieren Sonderfälle (z.B. nicht-faktische Wiedergabe). Zusätzlich werden Sprecher, Rahmenformeln und die redeeinleitenden Verben bzw. Nomen markiert. 
                
            Die Annotatoren arbeiten mit dem im Projekt Kallimachos (www.kallimachos.de) von Markus Krug entwickelten Eclipse-basierten Annotationswerkzeug ATHEN (
                    ), für welches eine spezielle Annotationsoberfläche für Redewiedergabeformen implementiert wurde. Es werden, zumindest in Teilen, Mehrfachannotationen durchgeführt und Annotatorenvergleiche angestellt. 
                
            Die zweite Projektphase, welche zum Einreichungszeitpunkt dieses Posters gerade beginnt, umfasst die Entwicklung eines automatischen Erkenners für Redewiedergabeformen. Hierbei dient das manuell annotierte Material als Test- und Trainingsmaterial. Die in Brunner 2015 implementierten Prototypen dienen als Ausgangspunkt, die Implementierung erfolgt unter Nutzung des UIMA-Frameworks sowie in Python. Geplant ist eine Verbesserung des maschinellen Lernens durch Optimierung der Attributauswahl sowie Tests mit verschiedenen Lernalgorithmen (RandomForest, SVM, eventuell Conditional Random Fields und Deep Learning) und verschiedenen Parametereinstellungen. Auch regelbasierte Ansätze sollen weiter verfolgt werden, eventuell auf Grundlage einer aufwendigeren Vorverarbeitung (z.B. Parsing). Zudem ist eine Ergänzung und Verfeinerung einer Liste von Wörtern geplant, die auf Redewiedergabe hinweisen, welche sich bereits in Brunner 2015 als wertvolles Werkzeug bei der automatischen Erkennung erwiesen hat.
            Der Redewiedergabe-Erkenner wird dann auf weitere Texte in unserem Untersuchungszeitraum angewendet, um größere Entwicklungslinien beobachten zu können und verschiedene offene narratologische und linguistische Forschungsfragen auf quantitativer Basis zu untersuchen, z.B.: Welche Entwicklungen in der Verwendung und Form von Redewiedergabe lassen sich im Untersuchungszeitraum beobachten? Welche Rolle spielen Textsortenunterschiede bei der Entwicklung von Redewiedergabeformen? Wie kommt die Dynamik im Bestand an Verben zustande, die als Redeeinleiter gebraucht werden?
            Sowohl das manuell annotierte Korpus als auch der automatische Erkenner werden am Ende des Projekts der Forschungsgemeinschaft zur Verfügung gestellt. Es werden dafür sowohl das CLARIN-D-Forschungsdatenrepositorium des Instituts für Deutsche Sprache als auch das DARIAH-DE-Repository genutzt.
         
      
      
         
            
               Bibliographie
               
                  Ágel, Vilmos (2000): "Syntax des Neuhochdeutschen bis zur Mitte des 20. Jahrhunderts", in: Besch, Werner / Betten, Anne / Reichmann, Oskar (eds.): 
                        Sprachgeschichte. Ein Handbuch zur Geschichte der deutschen Sprache und ihrer Erforschung. Berlin / Boston: de Gruyter 1855-1903.
                    
               
                  Brunner, Annelen (2015): 
                        Automatische Erkennung von Redewiedergabe. Ein Beitrag zur quantitativen Narratologie (= Narratologia 47). Berlin / Boston: de Gruyter. 
                    
               
                  Elson, David K. / Dames, Nicholas J. / McKeown, Kathleen (2010): "Extracting Social Networks from Literary Fiction", in: 
                       Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics 138-147.
                    
               
                  Fritz, Gerd (2005): 
                        Einführung in die historische Semantik. Tübingen: M. Niemeyer.
                    
               
                  Genette, Gérard (1998): 
                        Die Erzählung. München: Wilhelm Fink.
                    
               
                  Iosif, Elias / Mishra, Taniya (2014): "From Speaker Identification to Affective Analysis: A Multi-Step System from Analyzing Children's Stories", in: 
                       Proceedings of the Third Workshop on Computational Linguistics for Literature 40-49.
                    
               
                  Krestel, Ralf / Bergler, Sabine / Witte, René (2008): "Minding the Source: Automatic Tagging of Reported Speech in Newspaper Articles", in: 
                       Proceedings of the Sixth International Language Resources and Evaluation Conference
                  (LREC 2008) 2823-2828.
                    
               
                  Martínez, Matías / Scheffel, Michael (2007): 
                        Einführung in die Erzähltheorie. München: C. H. Beck.
                    
               
                  McHale, Brian (2014): "Speech Representation" in: Hühn, Peter / Pier, John / Schmid, Wolf / Schönert, Jörg (eds.): 
                       The living handbook of narratology. Hamburg: Hamburg University Press 434-446 
                        [letzter Zugriff 18. September 2017].
                    
               
                  Schöch, Christof / Schlör, Daniel / Popp, Stefanie / Brunner, Annelen / Henny, Ulrike / Calvo Tello, José (2016): "Straight talk! Automatic Recognition of Direct Speech in Nineteenth-century French Novels", in: 
                       Conference Abstracts. Jagiellonian University & Pedagogical University 346-353.
                    
               
                  Semino, Elena / Short, Mick (2004): 
                        Corpus stylistics. Speech, writing and thought presentation in a corpus of English writing. London / New York: Routledge.
                    
            
         
      
   



      
         
            Einleitung
            Dieser Beitrag behandelt die Frage, warum in den DH entwickelte und angewandte Software häufig schnell altert. Jede Software altert relativ zu der Umgebung, in der sie eingesetzt wird, unabhängig von der Qualität am Beginn ihrer Verwendung (Engels et al. 2009: 393). Wandeln sich Hardware, Infrastruktur oder Anforderungen an die Software, wird sie, um weiter brauchbar zu sein, angepasst. Je nach Beschaffenheit können sich diese Anpassungen positiv, oftmals aber auch negativ auf die Lebensdauer und Fitness einer Software auswirken. 
            Aus der Praxis behaupten wir, dass kontextuelle und inhaltliche Spezifika von DH-Software dazu führen, dass eine langfristige Lauffähigkeit und Brauchbarkeit erschwert werden. Unser Beitrag bringt allgemein die Bedeutung und Relevanz des Themas „Software Evolution“ (2) nahe, beschreibt Spezifika der Software Evolution aus der DH-Praxis (3) und zeigt welche konkreten Maßnahmen im Projekt 
                    monasterium.net (4) dahingehend gesetzt werden. 
                
         
         
            Software Evolution
            Software Evolution umfasst alle Aktivitäten und Prozesse, die Software verändern (Godfrey/German 2008). Änderungen der Hardware, der Infomationsübermittlung sowie der Anforderungen sind Kräfte die auf diesen Evolutionsprozess wirken. Softwareentwicklungsprozesse werden seit den 1970er Jahren definiert und systematisiert, um die Qualität von Software zu steigern. Aus dieser Zeit stammt auch das Konzept des sogenannten Software Lifecycles und die Idee, diesen Zyklus zu managen (Lehman 1980). Unterschiedliche Methoden und Techniken dazu haben sich seither für alle Phasen im Lebenslauf von Softwaresystemen etabliert. Dank der intensiven Auseinandersetzung mit der Qualitätssteigerung in der Softwareentwicklung wurden die Fehlerquoten gesenkt (Thaller 2000: 6). Hochwertige Software ist nicht nur (nahezu) fehlerfrei, sondern auch kompatibel zur ihrer Umgebung. Verläuft die Evolution einer Software nicht in diesem Sinne, spricht man vom „Software Aging“ beziehungsweise sogar von deren Verfall (Parnas 1994). Demeyer et al. (2013: 4f.) fassen die Symptome veralteter Software wie folgt zusammen: Unvollständige oder keine Dokumentation, fehlende Tests, Ausstieg ursprünglicher Entwickler, verlorengegangenes Insiderwissen, fehlender Überblick über Gesamtsystem, zeitintensive Anpassungen, ständige Fehlerkorrekturen und Wartung damit verbundener Abhängigkeiten, lange „Build“-Zeiten und schlechter Code.
            Parnas (1994: 280) erkennt zwei Hauptfaktoren für das Altern von Software. „Lack of movement“, also keine Änderungen an der Software vorzunehmen, und „Ignorant surgery“: Aus der Praxis weiß man, dass bei dringenden Korrekturen am Programmcode, die formale Kriterien für gute Software oftmals nicht eingehalten werden. Ein Beispiel ist das unreflektierte Copy-and-paste aus 
                    Stack Overflow
               . Kurzfristige werden den besten Lösungen vorgezogen. Derartige Eingriffe und nicht-systematisches Vorgehen beschleunigen den Prozess der Softwarealterung. Es wird immer aufwendiger, Änderungen an der Software vorzunehmen.
                
            Demzufolge werden Ideen zur systematischen und automatisierten Verjüngung von Software erforscht und erprobt: Refactoring-Tools, beispielsweise für Java in 
                    Eclipse, 
                    Python Rope, oder aber auch für HTML und CSS (Mazinanian/Tsantalis 2017, Harold 2008), wurden entwickelt. Sogenannte „Prediction“- Modelle werden ermittelt, um Softwareevolution besser verstehen zu können und vor allem dem Problem der „Legacy software“ zu begegnen (Goltz et al. 2015, Paech et al. 2016).
                
         
         
            Software Herausforderungen in der DH Praxis
            
               Diese teilweise schon seit Jahrzehnten bekannten Erkenntnisse aus dem Software Engineering haben für die DH eine besondere Relevanz, da die Projekte hier wesentlich kleinere Budgets, oftmals kurze Projektlaufzeiten und andere Unsicherheiten haben. Aus unserer Erfahrung wird Softwareentwicklung in den DH häufig sehr informell gehandhabt. Diesbezüglich nachhaltiger zu werden, haben unter anderem Czmiel (2017), Schrade (2017) oder Kasper/Grüntgens (2017) gefordert. Nicht nur der Entwicklungsprozess von DH-Software muss längerfristig gedacht werden (Hattrick 2016), auch der Kontext, in dem die Software entsteht und besteht, beeinflusst deren Entwicklung und Veränderung. 
                
            Erstens ist es nicht ungewöhnlich, dass Projekte in den DH von einer einzigen Person technisch umgesetzt werden, wie es etwa im Falle von Dissertationsprojekten typisch ist. Der entstandene Code ist bei Projektende lauffähig, es kann aber nicht vorausgesetzt werden, dass dieser auf einen langfristigen Einsatz ausgelegt ist und entsprechend gewissenhaft programmiert und dokumentiert ist. Forschungsergebnisse sind im Projektkontext meist wichtiger als die Qualität der entwickelten Software. Generell bedeutet ein Projektende nicht die Übergabe eines Produktes an einen Kunden, es bedeutet vielmehr: Die Finanzierung läuft aus und der/die Entwickler/in verlässt das Projekt. Was zurückbleibt, ist Software, die von anderen gewartet werden muss. Dazu ist es notwendig, die Dokumentation und Systemarchitektur zu verstehen, sich in den Fremdcode einzuarbeiten. Veränderungen am Code können oft nicht mehr ihrer ursprünglichen Intention entsprechend vorgenommen werden. Die Wartung wird aufwendig und zeitintensiv. Das heißt, die Organisationsstrukturen des Forschungsbetriebes beeinflussen die Alterung von Software.
            Zweitens bringen die komplexen Anforderungen der Forschungsdaten nicht-klassische Lösungsansätze mit sich. Mit diesen Ansätzen vertraute Entwickler/innen sind schwer zu finden und zu halten, Einarbeitungsphasen dauern lange. Besonders augenfällig wird das am in den DH weit verbreiteten Gebrauch von X-Technologien. Sie werden immer mehr zur Nischenanwendung. Während die Definitionen von XSLT 1.0 und XPath 1.0 noch von einer größeren Breite von Softwareprodukten implementiert wurden, sogar Teil der Browser wurden, gibt es nur noch wenige Implementationen der Weiterentwicklungen XSLT 2.0 und 3.0. Auch die Menge verwendbarer XML-Datenbanksysteme ist heute geringer als noch vor einigen Jahren. In den DH entwickelte Softwarelösungen sind also speziell auf die Bedürfnisse des Gegenstandes ausgelegt und stellen keine Standardlösungen dar. Sie brauchen spezifisches Know-how, um gewartet werden zu können. Fehlt dieses, beziehungsweise ist es nur mangelhaft vorhanden, droht die Software zum unbrauchbaren Altsystem zu verkommen.
            DH-Software verlangt drittens besondere Zuwendung, wenn der Code gleichzeitig die Forschungsergebnisse interpretiert. Wenn die Forschungsleistung also nicht allein in den Daten liegt, braucht es individuelle Wartungslösungen. Eine Digitale Edition kann beispielsweise als die Gesamtheit von Daten, Systemarchitektur, Anwendung und GUI verstanden werden (Andrews/Zundert 2018). Diese Interpretationsleistung als Teil der Forschung muss bei allen Phänomenen der Veränderung an der Edition mitbedacht werden. Die Gefahr ist groß, dass nach einiger Zeit das Argument durch Softwareanpassungen verwässert oder im schlimmsten Fall nicht mehr nachvollziehbar ist und für die Forschung unbrauchbar wird. 
            Zusammenfassend sehen wir in der nicht langfristigen Finanzierung, der hohen Fluktuation an Personen, der Notwendigkeit von Speziallösungen und im Forschungsgegenstand selbst erhöhten Bedarf an Maßnahmen, um unsere Softwareprojekte lauffähig zu halten. 
         
         
            Anti-Aging Maßnahmen im Projekt monasterium.net
            
               Seit 2008 basiert die Urkundenplattform 
                    monasterium.net auf 
                    eXist-db als Applikationsserver und Datenbank. Die Plattform wurde hauptsächlich von drei aufeinanderfolgenden Hauptentwicklern programmiert. Um die Software zu modularisieren, wurde seit 2011 das 
                    mom-ca-Framework entwickelt, eine Webapplikation in XRX-Architektur (XQuery, REST, XForms). Die Architektur galt damals in Verbindung mit XML-Datenbanken als Empfehlung, wird allerdings in der modernen Webentwicklung kaum mehr eingesetzt. Mit Auslaufen eines Projektes 2014 verließ der letzte Entwickler mit Überblick über das Gesamtsystem das Projekt. Zuvor wurde der Gesamtcode in ein öffentliches Repository überführt. Wissen und Intentionen gingen jedoch verloren. 
                    Wir, als das aktuelle, größtenteils projektfinanzierte Entwicklerteam, beschäftigen uns nun aktiv damit, wie der derzeitige Code-Bestand unter unsteten Umständen wartbar und aktuell gehalten werden kann. Im Folgenden beschreiben wir vier Anti-Aging-Maßnahmen, die einerseits Refactoring (das Überarbeiten des Codes), aber auch ganz grundsätzliche Umstellungen des Entwicklungsworkflows betreffen. 
                
            
               Softwareverwaltung durch 
                        Git und Nutzung der Services von 
                        GitHub.
                    
               Sowohl Entwicklung als auch Dokumentation erfolgen über ein öffentliches 
                        GitHub-Repository. Die dadurch verfügbaren Möglichkeiten der Versionsverwaltung, des Bugtracking und des Code Review werden genutzt, um die Qualität des Codes zu verbessern und diesen transparent und nachvollziehbar zu entwickeln.
                    
            
            
               Einrichtung einer Testumgebung. 
               Jede Neuentwicklung wird, vor ihrer Übernahme in das Produktivsystem anhand eines festgelegten Testszenarios evaluiert. Durch die Spiegelung des Livesystems auf einem Testserver soll reales Systemverhalten reproduziert werden. Fehler können so vorzeitig entdeckt und behoben werden.
            
            
               Refactoring von HTML und CSS.
               Die Verwendung eines auf den Konzepten von Material Design basierenden CSS-Frameworks garantiert ein konsistentes Gesamtdesign von 
                        monasterium.net. Teile des Benutzerinterfaces werden dadurch modularisiert und leichter anpassbar. Die Verwendung eines Präprozessors und das Einführen einer Namenskonvention sollen die Wartbarkeit, das Auffinden von Fehlern und die Umsetzung neuer Features erleichtern.
                    
            
            
               Entwicklung einer RESTful API zwischen Client und Datenbank.
               Die zukünftige Kommunikation zwischen Client und Datenbank übernimmt eine neudefinierte REST-API. 
                       Die Datenabfrage aus der XML-Datenbank erfolgt noch per XQuery, zurückgeliefert werden wahlweise in XML oder JSON serialisierte Daten. Diese Form des Reengineerings gewährt eine definierte, standardisierte Verarbeitungsweise sowie die Weiternutzung und Kombination multipler Datenquellen. Die Abstraktion von Datenbank, Programmlogik und Benutzeroberfläche erleichtert so in Zukunft deren entkoppelte Anpassung oder Austausch.
                    
            
         
         
            Fazit
            Softwarealterung ist nicht nur in der Softwareindustrie eine aktuelle und fordernde Problematik. Auch für DH-Forschungsinfrastrukturen ist diesbezüglich ein gezielter Umgang gefragt, um Software fit zu halten. Unwissenheit hinsichtlich der Wartung einer Software kann schlimmstenfalls zu einer zukünftigen Unbrauchbarkeit der Forschungsergebnisse führen. Eine dahingehende Bewusstseinsbildung kann über die empirische Betrachtung vorhandener Praktiken und Lösungswege geschehen.
            Anhand von 
                    monasterium.net haben wir exemplarisch mögliche Verjüngungsmaßnahmen dargestellt. Das Projekt eignet sich als Fallbeispiel, da seine Software eine über zehnjährige Laufzeit aufweist. Geringes Projektbudget und häufiger Personalwechsel mit daraus resultierenden Wissensverlusten haben die Codebasis gezeichnet. Das Projekt zeigt, dass Nachvollziehbarkeit des Entwicklungsprozesses, systematisches und standardisiertes Vorgehen, Modularisierung von Softwarekomponenten sowie kontinuierliches Testing in die Evolution von Software gewinnbringend eingreifen können.
                
            
               Die Verantwortung kann allerdings nicht allein bei den Entwickler/innen liegen. Um Wissensverluste vorzubeugen, müssen langfristige Strukturen aufgebaut und finanziell abgesichert werden. Es muss Teil der Förderungspolitik werden, die Unausweichlichkeit der Softwarealterung zu bedenken. Sollen Entwicklungen auch nach fünf Jahren noch benutzbar sein, muss der Aufwand der nachhaltigen Entwicklung und Wartung in der Antragsplanung verankert werden.
                
         
      
      
         
             Stack Overflow ist eine Online Community, zur gegenseitigen Unterstützung und zur Wissensgenerierung bei Fragen zur Softwareentwicklung: stackoverflow.com
             github.com/icaruseu/mom-ca
             material.io/guidelines/
         
         
            
               Bibliographie
               
                  Andrews, Tara / Zundert, Joris van (2018): “What are you Trying to Say? The Interface as an Integral Element of Argument”, in: Bleier, Roman et al. (eds.): 
                        Digital Scholarly Editions as Interfaces (=Schriften des Instituts für Dokumentologie und Editorik). Norderstedt: Books on Demand.
                    
               
                  Czmiel, Alexander (2017): “Funktionalität Digitaler Editionen“, in: 
                        DHd 2017. Digitale Nachhaltigkeit. Konferenzabstracts. Bern 138-141. 
                        http://www.dhd2017.ch/wp-content/uploads/2017/02/Abstractband_ergaenzt.pdf [letzter Zugriff 24. September 2017].
                    
               
                  Demeyer, Serge / Ducasse, Stéphane / Nierstrasz, Oscar (2013): 
                        Object-Oriented Reengineering Patterns. Bern: Square Bracket Associates. 
                        http://scg.unibe.ch/download/oorp/OORP.pdf
                        [letzter Zugriff 24. September 2017].
                    
               
                  Engels, Gregor et al. (2009) "Design for Future: Legacy-Probleme von morgen vermeidbar?", in: 
                        Informatik Spektrum 32, 5: 393-397. https://doi.org/10.1007/s00287-009-0356-3 [letzter Zugriff 24. September 2017].
                    
               
                  Godfrey, Michael W. / German, Daniel M. (2008): “The Past, Present, and Future of Software Evolution”, in: 
                        Proceedings of the 2008 Frontiers of Software Maintenance.
               
               New York: IEEE 129-138. 
                        https://doi.org/10.1109/FOSM.2008.4659256 [letzter Zugriff 24. September 2017].
                    
               
                  Goltz, Ursula et al. (2015): “Design for future: managed software evolution”, in: 
                        Computer Science - Research and Development 30, 3-4: 321-331. https://doi.org/10.1007/s00450-014-0273-9 [letzter Zugriff 24. September 2017].
                    
               
                  Harold, Rusty Elliotte (2008): 
                        Refactoring HTML. Improving the Design of Existing Web Applications. Upper Saddle River, NJ: Addison-Wesley.
                    
               
                  Hattrick, Simon (2016): Research Software Sustainability. Report on a Knowledge Exchange Workshop. JISC:
                        http://repository.jisc.ac.uk/6332/1/Research_Software_Sustainability_Report_on_KE_Work
                            shop_Feb_2016_FINAL.pdf
                         [letzter Zugriff 24. September 2017].
                    
               
                  Kasper, Dominik / Grüntgens, Max (2017): “Nachhaltige Konzeptionsmethoden für Digital Humanities Projekte am Beispiel der Goethe-Propyläen“, in: 
                        DHd 2017. Digitale Nachhaltigkeit. Konferenzabstracts. Bern 165-168. 
                        http://www.dhd2017.ch/wp-content/uploads/2017/02/Abstractband_ergaenzt.pdf [letzter Zugriff 24. September 2017].
                    
               
                  Lehman, Meir M. (1980): “Programs, life cycles, and laws of software evolution”, in: 
                        Proceedings of the IEEE 68, 9: 1060-1076.
                  https://doi.org/10.1109/PROC.1980.11805
                        [letzter Zugriff 24. September 2017].
                    
               
                  Mazinanian, Davood / Tsantalis, Nikolaos (2017): “CCSDev: Refactoring duplication in Cascading Style Sheets”, in: 
                        Proceedings of the 39th International Conference on Software Engineering Companion. New York: IEEE 63-66. 
                        https://doi.org/10.1109/ICSE-C.2017.7 [letzter Zugriff 24. September 2017].
                    
               
                  Paech, Barbara et al. (2016): “Empirische Forschung zu Software-Evolution”, in: 
                        Informatik Spektrum 39, 3: 186-193. 
                        https://doi.org/10.1007/s00287-015-0910-0
                         [letzter Zugriff 24. September 2017].
                    
               
                  Parnas, David L. (1994): “Software Aging”, in: 
                        Proceedings of 16th International Conference on Software Engineering. New York: IEEE 279-287. 
                        https://doi.org/10.1109/ICSE.1994.296790 [letzter Zugriff 24. September 2017].
                    
               
                  Schrade, Torsten (2017): “Nachhaltige Softwareentwicklung in den Digital Humanities. Konzepte und Methoden“, in: 
                        DHd 2017. Digitale Nachhaltigkeit. Konferenzabstracts. Bern 168-171. 
                        http://www.dhd2017.ch/wp-content/uploads/2017/02/Abstractband_ergaenzt.pdf [letzter Zugriff 24. September 2017].
                    
               
                  Thaller, Georg E. (2000): ISO 9001: 
                        Software-Entwicklung in der Praxis. Hannover: Heise.
                    
            
         
      
   



      
         
            Hintergrund und Zielsetzung
            Am 13. Oktober 2016 gab die Schwedische Akademie bekannt, dass sie den Nobelpreis in Literatur an Bob Dylan „für seine poetischen Neuschöpfungen in der großen amerikanischen Songtradition“ verleihen werde. 
                    
                    Dass Dylan als Musiker und Songwriter den Literaturnobelpreis erhielt wurde mitunter sehr kontrovers diskutiert. Auf Kritik stieß z.B. die unzulässige Herauslösung von Dylans Texten aus der Musik und die Deutung seiner Lieder als Gedichte. 
                    
                    Unbestritten ist nichtsdestotrotz Bob Dylans Rolle als einer der einflussreichsten Musiker des 20. Jahrhunderts. 
                    
                
            Die (welt-)politischen Entwicklungen, die das Schaffen Dylans inspirierten, sind im Kontext seines Wirkens umfassend diskutiert worden, unter anderem in „Bob Dylan und die sechziger Jahre: Aufbruch und Abkehr“, 
                   
                    und liefern noch immer Diskussionsstoff, wie etwa eine in jüngerer Zeit erschienene Arbeit von Taylor & Israelson (2015) 
                    
                   über Dylans politische Einflüsse zeigt. Erfolgte die Beschäftigung mit Dylans Werk bislang allenfalls episodisch, so muss eine systematische Analyse des Gesamtwerks als Desiderat gelten, welches in gewisser Weise bereits von Bob Dylan selbst formuliert wurde:
                
            All these people who say whatever it is I’m supposed to be doing – that’s all gonna pass, because, obviously, I’m not gonna be around forever. That day’s gonna come when there aren’t gonna be any more records, and then people won’t be able to say ‘Well, this one’s not as good as the last one.’ 
                    They’re gonna have to look at it all (eigene Hervorhebung). And I don’t know what the picture will be, what people’s judgement will be at that time. I can’t help you in that area. 
                    – Bob Dylan 
                
            
            Dieser Beitrag erprobt, inwiefern mithilfe digitaler Methoden im Sinne des 
                    Distant Reading-Paradigmas ein neuer Zugang zu Dylans Gesamtwerk geschaffen werden kann. Bezugnehmend auf das Konferenzmotto einer „Kritik der Digitalen Vernunft“ soll untersucht werden, wo die Grenzen und Möglichkeiten eines solchen digitalen Analyseansatzes liegen, indem überprüft wird, ob sich bestehende qualitative Einteilungen von Dylans Werk in unterschiedliche Schaffensperioden auch anhand statistisch signifikanter Wörter (Rayson, Berridge & Francis 2004) und N-Gramme (Evert 2005) belegen lassen. 
                
         
         
            Stand der Forschung
            Bereits vor der Auszeichnung Dylans mit dem Nobelpreis in Literatur, waren seine Texte Gegenstand wissenschaftlicher Betrachtungen im Sinne des 
                    Close Reading (vgl. etwa Brown 2014). Brown unterteilt Dylans Werk in einzelne Phasen und verknüpft diese jeweils mit allgemeinen, zeitgeschichtlichen Ereignissen sowie biographischen Meilensteinen des Künstlers. Taylor & Israelson (2015) gehen einen ähnlichen Weg, versuchen jedoch Dylans Werk abseits verbreiteter politischer Einordnungen zu betrachten. Etwas anders ausgerichtet ist die Arbeit von Wissolik et al. (1994): Hierbei handelt es sich um eine Art Wörterbuch, in dem Namen und Gegenstände, die in Dylans Texten auftauchen, erläutert werden. 
                
            Eine umfassende Untersuchung Dylans Werks mithilfe computerbasierter Methoden fand sich bis zum Abfassungszeitpunkt des vorliegenden Texts nicht. Allerdings sind quantitative Verfahren zur stilistischen und inhaltlichen Analyse von Liedtexten in den Digital Humanities durchaus verbreitet. So beschreiben etwa, 
                    
                    wie mithilfe von N-Gramm-Modellen ein Liedtext-Korpus anhand der Merkmale Textlänge, Textstruktur, Wortschatz und Semantik analysiert werden kann, um eine automatisierte Genrezuordnung vornehmen zu können. Daneben existieren stilometrische Untersuchungen von Liedern oder Gedichten, die sich mit der Berechnung von autoren- und genrespezifischen Merkmalen befassen, wie z.B. Suzuki & Hosoya (2014), die japanische Pop-Songs analysieren.
                
         
         
            Forschungsmethodik
            
               Bezugsrahmen der Analyse: Schaffensphasen Bob Dylans
               Den analytischen Bezugsrahmen dieser Studie stellt die phasenweise Einteilung von Dylans Schaffen nach Brown (2014)
                         
                        dar. 
                         
                        Brown unterscheidet dabei neun unterschiedliche Phasen, die mit „Becoming Bob Dylan“ (1960-1964) beginnen und vorläufig mit „Bob Dylan Revisited“ (2000-2012) enden. Diese Stilphasen umfassen z.B. Dylans Hinwendung zum Christentum oder seine elektronische „Folk Rock“-Phase (vgl. Brown, 2014).
                    
            
            
               Korpus und Datenaufbereitung
               In dieser Arbeit wurde ein Korpus bestehend aus 452 Liedtexten mit einem Umfang von 133.045 Tokens untersucht, die Bob Dylan zwischen den Jahren 1962 und 2016 auf Studio-Alben veröffentlicht hat. Die Liedtexte und Metainformationen wie etwa Titel, Album und Jahr stammen von der Plattform 
                        LyricsWikia
                  . Da es sich bei 
                        LyricsWikia um ein community-gestütztes Projekt handelt, erfolgte vorab ein stichprobenartiger Abgleich einzelner Lieder mit den offiziellen Texten nach, 
                        
                        wobei keinerlei Abweichungen festgestellt werden konnten. 
                    
               Das Korpus wurde weiterhin mit Methoden der Computerlinguistik aufbereitet, insbesondere unter Verwendung des 
                        Python Natural Language Toolkits (NLTK). Die Verarbeitung des Korpus umfasst die grundlegende Lemmatisierung mit dem 
                        WordNet-Lemmatizer (Teil des NLTK) und eine Stoppwortbereinigung (NLTK-Stoppwortliste für Englisch mit eigener Erweiterung) sowie die Wortartenannotation mithilfe des 
                        Stanford Log-linear Part-of-Speech-Taggers.
                        
                        Da Dylan in seinen Texten häufig umgangssprachliche Formulierungen wie etwa verkürzte Gerundformen (bspw. „savin“, „swimmin“) verwendet, wurde für den POS-Tagger ein Modell verwendet, welches auf der Grundlage von Twitter-Texten trainiert wurde und gute Ergebnisse für Texte mit nicht-standardisiertem Vokabular und Slang liefert. 
                        
                    
            
            
               Korpusvergleich – Assoziationsmaße und Referenzkorpus
               
                  Assoziationsmaße
                  Ein etabliertes Verfahren, um aus einem Korpus spezifische Wörter zu extrahieren, ist ein direkter Korpusvergleich mit dem 
                            Log-Likelihood-Test, 
                            
                            der sich zum Vergleich von Korpora unterschiedlicher Größe besonders eignet (Rayson, Berridge, & Francis, 2004). Damit können Wörter, die im untersuchten Korpus mit einem signifikanten Frequenzunterschied zum Referenzkorpus auftreten, als Schlagworte betrachtet werden. Dies kann besonders aussagekräftige Ergebnisse liefern, wenn zusätzlich eine POS-Filterung erfolgt, womit sich beispielsweise signifikante Nomen oder Verben eines Korpus berechnen lassen. Darüber hinaus wurde eine Berechnung von N-Grammen in Form von Bi- und Trigrammen umgesetzt. Die berechneten N-Gramme lassen sich in der Web-App unter der Wahl eines Assoziationsmaßes, wie dem 
                            Chi Quadrat-Test, dem Jaccard-Test, dem Poisson-Stirling-Test, dem Likelihood Ratio-Test sowie dem Pointwise Mutual Information-Test anzeigen. Dabei liefert jedes Verfahren zur N-Gramm-Berechnung eigene spezifische Ergebnisse. Dieser Freiraum wird ganz bewusst erhalten, um die verschiedenen Facetten eines Texts, die ein Assoziationsmaße jeweils anzeigt, für die spätere Datenanalyse nutzen zu können.
                        
               
               
                  Referenzkorpus 
                  Als Referenzkorpus dient das mündliche Subkorpus des 
                            Open American National Corpus (OANC; American National Corpus Project),
                            
                            welches insgesamt 3.862.172 Tokens umfasst. Das Korpus enthält viele Belege aus der mündlichen Kommunikation 
                             
                            und eignet sich dadurch in besonderer Weise als Vergleichskorpus für Dylans Texte, die wie bereits beschrieben einen hohen Anteil umgangssprachlicher Formulierungen und Slang-Ausdrücke enthalten.
                        
                  Beim Korpusvergleich kann entweder das gesamte Dylan-Korpus mit dem Referenzkorpus verglichen werden, oder mit den jeweiligen Dylan-Subkorpora, also bspw. all seinen Texten aus den 1970er-Jahren oder aus der ersten Schaffensperiode „Becoming Bob Dylan“ (1960-1964). Ein Vergleich der einzelnen Dylan-Subkorpora zum Gesamtwerk ist ebenso möglich. Letztere Option wird z.B. genutzt, um anhand jeweils signifikanter Wörter die einzelnen Schaffensperioden nach Brown (2014) zu überprüfen und damit die grundsätzliche Eignung solch quantitativer Verfahren zur Identifikation thematischer Verschiebungen zu untersuchen. Die Ergebnisse dieses Korpusvergleichs sind, zusammen mit allen anderen Ergebnissen der angewandten Analyseverfahren, in einer interaktiven Webanwendung über unterschiedliche Visualisierungen (Balkendiagramm, 
                            treemap, wordcloud, Tabelle) für weitere Interpretationen zugänglich. Wie schon bei den Assoziationsmaßen, so gilt auch hier, dass jede Visualisierungsform eine bestimmte Perspektive auf die Berechnungsergebnisse eröffnet.
                        
               
            
         
         
            Ergebnisse
            Im direkten Vergleich des gesamten Dylan-Korpus (1962-2016) mit dem OANC-Referenzkorpus treten einige interessante, signifikant-häufige Wörter im Werk Dylans hervor. Die von Bob Dylan verwendeten Adjektive erzeugen in der Gesamtschau tendenziell eher eine bedrückende Stimmung (blind, weary, lonely, drunken, scared, restless, ragged). Bei den Substantiven mischen sich unter viele Personen- und Ortsnamen auch religiöse Begriffe (soul, heaven, devil, eden, prayer, paradise). Viele der übrigen Begriffe sind erwartungsgemäß typisch für Folk-Musik (levee, rooster, train), was sich wiederum durch die Wahl des Referenzkorpus, das verschiedenartige mündliche Textquellen enthält, erklären lässt (Rayson, Berridge, & Francis, 2004: 8).
                
            Die Analyse signifikant-häufiger Wörter für die einzelnen Schaffensphasen Dylans liefert Ergebnisse mit hoher Aussagekraft. So fällt etwa für die Phase „The Changing of the Guard" (1978-1981), in der sich Dylan dem Christentum hinwendet, auf, dass das Vokabular tatsächlich viele christliche Motive aufweist (lord, Jesus, devil, altar, faith, confession, grace, power, serve). Insgesamt nimmt der Anteil an „düsterem“ Vokabular in dieser Phase ab, verschwindet jedoch nicht komplett (bspw. 
                    shot, destruction). Der Anteil an hoffnungsvollen Wörtern nimmt hingegen zu (bspw. 
                    beginning, ready, arise, wake, thank). Bei den übrigen Schaffensphasen fallen die Ergebnisse jedoch mitunter wesentlich weniger deutlich aus.
                
            Ein differenziertes Bild ergibt sich für die N-Gramm-Analyse, was einerseits der Vielfalt an verfügbaren Methoden zur Berechnung 
                     
                    und andererseits den unterschiedlichen N-Gramm-Längen geschuldet ist. Die Ergebnisse für Bigramme mit Hilfe des 
                    Pointwise-Mutual-Information-Tests (PMI) erschienen dabei am geeignetsten, um die thematischen Schwerpunkte von Dylans Schaffensphasen nach 
                     
                    nachzuvollziehen. So findet das PMI-Verfahren im Subkorpus der Phase „The Changing of the Guard“ (1978-1981) Bigramme wie 
                    close prayer, name lucifer, jesus good, jesus bone oder arise upon, die eindeutig religiöse Bezüge in Dylans Texten dieser Phase veranschaulichen. Generell fällt jedoch die Dominanz von Refrain-Versen in den Liedern bedeutend ins Gewicht (z.B. 
                    knock heaven door), was die Qualität der Ergebnisse insbesondere bei den Trigrammen beeinflusst. 
                
         
         
            Diskussion
            Im Sinne einer Kritik der Digitalen Vernunft bleibt demnach festzuhalten, dass sich Methoden der computergestützten Textanalyse und des statistischen Korpusvergleichs grundsätzlich dafür eignen, einen inhaltlichen Gesamtüberblick zu einem Liedtext-Korpus zu erhalten. Es können damit diachrone Entwicklungen des Wortschatzes und Verlagerungen thematischer Schwerpunkte als grobe Tendenzen aufgezeigt werden, um das Bild des Gesamtwerks zu ergänzen. Ein solcher Ansatz eignet sich demnach gut für die initiale Thesengenerierung und kann in gewisser Weise die Funktion eines Empfehlungs- bzw. Hinweissystems für erklärungsbedürftige Stellen
                     in den Geisteswissenschaften übernehmen.
                
            Die Identifikation konkreter Schaffensperioden, ausschließlich auf Basis signifikant häufiger Wörter ist aber – zumindest für das Werk Dylans – nicht ohne Weiteres erfassbar. Bei den N-Grammen zeigt sich, dass im Falle von Dylans Texten methodenübergreifend und mit zunehmender N-Gramm-Länge meist keine brauchbaren Ergebnisse erzielt werden konnten. Dies ist ein Hinweis darauf, dass die hier präsentierten Analysemethoden, die für andere Textsorten wie bspw. Parlamentsprotokolle bereits erfolgreich eingesetzt werden konnten (vgl. Sippl et al. 2016), auf Liedtexte nur eingeschränkt anwendbar sind. Ein möglicher Kritikpunkt am hier beschriebenen Vorgehen mag zudem das verwendete OANC-Referenzkorpus sein, welches trotz hoher Anteile mündlicher Kommunikation doch nur beschränkt vergleichbar mit der Textsorte „Liedtext“ ist. Für künftige Vergleichsstudien böte sich ggf. ein Vergleich mehrerer unterschiedlicher Künstler und deren Liedtexte an, also bspw. Bob Dylan vs. Johnny Cash.
         
      
      
         
             http://lyrics.wikia.com, alle Hyperlinks dieses Dokuments wurden zuletzt abgerufen am 10.01.2018
             Verfügbar unter http://www.nltk.org/
             Filterung von Stoppwörtern, wie „hey“, „ah“, „yeah“, und Verkürzungen, wie „‘ve“, „‘s“ etc.
             https://www.colin-sippl.de/dylan (Klick auf den Analyse-Button rechts oben)
             Diesen Gedanken äußerte Hubertus Kohle auf der #DigiCampus-Tagung im Juni 2017 in München, vgl. https://twitter.com/8urghardt/status/876725916487036928.
         
         
            
               Bibliographie
               
                  American National Corpus Project (2015a): 
                        American National Corpus. Frequency Data. http://www.anc.org/data/anc-second-release/frequency-data/ [Letzter Zugriff 10. März 2017].
                    
               
                  American National Corpus Project (2015b): 
                        The Open American National Corpus (OANC). http://www.anc.org/ [Letzter Zugriff 10. März 2017].
                    
               
                  Brown, Donald (2014): 
                        Bob Dylan: American troubadour. Lanham, Md. [u.a.]: Rowman & Littlefield.
                    
               
                  Cott, Jonathan (2006): 
                        Bob Dylan, the essential interviews. New York: Wenner Books.
                    
               
                  Derczynski, Leon et al. (2013): "Twitter part-of-speech tagging for all: Overcoming sparse and noisy data", in: 
                        Proceedings of the Recent Advances in Natural Language Processing September, 198–206. http://www.derczynski.com/sheffield/papers/twitter_pos.pdf [Letzter Zugriff 9. März 2017].
                    
               
                  Dylan, Bob (2016): 
                        The lyrics: 1961-2012. New York: Simon & Schuster.
                    
               
                  Evert, Stefan (2005): "The Statistics of Word Cooccurrences, Word Pairs and Collocations", in: 
                        Unpublished doctoral dissertation Institut fur maschinelle Sprachverarbeitung Universitat Stuttgart 98: August 2004, 353. http://en.scientificcommons.org/19948039 [Letzter Zugriff 3. März 2017].
                    
               
                  Fell, Michael / Sporleder, Caroline (2014): "Lyrics-based Analysis and Classification of Music", in: 
                        International Conference on Computational Linguistics 25: 23–29, 620–631.
                    
               
                  Geisel, Sieglinde (2016): 
                        Bob Dylan - Literaturnobelpreisträger wider Willen. Deutschlandradio Kultur. http://www.deutschlandradiokultur.de/bob-dylan-literaturnobelpreistraeger-wider-willen.1005.de.html?dram:article_id=373494 [Letzter Zugriff 7. März 2017].
                    
               
                  Rayson, Paul / Garside, Roger (2000): "Comparing corpora using frequency profiling", in: 
                        Proceedings of the workshop on Comparing Corpora 1–6.
                    
               
                  Rayson, Paul / Berridge, Damon / Francis, Brian (2004): "Extending the Cochran rule for the comparison of word frequencies between corpora", in: 
                        JADT 2004: 7es Journées internationales d’Analyse statistique des Données Textuelles: 1–12.
                    
               
                  Schmidt, Mathias R. (1983): 
                        Bob Dylan und die sechziger Jahre: Aufbruch und Abkehr. Frankfurt am Main: Fischer Taschenbuch Verlag.
                    
               
                  Sippl, Colin / Burghardt, Manuel / Wolff, Christian / Mielke, Bettina (2016): Korpusbasierte Analyse österreichischer Parlamentsreden. In: 
                        Netzwerke: Tagungsband des 19. Int. Rechtsinformatik Symposions IRIS 2016: 25.- 7. Feb. 2016, Univ. Salzburg, S. 139-148.
               
               
                  Suzuki, Takafumi / Hosoya, Mai (2014): "Computational Stylistic Analysis of Popular Songs of Japanese Female Singer-songwriters", in: 
                        Digital Humanities Quarterly 8: 1, .
                    
               
                  Svenska Akademien (2016): 
                        Der Nobelpreis in Literatur des Jahres 2016.
                    
               
                  Taylor, Jeff / Israelson, Chad (2015): 
                        The Political World of Bob Dylan: Freedom and Justice, Power and Sin. New York: Palgrave Macmillan.
                    
               
                  Toutanova, Kristina / Klein, Dan / Manning, Christopher D (2003): "Feature-rich part-of-speech tagging with a cyclic dependency network", in: 
                        Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1 (NAACL ’03), 252–259. http://nlp.stanford.edu/~manning/papers/tagging.pdf [Letzter Zugriff 3. März 2017].
                    
               
                  Wissolik, Richard David / McGrath, Scott. / Colaianne, A. J. (1994): 
                        Bob Dylan’s words: a critical dictionary and commentary. Greensburg, PA: Eadmer Press.
                    
            
         
      
   



      
         
            Introduktion
            Im Rahmen eines Softwareprojektes
                    , das sich mit der automatisierten Analyse von Märchen in deutscher Sprache befasst, hat sich die Notwendigkeit ergeben, eine formale Repräsentation von Märchen zu bestimmen, damit die einzelnen Komponente des Systems miteinander integriert werden können.
                
            Wir beschreiben in diesem Beitrag zum einen, welche Informationen in dieser formalen Repräsentation enthalten sind, und zum anderen, wie diese Informationen in XML bzw. Python konkret codiert werden.
         
         
            Kodierte Information
            Ein Märchen besteht im Sinne unseres Projektes aus den folgenden Bestandteilen:
            
               Eine Menge von Orten, an denen die Handlung spielt;
               Eine Menge von Charakteren, die an der Handlung beteiligt sind;
               Eine zeitliche Abfolge von Szenen, die jeweils an einem bestimmten Ort spielen und an denen jeweils eine Teilmenge der Märchencharaktere beteiligt ist;
               Jede Szene besteht ihrerseits aus einer zeitlichen Abfolge von Dialogakten zwischen den Märchencharakteren oder vom Erzähler zum Zuhörer. Zusammengenommen bilden diese Dialogakte den Märchentext.
            
            Im Folgenden werden die verschiedenen Bestandteile, sowie ihre Eigenschaften und Beziehungen untereinander, näher beschrieben.
            
               Orte, an denen das Märchen spielt, werden nur über ihren 
                    Typ (Attribut type) charakterisiert. Mögliche Ortstypen sind dabei z.B. Wald, Schloss oder Stall. Daneben existiert außerdem der Typ „Nirgendwo“ für Szenen ohne eindeutig bestimmbaren Ort (z. B. Abschnitte des Märchens, an denen nur der Erzähler beteiligt ist). Jeder Ort erhält eine spezifische ID der Form 
                    loc1, 
                    loc2 etc.
                
            
               Charaktere werden über eine Reihe von Eigenschaften beschrieben, welche zum einen inhärente demographische Eigenschaften (Name, Alter, Geschlecht, Typ), sowie zum anderen externe Eigenschaften (Einstellung, Propp-Archetyp – s. (Propp 1977)) beinhalten. Beim 
                    Namen (name) des Charakters handelt es sich um eine Zeichenkette, z.B. „Rapunzel.“ (Wird ein Charakter auf mehrere Arten gerufen, so wird die häufigste Bezeichnung gewählt.) 
                    Alter (age) des Charakters wird nicht in Zahlen, sondern in Stufen angegeben, da Märchen im Allgemeinen keine genauen Altersangaben enthalten; die möglichen Werte sind dabei „toddler“, „child“, „teenager“, „young adult“, „adult“ und „senior“. Das 
                    Geschlecht (gender) des Charakters wird den klassischen Vorstellungen folgend entweder mit „male“ oder „female“ angegeben. Zusätzlich gibt es den Wert „none“ für geschlechtlich unterspezifizierte Charaktere wie Tiere, Monster usw. Der 
                    Typ des Charakters unterscheidet z. B. zwischen „human“ oder „animal/monster“. Für „animal/monster“ unterscheiden wir zusätzlich nach 
                    Subtypen, z.B. für Tiere nach Größe, also „small“, „medium“ oder „big“, oder „witch“ und „demon“ für einen bestimmten Monstertyp. Eine binäre Feststellung der 
                    Einstellung bzw. Gesinnung des Charakters verortet diesen auf der Gut-/Böse-Achse: „evil“ oder „neutral“. Außerdem wird der 
                    Propp-Archetyp des Charakters angegeben: „hero“, „villain“ etc. (Propp, 1977). Jeder Charakter erhält eine spezifische ID von der Form 
                    ch1, 
                    ch2 usw. Außerdem gehören zu jedem Märchen zwei „Dummy“-Charaktere für Erzähler und Zuhörer, welche stets die IDs 
                    ch0 bzw. 
                    ch-1 und die Typen „narrator“ bzw. „listener“ zugewiesen bekommen. Dies ist nötig, um auch Passagen darstellen zu können, welche vom Erzähler gesprochen werden, der selbst ja kein eigentlicher Charakter der Handlung ist. Dies ist notwendig, um ein automatisches „Vorlesen“ des Märchens zu implementieren.
                
            
               Szenen werden im Hinblick auf Zeit, Ort, beteiligte Charaktere sowie Propp Funktionen (Propp, 1977) beschrieben. Der 
                    Zeitpunkt (time), zu dem die Szene spielt, wird anhand einer ID der Form 
                    t1, 
                    t2 usw. angegeben, wobei die IDs den linearen Ablauf der Zeit darstellen. Der 
                    Ort (location), an dem die Szene spielt, wird als String in Großbuchstaben angegeben, ausgewählt aus einer Liste mit Möglichkeiten. Der 
                    Übergang zur nächsten Szene (transition) wird ebenfalls codiert, indem das Bewegungsverb, das den Übergang von einem Ort zum anderen beschreibt, oder die Phrase, die stattdessen den Szenenwechsel einleitet, angegeben wird. Die an der Handlung der Szene beteiligten 
                    Charaktere werden mit ihren IDs angegeben, also z. B. 
                    ch2, 
                    ch3, 
                    ch5. Dabei werden alle Charaktere berücksichtigt, die in der Szene zugegen sind, auch wenn diese bspw. nicht sprechen. Die Propp-Funktionen und -Subfunktionen der Szene werden mit ihrem Symbol (nach der englischen Ausgabe Propp (1977)) angegeben, also z. B. A4 – „theft of daylight“. Jede Szene erhält eine spezifische ID der Form 
                    s1, 
                    s2 etc. Da die Märchenhandlung im Allgemeinen linear erzählt wird, ist der Index üblicherweise (aber nicht notwendigerweise) identisch mit demjenigen des Zeitpunkts der Szene, d. h. die Szene 
                    s1 wird üblicherweise zum Zeitpunkt 
                    t1 spielen usw. Jeder Szene sind 
                    Dialogakte untergeordnet, denen der zu dieser Szene gehörige Text entspricht.
                
            
               Dialogakte werden im Hinblick auf ihre Sprecher und Adressaten, ihren Inhalt sowie ihren Zeitpunkt beschrieben. Der 
                    Zeitpunkt (time), zu dem der Dialogakt geäußert wird, wird anhand einer ID angegeben, welche eine Spezifizierung der ID des Zeitpunkts der zugehörigen Szene darstellt. Spielt z. B. Szene 
                    s5 zum Zeitpunkt 
                    t5, so haben die zugehörigen Dialogakte die Zeitpunkte 
                    t5.1, 
                    t5.2 usw. Der 
                    Sprecher (speaker) des Dialogakts wird über seine ID angegeben. Der 
                    Adressat bzw. die 
                    Adressaten (receiver) des Dialogakts werden über eine Liste von Charakter-IDs angegeben, z.B. 
                    ch2, 
                    ch4, 
                    ch6. Passagen des Erzählers stellen dabei einen Spezialfall dar: Sie werden als Dialogakte des Erzählers mit dem Zuhörer bzw. Leser betrachtet, d. h. der „Dummy“-Charakter des Erzählers wird als Sprecher angegeben und der Dummy-Charakters des Zuhörers als Empfänger. Abgesehen davon werden sie behandelt wie Dialogakte zwischen Charakteren. Jeder Dialogakt erhält eine spezifische ID, die – unabhängig von der Szenestruktur – linear hochgezählt wird, also 
                    d1, 
                    d2 usw.
                
         
         
            XML-Repräsentation
            Die oben beschriebenen Informationen lassen sich im XML-Format darstellen. Dabei wird eine XML-Baumstruktur genutzt, um die Hierarchie der verschiedenen Objekte zu repräsentieren. Das Wurzelelement des Dokuments hat stets den Bezeichner Tale und die Attribute „title“ und „annotator“, welche Titel und den Namen des Annotators des jeweiligen Märchens enthalten:
            
               1: Struktur des Tale-Wurzelelements (Beispiel).
                ... 
               Diesem Element untergeordnet sind die Elemente Characters, Locations und Text. Das Characters-Element enthält Character-Subelemente, die jeweils die gesammelten Informationen für einen Charakter speichern:
            
            
               2: Struktur des Characters-Elements (Beispiel).
                
               Analog dazu enthält das Locations-Element untergeordnete Location-Elemente, die jeweils einen Ort codieren:
            
            
               3: Struktur des Locations-Elements (Beispiel).
                
               Das Text-Element enthält schließlich den eigentlichen Märchentext. Dieser ist auf die verschiedenen Szenen – repräsentiert durch Szene-Elemente – aufgeteilt, welche wiederum die verschiedenen Dialogakte (Dialog-Elemente) enthalten:
            
            
               4: Struktur des Text- und Szene-Elemente (Beispiel).
               
               ...
                Ach, du bist’s, alter Wasserpatscher, 
               ...
               
               Beim Entwurf des XML-Schemas wurde besonders Wert auf Übersichtlichkeit und Leserlichkeit gelegt. Trotz der Vielzahl der kodierten Informationen sind die resultierenden XML-Dateien daher vergleichsweise kompakt; so besteht die XML-Repräsentation des (vergleichsweise langen) Märchens „
                        Hänsel und Gretel“ bspw. nur aus 226 Zeilen.
                    
               Diese XML Repräsentation basiert auf und erweitert das Annotation Schema, das in (Scheidel & Declerck, 2010) beschrieben wird.
            
         
         
            Python-Repräsentation
            Auf der Grundlage der oben beschriebenen XML-Struktur kann eine Python-Klassenstruktur aufgebaut werden, die ein Märchen sowie seine einzelnen Teile als Python-Objekte repräsentiert.
            Neben einer Oberklasse Tale gibt es für jeden der oben beschriebenen Teile eine eigene Python-Klasse, d. h. die Klassen Location, Character, Scene und Dialogue. (Insgesamt bestehen die Dateien zur Märchen-Repräsentation aus 288 Zeilen Code.) Jede Klasse enthält dabei als Attribute die oben beschriebenen Eigenschaften, wobei diese auch Verweise auf andere Elemente darstellen können. So verweisen bspw. Dialogue-Objekte auf die Character-Objekte von Sprecher und Empfängern. Der Python-Code dient als Interface für drei Anwendungen. Erstens können Märchen aus bestehenden XML-Dateien eingelesen werden; zweitens können XML-Dateien anhand einer anderweitig (z. B. durch automatische Klassifizierung) erzeugten Python-Märchenstruktur generiert werden; und drittens kann anderer Python-Code auf die Märchen-Information zugreifen, was die Grundlage für Anwendungen wie Text-to-Speech oder Visualisierung bildet. Sowohl die XML Kodierung als auch die Python Objekte interagieren mit einer Märchen-Ontologie interagieren, die eine Erweiterung der in (Koleva et al., 2012) beschriebenen Ontologie ist.
            Somit haben wir eine formale Repräsentation von Märchen, die in verschiedenen Anwendungen zum Tragen kommen kann.
         
      
      
         
             Mit Beiträgen von Anastasija Aman, Stefan Grünewald, Matthias Lindemann, Lisa Schäfer, Natalia Skachkova.
         
         
            
               Bibliographie
               
                  Propp, Vladimir ; Scott, Laurence (Hrsg.): Morphology of the folktale. 2. überarbeitete Auflage. Austin, TX u.a., 1977
               
                  Antonia Scheidel and Thierry Declerck. 2010. Apftml - augmented proppian fairy tale markup language. In Sándor Dar´anyi and Piroska Lendvai, editors, First International AMICUS Workshop on Automated Motif Discovery in Cultural Heritage and Scientific Communication Texts. Szeged University.
               
                  Nikolina Koleva, Thierry Declerck, and Hans-Ulrich Krieger. 2012. An ontology-based iterative text processing strategy for detecting and recognizing characters in folktales. In Jan Christoph Meister, editor, Digital Humanities 2012 Conference Abstracts, pages 467–470, Hamburg, 7. University of Hamburg, Hamburg University Press
            
         
      
   



      
         
            Einleitung
            Der Sammelauftrag der Deutschen Nationalbibliothek (DNB) beginnt 1913 und bezieht sich auf »lückenlos alle deutschen und deutschsprachigen Publikationen« (»Wir über uns«, 16.03.2017). Der DNB-Katalog ist natürlich längst digitalisiert und die Arbeit mit ihm mittlerweile sehr komfortabel, da der Datendienst der DNB unter http://www.dnb.de/datendienst vierteljährlich einen Komplettabzug der Katalogdaten im RDF-Format bereitstellt, unter der freien Lizenz CC0 1.0. Momentan (Stand vom 23.06.2017) enthält er 14 102 309 Datensätze, also Metadaten zu von der DNB gesammelten Medien. Bisher gibt es aus geisteswissenschaftlicher Sicht nur wenige Versuche, diese Quelle nutzbar zu machen (eine Ausnahme bilden etwa Häntzschel u. a. 2009). Wir präsentieren ein einfaches Framework, mit dem verschiedene Aspekte des DNB-Katalogs untersucht werden können, seine Entwicklung über die knapp 105 Jahre seit Bestehen der Nationalbibliothek (vgl. auch Schmidt 2017, der für die Library of Congress einen ähnlichen Ansatz vorgestellt hat). Wir konzentrieren uns dabei auf Romane als Untersuchungsobjekt, von denen in der DNB rund 180 000 als solche rubriziert sind (dies entspricht nicht der Gesamtanzahl an Romanen, denn Nachauflagen und Übersetzungen zählen dort mit hinein – außerdem fehlen auch einige Romane, da sie nicht entsprechend verschlagwortet worden sind. Dieser Vortrag ist methoden-, nicht vorderhand ergebniszentriert, wobei wir an zwei Anwendungsszenarien aus der Praxis der digitalen Literaturwissenschaft demonstrieren, wie Katalogmetadaten bei der Bearbeitung konkreter Forschungsfragen behilflich sein können bzw. diese überhaupt erst ermöglichen.
         
         
            Beschreibung des Frameworks
            Die Titeldaten der DNB werden in typischen Linked-Data-Formaten (RDF/XML, JSON-LD usw.) angeboten. Der übliche Ansatz mit solchen Daten zu arbeiten ist, diese in eine geeignete Datenbank (Triple-Store) einzuladen und Anfragen mit Hilfe der entsprechenden Anfragesprache (i. A. SPARQL) zu stellen. Prinzipiell sind auch andere Systeme (z. B. relationale Datenbank, Suchmaschine) geeignet. Dies ermöglicht sehr flexible Anfragen und die leichte Einbindung weiterer Datenquellen. Da die Größe der Daten (unkomprimiert ca. 21 GB) jedoch gewisse Anforderungen an die Hardware stellt und die Konfiguration und Optimierung der Datenbank aufwendig ist, haben wir uns für eine andere, kompakte und leichter nachzuvollziehende Lösung entschieden. Langfristiges Ziel ist jedoch die Bereitstellung einer fertig konfigurierten Arbeitsumgebung in Form eines Docker-Containers, in der die Daten in einer Datenbank ad hoc verfüg- und analysierbar sind.
            Der Titeldatensatz ist mit 14 102 309 Datensätzen und 227 212 707 Tripeln (»Fakten«) sehr umfangreich und enthält neben Angaben zu Büchern auch Angaben zu weiteren Medientypen wie etwa Zeitschriften. Neben den üblichen Metadatenfeldern wie Titel und Erscheinungsjahr ist bei Buchobjekten meist auch die Seitenanzahl sowie das Format vermerkt. Ganz im Sinne von Linked Data werden viele Angaben mit Hilfe von standardisierten Vokabularien (z. B. Dublin Core oder Bibo) beschrieben und ermöglichen so die Verlinkung mit weiteren Datensätzen. Insbesondere ermöglicht die Angabe der Autor*innen durch die numerische Kennung aus der Gemeinsamen Normdatei (GND) die Verknüpfung der Daten mit Wikidata, der (zukünftig) hinter Wikipedia stehenden Faktendatenbank. Wikidata verwendet ein auf Linked Data basierendes Datenmodell und ermöglicht, ähnlich wie Wikipedia, jedermann das Hinzufügen und Bearbeiten von Daten. Neben Angaben zu Städten und Ländern (z. B. Fläche, Einwohnerzahl) sind in Wikidata auch Daten zu zahlreichen Persönlichkeiten gespeichert, etwa deren Namen, Geburtsdaten, Berufe, Werke und, falls vorhanden, GND-Kennung (als Beispiel sei auf die Seite zu Johann Wolfgang von Goethe verwiesen: https://www.wikidata.org/wiki/Q5879).
            Unser Framework umfasst derzeit vier Schritte, die im Folgenden beschrieben werden:
            
               Vorverarbeitung und Konvertierung der Daten von RDF/XML zu JSON (rdf2json.py)
               RDF/XML wird von den üblichen Softwaretools im Allgemeinen nicht als Datenstrom verarbeitet, sondern im Hauptspeicher abgelegt und dann weiterverarbeitet. Aufgrund der Größe der Daten scheidet diese Möglichkeit aus. Da jedoch alle wesentlichen Daten zu einem Medium typischerweise innerhalb eines XML-Tags "rdf:Description" abgelegt sind, können wir die Daten auch mit Hilfe eines SAX-Parsers als XML verarbeiten. Wir extrahieren die für die Analyse wesentlichen Metadaten (z. B. dcterms:contributor, dcterms:language, dc:title, dcterms:extent, rdau:P60493) und speichern diese als JSON ab. JSON ist im Allgemeinen platzsparender als RDF/XML und kann leicht in Elasticsearch eingeladen werden, was ein geplanter nächster Schritt ist.
            
            
               Extraktion von Daten zu Autoren aus Wikidata (WKD-Toolkit)
               Unser Ziel ist die Anreicherung der Autorenangaben im DNB-Datensatz mit Informationen aus Wikidata, beispielsweise Geburtsdatum- und ‑ort, Beruf und Verweis auf einen etwa vorhandenen Artikel in Wikipedia. Da die Python-Softwarebibliothek zur Verarbeitung von Wikidata-Datensätzen veraltet ist, greifen wir auf das Java-basierte Wikidata Toolkit zurück. Nach Herunterladen des aktuell (14.08.2017) 16 GB großen komprimierten Wikidata-Datensatzes extrahieren wir in zwei Durchgängen zunächst alle Elemente mit einer GND-Kennung einschließlich ausgewählter Merkmale und ergänzen im zweiten Durchlauf die Werte der Merkmale. Das Ergebnis speichern wir im JSON-Format.
            
            
               Normalisierung und Anreicherung der Daten (json2json.py)
               Unser Python-Skript implementiert eine Pipeline, die alle in den vorherigen Schritten extrahierten Daten einliest und mit Hilfe der GND-Kennung verknüpft, Metadatenangaben (wie z. B. Seitenanzahlen) extrahiert, vereinfacht und normalisiert, Datensätze mit fehlenden Angaben filtert und schließlich die gewünschten Datenfelder spaltenbasiert ausgibt. Die Vereinfachung umfasst vor Allem das Entfernen von Namespace-Präfixen (etwa http://id.loc.gov/vocabulary/iso639-2/ bei der Angabe der Sprache); Seitenanzahlen werden mit Hilfe eines regulären Ausdrucks extrahiert, der die häufigsten Fälle abdeckt; Jahreszahlen ebenso; Verlagsnamen können mit Hilfe einer Normtabelle normiert werden (dies ist nötig, da die Schreibung dieser Namen innerhalb des Katalogs nicht standardisiert ist).
            
            
               Analyse der Daten (Shell-Skripte und -Tools wie awk, sort, datamash, gnuplot, ...)
               Die entstandenen Dateien im TSV-Format können mit den üblichen Unix-Kommandozeilen-Werkzeugen wie awk, sort, uniq etc. leicht verarbeitet und analysiert werden; Visualisierungen wurden mit gnuplot erzeugt. Alle Schritte sind im GitHub-Repository dokumentiert.
            
         
         
            Zeitliche Entwicklung über 105 Jahre DNB
            Abbildung 1 zeigt die zeitliche Verteilung einiger Subdatensätze des Katalogs. Von den etwa 14,1 Mio. Objekten im originalen DNB-Datensatz weisen etwa 8,3 Mio. extrahierbare Seitenanzahlen auf (59 %). Beschränken wir diese Anzahl auf ›Romane‹ (über das Datenfeld "rdau:P60493"), bleiben 353 498 übrig, von denen wiederum 316 518 Umfangsangaben aufweisen und 180 219 einen Verfasser, der mindestens einen Wikipedia-Eintrag (in egal welcher Sprache) besitzt. Dieses Datenset ist die Grundlage für die unten folgenden Anwendungsszenarien.
            
               
                  
                  Abbildung 1: Fünf verschieden qualifizierte Subdatensätze des DNB-Katalogs in zeitlicher Verteilung.
               
            
         
         
            Repräsentativität
            Als möglicher Plausibilitäts- bzw. Repräsentativitätstest kann das Auszählen derjenigen Romanciers dienen, die mit den meisten Romanen im Katalog vertreten sind. Da der DNB-Katalog Vollständigkeit anstrebt, kann ein entsprechendes Ranking etwas über vergangene Realitäten auf dem deutschsprachigen Buchmarkt aussagen (Tab. 1), und tatsächlich stehen die Verfasser*innen von Romanbestsellern im Unterhaltungsbereich ganz oben (die Anzahl der Bücher umfasst von der DNB mitgesammelte Neuauflagen, Konsalik hat also nicht über 2 000 Romane geschrieben).
            
               
                  Autor*in
                  Romane
               
               
                  Heinz G. Konsalik
                  2232
               
               
                  Marie Louise Fischer
                  1264
               
               
                  Gert Fritz Unger
                  1013
               
               
                  Georges Simenon
                  783
               
               
                  Utta Danella
                  778
               
               
                  Edgar Wallace
                  654
               
               
                  Hedwig Courths-Mahler
                  647
               
               
                  Eleanor Hibbert
                  635
               
               
                  Pearl S. Buck
                  596
               
               
                  Alistair MacLean
                  582
               
               
                  Stephen King
                  577
               
               
                  Georgette Heyer
                  576
               
               
                  Agatha Christie
                  574
               
               
                  Theodor Fontane
                  565
               
               
                  Hans Ernst
                  563
               
               
                  Lion Feuchtwanger
                  501
               
               
                  Erich Maria Remarque
                  419
               
               
                  Hans Hellmut Kirst
                  411
               
               
                  Johannes Mario Simmel
                  403
               
               
                  Hans Fallada
                  396
               
               
                  Heinrich Mann
                  394
               
               
                  Fjodor Dostojewski
                  390
               
               
                  Barbara Cartland
                  390
               
               
                  Nora Roberts
                  381
               
               
                  Graham Greene
                  375
               
               
                  A. J. Cronin
                  370
               
               
                  Vicki Baum
                  366
               
               
                  Thomas Mann
                  359
               
               
                  Robert Ludlum
                  358
               
               
                  Gerd Hafner
                  357
               
               
                  Dean Koontz
                  354
               
               
                  Heinrich Böll
                  340
               
               
                  Alexandra Cordes
                  325
               
               
                  John le Carré
                  322
               
               
                  Marion Zimmer Bradley
                  321
               
               
                  Jason Dark
                  317
               
               
                  Willi Heinrich
                  313
               
               
                  Ludwig Ganghofer
                  311
               
               
                  Jack London
                  309
               
               
                  Joseph Roth
                  307
               
               
                  Danielle Steel
                  299
               
               
                  Johanna Lindsey
                  288
               
               
                  Erle Stanley Gardner
                  287
               
               
                  Siegfried Lenz
                  279
               
               
                  Jules Verne
                  277
               
               
                  Rosamunde Pilcher
                  274
               
               
                  Franz Kafka
                  271
               
               
                  Ernest Hemingway
                  271
               
               
                  Taylor Caldwell
                  269
               
               
                  Dorothy L. Sayers
                  269
               
            
            Tabelle 1: Romanautor*innen geordnet nach Anzahl der Werke (inkl. Nachauflagen) im DNB-Katalog.
         
         
            Anwendungsfall 1: Buchtitel
            Die Verfügbarkeit großer digitalisierter Kataloge ermöglicht Large-Scale-Analysen bibliografischer Metadaten, etwa die Entwicklung von Romantiteln. Ein Vorläufer auf diesem Gebiet, Werner Bergengruens immer noch zu empfehlende Bibliothekarsfantasie »Titulus« von 1960, musste sich noch auf eine manuelle Sammlung des Autors stützen. Mittlerweile gibt es mit Franco Morettis Studie »Style Inc.« (2009) ein prominentes datengestütztes Beispiel (wobei sich Moretti bei seiner Analyse von um die 7 000 Romantiteln auf Fachbibliografien stützte, nicht auf Katalogdaten).
            Um einen ersten Einblick in das Vokabular von Romantiteln zu bekommen, seien in Tabelle 2 die am häufigsten vorkommenden Substantive aufgelistet.
            
               
                  Substantiv
                  Frequenz
               
               
                  Liebe
                  3117
               
               
                  Mann
                  1906
               
               
                  Frau
                  1686
               
               
                  Tod
                  1537
               
               
                  Nacht
                  1505
               
               
                  Leben
                  1496
               
               
                  Welt
                  1188
               
               
                  Haus
                  1158
               
               
                  Zeit
                  1037
               
               
                  Schatten
                  1029
               
            
            Tabelle 2: Häufigste Substantive in Romantiteln im gesamten DNB-Katalog.
            Überzeitliche Konzepte – Liebe, Tod usw. – dominieren das Feld. Und nebenbei bemerkt: Ein wenig erinnert diese Liste an Jan Böhmermanns satirischen Song »Menschen, Leben, Tanzen, Welt«, mit dem auf die Beliebig- und Austauschbarkeit kontemporärer deutschsprachiger Liedproduktion angespielt wird (vgl. Pandzko/Böhmermann 2017), ein Befund, der sich analog auch auf Romantitel projizieren ließe.
            Diese Anfragetechnik kann – wie beim Google Ngram Viewer – auf n-Gramme ausgedehnt werden, die Top-10 der häufigsten Trigramme findet sich in Tabelle 3.
            
               
                  Trigramm
                  Frequenz
               
               
                  Das Geheimnis der
                  238
               
               
                  Das Haus der
                  224
               
               
                  Der Mann der
                  189
               
               
                  Das Geheimnis des
                  175
               
               
                  Die Tochter des
                  160
               
               
                  Im Schatten des
                  128
               
               
                  Der Mann im
                  128
               
               
                  Das Lied der
                  125
               
               
                  Die Frau des
                  124
               
               
                  Die Reise nach
                  108
               
            
            Tabelle 3: Häufigste Trigramme in Romantiteln im DNB-Katalog.
            Ebenfalls analog zum Ngram Viewer lässt sich die zeitliche Entwicklung von n-Gramm-Frequenzen darstellen. Die unterschiedlichen Darstellungen in absoluten (Abb. 2) und relativen Zahlen (Abb. 3) kann etwa zeigen, dass sich zwischen Mitte der 1970er-Jahre und Mitte der 1990er-Jahre die Zahl an Romanen mit »Liebes«-Titeln zwar nahezu verdoppelt, dass sich diese Titel aber in relativen Zahlen nicht großartig vermehren.
            Für genauere Analysen auf Grundlage dieser Extraktions- und Visualisierungsmethoden stellt das von uns vorgestellte Framework eine ideale Basis dar.
            
               
                  
                  Abbildung 2: Vorkommen ausgewählter Wörter in Romantiteln im zeitlichen Verlauf (absolut).
               
            
            
               
                  
                  Abbildung 3: Vorkommen ausgewählter Wörter in Romantiteln im zeitlichen Verlauf (relativ).
               
            
         
         
            Anwendungsfall 2: Textumfang
            Unser zweites Anwendungsszenario betrifft die Erforschung des literarischen Textumfangs. Abbildung 4 zeigt die durchschnittliche Seitenanzahl von Romanen im Katalog der DNB.
            Als Zuarbeit zu einer Theorie des literarischen Textumfangs haben wir mit dem von uns hier vorgestellten Framework in einer umfangreicheren Studie untersucht, wie sich der Umfang von Romanen etwa auf die Kanonbildung auswirkt (längere Romane, speziell solche von mehr als 1 000 Seiten Umfang, haben es leichter, in Kanonlisten zu landen). Außerdem ist es uns gelungen zu zeigen, wie umfangreiche Romane die DNA von Verlagen bestimmen können (vgl. Fischer/Jäschke 2018).
            
               
                  
                  Abbildung 4: Entwicklung der mittleren Seitenanzahl pro Jahr seit 1913.
               
            
         
         
            Fazit
            Katalogdaten als Untersuchungsobjekt der quantifizierenden Literaturwissenschaften sind keine sich selbst erklärende Quelle, sondern ein über Jahrhunderte gewachsenes, überaus komplexes System. Die bibliothekarische Betreuung dieser Daten zielt nicht per se auf literaturwissenschaftliche Anwendungsfälle. Die Verschlagwortung kann lückenbehaftet sein, bestimmte Angaben wie etwa zum Textumfang können Fehler aufweisen. Die literaturwissenschaftliche Beschäftigung mit Katalogdaten setzt deren Explorier- und Kontrollierbarkeit voraus, wozu das hier vorgestellte Framework einen ersten Beitrag leisten soll. Zwei konkrete Anwendungsfälle sollten als Praxisbeispiele und ausdrücklich als Anreiz für weitere Szenarien dienen.
         
      
      
         
            
               Bibliographie
               Das 
                        Arbeitsrepositorium ist unter > zu finden.
                    
               
                  Bergengruen, Werner (1960): Titulus. Das ist: Miszellen, Kollektaneen u. fragmentar., mit gelegentl. Irrtümern durchsetzte Gedanken zur Naturgeschichte d. dt. Buchtitels oder unbetitelter Lebensroman e. Bibliotheksbeamten. Zürich: Verlag der Arche.
                    
               
                  DNB (2017): »Wir über uns«, Stand 16.03.2017. URL: >.
                    
               
                  Fischer, Frank; Jäschke, Robert (2018): Ein Quantum Literatur. Empirische Daten zu einer Theorie des literarischen Textumfangs. DFG-Symposium »Digitale Literaturwissenschaft«. Villa Vigoni, 9.–13. Oktober 2017. (Entsprechender Sammelband erscheint demnächst.)
                    
               
                  Häntzschel, Günter; Hummel, Adrian; Zedler, Jörg (2009): Deutschsprachige Buchkultur der 1950er Jahre. Fiktionale Literatur in Quellen, Analysen und Interpretationen. Wiesbaden: Harrassowitz 2009. URL: >.
                    
               
                  Moretti, Franco (2009): Style Inc. Reflections on Seven Thousand Titles (British Novels, 1740–1850). In: Critical Inquiry, Vol. 36, No. 1 (Autumn 2009), S. 134–158.
                    
               
                  Pandzko, Jim
                  ; 
                  Böhmermann, Jan (2017): Menschen Leben Tanzen Welt [Musikvideo]. In: Neo Magazin Royale, 05.04.2017. URL: >.
                    
               
                  Schmidt, Ben (2017): A brief visual history of MARC cataloging at the Library of Congress. In: Sapping Attention [Blog], 16.05.2017. URL: >.
                    
            
         
      
   



      
         
            Introduction and Motivation
            The emergence of computational methods of text processing has created new paradigms of research in literary studies in recent years (Jockers & Underwood, 2016), for instance 
                    distant reading to find patterns and regularities (Moretti, 2005). Network analysis and extraction of information about relations between characters from literary texts is an example for distant reading methods. Such information can not only be helpful for better understanding of character interactions but can also facilitate the comparison of thereof in different texts.
                
            Existing tools of text analysis and network visualization such as Voyant
                     or Gephi
                     are either missing modules for character network analysis or require preliminary steps on data preprocessing from the user and therefore are not easy-to-use for some humanities scholars who lack programming skills. Interactive tools in addition often lack features to ensure reproducibility of results.
                
            We present our ongoing effort on closing this gap by developing a literary analysis reporting tool 
                    rCAT
               
                  
               , whose primary purpose is to provide an easy-to-use, stable, and reusable solution for automatic extraction of relational information from text and to characterize these relationships automatically to provide the user with deeper qualitative insight. We opt for implementation as a web-based reporting tool instead of an interactive tool for two reasons: (1) automatically generated reports in PDF format can serve as a stable foundation for discussion and can be reused in publications and visualizations easily, and (2) the results are clearly connected to the chosen input parameters such that reproducibility of results is ensured.
                
            As a use-case study, we apply 
                    rCAT to Johann Wolfgang von Goethe's epistolary novel 
                    Die Leiden des jungen Werthers. On the basis of this epistolary novel, we show that not only the network can be generated, but also the characteristic triangular relationship of the protagonists is easily identified. The goal is to automatically determine this triad in the original text and in the adaptations that have been published since the publication of 
                    Werther in 1774.
                
         
         
            
               Previous Work
                
            Previous research on social networks in literary fiction generally fall into one of the two categories: (1) works that explore methods for extracting and formalizing character networks (
                    cf., Elson et al. (2010), Agarwal et al. (2012, 2013), Park et al. (2012)), and (2) works that primarily focus on qualitative implications of network analysis (
                    cf., Rydberg-Cox (2011), Moretti (2011), Nalisnick & Baird (2013), Jayannavar et al. (2015)). It is common to address both tasks at the same time, as in Beveridge & Shan (2016), who introduce a number of formal measures for analyzing the centrality of the characters in 
                    Game of Thrones books, which results in both expected and surprising findings. 
                
            Building on graph theory extensively elaborated in the past fifty years (e.g., Bondy and Murty, 1976 or West, 2001), our work is similar to Beveridge & Shan (2016), in particular, in terms of the weighted degree measure, and to Park et al. (2012), in terms of distance measure for detecting closely related characters in a text.
         
         
            
               Methods
                
            In the following, we explain the different components in 
                    rCAT, which are available for text analysis. After that, we discuss the results based on a use-case study.
                    
            
            
               Character lists and character identification
               To detect character mentions in the text we use a fundamental named-entity recognition approach based on dictionaries. This approach is suitable for scholars who analyze texts they already know. Consequently, we opt for a transparent and simple character recognition procedure: The user provides a list of character names to be included in the analysis specifying a canonical name form and all variations thereof she would like to take into account (
                         e.g., “Lotte” is the canonical name and “Lotten”, “Lottens”, “Lottgen”, “Lottchen”, “Charlotten S.”. are its variants).
                     
            
            
               Relation detection and context words
               We define the closeness of relationship between two characters using a 
                    distance measure
                  dist
                  X
                  (p,q), where 
                    p and 
                    q are the strings corresponding to these characters and 
                    X is the number of tokens between them (Park et al., 2012). In addition, we introduce the 
                    context measure
                  cont
                  Y
                  (p,q), where 
                    p and 
                    q are the strings corresponding to these characters and 
                    Y is the number of tokens before the character 
                    p and after the character 
                    q. While the former measure allows for detecting those characters that are closely related to each other, the latter one enables a contextual analysis of their relationship.
                
            
            
               Network analysis
               We visualize the network of characters with an undirected graph 
                    G=(V,E), where 
                    V are the vertices, each vertex corresponding to one character, and each edge 
                    E=(V
                  i,
                  ,V
                  j
                  ) corresponding to relations between pairs of characters. We output the following measures for each character node: 
                    degree, 
                    edge weight, 
                    weighted degree and 
                    density. The degree is the number of edges occurring with a given vertex. The edge weight, 
                    w
                  i,j
                   ≥ 0, is defined as the number of interactions between the vertices V
                    i and V
                    j. The weighted degree is the sum of weights of the edges occurring with a vertex 
                    i. Density is the ratio of occurring edges between two vertices and all possible vertex pairs.
                    
            
            
               Word clouds
               Word clouds are an approach to visualize the vocabulary of a text. The size of one word corresponds to its frequency. We use two different kinds of word clouds: For each character in the character list, we show word clouds based on the context of a window size 
                        n. For each pair of characters occurring in the network, we present a word cloud based on the words between them as well as on the words found in the context. Both types of word clouds can be filtered to the specific word fields (words from specific domains) which is helpful in gaining a focused insight into the characters relations.
                    
            
            
               Word Field developments
               We plot the timeline of multiple predefined world fields (specified by word lists) in the text. This feature is helpful in representing how certain fields (
                      e.g., concepts, emotions) develop throughout the narrative (Kim et al., 2017).
                   
            
            
               Implementation
               The tool was developed using Python v.3.6 and the Flask
                     web development framework. The tool outputs a single PDF report. The resulting document contains information from the analysis modules described in the previous section. Network graphs included in the report are generated with 
                    graphviz. Additionally, the tool can generate a CSV file that can be used as input to Gephi. 
                    
            
         
         
            
               Use-case Demonstration
                
            For a use-case analysis, we apply 
                    rCAT to 
                    Die Leiden des jungen Werther by Johann Wolfgang Goethe with the following parameters: X=8, Y=5, stop words removed (previous work focused on this analysis without rCAT, 
                    cf. Murr, 2017).
                
            In Goethe's epistolary novel, the protagonist Werther describes his unhappy love for Lotte, who is engaged to Albert. The characteristic triangular relationship in the novel arises from this constellation (protagonist - beloved woman - antagonist). With 
                    rCAT we expect to identify and characterize this relationship. Figures 1 and 2 show a sample network analysis output (tables are shown only partly).
                
            The protagonist Werther shows a degree of 21, which is the number of characters with whom he interacts. The closest relationship measured by edge weight (Figure 2) is observed between Werther and Lotte (81 interactions). The antagonist Albert has a low degree of 3. However, his weighted degree is 36 (third highest after Werther and Lotte), which confirms his important role in the triangular relationship.
            
               
                  
                  Illustration 1: Degrees and weighted degrees for most important characters of Goethe’s Werther
               
            
            
               
                  
                  Illustration 2: Edge weights
               
            
            
               
                  
                  Illustration 3: Complete network of Goethe’s Werther
               
            
            Highlighted in red is the typical triangular relationship in Goethe’s novel, which corresponds to the three highest weighted degrees. In further steps, we will use 
                    rCAT to analyze the adaptations of Goethe's novel with a focus on this triad.
                
            To better characterize the edges, the tool outputs top-
                    n word clouds sorted by edge weight (
                    n is specified by the user) for character pairs and by degree for single characters. Figure 4 and 5 show examples of the word clouds for character pairs filtered to the words from the emotion domain.
                
            
               
                  
                  Illustration 4: Word clouds for Werther-Lotte
               
               
                  
                  Illustration 5: Werther-Albert
               
            
            The word clouds enable first conclusions about the relationships of the characters. Werther and Lotte's word cloud characterizes their ambivalent relationship. The key words "Leidenschaft" and "Freude" reflect Werther's love, whereas the mentions of "sterben" and "Verblendung" are characteristic of the unrequited love, which leads Werther into his "disease unto death". As Werther and Albert’s word cloud reveals, their relationship is dominated by the "Unruhe" that Werther feels through his adversary. 
            Additionally, the tool plots the development of the narrative (not bound to specific characters) based on the word fields, an example of which is shown on Figure 6. In this case we used words from the emotion domain (with emotion dictionaries by Klinger et al. (2016)).
            
               
                  
                  Illustration 6: Word field development for Goethe’s Werther
               
            
            The word field development can highlight the prevalence of individual emotion domains across the text. The accumulation of the negative emotion words (Wut,Trauer, Furcht) towards the end suggests, for example, that Goethe’s novel has no “happy ending”. The striking rash on “Freude”, however, captures the last happy hours Werther spends with Lotte in the second part of the narration before he kills himself.
            
               
                  Future Work
                    
               The next version of the tool will include a character-oriented word field development calculated and plotted for the main characters of the stories. In addition, future releases will include more analysis features and bulk file processing.
            
         
      
      
         
            
               
            
            
               
            
            
               
                  www.ims.uni-stuttgart.de/data/rcat
               
            
            
               
            
         
         
            
               Bibliographie
               
                  Agarwal, A. / Corvalan, A. / Jensen, J. / Rambow, O. (2012): “Social Network Analysis of Alice in Wonderland”, in: CLfL@ NAACL-HLT 88-96.
               
                  Agarwal, A. / Kotalwar, A. / Rambow, O. (2013): “Automatic Extraction of Social Networks from Literary Text. A Case Study on Alice in Wonderland”, in: IJCNLP 1202-1208.
               
                  Beveridge, A. / Shan, J., (2016): “Network of thrones”, in: Math Horizons, 23(4): 18-22.
               
                  Bondy, J.A. / Murty, U.S.R. (1976): Graph theory with applications (Vol. 290). London: Macmillan.
               
                  Burrows, J.F. (1987): “Word-patterns and story-shapes: The statistical analysis of narrative style”, in: Literary & Linguistic Computing, 2(2): 61-70.
               
                  Elson, D.K. / Dames, N. / McKeown, K.R. (2010): “Extracting social networks from literary fiction”, in: Proceedings of the 48th annual meeting of the association for computational linguistics 138-147. Association for Computational Linguistics.
               
                  Heuser, R., F. Moretti / E. Steiner (2016): The Emotions of London. Technical report. Stanford University. Pamphlets of the Stanford Literary Lab.
               
                  Jayannavar, P. / Agarwal, A. / Ju, M. and Rambow, O. (2015): “Validating Literary Theories Using Automatic Social Network Extraction”, in CLfL@ NAACL-HLT 32-41.
               
                  Jockers, M.L. / Underwood, T. (2016): “Text‐Mining the Humanities”, in: Schreibman, Susan / Siemens, Ray / Unsworth, John (eds.): A New Companion to Digital Humanities 291-306.
               
                  Kim, E. / Padó, S. / Klinger, R. (2017): “Investigating the Relationship between Literary Genres and Emotional Plot Development”, in: Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature 17-26.
               
                  Klinger, R. / Sulliya S.S. / Reiter N. (2016): “Automatic Emotion Detection for Quantitative Literary Studies -- A Case Study on Kafka's ‘Das Schloss’ and ‘Amerika’”, in: Digital Humanities (DH), Conference Abstracts, Kraków, Poland, 2016.
               
                  Michel, J.B. / Shen, Y.K. / Aiden, A.P. / Veres, A. / Gray, M.K. / Pickett, J.P. / Hoiberg, D. / Clancy, D. / Norvig, P. / Orwant, J. / Pinker, S. (2011): “Quantitative analysis of culture using millions of digitized books”, in: science, 331(6014) 176-182.
               
                  Moretti, F. (2005): Graphs, maps, trees: abstract models for a literary history. Verso.
               
                  Moretti, F. (2011). Network theory, plot analysis. Stanford Literary Lab Pamphlet Series 2. Available at: https://litlab.stanford.edu/LiteraryLabPamphlet2.pdf
               
                  Murr, S. / Barth, F. (2017): Digital Analysis of the Literary Reception of J.W. v. Goethe’s ‘Die Leiden des jungen Werthers’, in: Digital Humanities (DH), Conference Abstracts, Montreal, Canada 2017.
               
                  Nalisnick, E.T. / Baird, H.S. (2013): “Extracting sentiment networks from Shakespeare's plays”, in: Document Analysis and Recognition (ICDAR), 2013 12th International Conference on IEEE 758-762.
               
                  Park, G.M. / Kim, S.H. / Cho, H.G. (2013): “Structural analysis on social network constructed from characters in literature texts”, in: Journal of Computers, 8(9): 2442-2447.
               
                  Rydberg-Cox, J., (2011): “Social networks and the language of greek tragedy”, in: Journal of the Chicago Colloquium on Digital Humanities and Computer Science (Vol. 1, No. 3).
               
                  West, D.B. (2001): Introduction to graph theory (Vol. 2). Upper Saddle River: Prentice Hall.
            
         
      
   



      
         
            1. Einleitung: Verfassungsvergleich als Spiegel staatlichen Wandels?
            Staatlich verfasste Gesellschaften sind komplex und differenziert (Mayntz 1997; Schimank 1999). Will man etwas über die „Identität“ von Staaten und deren Wandel erfahren, dann eignen sich Verfassungen, da diese spezfische Dokumentensorte soziologisch als kodifizietre Selbstbeschreibung von Gesellschaften verstanden werden kann (Boli-Bennett und Meyer 1978; Go 2003; Heintz und Schnabel 2006; Boli-Bennett 1979). Moderne Staaten produzieren aber nicht nur Unmengen an amtlichen Dokumenten, sondern sind darüber hinaus durch ihre konstitutionelle sowie rechtsstaatlich-bürokratische Verfasstheit grundsätzlich textlich strukturiert (Weber 1972). Für die historsiche Dokumentenanalyse spielen die Erstellung des Korpus und die Auswahl der Untersuchungsmethoden wichtige Rollen, um sowohl die Textlichkeit, als auch den Kontext angemessen zu berücksichtigen.
            In diesem Vorhaben wird mit Verfassungsdokumenten europäischer Staaten gearbeitet, anhand derer staatlicher Wandel von der ersten Verfassungsgebung bis heute sichtbar gemacht wird. Verfassungen beinhalten u.a. Vorstellungen darüber, wie die Gesellschaft beschaffen ist. Konkret wird der historisch-wissenssoziologischen Frage nach der sozialen Konstruktion des (Staats-)Bürgers nachgegangen. Denn historisch betrachtet reflektieren Verfassungen den sukzessiven Umbau von ständisch stratifizierten hin zu souveränen Bürgergesellschaften und damit reflektieren sie ebenso den Wandel des gesellschaftlichen Personals, das es in Form von Personenkategorien und Zugehörigkeitsdimensionen aus dem Material herauszuarbeiten gilt. Was aber genau unter „Verfassung“ verstanden wurde, wie sich die Staaten und ihr ‚Personal’ über dieses Dokument selbst beschreiben und welches Wissen in selbiges eingeht, variiert erheblich (Gosewinkel, Masing und Würschinger 2006; Vorländer 2007). Daher bedarf es eines geeignetne methodisch-analytischen Instrumentariums, um die aufgeworfene Fragen zu beantworten.
         
         
            2. Dokumentenanalyse im Schnittfeld von historischer Soziologie und Computerlinguistik 
            Um Verfassungen strukturell und inhaltlich untersuchen zu können, werden Ansätze der historischen Wissenssoziologie (Thelen 1999, 2002; Jepperson 1991) und der Computerlinguistik (z.B. Hausser 2014; Lobin 2010) miteinander verschränkt. Die Entwicklung dieses methodischen Werkzeugs zur „Dokumentenarbeit“ umfasst Verfahrensschritte der Datenerhebung, -aufbereitung und -auswertung, wobei hier vor allem auf methodologische Herausforderungen, d.h. die Korpuserstellung und die semi-automatische Analyse von Dokumentenstrukturen eingegangen wird. 
            Rechtstexte im Allgemeinen und Verfassungen im Besonderen, weisen eine Dokumentenlogik auf, die stark durch eine formale hierarchische Struktur gekennzeichnet ist. Bei dieser Dokumentenart ist daher davon auszugehen, dass der Struktur eine besonders sinnstiftende Bedeutung zukommt, die es vor allem bei vergleichenden Untersuchungen (synchron wie auch diachron) zu berücksichtigen gilt. Insofern sollte ein computerlinguistisches Verfahren die Strukturinformationen bspw. in welche (Sinn-)Abschnitte sich ein Dokument gliedert für den Vergleich nutzen. Diese Strukturauswertungen können dann wiederum mit statistischen Häufigkeits- und Ähnlichkeitsberechnungen von Worten innerhalb von Fließtexten – wie das u.a. die gängigen Vektorraummodelle (Manning, Raghavan und Schütze 2008; Salton, Wong und Yang 1975) oder insbesondere die derzeit populären „word embedding“ Modell (z.B. Mikolov et al. 2013) machen – kombiniert werden. 
            Bei der hierarchischen Struktur von Dokumenten anzusetzen bietet einen klaren Ausgangspunkt für die systematische Analyse von großen Textmengen und stiftet zugleich Orientierung im Feld der inhaltsanalytischen Methoden (Kuckartz 2012; Mayring 2015). Diese unterscheiden sich vor allem in Bezug auf ihre Anlage, d.h. entweder Häufigkeiten zählende oder hermeneutisch interpretierende Ausrichtung, und firmieren in den Sozialwissenschaften oftmals unter dem Label „Dokumentenanalyse“. Zwar verbindet alle diese Ansätze, dass sie sich durch eine ständige Korrespondenz von Forschungsfrage und Arbeit am Material auszeichnen und in der Regel mehrere Iterationen durchlaufen, bevor valide Ergebnisse vorliegen. Dennoch bringt vornehmlich die manuelle Bearbeitung von umfangreichen Textmengen, etwa in Form von Kodier- und Kategorisierschritten der 
                    Grounded Theory (Strauss und Corbin 1996), Probleme der methodisch kontrollierten Auswertung und damit der Reliabilität der Ergebnisse mit sich.
                
            Außerdem lassen sich über den strukturellen Zugang Fragen erschließen, die über den „reinen“ Inhalt hinausgehen, und die die Verwendung wie auch die Art und Weise in den Vordergrund rücken, in der Verfassungen im Zeitverlauf politisch unter Druck geraten, sich also aufgrund wechselnder politischer Machtverhältnisse wandeln. Damit werden die Relation von Dokument und (Entstehungs-)Kontext und besonders die Verfasser von Dokumenten und deren Konstruktion sozialer Wirklichkeit durch die schriftliche Fixierung gesellschaftlichen Wissens (Prior 2011) fokussiert.
            Von der Dokumentenstruktur auszugehen heißt, zunächst methodologisch zu fragen, inwieweit sich Dokumente formal wie auch inhaltlich ähneln. Das setzt wiederum voraus, dass sich Dokumente überhaupt vergleichen lassen und so einer Analyse etwaiger struktureller Ähnlichkeiten und spezifischer Unterschiede allererst zugänglich gemacht werden. Hier bieten computergestützte Verfahren einen produktiven Ausgangspunkt, um einerseits bestehende Methoden zu reflektieren und andererseits ein dann auch verallgemeinerbares Werkzeug zur Dokumentenanalyse von Rechtstexten zu entwickeln. Zur Beantwortung der formulierten Frage wird eine innovative, von uns eigens entwickelte Software vorgestellt, mit der sich Verfassungen in ihrer historischen Entwicklung vergleichen lassen. Hierdurch werden Impulse zur Generierung neuer methodischer Ansätze gegeben werden. 
         
         
            3. Arbeitsschritte: Vom Download zur Analysesoftware 
            Um mit der Auswertung der Dokumente beginnen zu können, muss das Korpus erstellt werden. Ein normales PDF beinhaltet in der Regel kaum explizite Strukturinformationen, lediglich einzelne Worte ließen sich automatisch erfassen, nicht aber die zugrunde liegenden Strukturen abbilden. Hierfür bedürfte es bspw. der Kennzeichnung von Überschriften, Absätzen oder inhaltlich unterscheidbaren Abschnitten. Das Dokument muss also mit weiteren Informationen in seiner Struktur beschrieben werden.
            Das konkrete methodische Vorgehen, das hier als „Dokumentenarbeit“ zur Korpuserstellung bezeichnet wird, gliedert sich in drei Schritte: das Zusammenstellen der Ausgangsdaten als HTML-Dateien, die Transformation der Ausgangsdaten in das XML-Format sowie die eigentliche Auszeichnung der Ausgangsdaten mit Metadaten zur Modellierung der Struktur. 
            Entgegen der Annahme, dass derart politisch relevante Dokumente wie Verfassungen als elektronische Ausgangsdaten vorliegen sollten, müssen diese zunächst hergestellt und in ein bearbeitbares Datenformat transformiert werden. Zwar finden sich aktuelle Verfassungen als elektronisch veröffentlichte Ressourcen bspw. in den Rechtsdatenbanken und -portalen der jeweiligen Staaten, im deutschen Fall bspw. „Juris“.Es existiert jedoch kein lückenloser, chronologischer Verlauf, aus dem sich alle Änderungen computergestützt entnehmen ließen. 
            Auf der Seite 
                    www.verfassungen.org lassen sich die meisten Verfassungen online (auf Deutsch) abrufen und downloaden. Zudem beinhalten die dortigen Dokumente farblich abgesetzte Änderungen in Textform und nicht etwa als Kommentar oder gesonderte Liste sowie jeweils Totalrevisionen als separate Dokumente. Diese Dokumente werden dann mit offiziell veröffentlichen Verfassungen abgeglichen, um gegebenenfalls inhaltliche Fehler aufzuspüren und zu beheben. Anschließend werden die Daten manuell bereinigt und standardisiert, d.h. nicht benötigte Beschreibungen der Autor*innen der Webseiten oder andere irrelevante Informationen werden entfernt. Diese Dokumente bilden sodann die Grundlage für das Korpus. Die einzelnen Verfassungen beinhalten eine Fülle an textlichen Ergänzungen, Streichungen und anderweitigen textlichen Veränderungen, die einerseits schwer zu identifizieren sind und andererseits nicht chronologisch sortiert, sondern der Texthierarchie folgend vorliegen. Aus diesen Gründen wird zuerst eine Ausgangsverfassung des Jahres 2011, also dem Ende des Untersuchungszeitraums, erstellt, um ausgehend davon jede weitere Änderung als eigenständige, nicht offizielle, „Phantom-Verfassung“ zu rekonstruieren. Durch diesen iterativen, historischen Rekonstruktionsprozess wird schließlich die Datengrundlage geschaffen. 
                
            Die Verfassungen liegen zunächst als HTML-Dokumente, mit einer sehr flachen Dokumentenstruktur vor. Die zu entwickelnde Analysesoftware benötigt jedoch ein Datenformat, das Metadaten mit Dokumentendaten assoziieren kann. Dem aktuellen Entwicklungsstand entsprechend verwenden wir ein XML-Format (Bubenhofer und Scharloth, 2015). 
            Deshalb wird im nächsten Schritt der HTML-Code mittels eines XSL-Skripts (Extensible Stylesheet Language) in das technische XML Format (vgl. XML Schema 2001; XQuery 2002; XSLT 1999) überführt und formatiert. Bei XSL bzw. XSLT handelt es sich um eine Programmiersprache zur Transformation (und Formatierung) von XML Derivaten – in unserem Fall die HTML-Dateien – in XML Dokumente. Das XML Format eignet sich in erster Linie dafür, die informationsarmen Ausgangsdaten mit Metadaten (z.B. Attribute, Codes oder Variablen) zur systematischen Beschreibung der Strukturen und der Inhalte anzureichern. Bspw. ließe sich das Ausgangsdatum „Herbert“ mit dem Attribut „Vorname“ verknüpfen und so systematisch alle Vornamen erschließen. 
            Die Umwandlung von HTML zu XML ist die Grundlage des Mappings der einzelnen Versionen auf einander. Hierfür wurde kein existierender Standard verwendet, sondern das Format so entwickelt, dass es die Struktur der Texte möglichst treu abbildet. Dafür sollen möglichst wenige Elemente verwendet und unnötig tiefe Einbettungen vermieden werden. 
            Jeder Version wird ein Vorspann vorangestellt () der den Titel () und das Datum der jeweiligen Version () enthält. Diesem Vorfeld folgt der eigentliche Text der Verfassung (). Dieser ist zunächst in Hauptteile gegliedert (). Diese können wiederum aus Sektionen () bestehen, welche die Artikel () der Verfassung enthalten. Die Artikel setzen sich aus Sätzen () und gegebenenfalls auch aus listenartigen Aufzählungen () zusammen. Letztere bestehen aus einer Reihe von Listenelementen ( ). 
            Schwesterknoten werden mit eins anfangend durchnummeriert (n). Ein Hauptteil mit n="0" ist eine Präambel. Diese enthalten keine Artikel, sondern eine Reihe von Sätzen () und gegebenenfalls Aufzählungen. Hauptteile, Sektionen und Artikel weisen jeweils ein Element für ihre Überschriften auf. Darüberhinaus enthalten nur Paragraphenelemente () Text. Eine Validierung gegen eine DTD findet nicht statt.
            Es ergibt sich folgendes Format:
            
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
             
              
             
             
             
             
             
             
             
             
             
            
            An dieser Stelle der Beschreibung der Ausgangsdaten im XML Format setzen wiederum qualitative Dokumentenarbeitsschritte ein, die sich an der analytischen Strategie des Kodierens und Kategorisierens anlehnen. In diesen Vorgang fließen einerseits Kontextinformationen ein, andererseits werden während des Arbeitsschritts wichtige empirische Beobachtungen gemacht, die in Form von Kodiermemos dokumentiert werden. So können die gewonnenen Informationen zu einem späteren Zeitpunkt für die Tiefenstrukturanalyse des Materials oder die Ausdifferenzierung der Analysesoftware genutzt werden.
            
               Mapping als Strukturvergleich
               Das Mapping der Strukturelemente der im Vergleich stehenden Versionen aufeinander wird automatisiert vollzogen, indem jedes Element einer strukturellen Ebene (Hauptteil, Sektion, Artikel) mit jeder anderen entsprechenden Ebene der Vergleichsversion abgeglichen wird. Dafür verwenden wir das gängige Cosinus-Maß, das TextÄhnlichkeit durch Modellierung im hochdimensionalen Vektorraum misst. Auf diese Weise wird eine Matrix von Ähnlichkeitswerten aufgebaut. 
               Währen des Aufbaus der Matrix wird die Anzahl der Berechnungen reduziert, indem, sobald eine Ähnlichkeit vom Wert 1 zu einem Element der Vergleichsversion gefunden wird, also eine perfekte Übereinstimmung vorliegt, das Elementpaar als unveränderte Übereinstimmung abgespeichert und die Elemente aus dem weiteren Vergleich ausgeschlossen werden. 
               Liegt keine genaue Übereinstimmung vor, wird getestet, ob die beiden zu vergleichenden Texte unterschiedlicher Länge sind. Ist dies der Fall, wird ferner geprüft, ob der kürzere der beiden Texte einen Teilstring des längeren bildet. In solchen Fällen werden die Texte einander als Änderungen (Erweiterungen oder Kürzungen) zugeordnet, abgespeichert und ebenfalls aus der weiteren Berechnung ausgeschlossen.
               Schließich bleibt eine Matrix der Ähnlichkeitswerte ausschließlich der Elemente übrig, für die keine Entsprechung gefunden werden konnte. 
               Die Paare, die die höchsten Cosinusähnlichkeiten (
               
                  
               
            
            
               Entwicklungsergebnisse
               Die Software zur Verfassungsanalyse ist in der Programmiersprache 
                        Python geschrieben. Im Zuge der ersten Entwicklungsphase lassen sich rein formal die hierarchische Struktur und die jeweiligen Abschnittslängen der Dokumente vergleichen und auch quantifizieren. 
                    
               In der vorliegenden Fassung des Werkzeugs können vier verschiedene Operationen ausgeführt werden: 
               1. Vergleichen 
               Für den Vergleich wird zunächst ein Land und ein Zeitraum ausgewählt für den die Änderungen ausgegeben werden sollen. Um die Suche weiter einzuschränken, wird zunächst gezeigt, wie viele Änderungen in welchen Hauptstücken in dem angegebenen Zeitraum stattgefunden haben. Nach der Auswahl eines Hauptteils werden die gefundenen Änderungen (in Sektionen und Artikeln), Tilgungen und Hinzufügungen ausgegeben (Abbildung 2).
               
                  
               
               2. Cosinusähnlichkeiten 
               Mit dieser Funktion (Abbildung 3) lassen sich die Cosinusähnlichkeiten ganzer Versionstexte untereinander berechnen und ausgeben.
               
                  
               
               3. Wortzählungen und Wortprofile
               Der Nutzer kann sich unter Angabe der Version, die von Interesse ist, die Aufkommenshäufigkeiten von Wörtern ausgeben lassen. Zudem lassen sich die Textstellen, die das gezählte Wort enthalten, zusammen mit einer Liste der Wörter ausgeben, die häufiger als ein definierter Schwellenwert (bspw. fünf Mal) in derselben textuellen Umgebung vorkommen (Abbildung 4).
               
                  
               
               Das Beispiel zeigt die politische Kernkategorie, den Bürger, in Version 7 der irländischen Verfassung. Die textuelle Umgebung ist durch Begriffe wie Gesetz (42 mal), Gerichtshof (37 mal) und Präsident (28 mal), die Hinweise dazu liefern in welchen Sinnzusammenhängen der Bürger theamtisiert wird, gekennzeichnet. Die Begriffe Person (26 mal) und Staat (25 mal) weisen darauf hin, dass es sich beim Bürger offenbar tatsächlich um eine Kategorisierung als Person handelt, die wiederum in irgendeiner Beziehung zum Staat steht. Dieser Zusammenhang, die Beziehung von Bürger und Staat kann nun mithilfe von (historischen) Kontextrecherchen und Literatur basierten Konzepten und Theorien genauer untersucht werden. 
               Zur Erstellung der Wortprofile, d.h. für die Anreicherung der Daten mit bspw. Lemmata, POS-Taggs und Dependenzrelationen wurde die Pipeline des DARIAH-DKPro-Wrapper
                         des NLP-Toolkits DKPro Core (vgl. Eckart de Castilho und Gurevych (2014)) benutzt. Das ermöglicht die Ausgabe der syntaktischen Relationen, die das gezählte Wort mit anderen Wörtern eingeht, zusammen mit ihren Häufigkeiten (Abbildung 5).
                    
               
                  
               
               Das verwendete Beispiel, die Personenkategorie Mitglied, kommt in der gewählten Verfassungsversion 78 mal vor. Die Informationen, wovon Mitglied das Subjekt ist oder inwiefern Mitglied als Genitivattribut oder Akkusativobjekt von bestimmten Termini vorkommt können u.a. dabei helfen, die Eigenschaften dieser Kategorie oder auch die Prozesse in die diese kategorie eingebunden sein kann, genauer zu bestimmen. So kann ein Mitglied hinzugefügt oder auch ernannt werden. In weiteren Betrachtungen kann dann herausgearbeitet werden, wozu ein Mitglied ernannt oder hinzugefügt werden kann. Das Genitivattribut der Kategorie König gibt bspw. Auskunft darüber, von welcher Bezugsgruppe diese Person überhaupt der König ist. Diese automatisiert verfügbaren Informationen tragen dazu bei, die kategoriale Wissensbestände der Verfassungsstaaten historisch-vergleichend zu untersuchen.
               4. Graphische Darstellungen 
               Nutzende können sich in der aktuellen Version die Auftretensverteilungen von Wörtern in einem anzugebenden Zeitraum als Graph ausgeben lassen. Dabei werden pro eingegebenes Wort ein Balkendiagramm sowie ein Diagramm generiert, das die Kurven aller angegebenen Wörter in einem Graph zugleich darstellt (Abbildung 6). 
               
                  
               
               In dem verwendeten Beispiel werden die Verkommenshäufkeiten von drei Personenkategorien – Gott, König und Bürger – in den Verfassungen der Niederlande für den Zeitraum 1815 bis 1948 dargestellt. Dabei fällt auf, dass der Bürger im Vergleich zum König als Repräsentation des Souveräns eine deutlich untergeordnete Rolle spielt und ab 1848 bis 1948 im Prinzip nicht vorkommt. Die Verfassungen spiegeln das politische System der Monarchie und nicht die Staatsbürgergesellschaft wieder. Ab 1848 nimmt auch das Vorkommen der Kategorie Gott signifikant ab. Während 1840 Gott noch 30 Mal vorkommt, verringert sich die Häufigkeit in den darauffolgenden 100 Jahren auf durchscnittlich sieben. Anahnd dieser Ergebnisse lassen sich ganz verschiedene Interpretationen und tiefergehende Analysen anschließen, um bspw. Erkenntniss über die religiöse Semantiken staatlicher Selbstbeschreibung (Gottesbezog, Gründungsmythen, Vorstellungen der Nation usw.) oder den Wandel des politischen Gemeinwesens und politischer Zugehörigkeit (wer gehört eigentlich dazu?) zu erlangen.
               Diese Funktion wird demnächst um weitere bereichert werden, um die Potentiale von Visualisierungen als darstellende Klammer des Strukturvergleichs, der Wortsuche und der syntaktischen Wortrelationen wie auch als analytisches Werkzeug (Lupton 2014) selbst zu sondieren.
            
         
         
            4. Zusammenfassung und Ausblick
            Derzeit kann die Software alle Änderungen – unabhängig von formalen Totalrevisionen innerhalb der historischen Verfassungsentwicklungen aufzeigen. So lässt sich bspw. feststellen, welche Teile besonders häufig geändert werden oder welche Teile bis heute unangetastet geblieben sind. 
            Die Vorteile dieser Software gegenüber frei im Internet zugänglichen Versionierungstools (github, gitlab o.ä.) liegen auf der Hand: Zwar bieten solche Programme relativ einfach die Möglichkeit Textänderungen nachzuverfolgen, das gezielte Nachvollziehen von der Änderungshistorie spezifischer Textabschnitte ist ungleich schwieriger. Darüber hinaus bieten die beschrieben Funktionalitäten viel weitgehendere Auswertungsszenarien, als das reine Mapping, das durch ein Versionierungstool angeboten wird.
            Beispielsweise kann mit dem Programm untersucht werden, welche neuen (normativen) Vorgaben Einzug in die Verfassung finden. Diese Änderungen in den Zeitreihen lassen sich dann ihrerseits im Zuge der weitergehenden Untersuchung historisch kontextualisieren und bspw. mit Blick auf staatlichen Wandel oder die Institutionalisierung bzw. Legitimierung neuer Werte, Normen, staatlicher Handlungsverpflichtungen und kultureller Leitideen (Meyer et al. 2005) interpretieren. Das Programm stellt das technische Werkzeug dafür dar, beide Ebenen, Mikro- und Makroebene, gleichermaßen zu betrachten, indem die Änderungshistorie einzelner Abschnitte ins Verhältnis zur Distanzsicht auf die gesamttextlichen Änderungen vieler Verfassungen gesetzt werden können.
            Künftig sollen auch Fallvergleiche zwischen verschiedenen europäischen Staaten möglich sein, bspw. indem für frei wählbare Textbereiche die Cosinusähnlichkeit berechnet wird. Dadurch wird u.a. die Herausforderung des Vergleichs verschiedener historischer Kontexte tangiert. Wie kann ein informationstheoretisches Modell aussehen, das verschiedene Vektorräume, die definierte temporäre Sequenzen umfassen, zusammenbringt und miteinander vergleicht? Wie können Okkurrenzen kategorisiert werden, wenn diese bspw. häufiger auftreten?
            Die dargestellte Form der Dokumentenarbeit macht Verfassungen nicht nur einer breiten Öffentlichkeit und vielfältigen wissenschaftlichen Erhebungen zugänglich. Vielmehr reflektiert sie Methoden der Dokumentenanalyse, indem sie der spezifischen Dokumentengattung „Verfassung“ besondere Aufmerksamkeit schenkt. Hermeneutisch-interpretative Verfahren versuchen Kontextwissen zumindest zu Beginn der Analyse weitestgehend auszublenden, wohingegen beim skizzierten Vorgehen eben dieses Wissen über die Dokumentenart, deren struktureller Aufbau sowie etwaige kulturell-historische Besonderheiten in die Auszeichung des Textes mit Metadaten für die computerbasierte Bearbeitung einfließt. 
            Insgesamt leistet die methodische Verschränkung von historischer Wissenssoziologie und Computerlinguistik als Dokumentenarbeit und Entwicklung einer Analysesoftware einen Beitrag zur Untersuchung der Ko-Fabrikation von Sprache und Verfassungsrecht in Europa, indem über einzelene Begriffe und Begriffskombinationen spezifische Wissensbestände und Semantiken in den Blick genommen werden können. Dieses Vorgehen kann dazu beitragen, neue Textkorpora zu erschließen und weitere gesellschaftliche Wissensbestände (bspw. Bibelversionen, Dramen usw.) zu erkunden. Diese Analysen ließen sich mit anderen methodisch ähnlichen, aber gegenständlich anders ausgerichteten Untersuchungen koppeln. Bspw. könnten Verfassungen in Beziehung zu Presseartikeln und den sich darin ablesbaren Diskursen gesetzt und miteinander verglichen werden.
         
      
      
         
             Im Zuge dieses Entwicklungsschrittes wurden u.a. folgende Tagger und Parser verwendet: Open NLP Segmenter, Mate Tools POS-Tagger, Mate Tools Lemmatizer, Open NLP Chunker, Mate Tools Morphological Analyzer, Hyphenation Annotator, CoreNLP Named Entity Recognizer, Mate Tools Dependency Parser. 
         
         
            
               Bibliographie
               
                  Boli-Bennett, John (1979): 
                        The Ideology of Expanding State Authority in National Constitutions, 1870-1970, in: Meyer, John W. / Michael Thomas Hannan (eds.): National development and the world system: educational, economic and political change. Chicago: University of Chicago Press 222-237. 
               
               
                  Boli-Bennett, John / John W. Meyer (1978): The ideology of childhood and the state: Rules distinguishing children in national constitutions, 1870-1970, in: 
                        American Sociological Review 43: 797–812
                        .
               
               
                  Bubenhofer, Noah / Joachim Scharloth (2015): Themenheft „Maschinelle Textanalyse“,
                         in: Zeitschrift für germanistische Linguistik, 43.1.
               
               
                  Eckart de Castilho, Richard / Gurevych, Iryna (2014): A broad-coverage collection of portable NLP components for building shareable analysis pipelines. In: 
                        Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT (OIAF4HLT) at COLING 2014, 1-11, Dublin, Ireland.
                    
               
                  Go, Julian (2003): A Globalizing Constitutionalism? Views from the Postcolony, 1945-2000, in: 
                        International Sociology 18.1: 71-95.
                    
               
                  Gosewinkel, Dieter / Johannes Masing / Andreas Würschinger (2006): 
                        Die Verfassungen in Europa 1789-1949. München: Beck
                        .
               
               
                  Hausser, Roland (2014): 
                        Foundations of Computational Linguistics: Human-Computer Communication in Natural Language
                        . Berlin: Springer.
                    
               
                  Heintz, Bettina / Annette Schnabel (2006): Verfassungen als Spiegel globaler Normen? Eine quantitative Analyse der Gleichberechtigungsartikel in nationalen Verfassungen, in: 
                        Koelner Zeitschrift für Soziologie und Sozialpsychologie, 58.4, 685-716.
                    
               
                  Jepperson, Ronald L (1991): 
                        Institutions, Institutional Effects, and Institutionalism, in: DiMaggio, Paul / Walter W. Powell (eds.): The New Institutionalism in Organizational Analysis. Chicago: University of Chicago Press 143-163.
               
               
                  Kuckartz, Udo (2012): 
                        Qualitative Inhaltsanalyse. Methoden, Praxis, Computerunterstützung. Weinheim / Basel: Beltz.
                    
               
                  Lobin, Henning (2010): 
                        Computerlinguistik und Texttechnologie. Paderborn / München: Fink.
                    
               
                  Lupton, Deborah (2014): 
                        Digital sociology. New York: Routledge.
                    
               
                  Manning, Christopher D. / Prabhakar Raghavan / Hinrich Schütze (2008): 
                        Introduction to information retrieval. New York: Cambridge University Press.
                    
               
                  Mayntz, Renate (1997): 
                        Soziale Dynamik und politische Steuerung: theoretische und methodologische Überlegungen. Frankfurt/Main: Campus.
                    
               
                  Mayring, Philipp (2015): 
                        Qualitative Inhaltsanalyse. Grundlagen, Techniken. Weinheim / Basel: Beltz.
                    
               
                  Meyer, John W. (2005): 
                        Die Weltgesellschaft und der Nationalstaat, in: ders., Weltkultur: wie die westlichen Prinzipien die Welt durchdringen. Frankfurt/Main: Suhrkamp 85-132.
               
               
                  Mikolov, Thomas / Wen-tau Yih / Geoffrey Zweig (2013): Linguistic Regularities in Continuous Space Word Representations, in:
                         Proceedings of the HLT-NAACL conference 746-752. 
                    
               
                  Prior, Lindsay (2011): 
                        Using documents in social research. Los Angeles: Sage.
                    
               
                  Salton, Gerard / Andrew Wong / Shungshu Yang (1975): A vector-space model for information retrieval, in: 
                        Journal of the American Society for Information Science 18: 613-620.
                    
               
                  Schimank, Uwe (1999): 
                        Funktionale Differenzierung und Systemintegration der modernen Gesellschaft: Soziale Integration. Opladen: Westdeutscher Verlag.
                    
               
                  Strauss, Anselm L. / Juliet M. Corbin (1996): 
                        Grounded theory: Grundlagen qualitativer Sozialforschung. Weinheim: Beltz.
                    
               
                  Thelen, Kathleen (1999): Historical Institutionalism in Comparative Politics, in: 
                        Annual Review of Political Science 2.1: 369-404.
                    
               
                  Thelen, Kathleen (2002): 
                        The explanatory power of historical institutionalism, in: Mayntz, Renate (eds.): Akteure-Mechanismen–Modelle. Zur Theoriefähigkeit makro-sozialer Analysen. Frankfurt / New York: Campus) 91-107.
               
               
                  Vorländer, Hans (2007): Europas multiple Konstitutionalismen, in: 
                        Zeitschrift für Staats- und Europawissenschaften 5.2: 160-180.
                    
               
                  Weber, Max (1972): 
                        Wirtschaft und Gesellschaft: Grundriss der verstehenden Soziologie. Tübingen: Mohr.
                    
               
                  „XML Schema”. World Wide Web Consortium (W3C)
                  http://www.w3c.org/XML/Schema, 
                        [letzter Zugriff] 10.09.2017).
                    
               
                  „XQuery 1.0: An XML Query Language“. World Wide Web Consortium (W3C) 
                  http://www.w3.org/TR/xquery
                         [letzter Zugriff 10.09.17]. 
                    
               
                  „XSL Transformation Version 1.0“. World Wide Web Consortium (W3C)
                  http://www.w3c.org/TR/xslt
                         [letzter Zugriff 10.09.17].
                    
            
         
      
   



      
         
            
               Introduction
                
            
               Detecting direct speech in fiction allows gaining insight into an important element of its narrative structure. In literary studies, there are assumptions on the factors influencing the distribution of direct speech, like genre, period and aesthetic complexity.
                
            This paper aims to provide a detailed analysis of the use of direct speech across different time periods and domains. To create a reliable database for these analyses, we need to measure the usage of direct speech in a large and representative corpus. This task is more challenging than it may sound: While, nowadays, direct speech is often marked very explicitly by the use of quotes, this has not always been consistently the case. Many historical novels are not available in a well-edited form, meaning that there may be inconsistent use of quotation, or no quotation at all (Brunner, 2013). In this case, a more robust method for detecting direct speech is necessary.
            Our first contribution is therefore a deep learning-based method to detect direct speech using large amounts of rule-based, but slightly flawed, labelled data extracted from raw text.  This has multiple advantages over the use of manually annotated training data: First, manually annotating large amounts of text is very time-intensive and therefore costly. Furthermore, annotations for one type of texts may not be transferable to other types, leading to the necessity of new annotated data for new corpora. Being able to learn from the already existing weakly labelled data is therefore desirable, as this data can automatically be extracted for a new corpus.
            Our second contribution is the application of this approach on curated texts to gain insight in trends of direct speech distribution. On one hand we try to look for development of direct speech over time, analysing a large dataset of novels from the nineteenth century, on the other hand we focus on differences in genre comparing contemporary high and low brow literature.
         
         
            Related Work and Task Description
            
               There have been several previous approaches to direct speech detection applying machine learning methods.
                
            For example, Brunner (2013) tests rule-based and machine learning driven classification, as well as combinations of both, on German novels. She recommends using a pure machine learning approach (Random Forest), reaching an F1 score of 0.87.
            Scheible et al. (2016) employ a simple greedy algorithm and a semi-Markov model, showing that the latter outperforms the previous state-of-the-art by achieving a precision of 0.88.
            Although the results seem quite satisfying, these systems require a relatively large amount of labelled data for training. As stated above, this is problematic because of the need for expensive annotation and lack of transferability to other domains. Thus, our goal in this paper differs from that in previous work. We do not aim to set a new state-of-the-art in direct speech detection, but instead: 
            a) present a method that can leverage large amounts of weakly labelled data extracted from raw text, and 
            b) use this model for the analysis of different distributions of direct speech across genres or time-periods. 
            To the best of our knowledge, the second task has never been done on a large collection of texts.
         
         
            Corpus and Resources
            
               The following experiments are based on three German corpora. The first one is a large corpus containing 4600+ public domain novels including texts from the TextGrid digital libraryand Project Gutenberg. We will refer to this as the Corpus 
                    Public Domain, PD. The second one contains 800+ texts of current popular genres like romance, crime or science-fiction (Corpus 
                    Low Brow, LB). Finally, we use a corpus with 200 novels nominated for the 
                    German Book Prize or the 
                    Georg Büchner Prize (Corpus 
                    High Brow, HB).
                
            In order to train and evaluate our classifiers, we need to obtain labels specifying which parts of the texts contain direct speeches. To this end, we chose two strategies:
            For training our classifiers, we decided to extract weak labels using a simple rule based on quotation, implying everything written between quotation marks is direct speech. To yield high accuracy for this approach, it is necessary to use a well-edited collection of texts. Our PD corpus contains such a subset, which we refer to as our 
                    Kerncorpus. This 
                    Kerncorpus consists of 250 high and middle brow texts (those from the TextGrid digital library), has been manually edited and is assumed to have a mostly consistent use of quotation. 
                
            Using our quotation rule on the 
                    Kerncorpus resulted in a dataset where about 36% of tokens were marked as direct speech. In order to assess the quality of these weak labels, we gave 500 of the sentences to domain experts for manual correction. We found that there was an error-rate of about 3% in those sentences, mostly caused by nested direct speech or inscriptions being enclosed by quotation marks.
                
            For further evaluation, we chose to annotate a smaller subset of the corpus 
                    LB by hand. We selected 50 snippets from texts of low brow literature. This dataset, referred to as 
                    ALB, is relatively skewed towards text outside direct speech, with only about 18% of tokens in a direct speech.
                
         
         
            Experiments
            
               The following experiments use both labelled subsets described above, the large 
                    Kerncorpus
               
                    and the smaller 
                    ALB. For all experiments, quotation marks are removed from the texts. This is done to avoid training models that rely only on the formal style of qualifying direct speech, but also consider implicit signs like the use of first person verbs or speech words.
                
            We conducted experiments on two different levels, starting with a sentence classification task, which is then refined to detect direct speech on word-level.
            
               Sentence-Level Classification
               In our first classification task, documents are split into sentences and vectorised by storing each sentence in a bag-of-words representation. To create a baseline for measuring the advantage using deep learning for direct speech recognition, we compared the performance of traditional machine learning algorithms on our labelled datasets. Training and testing some of the most common machine learning classifiers to detect sentences containing at least one word of direct speech leads to an accuracy of 
                        0.85 using Logistic Regression; for more results see Table 1.
                    
               
                  Using the same setting and replacing machine learning with a combination of recurrent and convolutional neural networks (see Chollet 2017 and Goodfellow 2017) ended up with an accuracy of 
                        0.84.
                    
               
                  
                     
                  
               
               Since we noticed that three of our classifiers all ended up with about the same score, we decided to give the task to two human annotators to establish an upper bound. We selected 250 sentences for manual annotation and again removed all quotation marks. Both annotators ended up with an accuracy comparable to that of the best machine learning methods, 84% and 82.8% respectively. From this result we concluded that it is not expedient to further optimise the sentence classification task, as we had already reached human-level accuracy.
            
            
               Word-Level Classification
               Because of the results from the previous section, we decided to modify our task to a word-level prediction, which enables us to include more context by ignoring sentence boundaries and at the same time make more fine-grained predictions. In this second classification task, each word is to be classified separately as inside or outside a direct speech. As baseline for this task, we trained a Linear Chain Conditional Random Field (CRF) that was only given the word itself and its part-of-speech tag. This CRF stagnated at a comparably low accuracy of 0
                        .71 using cross-validation on the 
                        Kerncorpus. 
                    
               Since our goal was to provide the classifier with more context, we chose to use an architecture based on recurrent neural networks, which are able to deal with relatively large contexts. Our assumption here is that, for a good classification, we need context from both before and after the target word itself, as markers for direct speech can be found at the beginning or the end of the direct speech. We thus designed a two-branch network, visualised in Figure 1. This network receives as input a text-segment, specifically the target word in its context. The words of the input are then passed through an embedding layer and split into two parts, where the first part contains the context up to the target word and the second part contains the context following the target word. The target word itself is contained in both parts. Each part is passed through three separate LSTM-layers. In the future-branch, the context is passed through the layers in reverse, so that the target word is the last word to be read in both branches. The LSTM-layers in the past-branch are stateful and can therefore theoretically retain the entire context of the novel up to the target word. The outputs of the final LSTM-layer of both branches are concatenated. The final prediction is made based on this concatenation by a fully connected layer.
               In our best setup, we used 60 words before and after the target word as context. 
               Training on one half of the 
                        Kerncorpus and evaluating on the other half, this setup yielded an accuracy of 
                        0.83. Training on the full 
                        Kerncorpus and evaluating on the manually annotated 
                        ALB reached an even better accuracy of 
                        0.90.
                        
               
               
                  
                     
                     
                        Figure 1: Architecture of recurrent network to detect direct speech.
                            
                  
               
            
         
         
            Distribution of direct speech
            
               In the following experiments, we used the model based on the architecture described above. We trained this model on the 
                    Kerncorpus and used it to detect direct speech in the complete corpora 
                    PD, 
                    LB and 
                    HB. Here, we describe our findings on these corpora.
                
            
               Direct speech in 19th Century Fiction
               
                  
                  
                     Figure 2: Ratio of direct speech in German novels from 1800 – 1900.
                                    
               
               
                  Figure 2 shows the ratio of direct speech in German novels from 1800 till 1900 based on the texts from Corpus 
                        PD. The regression line indicates a decline of direct speech over time; at the same time, we can observe a decrease of variance. The strong variations between certain years, especially in the early 19th century, are caused by low numbers of provided texts (see Fig. 3). For instance, the peak in 1805 can be explained by the first publication of Denis Diderots ”Herrn Rameaus Neffe”, a philosophical dialogue-based novel.
                    
               
                  
                  
                     Figure 3: Number of provided novels per year.
                                    
               
            
            
               Distribution of direct speech in low and high brow literature
               
                  
                  
                     Figure 4: Ratio of direct speech in German low and high brow novels after 1945.
                                    
               
               
                  There is an assumption in literary studies that a huge amount of direct speech is an indicator of low brow fiction. Figure 4 shows the ratio of direct speech between Corpus 
                        LB and 
                        HB. While the mean usage of direct speech is nearly equal in both groups, the high brow literature is far more variable.
                    
               
                        This finding is contrary to the assumption mentioned above. We propose that, while there is no clear difference in the average use of direct speech between high and low brow literature, authors in high brow literature are far more flexible in choosing how much direct speech they use in their novels. Low brow literature, on the other hand, is expected to have a rather constant amount of dialogue.
                    
            
         
         
            Conclusion and Future Work
            
               In this paper, we introduced a neural network architecture that is able to learn the classification of direct speech by training on weakly labelled data. This network works purely on the raw text of a novel by taking into account a relatively large context. We also demonstrate that training on weakly labelled data leads to satisfying results.
                
            While an accuracy of 0.9 is remarkable, there is still need for optimisation. Recent developments in the performance of neural networks by adding an attention mechanism (see Rush 2015) could improve the results.
            We used our neural network to analyse the distribution of direct speech over time and genres. Besides algorithmic refinements, there is a lot of potential in adding more text to our corpus and refining metadata to allow more sophisticated research questions like differences between or development of direct speech in certain genres.
         
      
      
         
            
               
            
            
               https://gutenberg.spiegel.de
            
            
                We cannot use the remaining texts for either training or evaluation, as we do not have any reliable source of labels for these texts.
            
         
         
            
               Bibliography
               
                  Brunner, Annelen (2013): “Automatic recognition of speech, thought, and writing representation in German narrative texts”, in 
                        Literary and Linguistic Computing. Vol. 28 (2013).
               
               
                  Chollet, Francois (2017): “Deep Learning with Python”. Manning Publications. New York. (Preprint: https://www.manning.com/books/deep-learning-with-python)
                    
               
                  Goodfellow, Ian /  Bengio Yoshua / Courville,  Aaron (2016): “Deep Learning”. MIT Press. (URL: 
                        
                     http://www.deeplearningbook.org
                  )
                    
               
                  Rush, Alexander M. / Chopra, Sumit / Weston, Jason (2015): “A Neural Attention Model for Abstractive Sentence Summarization”. 
                        arXiv preprint arXiv:1509.00685.
               
               
                  Scheible, C., Klinger, R. & Padó, S. (2016): “Model Architectures for Quotation Detection”, in 
                        Proceedings of ACL (p./pp. 1736–1745).
               
            
         
      
   



      
         
            LDA (Latent Dirichlet Allocation) Topic Modeling ist ein computergestütztes Verfahren zur semantischen Analyse digitaler Textsammlungen. Hierbei werden mit Hilfe eines probabilistischen Verfahrens aus Texten eine Reihe sogenannter “Topics” generiert: Gruppen semantisch ähnlicher Begriffe, die über mehrere Texte gemeinsam auftreten und im Modell als Wahrscheinlichkeitsverteilungen über die Gesamtheit des analysierten Vokabulars repräsentiert werden. Das heißt, daß zum Beispiel in einem Topic zum Thema Seefahrt nautische Begriffe besonders hohe Wahrscheinlichkeiten haben (Blei 2012, Steyvers und Griffiths 2006).
            
         In den letzten Jahren ist das Interesse an LDA als Verfahren für die Analyse literarischer Textcorpora auf Seiten der digitalen Geisteswissenschaften stark gestiegen. Im Kontrast zu diesem gesteigerten Interesse ist die Anwendung der Methode allerdings nicht wesentlich leichter geworden. Gängige Implementierungen des LDA-Algorithmus werden entweder über ein kommandozeilenbasiertes Java-Programm (MALLET von McCallum 2002) oder über Skripte in der Programmiersprache Python (Gensim von Rehurek und Sojka 2010) angesprochen. Die Aufbereitung der Daten vor dem Topic Modeling, das sog. “Preprocessing” und die Analyse der Ergebnisse hinterher geschieht dann zumindest in Teilen häufig unter Verwendung weiterer Programme bzw. Arbeitsumgebungen. Alles in allem erfordert die Durchführung einer LDA-basierten Inhaltsanalyse damit zur Zeit relativ umfangreiche technische Kenntnisse.
         Um den Zugang zu dieser Methode zu erleichtern entwickeln wir im Rahmen von DARIAH-DE (https://de.dariah.eu/) zur Zeit eine ausführlich dokumentierte Python-Programmbibliothek, die es ermöglichen soll, den gesamten Arbeitsprozess einer LDA-basierten Analyse in einer einzigen Umgebung durchzuführen (). Neben der Schaffung integrierter, flexibler Arbeitsabläufe, die vollständig in einer Programmiersprache und Umgebung stattfinden können,  wollen wir auch Forscherinnen und Forschern ohne vorherige Programmierkenntnisse eine Möglichkeit zu bieten, Topic Modeling als Verfahren kennen zu lernen und an eigenen Daten auszuprobieren.
            
         Um einen leichtgewichtigen Einstieg in diese Thematik zu bieten haben wir auf Basis unserer Programmbibliothek, der Python-nativen LDA-Implementierung von Allan Riddell (https://pypi.python.org/pypi/lda) und dem Python-Microframework “Flask” () einen sogenannten GUI-Demonstrator entwickelt (Abb. 1). Dabei handelt es sich um eine browserbasierte graphische Benutzeroberfläche für die DARIAH-Topics Bibliothek, mit der sich ein basaler Topic-Modeling Analysevorgang lokal, mit eigenen Daten, aber eben ohne jegliche Programmierkenntnisse durchführen lässt.
            
         Der GUI-Demonstrator übernimmt und erklärt hierbei exemplarisch alle Arbeitsschritte einer einfachen Analyse. Zunächst werden Textdateien über ein Auswahlmenü eingelesen und tokenisiert. Nutzerinnen und Nutzer können zur Reduktion des Vokabulars auf die Funktionswörter vorgeben, wie viele der häufigsten Wörter aus den Texten entfernt werden sollen, oder alternativ über ein weiteres Auswahlmenü eine externe Stopwortliste einbinden. Die Anzahl der zu berechnenden Topics und die Zahl der Iterationen, über die die Berechnung durchgeführt werden soll, ein Faktor, der die Qualität der Ergebnisse entscheidend beeinflusst, können ebenfalls über das Interface gesteuert werden. In der derzeitigen Form generiert das Programm als Output eine Tabelle mit den zehn am stärksten gewichteten Wörtern in jedem Topic, sowie ein Heatmap als Übersicht über die Verteilung der Topics über die Texte.
         Im Fokus der gegenwärtigen Weiterentwicklung steht die Gestaltung interaktiver Outputs mit Hilfe von Bokeh (), die einen flexibleren Zugriff auf eine größere Zahl von Aspekten der Modellierungsergebnisse ermöglichen sollen.
            
         Das Ziel dieser Entwicklung bleibt aber in erster Linie ein didaktisches: Der GUI-Demonstrator führt die  grundsätzlichen Möglichkeiten der Methode vor und informiert gleichzeitig über die Abläufe im Hintergrund, so dass der Schritt hin zur Verwendung der gleichen Funktionalitäten in einem vorbereiteten Notebook mit interaktiven Codeblöcken, das schnell an die spezifischen Bedürfnisse eine bestimmten Forschungsfrage angepasst werden kann, nur noch klein ist.
         
            
               
               Abbildung 1.: Screenshot
            
         
      
      
         
            
               Bibliographie
               
                  
                  Blei, David M.
                        (2012): „Probabilistic Topic Models“, in 
                        Communication of the ACM
                        55, Nr. 4 (2012): 77–84. doi:10.1145/2133806.2133826.
                    
               
                  McCallum, Andrew K.
                        (2002): MALLET : A Machine Learning for Language Toolkit.
                        
                     http://mallet.cs.umass.edu
                  .
                    
               
                  Rehurek, Radim/ Sojka, Petr
                        (2010): "Software framework for topic modelling with large corpora."
                        In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks.
                    
               
                  Steyvers, Mark/ Griffiths, Tom
                        (2006): „Probabilistic Topic Models“, in
                        Latent Semantic Analysis: A Road to Meaning, herausgegeben von T. Landauer, D. McNamara, S. Dennis, und W. Kintsch. Laurence Erlbaum.
                    
            
         
      
   



      
         
            Einleitung
            Das Ziel dieses Tutorials ist es, den Teilnehmerinnen und Teilnehmern konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt.
            Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den "Maschinenraum" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden.
            Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im "Center for Reflected Text Analytics" (CRETA)
                     in den letzten zwei Jahren annotiert wurden, und deckt Texte verschiedener Disziplinen und Sprachstufen ab.
                
         
         
            Entitätenreferenzen
            Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke).
            Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. "Peter"), zum anderen Gattungsnamen (z.B. "der Bauer"), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert.
            In 
                    literarischen Texten sind vor allem Figuren und Räume als grundlegende Kategorien der erzählten Welt von Interesse. Über die Annotation von Figurenreferenzen können u.a. Figurenkonstellationen und -relationen betrachtbar gemacht sowie Fragen zur Figurencharakterisierung oder Handlungsstruktur angeschlossen werden. Spätestens seit dem 
                    spatial turn rückt auch der Raum als relevante Entität der erzählten Welt in den Fokus. Als "semantischer Raum" (Lotmann, 1972) übernimmt er eine strukturierende Funktion und steht in Wechselwirkung mit Aspekten der Figur.
                
            In den 
                    Sozialwissenschaften sind politische Parteien und internationale Organisationen seit jeher zentrale Analyseobjekte der empirischen Sozialforschung. Die Annotation der Entitäten der Klassen ORG, PER und LOC in größeren Textkorpora ermöglicht vielfältige Anschlussuntersuchungen, unter anderem zur Sichtbarkeit oder Bewertung bestimmter Instanzen, beispielsweise der Europäischen Union.
                
         
         
            Textkorpus
            Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (cf. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches Teilkorpus.
                    
            
            Der 
                    Parzival
                Wolframs von Eschenbach ist ein arthurischer Gralroman in mittelhochdeutscher Sprache, entstanden zwischen 1200 und 1210. Der 
                    Parzival zeichnet sich u.a. durch sein enormes Figureninventar und seine komplexen genealogischen Strukturen aus, wodurch er für Analysen zu Figurenrelationen von besonderem Interesse ist. Der Text ist in 16 Bücher unterteilt und umfasst knapp 25.0000 Verse.
                
            
               Johann Wolfgang von Goethes 
               Die Leiden des jungen Werthers ist ein Briefroman aus dem Jahr 1774. Unsere Annotationen sind an einer überarbeiteten Fassung von 1787 vorgenommen und umfassen die einleitenden Worte des fiktiven Herausgebers sowie die ersten Briefe von Werther an seinen Freund Wilhelm.
                
            Das 
                    Plenardebattenkorpus des deutschen Bundestages besteht aus den von Stenografinnen und Stenografen protokollierten Plenardebatten des Bundestages und umfasst 1.226 Sitzungen zwischen 1996 und 2015.
                     Unsere Annotationen beschränken sich auf Auszüge aus insgesamt vier Plenarprotokollen, die inhaltlich Debatten über die Europäische Union behandeln. Hierbei wurde pro Protokoll jeweils die gesamte Rede eines Politikers bzw. einer Politikerin annotiert.
                
         
         
            Ablauf
            Der Ablauf des Tutorials orientiert sich an sog. 
                    shared tasks aus der Computerlinguistik, wobei der Aspekt des Wettbewerbs im Tutorial vor allem spielerischen Charakter hat. Bei einem traditionellen 
                    shared task arbeiten die teilnehmenden Teams, oft auf Basis gleicher Daten, an Lösungen für eine einzelne gestellte Aufgabe. Solch eine definierte Aufgabe kann z.B. 
                    part of speech-tagging sein. Durch eine zeitgleiche Evaluation auf demselben Goldstandard können die entwickelten Systeme direkt verglichen werden. In unserem Tutorial setzen wir dieses Konzept live und vor Ort um.
                
            Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann.
            Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Ähnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen (``brute force'') zeitlich Grenzen gesetzt sind.
            Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (cf. Reiter et al., 2017b) -- stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden.
            Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit.
         
         
            Lernziele
            Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmerinnen und Teilnehmer bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar.
            Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können.
            
               Zeitplan
            
            
               Im Vorfeld der Veranstaltung: Installationsanweisungen und Online-Support
            
            
               Dauer in Minuten (ca.)
            
            
               10 Lecture
                        
                     Intro & Ablauf
                  
               
               15 Hands-On
                        
                     Test der Installation bei allen
                  
               
               50 Lecture
                        
                     Einführung in Korpus und Annotationen
                     Grundlagen maschinellen Lernens
                     Überblick über das Skript (where can you edit what?)
                                
                           Grundlagen Python Syntax
                           Bereitgestellte Features
                        
                     
                  
               
               15 Hands-On
                        
                     Erste Schritte
                  
               
               30 Kaffeepause
               60 Hands-On
                        
                     Hack
                  
               
               30 Evaluation & Preisverleihung
            
         
         
            Beitragende (Kontaktdaten und Forschungsinteressen)
            Der Workshop wird ausgerichtet von Mitarbeiterinnen und Mitarbeitern des "Center for Reflected Text Analytics" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine "black box" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.
            
               Nils Reiter
               
                  nils.reiter@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
                        Pfaffenwaldring 5b
                        70569 Stuttgart
                    
               Die Forschungsinteressen von Nils Reiter liegen generell in der Anwendung computerlinguistischer Methoden auf Fragen aus den Geistes- und Sozialwissenschaften. Insbesondere die Operationalisierung literarischer Forschungsfragen und die adäquate Interpretation von Ergebnissen ist dabei ein Schwerpunkt, neben der regelgeleiteten Annotation und damit zusammenhängenden Fragen.
            
            
               Nora Ketschik
               
                  nora.ketschik@ilw.uni-stuttgart.de
                  Institut für Literaturwissenschaft
                        Keplerstraße 17
                        70174 Stuttgart
                    
               Nora Ketschik ist Promotionsstudentin in der Abteilung für Germanistische Mediävistik. Im Rahmen des CRETA-Projekts nimmt sie Analysen narratologischer Kategorien (u.a. Figur, Raum) an ausgewählten mittelhochdeutschen Romanen vor und setzt sich dabei mit der Verwendung computergestützter Methoden für literaturwissenschaftliche Analysezwecke auseinander.
            
            
               Gerhard Kremer
               
                  gerhard.kremer@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
                        Pfaffenwaldring 5b
                        70569 Stuttgart
                    
               Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen.
            
            
               Sarah Schulz
               
                  sarah.schulz@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
                        Pfaffenwaldring 5b
                        70569 Stuttgart
                    
               Sarah Schulz beschäftigt sich überwiegend mit der automatischen Verarbeitung von Texten, die syntaktischen oder lexikalischen Eigenschaften aufweisen und damit vom 
                        Standard abweichen. Sie hat einen Hintergrund in sowohl Computerlinguistik als auch Theater-und Medienwissenschaften und Germanistik.
                    
            
         
         
            Zahl der möglichen Teilnehmerinnen und Teilnehmer
            Zwischen 15 und 25.
         
         
            Benötigte technische Ausstattung
            Es wird außer einem Beamer keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem es möglich ist, den Teilnehmenden über die Schulter zu blicken und durch die Reihen zu gehen.
         
      
      
         
            
               www.creta.uni-stuttgart.de
            
             Aus urheberrechtlichen Gründen wird das Tutorial ohne das Teilkorpus zu Adornos ästhetischer Theorie stattfinden, das in den Publikationen erwähnt wird.
             Die Texte wurden im Rahmen des PolMine-Projekts verfügbar gemacht: http://polmine.sowi.uni-due.de/polmine/
         
         
            
               Bibliographie
               
                  Kuhn, Jonas / Reiter, Nils (2015): "A Plea for a Method-Driven Agenda in the Digital Humanities" in: 
                        Digital Humanities 2015: Conference Abstracts, Sydney.
                    
               
                  Reiter, Nils / Blessing, Andre / Echelmeyer, Nora / Koch, Steffen / Kremer, Gerhard / Murr, Sandra / Overbeck, Maximilian / Pichler, Axel (2017a): "CUTE: CRETA Unshared Task zu Entitätenreferenzen" in 
                        Konferenzabstracts DHd2017, Bern.
                    
               
                  Reiter, Nils / Kuhn, Jonas / Willand, Marcus (2017b): "To GUI or not to GUI?" in 
                        Proceedings of INFORMATIK 2017, Chemnitz.
                    
               
                  Blessing, Andre / Echelmeyer, Nora / John, Markus / Reiter, Nils (2017): "An end-to-end environment for research question-driven entity extraction and network analysis" in 
                        Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, Vancouver.
                    
               
                  Lotman, Juri (1972): 
                        Die Struktur literarischer Texte, München.
                    
            
         
      
   



      
         
            Sentiment Analyse und Dramenanalyse
            Sentiment Analyse (SA) beschreibt eine Reihe von computergestützten Methoden zur Prädiktion der Polarität eines Texts, versucht also vereinfacht gesagt automatisiert herauszufinden, ob ein Text ein positives oder negatives Gefühl ausdrückt (Liu 2016). Darüber hinaus werden teilweise auch komplexere emotionale Kategorien (wie z.B. Zorn und Freude) betrachtet (Mohammad & Turney 2010). Zentrale Anwendungsfelder der SA sind bislang vor allem die Analyse von Online-Reviews (McGlohan, Glance & Reiter 2010) und Social Media-Daten (Kouloumpis, Wilson & Moore 2011). 
            Zur Analyse von literarischen Texten mittels SA-Techniken finden sich bislang nur wenige Studien, z.B. zu Märchen (Alm, Roth & Sproat 2005) und Romanen (Kakkonen & Kakkonen 2011; Elsner 2012; Jannidis et al. 2016). Auf größeren Textkorpora wurde getestet, inwiefern SA-Werte eines Textes und Emotionskurven von Texten zur Genreklassifikation verwendet werden können (Kim, Padó & Klinger 2017) und wie begriffsgeschichtliche Bedeutungsverschiebungen in literarischen Texten mithilfe von erweiterten SA-Methoden erforscht werden können (Buechel, Hellrich & Hahn 2017). In Dramentexten hat man bisher die Verteilung von emotionalen Kategorien (Mohammad 2011) oder die Entwicklung von Figurenbeziehungen (Nalisnick & Baird 2013) in Shakespeare-Dramen untersucht. Auch der vorliegende Beitrag beschäftigt sich mit dem Einsatz von SA im Bereich der Dramenanalyse. Es werden erstmals systematisch verschiedene Methoden der SA für Dramen getestet und evaluiert. Zudem wird exploriert, inwiefern bisher in der Literaturwissenschaft erforschte Aspekte von Dramen mithilfe der SA erfasst werden und inwiefern die SA auch für die Gewinnung neuer literaturwissenschaftlicher Erkenntnisse eingesetzt werden kann.
            Das im Rahmen dieser Studie verwendete Lessing-Korpus umfasst ein mit Strukturinformationen annotiertes Dramenkorpus mit 11 Dramen, bestehend aus insgesamt 8224 Einzelrepliken. Sämtliche Dramen wurden über die Plattform 
                    TextGrid
                bezogen, so dass alle im Rahmen dieses Beitrags entwickelten Tools auch auf andere 
                    TextGrid-Dramen anwendbar sind. Mit dem am besten evaluierten SA-Verfahren wurde eine webbasierte Anwendung zur Analyse und Visualisierung von Sentiment-Verteilungen und -Verläufen implementiert.
                
         
         
            Evaluation unterschiedlicher SA-Verfahren
            
               Lexikonsbasierte SA
               Innerhalb der SA unterscheidet man zwei wesentliche Ansätze: (1) die Nutzung maschinellen Lernens und (2) die Verwendung lexikonbasierter Verfahren. Für das erstgenannte Vorgehen ist typischerweise ein mit Sentiment-Informationen annotiertes Trainingskorpus notwendig (D‘Andrea et al. 2015), welches für die Dramenanalyse bislang nicht vorliegt. Aus diesem Grund werden in der vorliegenden Arbeit lexikonbasierte Verfahren eingesetzt. Ein Sentiment-Lexikon ist dabei eine Wortliste, in der für jedes Wort Sentiment-Informationen angegeben sind (Liu 2016: 10), also z.B. ob es positiv oder negativ konnotiert ist und in welchem Ausmaß (Polaritätsstärke). Ein derartiges Wort nennt man auch 
                        sentiment bearing word (SBW; Liu 2016: 189).
                    
            
            
               SA-Parameter
               Folgende SA-Optionen wurden in unterschiedlichen Kombinationen systematisch evaluiert: 
               
                  i) Lexika – Es wurden fünf zentrale Sentiment-Lexika für den deutschsprachigen Bereich herangezogen: 
                        SentiWortschatz (SentiWS; Remus, Quasthoff & Heyer 2010), die 
                        Berlin Affective Word List – Reloaded (Bawl-R; Vo et al. 2009), die deutsche Version des 
                        NRC Emotion-Association Lexicon (NRC, Mohammad & Turney 2010), ein Lexikon von Clematide & Klenner (2010; im folgenden CK genannt) und das 
                        German Polarity Clues (GPC; Waltinger 2010). SentiWS, Bawl-R und CK enthalten Polaritäten und Polaritätsstärken, das NRC und GPC nur Polaritätsangaben. Das NRC enthält des Weiteren Annotationen zu acht unterschiedlichen Emotionen (Zorn, Furcht, Erwartung, Freude, Vertrauen, Ekel, Traurigkeit, Überraschung).
                    
               
                  ii) Historisch-linguistische Varianten – Über ein Tool des Deutschen Text-Archivs von Jurish (2011) wurde die Option der Lexikon-Erweiterung mit historischen linguistischen Varianten der Originalwörter untersucht.
                    
               
                  iii) Stoppwortlisten – Analog zu Saif et al. (2014) wurde der Einfluss der Verwendung von insgesamt drei unterschiedlichen Stoppwortlisten auf die Qualität der SA untersucht. Grund hierfür ist, dass durch verschiedene Kombination der Verfahren Sentiment-tragende Stoppwörter entstehen. Neben herkömmlichen Stoppwörtern wurden dabei auch Listen mit hochfrequenten Wörtern des Korpus untersucht. Dadurch wird der Einfluss von Wörtern analysiert, die zwar als sentiment-tragend in SA-Lexika ausgezeichnet werden, aber aufgrund der häufigen Nutzung im Korpus ein ungleichmäßiges Sentiment-Gewicht erzeugen (z.B. Herr, Fräulein).
                    
               
                  iv) Lemmatisierung – Eine weitere untersuchte Verarbeitungsform für die SA ist die Lemmatisierung. Als Lemmatisierer werden der 
                        Pattern-Lemmatisierer (De Smedt & Daelemans 2012) der Python-Bibliothek 
                        textblob und der Python-Wrapper des 
                        treetagger-Tools (Schmid 1995) evaluiert. Viele SA-Lexika enthalten lediglich Grundformen. Aufgrund der Probleme und Schwierigkeiten der Lemmatisierung im Deutschen (Eger, Gleim & Mehler 2016) soll vergleichend untersucht werden, welcher Lemmatisierer die besten Ergebnisse in Kombination mit Lexika erzielt. Ferner enthalten einige SA-Lexika manuell angegebene flektierte Wortformen. Es wird somit auch die automatische Lemmatisierung mit der manuellen Erweiterung verglichen.
                    
            
            
               SA-Metriken
               Alle nachfolgenden Berechnungen wurden bezüglich aller kombinatorischen Möglichkeiten der soeben beschriebenen SA-Parameter durchgeführt. Dabei werden die jeweiligen SA-Metriken nach Term-Zähl-Methodik (Kennedy & Inkpen 2006) berechnet, d.h. ein Text wird hinsichtlich vorhandener SBWs untersucht, positive und negative Wörter ausgezählt und für einen Polaritätswert die positive von der negativen Zahl subtrahiert. SA-Metriken wurden auf folgenden Ebenen über die jeweils zugehörigen Texte kalkuliert: Drama, Akte, Szenen, Repliken sowie Sprecher und Sprecherbeziehungen pro Drama, Akt, Szene und Replik. Die Beziehungen zwischen den Figuren wurden nach einer Heuristik von Nalisnick & Baird (2013) berechnet. 
            
            
               Erstellung des Gold Standards
               Zur systematischen Evaluation der Prädiktionsleistung der verschiedenen SA-Ansätze wurde ein Evaluationskorpus bestehend aus 200 Repliken erstellt. Bei der Auswahl der Repliken wurde darauf geachtet, dass die dramenspezifische Verteilung berücksichtigt wird, längere Dramen sind also mit mehr Repliken vertreten. Ferner wurden nur solche Repliken aufgenommen, die mindestens 19 Wörter umfassen. Diese Länge entspricht etwa -25% des Mittelwerts des Gesamtkorpus und vermeidet damit die Selektion von zu kurzen Repliken. Es wurde insgesamt auf eine gleichmäßige Längenverteilung geachtet.
               Die Repliken wurden von insgesamt fünf Personen (4 weiblich, 1 männlich; alle jeweils mit Deutsch als Muttersprache) jeweils unabhängig voneinander bezüglich deren Polaritätswirkung bewertet. Die Polarität jeder Replik wurde jeweils sechswertig (sehr negativ, negativ, neutral, gemischt, positiv, sehr positiv) und binär (positiv, negativ) bewertet. Die Annotationen wurden bezüglich des Übereinstimmungsgrades analysiert. Dazu wurden das Übereinstimmungsmaß Fleiss‘ Kappa (Fleiss 1971) sowie der Durchschnittswert der prozentualen Übereinstimmung aller Annotatoren und Annotatorinnen berechnet (vgl. Tabelle 1).
               
                  
                  
                     Tabelle 1. 
                            Annotator agreement.
                        
               
               Man erkennt eine geringe Übereinstimmung für die Bewertungsskala mit sechsstufiger Polarität und eine moderate Übereinstimmung für die binäre Variante. Die Ergebnisse verhalten sich konform zu verwandten Studien bei der Interpretation literarischer Texte (Alm & Sproat 2005). Als finale Annotation für eine Replik wird die binäre Polarität gewählt, die die Mehrheit der Annotatoren und Annotatorinnen ausgewählt haben (Endresultat: 139 negativ, 61 positiv).
            
            
               Evaluationsmaße 
               Als Evaluationsmaße wurden Genauigkeit (accuracy), Recall, Precision und F-Werte (Gonçalves et al. 2013) herangezogen. Abb. 1 zeigt einen Ausschnitt aus den je fünf besten Kombinationen pro Lexikon, geordnet nach Genauigkeit.
                        
               
               
                  
                  
                     Abbildung 1: Ausschnitt aus der detaillierten Ergebnistabelle zur Evaluation der SA-Kombinationsmöglichkeiten.
                        
               
            
            
               Ergebnisse der Evaluation
               Nachfolgend erfolgt eine überblicksartige Zusammenstellung einiger zentraler Ergebnisse aus der Evaluation:
               
                  Eine explizite Lemmatisierung führt zu einer verbesserten Leistung. Beide Lemmatisierer erzielen dabei meist ähnliche Ergebnisse. Die Lexikonerweiterung durch historische Varianten macht die explizite Lemmatisierung jedoch weitestgehend unnötig, da hierbei auch eine grundlegende Lemmatisierung inkludiert ist. 
                  Es zeigt sich eine konsistente Verbesserung durch die Lexikonerweiterung mittels der Wort-Varianten aus dem Tool von Jurish (2011). 
                  Stoppwortlisten haben nur auf vereinzelte Lexika (GPC, CK) einen merklich positiven Einfluss. 
                  Lexika mit Polaritätsstärken sind meist besser als reine Term-Zähl-Verfahren desselben Lexikons. 
                  Das Lexikon, dass die höchsten Genauigkeiten für die SA erzielt, ist SentiWS 
                  Die beste Leistung (unter Analyse aller Metriken) erzielt das erweiterte SentiWS mit den Polaritätsstärken, lemmatisiert mittels Pattern-Lemmatisierer und ohne Stoppwortliste (Genauigkeit = 0,67; F-Wert = 0,64). Die Erkennungsrate ist besser als die random baseline von 0,576 aber schlechter als viele Erkennungsraten auf anderen Anwendungsgebieten der SA (Vinodhini & Chandrasekran 2012). 
               
               Aufgrund der Tatsache, dass hier ein verhältnismäßig simpler SA-Ansatz gewählt wurde und bereits menschliche Annotatoren und Annotatorinnen Schwierigkeiten mit der Polaritätsbestimmung haben, sind die Ergebnisse insgesamt durchaus positiv zu bewerten.
            
         
         
            Online-Tool
            Abschließend wurde auf Basis des besten SA-Ansatzes ein Web-Tool für die SA bei Dramen entwickelt. Dieses bietet interaktive Visualisierungen der Sentiment-Verteilungen und -Verläufe für alle berechneten Ebenen. Neben den SentiWS-Metriken wurden auch die Emotionskategorien des NRC integriert. Über das Tool kann man erste Fallstudien auf Dramen-, Akt-, Szenen-, Repliken-, Sprecher- und Sprecherbeziehungsebene durchführen. Die SA-Komponente ist online verfügbar.
                    
            
            Trotz der historischen Differenz stimmen die Ergebnisse der automatischen SA tendenziell mit dem überein, was man in der Dramengeschichte über Bewertungen von Figuren und deren Verhalten weiß. Zusätzlich ist aber ein wichtiger heuristischer Mehrwert zu beobachten: eine Analyse allein auf der Basis von Sentiment-Zuschreibungen führt dazu, dass man das Augenmerk gezielt auf Fakten des Textes richtet, die bisher nicht berücksichtigt wurden. 
            Im Folgenden einige Beispiele für die Bestätigung bekannter Ergebnisse und für Entscheidungen von Analysefragen: 
            
               Fallstudie: Minna von Barnhelm
               Die Analyse von Minna von Barnhelm zeigt, dass die negativen emotionalen Bewertungen insgesamt gegenüber den positiven deutlich überwiegen (vgl. Abb. 2). Dieser Befund bestätigt die bekannte Erkenntnis, dass Lessing das Schema des rührenden Lustspiels verwendet hat. Während die Komik im Stück eher das Ergebnis von Schlussprozessen ist, geht es auf der wörtlichen Ebene überwiegend um ernste Vorwürfe und drohenden Identitäts- und Beziehungsverlust.
               
                  
                  
                     Abbildung 2: Polaritätsverteilung im Drama – 
                            Minna von Barnhelm
                  
               
               Es ist verschiedentlich behauptet worden (Saße 1993), Minna und nicht Tellheim sei die lächerliche Figur des Stücks. Die Sympathielenkung auf der wörtlichen Ebene des Textes, die in der unten stehenden Sentimentverteilung pro Akt abgebildet ist, kann dazu herangezogen werden, diese Frage negativ zu bescheiden (vgl. Abb. 3). Es ist eine auffällige Abweichung der Polarität im zweiten Akt erkennbar. In diesem Akt tritt Minna von Barnhelm zum ersten Mal auf, Tellheim jedoch nicht.
               
                  
                  
                     Abbildung 3: Polaritätsverlauf pro Akt – 
                            Minna von Barnhelm
                  
               
            
            
               Fallstudie: Emilia Galotti
               Die letzte Visualisierung kann genutzt werden die Frage zu diskutieren, warum Emilia in Lessings Drama „Emilia Galotti“ sterben muss (vgl. Abb. 4). Auffällig ist hier die starke negative Bewertung Emilias im zweiten Akt. Entgegen bisheriger Interpretationen, in denen nur die Intrige des Prinzen und Marinelli dafür verantwortlich gemacht werden, dass Emilia um ihre Tugend fürchten und ihren Vater dazu bringen muss, sie umzubringen, wird dadurch die Abwertung allein durch die Avancen des Prinzen sichtbar, die später sowohl Emilias als auch für Odoardos Einschätzung der Ehrbarkeit Emilias in ihrem zukünftigen Leben bestimmen.
               
                  
                  
                     Abbildung 4: Polaritätsverlauf von Sprechern pro Akt – 
                            Emilia Galotti
                  
               
            
            
               Fazit 
               Insgesamt sind die ersten Analyse-Ergebnisse über das Web-Tool sehr vielversprechend. Dabei ist zu bedenken, dass über die Verwendung von SA-Lexika ein sehr einfacher SA-Ansatz gewählt wurde. Über ML- oder Hybrid-Ansätze können Besonderheiten der poetischen und veralteten Sprache möglicherweise besser beachtet werden. Ferner ist fraglich, ob eine Reduktion auf das sonst in der SA übliche binäre System positiv/negativ ausreichend ist für komplexe Interpretationen von Emotionen in Dramen.
               Durch Optimierung des SA-Verfahrens, Ausbau der Funktionen im Front-End und Erweiterung des Tools mit zusätzlichen Dramen sollen künftig Möglichkeiten und Nutzen der SA in der Dramenanalyse weiter exploriert werden.
            
         
      
      
         
             https://textgridrep.org/repository.html; Hinweis: alle im Beitrag erwähnte URLs wurden zuletzt am 12.1.2018 überprüft
             Die vollständige Tabelle ist online verfügbar unter https://drive.google.com/open?id=1cvyqiiLJ03XT1VNaWgSDoajeTE3wgeqxxr2PXp-VM4w
             http://lauchblatt.github.io/QuantitativeDramenanalyseDH2015/FrontEnd/sa_selection.html
         
         
            
               Bibliographie
               
                  Alm, Cecilia Ovesdotter / Sproat, Richard (2005): "Emotional sequencing and development in fairy tales.", in:
                         International Conference on Affective Computing and Intelligent Interaction 668-674.
                    
               
                  Alm, Cecilia Ovesdotter / Roth, Dan / Sproat, Richard (2005): "Emotions from text: machine learning for text-based emotion prediction.", in: 
                        Proceedings of the conference on human language technology and empirical methods in natural language processing 579-586.
                    
               
                  Buechel, Sven / Hellrich, Johannes / Hahn, Udo (2017): “The Course of Emotion in Three Centuries of German Text – A Methodological Framework.”, in: 
                        Digital Humanities 2017 176-179.
                    
               
                  Clematide, Simon / Klenner, Manfred (2010): "Evaluation and extension of a polarity lexicon for German.", in: 
                        Proceedings of the First Workshop on Computational Approaches to Subjectivity and Sentiment Analysis 7-13.
                    
               
                  D’Andrea, Alessia et al. (2015): "Approaches, tools and applications for sentiment analysis implementation.", in 
                        International Journal of Computer Applications 125.3: 26-33.
                    
               
                  De Smedt, Tom / Daelemans, Walter (2012): "Pattern for python.", in: 
                        Journal of Machine Learning Research 13: 2063-2067.
                    
               
                  Eger, Steffen / Gleim, Rüdiger / Mehler, Alexander. (2016). “Lemmatization and Morphological Tagging in German and Latin: A Comparison and a Survey of the State-of-the-art.”, in: 
                        LREC 1507–1513.
                    
               
                  Elsner, Micha (2012): "Character-based kernels for novelistic plot structure.", in: 
                        Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics 634-644.
                    
               
                  Fleiss, Joseph L. (1971): "Measuring nominal scale agreement among many raters.", in: 
                        Psychological bulletin 76.5: 378-382.
                    
               
                  Gonçalves, Pollyanna, et al. (2013): "Comparing and combining sentiment analysis methods.", in: 
                        Proceedings of the first ACM conference on Online social networks 27-33.
                    
               
                  Jannidis, Fotis, et al. (2016): "Analyzing Features for the Detection of Happy Endings in German Novels.", in: 
                        arXiv preprint arXiv:1611.09028
                    
               
                  Jurish, Bryan (2011): 
                        Finite-state canonicalization techniques for historical German. Diss. Universitätsbibliothek der Universität Potsdam.
                    
               
                  Kakkonen, Tuomo / Kakkonen, Gordana Galić (2011): "SentiProfiler: creating comparable visual profiles of sentimental content in texts.", in: 
                        Language Technologies for Digital Humanities and Cultural Heritage 62-67.
                    
               
                  Kennedy, Alistair / Inkpen, Diana (2006): "Sentiment classification of movie reviews using contextual valence shifters.", in: 
                        Computational intelligence 22.2: 110-125.
                    
               
                  Kim, Evgeny / Padó, Sebastian / Klinger, Roman (2017): “Investigating the relationship between Literary Genres and Emotional Plot Development.”, in: 
                        Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature 17–26.
                    
               
                  Kouloumpis, Efthymios / Wilson, Theresa / Moore, Johanna D.  (2011): "Twitter sentiment analysis: The good the bad and the omg!.", in: In 
                        Proceedings of the Fifth International Conference on Weblogs and Social Media 538-54.
                    
               
                  Liu, Bing (2016): 
                        Sentiment analysis: Mining opinions, sentiments, and emotions. New York: Cambridge University Press.
                    
               
                  McGlohon, Mary / Glance, Natalie S. / Reiter, Zach (2010) "Star Quality: Aggregating Reviews to Rank Products and Merchants.", in: 
                        Proceedings of the International Conference on Weblogs and Social Media (ICWSM-2010) 114-121.
                    
               
                  Mohammad, Saif (2011): "From once upon a time to happily ever after: Tracking emotions in novels and fairy tales.", in: 
                        Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities 105-114.
                    
               
                  Mohammad, Saif M. / Turney, Peter D. (2010): "Emotions evoked by common words and phrases: Using Mechanical Turk to create an emotion lexicon.", in: 
                        Proceedings of the NAACL HLT 2010 workshop on computational approaches to analysis and generation of emotion in text 26-34.
                    
               
                  Nalisnick, Eric T. / Baird, Henry S. (2013): "Character-to-character sentiment analysis in shakespeare’s plays.“, in:
                         Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics 479–483.
                    
               
                  Remus, Robert / Quasthoff, Uwe / Gerhard, Heyer (2010): "SentiWS-A Publicly Available German-language Resource for Sentiment Analysis.", in: 
                        LREC 1168-1171.
                    
               
                  Saif, Hassan, et al. (2014): "On stopwords, filtering and data sparsity for sentiment analysis of twitter.", in: 
                        Proc. 9th Language Resources and Evaluation Conference (LREC) 810-817.
                    
               
                  Saße, Günter (1993): 
                        Liebe und Ehe: oder, wie sich die Spontaneität des Herzens zu den Normen der Gesellschaft verhält. Lessings Minna von Barnhelm. Tübingen: Niemeyer.
                    
               
                  Schmid, Helmut (1995): "Improvements in part-of-speech tagging with an application to German.", in: 
                        Proceedings of the acl sigdat-workshop.
                    
               
                  Vinodhini, G. / Chandrasekaran, R. M. (2012): "Sentiment analysis and opinion mining: a survey.", in: 
                        International Journal of Advanced Research in Computer Science and Software Engineering 2.6: 282-292.
                    
               
                  Võ, Melissa LH, et al. (2009): "The Berlin affective word list reloaded (BAWL-R) ", in: 
                        Behavior research methods 41.2: 534-538.
                    
               
                  Waltinger, Ulli (2010): "Sentiment Analysis Reloaded-A Comparative Study on Sentiment Polarity Identification Combining Machine Learning and Subjectivity Features.", in: 
                        Proceedings of the 6th International Conference on Web Information Systems and Technologies (WEBIST '10).
                    
            
         
      
   



      
         
            Einleitung
            Der vorliegende Beitrag enthält methodische Überlegungen und Experimente zu “Zeta”, einem von John Burrows (2007) vorgeschlagenen Maß für die Distinktivität oder “keyness” von textuellen Merkmalen (Wortformen, Lemmata, etc.). Mit solchen Maßen werden Merkmale ermittelt, die für eine bestimmte Gruppe von Texten gegenüber einer Vergleichsgruppe charakteristisch sind.
            Das Exposé gibt einen Überblick zu solchen Maßen, bevor die Funktionsweise von Zeta erläutert wird. Aufbauend auf einer Neu-Implementierung in Python (“pyzeta”, https://github.com/cligs/pyzeta) und Vorarbeiten (Schöch im Druck) liegt der spezifische Forschungsbeitrag dann in den folgenden Schritten: erstens werden mehrere Varianten von Zeta vorgeschlagen und implementiert; zweitens werden Verfahren zum Vergleich und der Evaluation der Ergebnisse erprobt. Ziel ist es, Zeta in seiner Funktionsweise und in seiner Beziehung zu vergleichbaren Maßen besser zu verstehen und vorhandene Nachteile des Maßes durch gezielte Modifikationen zu beheben.
         
         
            Überblick und Stand der Forschung
            Die vergleichende, kontrastierende Analyse zweier Gruppen von Texten ist ein in den Sprach- und Literaturwissenschaften weit verbreitetes Verfahren. Entsprechend wurden zahlreiche Maße der Distinktivität oder “keyness” von Merkmalen entwickelt und für vielfältige Fragestellungen eingesetzt. Die grundlegende Annahme solcher Maße ist, dass ein Merkmal nicht schon durch seine reine Häufigkeit in einer Textgruppe für diese charakteristisch ist, sondern dass dies auch davon abhängt, wie häufig das Merkmal in einer Vergleichsgruppe ist. Diejenigen Merkmale bekommen einen besonders hohen Wert zugewiesen, die in der einen Gruppe sehr häufig sind und zugleich in der Vergleichsgruppe sehr selten sind (Scott 1997, 236). Man kann vier Arten von Verfahren unterscheiden: 
            
               Verfahren, welche erwartete und beobachtete Werte vergleichen (wie “log-likelihood-ratio”; siehe Rayson und Garside 2000);
               Verfahren, die eine Gewichtung der Häufigkeiten vornehmen (wie “tf-idf”, “term frequency / inverse document frequency”; siehe Robertson 2004);
               Statistische Hypothesentests, die Verteilungseigenschaften vergleichen (wie “Welch’s t-Test”; siehe Bortz und Schuster 2010); 
               Dispersionsmaße, die nicht die Häufigkeit, sondern den Grad der konsistenten Verwendung von Merkmalen in Beziehung setzen (wie “deviation of proportions”; Gries 2008). 
            
            Die praktische Bedeutung von Distinktivitätsmaßen ist daran erkennbar, dass Korpusanalyse-Software meist eine entsprechende Funktion anbietet, so “keyness” in WordCruncher (Scott 1997) oder “spécificity” in TXM (Heiden et al. 2012). Kilgariff 2004 und Lijfijt et al. 2014 sind wichtige Arbeiten zur Evaluation von Distinktivitätsmaßen. 
         
         
            Was ist Zeta?
            Das von John Burrows (2007) vorgeschlagene “Zeta” beruht auf einem Dispersionsmaß. Vor der Berechnung werden die Texte in kleinere Segmente gesplittet, wobei die Segmentlänge ein wichtiger Parameter ist. Dann wird für jedes Merkmal der Anteile der Segmente erhoben, in denen das Merkmal mindestens einmal vorkommt (die “document proportion”). Von diesem Anteil in der untersuchten Gruppe wird der entsprechende Anteil in der Vergleichsgruppe subtrahiert, woraus sich ein Zeta-Wert zwischen -1 und 1 ergibt.
            Ein Effekt dieser Berechnungsweise ist, dass Zeta Inhaltswörter als distinktive Wörter favorisiert, Funktionswörter sowie Eigennamen hingegen penalisiert. Daraus ergibt sich eine hohe Interpretierbarkeit der Ergebnisse, die Zeta im Vergleich zu anderen Maßen für die (digitalen) Literaturwissenschaften besonders attraktiv macht. Ein Nachteil ist, dass Merkmale durch die Subtraktion niemals einen Zeta-Wert bekommen können, der höher ist als ihre “document proportion” in der untersuchten Textgruppe, selbst wenn sie gegenüber der Vergleichsgruppe deutlich überrepräsentiert sind (Abbildung 1, Wörter in den roten Rahmen; Schöch im Druck).
            
               
                  
                  Abbildung 1: Scatterplot der Wörter in zwei Textgruppen (französische Komödien und Tragödien): “document proportions” der Wörter in zwei Textgruppen (x- und y-Achse) und resultierende Zeta-Werte (Distanz von der Diagonale).
               
               Eine bekannte Implementierung von Zeta existiert im stylo-Paket für Rin der Funktion "oppose()" (Eder et al. 2016). Abbildung 2 zeigt für ein Beispiel die Ergebnisdarstellung in der hier verwendeten “pyzeta”-Implementierung. 
            
            
               
                  
                  Abbildung 2: Positive und negative Keywords für französische Komödien (rechts) im Vergleich mit Tragödien (links). Zeta-Werte auf der horizontalen Achse.
               
            
            Anwendungsbeispiele von Zeta gibt es in der Shakespeare-Forschung (Craig und Kinney 2009), der modernen englischsprachigen Literatur (Hoover 2010; Weidman und O’Sullivan 2017) und der Romanistik (Schöch im Druck). In der zuletzt genannten Arbeit zum französischen Theater der Klassik und Aufklärung konnte nicht nur die erwartbare, klare Differenzierung von Komödien und Tragödien gezeigt werden. Vielmehr wurde auch die spezifische Verortung der Tragikomödien deutlich, die nicht als Mischform zwischen Komödien und Tragödien zu verstehen sind, sondern eine besondere Affinität zur Tragödie aufweisen (Abbildung 3). 
            
               
               Abbildung 3: Hauptkomponentenanalyse auf Grundlage der 50 Wörter, die für Komödien und Tragödien die höchsten Zeta-Werte erhalten. Komödien in rot, Tragödien in blau, Tragikomödien in grün. Quelle: Schöch im Druck.
            
         
         
            Varianten von Zeta
            Ausgehend von der ursprünglichen Formulierung von Zeta durch Burrows als Subtraktion der “document proportions” lassen sich mehrere Faktoren identifizieren, die zur Formulierung von Varianten von Zeta geeignet erscheinen: 
            
               Statt “document proportions” werden relative Häufigkeiten verwendet; 
               Statt der Subtraktion erfolgt eine Division;
               Statt nicht-transformierter Werte wird eine log2-Transformation der Werte vorgenommen.
            
            Die Kombination dieser Faktoren ergibt 8 Varianten von Zeta (Tabelle 1). 
         
         
            
               
               document proportions
               relative Häufigkeiten
            
            
               
               keine Transformation
               log2-Transformation
               keine Transformation
               log2-Transformation
            
            
               Subtraktion
               sd0
               sd2
               sr0
               sr2
            
            
               Division
               dd0
               dd2
               dr0
               dr2
            
         
         Tabelle 1: Übersicht über die getesteten Varianten von Zeta. Die Variante mit Label „sd0“ entspricht Burrows‘ Zeta. 
         Einige der Varianten sind mathematisch gut motivierbar und versprechen, den oben genannten Nachteil der begrenzten Werte für bestimmte Wörter auszugleichen und damit Zeta zu verbessern, es wurden aber alle implementiert und auf zwei Datensätzen evaluiert.
         
            Datensätze
            Es wurden zwei unterschiedliche Korpora verwendet. Erstens ein Korpus aus der textbox-Sammlung (Schöch et al. 2017), das Romane enthält, die zwischen 1880 und 1940 veröffentlicht wurden: jeweils 24 Texte aus Spanien und aus Lateinamerika (ca. 2,8 Millionen Tokens). Zweitens, ein Teil der Sammlung 
                    Théâtre classique (Fièvre 2007-2017) mit französischen Dramen: 134 Tragödien und 158 Komödien aus Klassik und Aufklärung (ca. 4,9 Millionen Tokens).
                
         
         
            Evaluation
            Die 8 Varianten führen zu unterschiedlichen Wortlisten, geordnet nach absteigenden Zeta-Werten. Vergleicht man den Beginn der Wortlisten für zwei Varianten, fällt auf, dass es wie erwartet zu Verschiebungen im Rang der distinktivsten Wörter kommt. 
            
               Ähnlichkeit der Varianten
               Um den Grad der Abweichung der Ergebnisse für alle Varianten zueinander auf der Grundlage längerer Wortlisten zu erheben, ist ein quantifizierendes Verfahren unerlässlich. Ein Ansatz ist, ein Clustering der Maße auf Basis der Zeta-Werte ihrer Wörter vorzunehmen (Abbildung 4). 
               
                  
                     
                     Abbildung 4: Dendrogramm auf Grundlage einer Cluster Analyse der Zeta-Werte für die 8 Zeta-Varianten (Théâtre-classique-Datensatz; 500 distinktive Wörter; Ward-Verfahren).
                  
                  Abbildung 4 zeigt, dass der wichtigste Faktor für die Unterschiedlichkeit der Varianten ist, ob subtrahiert oder dividiert wird (zwei Haupt-Cluster). Die beiden anderen Variablen spielen eine viel kleinere Rolle. (Die Ergebnisse weiterer Analysen, u.a. auf Basis der RBO-Ähnlichkeit (“ranked biased order”, Webber et al. 2010), werden aus Platzgründen hier nicht diskutiert.)
               
            
            
               Evaluation mit Klassifikationstask
               Unabhängig von den Beziehungen der Varianten zueinander stellt sich die Frage, welche der Varianten von Zeta besonders gut distinktive Wörter identifiziert. Dabei kann zur Evaluation nicht auf einen Goldstandard zurückgegriffen werden: eine händische Annotation der Wörter nach dem Grad ihrer Distinktivität ist nicht möglich, weil niemand das zugrunde liegende Korpus überblicken kann. Die Qualität eines Distinktivitätsmaßes kann aber evaluiert werden, indem es als Merkmalsselektor für einen Klassifikationstask verwendet wird.
               Wenn die durch Zeta am höchsten bewerteten Wörter als Features für einen Klassifikator verwendet werden, sollte dieser Klassifikator eine höhere Genauigkeit erreichen als bei einfacher Verwendung der häufigsten Wörter. Tatsächlich lässt sich dieser Effekt auf dem Korpus der spanisch-sprachigen Romane nachweisen (Tabelle 2). Zur Ermittlung einer Baseline wurde für die Klassifikation in spanische und lateinamerikanische Romane ein linearer SVM-Classifier auf den häufigsten 80, nach TF-IDF gewichteten Wörtern (ohne Stoppwörter) trainiert. Dieser Classifier erreichte lediglich eine Klassifikationsgüte (F1-Score) von 0.49, ist also nicht vom Zufall zu unterscheiden.
               Trainiert man stattdessen auf den 40 distinktivsten Wörtern nach Zeta (oder einer der Varianten), lassen sich Genauigkeiten von deutlich über 90% erzielen. Diese Genauigkeit kann nicht als tatsächliches Klassifikationsergebnis gesehen werden, da die distinktivsten Merkmale auf dem gesamten Korpus extrahiert wurden, ohne Aufteilung in Trainings- und Testdaten. Dennoch zeigt das Ergebnis, dass die von Zeta selektierten Merkmale tatsächlich sehr nützlich für eine Klassifikation sind. Zudem zeigen sich deutliche Unterschiede in der Performanz je nach verwendeter Variante: während mit “sd0” (=Burrows Zeta) 81% Genauigkeit erreicht wird, erhöht sich dieser Wert bei der Variante mit log2-Transformation, “sd2”, auf 98%. 
               
                  
                     baseline
                     sd0
                     sd2
                     sr0
                     sr2
                     dd0
                     dd2
                     dr0
                     dr2
                  
                  
                     0.49
                     0.81
                     0.98
                     0.48
                     0.83
                     0.79
                     0.85
                     0.75
                     0.79
                  
               
               
                  Tabelle 2: Klassifikationsergebnisse bei Verwendung einer linearen SVM, trainiert auf den 40 am höchsten gerankten Wörtern verschiedener Maße im Vergleich zur Baseline. Alle Werte sind der Durchschnitt einer dreifachen Kreuzvalidierung.
               
            
         
         
            Fazit
            Wichtigste Ergebnisse dieses Beitrags sind ein differenziertes Verständnis davon, wie Zeta im Kontext anderer Distinktivitätsmaße einzuordnen ist und wie bestimmte mathematischen Parameter sich auf die Ergebnislisten auswirken: als ein auf dem Grad der Dispersion der Merkmale beruhendes Maß, dessen entscheidende Eigenschaft die Subtraktion der Werte ist. Ein weiteres wesentliches Ergebnis sind die beiden vorgeschlagenen Strategien zum Vergleich und der Evaluation von Distinktivitätsmaßen, wenn eine direkte Evaluation auf Goldstandard-Daten nicht möglich ist.
            Nächste Schritte: Wir möchten als weitere Evaluationsstrategie künstliche Texte generieren, in denen wir kontrolliert einzelne Wörter mit unterschiedlich stark abweichender Verteilung einfügen. So können verschieden Zeta-Varianten direkt dahingehend evaluiert werden, wie gut sie diese Wörter korrekt identifizieren. Zudem möchten wir neben der “document proportion” von Zeta ein weiteres Dispersionsmaß, die von Gries (2008) vorgeschlagene “deviation of proportions” als Grundlage für eine weitere Zeta-Variante verwenden. Schließlich möchten wir untersuchen, ob die hohe Interpretierbarkeit des Original-Zeta bei den Varianten mit noch höherer Klassifikationsgüte erhalten bleibt.
            Eine separate Untersuchung ist in Vorbereitung zu zwei eng zusammenhängenden Fragen: wie sich unterschiedliche Segmentlängen einerseits auf die Ergebnisse auswirken, und wie sich die Ergebnisse verändern, wenn unterschiedlich lange Texte nicht mit allen Segmenten in die Berechnung eingehen, sondern aus jedem Einzeltext zufällig eine identische Anzahl von Segmenten gesampelt wird. 
            Übergeordnetes Ziel all dieser Arbeiten zu Zeta ist es letztlich weniger, ein perfektes Distinktivitätsmaß zu identifizieren, als ein justierbares Maß vorzuschlagen, bei dem in Abhängigkeit von Daten und Forschungsfragen dynamisch Parameter verändert und die resultierenden Verschiebungen in den Ergebnissen visualisiert werden können. 
         
      
      
         
            
               Bibliographie
               
                  Bortz, Jürgen, and Christof Schuster (2010). 
                        Statistik für Human- und Sozialwissenschaftler. 7. Auflage. Berlin: Springer.
                    
               
                  Burrows, John (2007). “All the Way Through: Testing for Authorship in Different Frequency Strata.” 
                        Literary and Linguistic Computing 22, no. 1: 27–47. doi:10.1093/llc/fqi067.
                    
               
                  Craig, Hugh, and Arthur F. Kinney, eds. (2009). 
                        Shakespeare, Computers, and the Mystery of Authorship. 1st ed. Cambridge University Press.
                    
               
                  Eder, Maciej, Mike Kestemont, and Jan Rybicki(2016). “Stylometry with R: A Package for Computational Text Analysis.” 
                        The R Journal 16, no. 1: 1–15.
                    
               
                  Fièvre, Paul, ed. (2007-2013). “Théâtre classique.” Paris: Université Paris-IV Sorbonne. http://www.theatre-classique.fr.
                    
               
                  Gries, Stefan Th. (2008). “Dispersions and Adjusted Frequencies in Corpora.” 
                        International Journal of Corpus Linguistics 13, no. 4: 403–37. doi:10.1075/ijcl.13.4.02gri.
                    
               
                  Heiden, Serge (2010). “The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme.” In 
                        24th Pacific Asia Conference on Language, Information and Computation - PACLIC24, edited by Ryo Otoguro, Kiyoshi Ishikawa, Hiroshi Umemoto, Kei Yoshimoto, and Yasunari Harada, 389–98. Sendai: Waseda University. https://halshs.archives-ouvertes.fr/halshs-00549764/en.
                    
               
                  Hoover, David L. (2010). “Teasing out Authorship and Style with T-Tests and Zeta.” In 
                        Digital Humanities Conference. London: ADHO. http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/html/ab-658.html.
                    
               
                  Kilgarriff, Adam (2001). “Comparing Corpora.” 
                        International Journal of Corpus Linguistics 6, no. 1: 97–133. doi:10.1075/ijcl.6.1.05kil.
                    
               
                  Lijffijt, Jefrey, Terttu Nevalainen, Tanja Säily, Panagiotis Papapetrou, Kai Puolamäki, and Heikki Mannila (2014). “Significance Testing of Word Frequencies in Corpora.” 
                        Digital Scholarship in the Humanities 31, no. 2: 374–97. doi:10.1093/llc/fqu064.
                    
               
                  Rayson, Paul, and R. Garside (2000). “Comparing Corpora Using Frequency Profiling.” In 
                        Proceedings of the Workshop on Comparing Corpora, 1–6. Hong Kong: ACM.
                    
               
                  Robertson, Stephen (2004). “Understanding Inverse Document Frequency: On Theoretical Arguments for IDF.” 
                        Journal of Documentation 60, no. 5 : 503–20.
                    
               
                  Schöch, Christof (im Druck). “Zeta für die kontrastive Analyse literarischer Texte. Theorie, Implementierung, Fallstudie.” In 
                        Quantitative Verfahren in der Literaturwissenschaft. Von einer Scientia Quantitatis zu den Digital Humanities, edited by Andrea Albrecht, Sandra Richter, Marcel Lepper, Marcus Willand, and Toni Bernhart. Berlin: de Gruyter. https://cligs.hypotheses.org/files/2017/09/Schoech_2017-preprint_Zeta-fuer-die-kontrastive-Analyse.pdf.
                    
               
                  Schöch, Christof, José Calvo Tello, Ulrike Henny-Krahmer, and Stefanie Popp (angenommen). “The CLiGS Textbox: Building and Using Collections of Literary Texts in Romance Languages Encoded in XML-TEI.” 
                        Journal of the Text Encoding Initiative http://cligs.hypotheses.org/files/2017/09/Schoech-et-al_2017_Textbox.pdf.
                    
               
                  Scott, Mike (1997). “PC Analysis of Key Words and Key Key Words.” 
                        System 25, no. 2: 233–45.
                    
               
                  Webber, William, Alistair Moffat, and Justin Zobel (2010). “A Similarity Measure for Indefinite Rankings.” 
                        ACM Trans. Inf. Syst. 28, no. 4: 20:1–20:38. doi:10.1145/1852102.1852106.
                    
            
         
      
   



      
         
            I. Motivation 
            In den textbasierenden Geisteswissenschaften ist eine optimale Beschaffenheit der zu Grunde liegenden Texte eine wesentliche Bedingung für wissenschaftliches Arbeiten. Wie sehr sich die Bedeutung eines Textes schon durch scheinbar minimale Unterschiede wie die der Interpunktion verschieben kann, hat sich einer breiteren Öffentlichkeit zuletzt in der Diskussion über einen Punkt in einer Abschrift der amerikanischen Unabhängigkeitserklärung gezeigt (The Atlantic 7/2014).
            Während die historische Diskussion über digitale Editionen vielfach mit dokumentarischen Editionstypen bzw. der Frage nach dem Dokumentcharakter der Grundlagen einer Edition (Manuskripte etc.) und der Frage nach der Essenz des Textbegriffes beschäftigt ist (P. Sahle 2013), ergibt sich insbesondere für diejenigen historischen und altertumswissenschaftlichen Disziplinen, die weiterhin auf die klassische Form kritischer Texteditionen angewiesen sind, ein anderes Problem: Da sie traditionell in hohem Maße mit Texten beschäftigt sind, die notwendig als Interpretationen und Rekonstruktionen gelten müssen (West 1973,32), ist hier vor allem die Frage nach der Beschaffenheit und Begründung der zum Teil massiven editorischen Eingriffe nicht nur in die Lesart, sondern auch in den Umfang und die Zuschreibung von Texten, etwa im Falle sogenannter „Fragmente“, von vorrangiger Bedeutung. Der Workshop soll in diese allgemeine Problematik einführen und sich im Hinblick auf das Thema der Konferenz dem Bereich Kritik der digitalen Geisteswissenschaften (traditionelle Fächer und DH) zuordnen. Den Teilnehmenden soll dies praktisch anhand der Anwendung der Software eCOMPARATIO vermittelt werden. eCOMPARATIO bietet eine einfache Möglichkeit der Kollationierung verschiedener Varianten eines Textes und ermöglicht eine digitale, auf dem automatischen Textvergleich beruhende Form des kritischen Apparates.
            Dazu sind einzelne Use Cases ausgearbeitet worden (anhand der Fragmentsammlung der Vorsokratiker von Diels/Kranz [griechisch], der Res Gestae des Augustus [lateinisch], des Genfer Gelöbnisses [deutsch], der Gettysburg Address von Abraham Lincoln [englisch]), anhand derer den Teilnehmern die Funktionalitäten demonstriert werden.
         
         
            II. Die Software
            Im Projekt eCOMPARATIO, das von 2014 bis 2016 von der DFG gefördert wurde und in Leipzig am Lehrstuhl für Alte Geschichte in Kooperation mit dem Center of E-Humanities in History and Social Sciences (ICE) am Max-Weber-Kolleg für kultur- und sozialwissenschaftliche Studien durchgeführt wurde, ist vor dem Hintergrund dieser Problematik ein einfach zu bedienendes Tool für den Vergleich prinzipiell beliebig vieler und beliebig langer digitalisierter Editionen vorrangig griechischer und lateinischer, technisch gesehen aber auch sämtlicher anderer in einem UNICODE-Format zugänglicher Texte entstanden. Der Textvergleich arbeitet auf der Basis der Identifikation von Ungleichheiten. Hierbei reicht die Spanne der programmiertechnisch unterscheidbaren Differenzen von Ungleichheiten innerhalb von Wörtern und Buchstabenfolgen bis hin zu vertauschten Passagen. Im Unterschied zu anderen Vergleichsprogrammen benötigen Anwender weder eine Installation von Python, eine Levenshtein Bibliothek oder ein Java-Plugin, sondern erhalten eine für die Anwender ohne Vorkenntnisse sofort im Browser nutzbare und mit Copy-and-Paste einfach zu bedienende Oberfläche. Auch Fallbeispiele, eine Textdokumentation sowie Videoanleitungen stehen zur Verfügung. In zwei derzeit (2016-17) von der DFG und der Andrew W. Mellon Foundation geförderten Projekten an der Universität Leipzig in Kooperation mit Christopher Blackwell von der Furman University in Greenville, SC/ USA, erfolgt eine Einbindung des Vergleichstools in ein Interface (zu dem Protokoll Canonical Text Services (CTS) als Teil der CITE Architecture: http://cite-architecture.github.io/cts/), dessen Ziel nicht nur die Bereitstellung möglichst vieler digitalisierter Editionen einzelner Texte, sondern damit auch erweiterte Möglichkeiten des Textvergleiches, der Suche nach Parallelstellen und weiterer Formen des sog. „Text-Mining“ ist.
            
               Die Texteingabemaske
               Die eigenständige Version der Vergleichssoftware (unter http://ecomparatio.net/~khk/instanzen/ecompp/ ist ein Beispiel frei zum Gebrauch bereitgestellt) ermöglicht weiterhin eine browserbasierte oder auch offline verwendbare Anwendung für Texte nach dem Copy-and-Paste-Prinzip: Hier können beliebig viele Versionen eines Textes eingefügt werden.
            
            
               Die Darstellung
               Das Tool ermöglicht verschiedene Ausgaben des Vergleiches:
            
            
               
               Abbildung 1, Detail-Vergleich
            
            
               
               Abbildung 2, Synopse
            
            
               
               Abbildung 3, Buch-Darstellung mit digitalem Apparat zum Textvergleich
            
            
               Ausgabe/Export der Ergebnisse
               Die Ausgabe der Ergebnisse kann zur weiteren Integration in den Arbeitsprozess als TEI XML oder LaTeX Code erfolgen. Will man die Vergleichsdaten abfragen, so steht ein JSON Interface zur Verfügung, über das weitere Software angebunden werden kann. 
            
         
         
            III. Ziele und Zielgruppe
            Ziel des Workshops ist eine Einführung in die Anwendung der eCOMPARATIO Vergleichssoftware, auch in Abgrenzung zu bereits verfügbarer Software (collateX, Juxta) mit ähnlichen Anwendungsbereichen. Dazu soll zunächst eine Einführung in die Relevanz der Frage nach scheinbar marginalen textlichen Unterschieden anhand prägnanter Beispiele aus verschiedenen historischen Epochen und in verschiedenen wissenschaftlichen Diskurssprachen gegeben werden (anhand griechischer, lateinischer, englischer und deutscher Texte).
            Darüber hinaus soll den Teilnehmenden die Möglichkeit gegeben werden, auch Texte aus den eigenen Disziplinen oder beliebige im Internet verfügbare Versionen von Texten selbst miteinander zu vergleichen und sich so mit den Funktionen der Software vertraut zu machen.
            Zielgruppe des Workshops sind prinzipiell alle Interessierten aus dem Bereich der textbasierten Geisteswissenschaften. Dabei sind diejenigen, die, wie oben angesprochen, vor allem mit der Vielzahl digitalisierter Editionen arbeiten, ebenso angesprochen wie solche, die selbst an der Erstellung von Editionen etc. arbeiten, und für die eCOMPARATIO ein hilfreiches Mittel bei der Sichtung und Kollationierung von Textzeugnissen sein kann.
            Besondere technische Kenntnisse sind nicht erforderlich, da sich eCOMPARATIO bewusst an Anwender richtet, die entweder selbst an einer kritischen Edition arbeiten oder auf der Grundlage mehrerer kritischer Editionen wissenschaftliche Fragestellungen verfolgen.
         
         
            IV. Ablauf und Teilnehmerzahl
            Im Workshop soll die browsergestützte Version zur Anwendung kommen, ein Download der Software wird aber auch möglich sein. Neben der bereits beschriebenen Einführung in die wissenschaftliche Grundproblematik erfolgt zunächst eine kurze Präsentation eigener Ergebnisse im Zuge der Forschung mit eCOMPARATIO.
            Hauptsächlich soll im praktischen Teil den Teilnehmenden die Möglichkeit gegeben werden, auf ihren eigenen Rechnern in der browsergestützten Version selbst Vergleiche der für sie relevanten Texte vorzunehmen, die mitgebracht werden sollten oder vor Ort aus Onlinedatenbanken heruntergeladen werden können.
            Während des Workshops soll dann von Seiten der Organisatoren auf eventuelle individuelle Probleme und Schwierigkeiten eingegangen werden. Die Organisatoren des Workshops erhoffen sich hiervon einen eigenen Erkenntnisgewinn auch im Hinblick auf die Anwendungsmöglichkeiten und notwendige Ergänzungen im Hinblick auf die Arbeit gerade mit nicht-lateinischen oder nicht-griechischen Texten.
         
         
            V. Ablauf
            Die TeilnehmerInnen erhalten vorab eine detaillierte Anleitung (Handbuch eCOMPARATIO).
            Im eigentlichen Workshop werden die jeweiligen Arbeitsschritte von einem der Organisatoren live vorgeführt (dafür wird ein leistungsstarker Beamer benötigt). Die konkreten Inhalte orientieren sich dabei an den bisher von den Organisatoren ausgearbeiteten Use Cases (s.o.), die von den Organisatoren präsentiert werden.
            Während des Workshops werden wir bei auftretenden Fragen und Problemen den Teilnehmenden helfend zur Seite stehen, da sie auch die Möglichkeit haben sollen, anhand eigener Texte zu arbeiten. Um eine möglichst gute Betreuung der TeilnehmerInnen gewährleisten zu können, sollte die Teilnehmerzahl 25-30 nicht überschreiten.
         
         
            VI. Organisatoren
            Charlotte Schubert ist Althistorikerin, hat zu Themen der Mentalitätsgeschichte, Medizin- und Wissenschaftsgeschichte sowie zu verschiedenen Bereichen der griechischen Geschichte gearbeitet; seit 2006 verantwortliche Koordinatorin in verschiedenen DH-Projekten, die vom BMBF (eAQUA, eXChange), der DFG (eCOMPARATIO, CTS, Etablierung eines Open Access Online eJournals: Digital Classics Online), der VolkswagenStiftung (Digital Plato) und der Andrew W. Mellon Foundation (CTS) gefördert wurden und werden.
            Hannes Kahl ist Informatiker, Berufserfahrung aus diversen DH-Projekten, Entwickler von eCOMPARATIO, arbeitet an der Weiterentwicklung von CTS und an einer Dissertation zu dem Thema „Form und Formalisierung – mit Anwendung innerhalb automatischer Ermittlung von Buchstabenwerten aus digitalen Abbildungen griechischer Lettern innerhalb wissenschaftlicher Editionen sowie deren digitaler Formatierung“ (Betreuer: Prof. Ch. Schubert, Alte Geschichte/Universität Leipzig/ Prof. O. Arnold, Informatik/FH Erfurt).
            Friedrich Meins ist Althistoriker und hat eine Dissertation zum Thema „Literarische Kritik, rhetorische Theorie und historische Methode bei Dionysios von Halikarnassos“ geschrieben. Im Bereich der eHumanities hat er in den Projekten „eAQUA“ und „eAQUA Dissemination“ an der Uni Leipzig sowie im Projekt „eCOMPARATIO“ am ICE der Universität Erfurt mitgearbeitet. Derzeit arbeitet er im von der Andrew W. Mellon Foundation geförderten Kooperationsprojekt der Universität Leipzig und der Furman University (Greenville, SC/ USA) „Annotating and Editing With Canonical Text Services (CTS)“.
            Oliver Bräckel ist Althistoriker und arbeitet an einer Dissertation zu Politischen Flüchtlingen im Römischen Reich. Im Bereich der eHumanities hat er im Projekt „eCOMPARATIO“ am ICE der Universität Erfurt sowie im Projekt eXChange an der Uni Leipzig mitgearbeitet. Derzeit arbeitet er im von der Andrew W. Mellon Foundation geförderten Kooperationsprojekt der Universität Leipzig und der Furman University (Greenville, SC/ USA) „Annotating and Editing With Canonical Text Services (CTS)“.
         
      
      
         
            
               Bibliographie
               
                  A. Olheiser, Have We Been Reading the Declaration of Independence All Wrong?, 
                        
                            https://www.theatlantic.com/entertainment/archive/2014/07/typo-could-mean-weve- been-reading-the-declaration-of-independence-all-wrong/373915/
                         (letzter Zugang 7.7.2017).
                    
               
                  P. Sahle, Digitale Editionsformen. Zum Umgang mit der Überlieferung unter den Bedingungen des Medienwandels, Norderstedt 2013, 3 Bde. (Schriften des Instituts für Dokumentologie und Editorik Bd. 7).
               
                  M.L. West, Textual Criticism and Editorial Technique, Stuttgart 1973.
               
                  Links:
               
               The Atlantic: https://www.theatlantic.com/entertain
                        ment/archive/2014/07/typo-could-mean-weve-been-reading-the-declaration-of-independence-all-wrong/373915/
                        (letzter Zugang 7.7.2017)
                    
               http://www.eaqua.net
               
                  http://www.ecomparatio.net/
               
               
                  http://www.ecomparatio.net/~khk/instanzen/ecompp/ (letzter Zugang17.9.2017)
               http://digital-plato.org
               http://digital-classics-online.eu
               
                  http://cite-architecture.github.io/cts/
                    
            
         
      
   



      
         Ziel dieses Posters ist es, anhand von 32 deutschsprachigen Dramen in die Netzwerkanalyse literarischer Texte einzuführen, eine didaktische Intervention für eine zwar mittlerweile etablierte Methode der literaturwissenschaftlichen Analyse, die aber nicht immer genügend reflektiert wird: Der Errechnung teils komplexer netzwerktheoretischer Maße entspricht nicht immer ein entsprechender Sprung zur Bedeutungsebene. Was bedeutet es zum Beispiel wirklich, dass die durchschnittliche Pfadlänge in Goethes »Faust. Der Tragödie erster Theil« genau 1,79 beträgt? Wenn man jedoch diesen Wert in Beziehung zu entsprechenden Werten anderer Stücke setzt, gewinnt er an komparatistischer Bedeutung. Die Anschaulichkeit der Wert und ihre spielerisch erfahrene Dimensionierung ermöglichen so die Einübung in die strukturalistische Betrachtung von Netzwerken am Beispiel von Dramen, wobei damit zugleich kulturelles Grundwissen über die Strukturation von Netzwerken – immerhin ubiquitäre Gegenstände der sozialen und technischen Welt – erworben werden kann. 
         Um den komparatischen Blick im Kontext der literaturwissenschaftlichen Netzwerkanalyse zu schulen, setzen wir mit unserem Poster auf einen Gamification-Ansatz. Anders als bei unserem ersten Experiment in dieser Richtung – der auf der DHd2016 präsentierten Android-App »Play(s)« (vgl. Göbel/Meiners 2016), in deren Mittelpunkt die spielerische Korrektur und Anreicherung unserer Korpusdaten stand –, handelt es sich diesmal um eine nicht-technische Anwendung, die auf spielerische Weise netzwerkanalytisches Datenmaterial explorierbar macht.
         Dabei wird das Posterformat in zweierlei Hinsicht bespielt: Das Poster ist einerseits eine Datenvisualisierung auf Grundlage eines selbst gepflegten größeren Dramenkorpus. Andererseits ist es ein in 32 Teile zerlegbares Dramenquartett, das spielerisch mit den Bedeutungshorizonten verschiedener netzwerktheoretischer Größen bekannt macht und ein Bewusstsein für komparatistische Möglichkeiten trainiert. Dieser Ansatz ist in den Geisteswissenschaften nicht neu, verwiesen sei etwa auf das architekturgeschichtliche Quartettspiel »Plattenbauten. Berliner Betonerzeugnisse« (Mangold u. a. 2001), in dem technische Daten verschiedener Plattenbautypen gegenübergestellt werden (vgl. auch Richter 2006).
         Die Didaxe des Dramenquartetts bezieht sich auf mehrere Dimensionen: eine literaturgeschichtliche, eine quantitative, eine netzwerktheoretische. Die 32 Stücke bilden einen Minimalkanon, der von der Zeit der Gottschedischen Theaterreformen bis in die Moderne reicht. Statt der lexikonartigen Beschreibung eines solchen Kanons (wie etwa im »Dramenlexikon des 18. Jahrhunderts«, Hollmer/Meier 2001), besteht das Beschreibungsinstrument hier in visuellen und quantitativen Werten, die Vergleichbarkeit herstellen – erst dieser Umstand vereint die verschiedenen Karten zu einem kompetitiven Spiel.
         Als visueller Catch der Quartettkarten dient eine Visualisierung des jeweiligen extrahierten sozialen Netzwerks (vgl. Fischer u. a. 2016). Die weiteren Informationen auf den Karten setzen sich aus (Kanonwissen präsentierenden) Metadaten (Autor*in – Titel – Untertitel – Genre – Jahr) und vor allem aus statischen und dynamischen Netzwerkdaten zusammen (Anzahl von Subgraphen – Netzwerkgröße – Netzwerkdichte – Clustering-Koeffizient – Durchschnittliche Pfadlänge – Höchster Degreewert und Name der entsprechenden Figur –), wie sie im Rahmen des dlina-Projekts berechnet wurden.
                 Das Deckblatt enthält eine Einführung zum Projekt und seinen Hintergründen sowie Kurzdefinitionen der auf den einzelnen Karten enthaltenen netzwerktheoretischen Maßzahlen, die damit nicht nur spielerisch erkundet, sondern auch konzeptuell verstanden werden können. 
            
         Das Poster wird mit unserer Python-Skriptsammlung ›dramavis‹ generiert, die in der neuen Version 0.4 eine entsprechende Funktion erhalten hat (Kittel/Fischer 2017). Für das Konferenzposter haben wir einen Fallback-Kanon zusammengestellt (Stücke von Johann Christoph und Luise Adelgunde Victorie Gottsched, von Gellert, J. E. Schlegel, Caroline Neuber, Klopstock, Lessing, Gerstenberg, Goethe, Lenz, Klinger, Schiller, Kotzebue, Kleist, Zacharias Werner, Müllner, Grillparzer, Grabbe, Büchner, Hebbel, Gustav Freytag, Anzengruber, Arno Holz, Wedekind, Schnitzler, Erich Mühsam). Über eine individualisierbare Kanon-Datei können aber auch eigene Quartette zusammengestellt werden, sodass sich etwa auch epochenspezifische Sets (Dramen der Aufklärung, Dramen der Klassik, Romantische vs. Klassische Dramen, Dramen des Sturm und Drang vs. Dramen des Naturalismus) oder gattungsspezifische Sets erstellen lassen. 
         Auf der Konferenz werden wir neben einem Poster, das das didaktisch-interventionistische Konzept veranschaulicht, auch diverse Quartett-Sets präsentieren. 
      
      
         
             Vgl. das Blog 
                        https://dlina.github.io/ und das Github-Repo 
                        https://github.com/dlina.
                    
         
         
            
               Bibliographie
               
                  Fischer, Frank / Göbel, Mathias / Kampkaspar, Dario / Kittel, Christopher / Trilcke, Peer (2016): “Distant-Reading Showcase. 200 Years of Literary Network Data at a Glance”, DHd2016, Leipzig. DOI: https://dx.doi.org/10.6084/m9.figshare.3101203.v1
                        >.
                    
               
                  Göbel, Matthias / Meiners, Hanna-Lena: Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus”. DHd2016, Leipzig.
                    
               
                  Hollmer, Holmer / Meier, Albert (eds.) (2011): Dramenlexikon des 18. Jahrhunderts. München: C.H. Beck.
                    
               
                  Kittel Christopher / Fischer, Frank: “dramavis v0.4” (September 2017). Repo: https://github.com/lehkost/dramavis
                        >.
                    
               
                  Mangold, Cornelius u.a. (2001): 
                        Plattenbauten. Berliner Betonerzeugnisse. Ein Quartettspiel. Berlin
                    
               
                  Richter, Peter (2006): 
                        Der Plattenbau als Krisengebiet. Die architektonische und politische Transformation industriell errichteter Wohngebäude aus der DDR am Beispiel der Stadt Leinefelde. Hamburg: Univ., Diss.
                    
            
         
      
   



      
         
            Fragen
            Wie gut lassen sich Gattungen und Untergattungen durch Maschinelles Lernen über eine längere Periode erkennen? Obwohl eine Reihe von Artikeln die Frage hauptsächlich für Englisch (Kessler, Numberg, und Schütze 1997; Petrenz und Webber 2011; Underwood 2014) und Deutsch (Hettinger et al. 2016) beantwortet hat, befasst sich wenig Forschung mit diesem Thema aus einer diachronischen Perspektive oder wird auf spanischen Texte angewendet (Henny-Krahmer 2018). Welche Gattungen sind leichter zu erkennen, welche komplizierter? Welche Algorithmen, Transformationen und Anzahl der lexikalischen Einheiten funktionieren am besten?
         
         
            Datensatz: CORDE 1475-1975
            Zur Beantwortung ob verschiedene Gattungen durch Maschinelles Lernen erkannt werden können, wurde das umfangreichste historische Korpus des Spanischen analysiert, CORDE. Dieses Korpus wurde von der Real Academia Española kompiliert (Rojo Sánchez 2010; Sánchez Sánchez und Domínguez Cintas 2007) und ist ein standard-Tool in der Hispanistik über das online Such-Interface (Kabatek und Pusch 2011). Für die Analyse wurden die Frequenzen der Tokens und die Metadaten jedes Texts an Forscher weitergegeben. Das Korpus beinhaltet ca. 300 Millionen Tokens (34.000 Texte) und die Texte sind mit expliziten Metadaten über Jahrhunderte, Länder und Gattungen markiert.
            Die Daten der mittelalterlichen Sektion des Korpus präsentieren mehrere Probleme (Rodríguez Molina und Octavio de Toledo y Huerta 2017), wie beispielsweise ausgeprägte Unausgewogenheit der Anzahl der Texten im Vergleich zu anderen Jahrhunderten oder schwankende philologische Qualität. Deswegen wurden für diese Analyse nur die Texte der letzten 500 Jahre des Korpus selektiert, die länger als 100 Tokens sind. Somit beinhaltet das analysierte Korpus über 22.000 Texte (über 244 Millionen Tokens). Die Metadaten unterscheiden:
            
               Fachtexte in Themen (Jura, Geschichte, Geisteswissenschaften…)
               Gattungen und Untergattungen (lyrischer Vers, kurzer dramatischer Vers…)
               oder Medien (journalistische Texte, Briefe…).
            
            Eine komplette Liste der Gattungen ist auf den Abbildungen zu finden.
         
         
            Methoden der Evaluation
            Die Klassifikation wurde binarisiert durchgeführt, d. h. jeder Text könnte zu jeder Gattung gehören oder nicht. Verschiedene Parameter wurden evaluiert:
            
               Transformation der lexikalischen Information: relative Frequenz, binäre Frequenz, z-scores, TF-IDF, logarithmierte relative Frequenz
               Algorithmen: k-Nearest Neighbors, Random Forest, Logistic Regression und Support Vector Machine
               Anzahl der Tokens: 10, 50, 100, 500, 1.000, 2.000, 3.000, 4.000, 5.000 und 6.000
            
            Das Korpus wurde für jede Gattung undersampled: die gleiche Anzahl an positiven wie an negativen Fällen wurden für jede Gattung gesamplet. Die Evaluation wurde mit Hilfe von Cross-Validation (10 folds) durchgeführt und der Mittelwert der F1 Scores berechnet. Der Code wird als Python Notebook über GitHub zugänglich sein.
         
         
            Ergebnisse und Diskussion
            Die höchsten F1 Scores der Kombinationen von Parametern für jede Gattung lagen zwischen 0,9 und 1,0 mit einem Mittelwert der verschiedenen Gattungen von 0,96 (Standardabweichung von 0,03). Diese sehr hohen Ergebnisse ähneln sich denen von Underwood (2014), der an einem sehr großen Datensatz forschte. Die häufigsten Parameter bei den besten Ergebnissen waren Logistic Regression (16 Fälle von 27), binäre Häufigkeit (16, was nicht zu erwarten war) und 6.000 MFW (9).
            Auf den nächsten Boxplots sind die 10 besten Kombinationen zu sehen. Jeder Punkt entspricht dem Mittelwert der F1 Scores der Cross-Validation der 10 besten Kombinationen von Parametern. Diese sind nach Gattung differenziert aufgelistet:
            
               
                  
               
            
            Folgende Gattungen wurden am besten erkannt: Theater (Vers und Prosa), Romane, lyrischer Vers, und Fachtexte über Naturwissenschaften und Kunst. Lyrische Prosa zeigt heftige Schwankungen, außerdem wurden die niedrigsten Ergebnisse von folgenden Gattungen erreicht: Autobiografie, narrativer Vers, Essay, lyrische Prosa, Prosa sowie Fachtexte über Gesellschaft, Geschichte und Geisteswissenschaften.
            Ein interessanter Aspekt ist, welche die allgemeinen Tendenzen der Parameter und dessen Kombinationen sind. Dafür eignet sich ein Facet Grid Scatter Plot mit den Algorithmen als Spalten und den Transformationen als Reihen (einzelne Punkte entsprechen den Mittelwert der F1 Scores pro Gattung):
            
               
                  
               
            
            Hinsichtlich der Transformation (Reihen) zeigen die relative und die logarithmierte Häufigkeit niedrigere Ergebnisse als TF-IDF, z-scores und die binäre Häufigkeit. Bei den Algorithmen (Spalten) ist KNN merklich schlechter als die anderen drei. Zuletzt ist noch zu erkennen, dass die Qualität der Ergebnisse bis zu einer Anzahl von 2.000 Tokens zunimmt, und mit Schwankungen bis 6.000 stabil bleibt. Ein interessanter Aspekt ist die Tatsache, dass spezifische Kombinationen (SVC + TF-IDF, binäre + Logistic Regression, relative + Random Forest) von Vorteil im Vergleich zu anderen sind.
         
      
      
         
            Bibliographie
            
               Henny-Krahmer, Ulrike (2018): “Exploration of Sentiments and Genre in Spanish American Novels.” In DH Conference. Mexico City: ADHO.
                
            
               Hettinger, Lena / Reger, Isabella / Jannidis, Fotis / Hotho, Andreas (2016): “Classification of Literary Subgenres.” In DHd Konferenz, 154–58. Leipzig: Universität Leipzig.
                
            
               Kabatek, Johannes / Pusch, Claus D. (2011): Spanische Sprachwissenschaft: eine Einführung. Tübingen: Narr.
                
            
               Kessler, Brett/ Numberg, Geoffrey / Schütze, Hinrich (1997): “Automatic Detection of Text Genre.” In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics, 32–38. ACL ’98. Stroudsburg, PA, USA: Association for Computational Linguistics.
                
            
               Petrenz, Philipp / Webber, Bonnie (2011): “Stable Classification of Text Genres.” Computational Linguistics 37 (2): 385–93.
                
            
               Rodríguez Molina, Javier / Octavio de Toledo y Huerta, Álvaro Sebastián (2017): “La imprescindible distinción entre texto y testimonio: el CORDE y los criterios de fiabilidad lingüística.” Scriptum digital: revista de corpus diacrònics i edició digital en llengües iberoromàniques, no. 6: 5–68.
                
            
               Rojo Sánchez, Guillermo (2010): “Sobre codificación y explotación de corpus textuales: Otra comparación del Corpus del español con el CORDE y el CREA.” Lingüística, no. 24: 11–50.
                
            
               Sánchez Sánchez, Mercedes / Domínguez Cintas, Carlos (2007): “El banco de datos de la RAE: CREA y CORDE.” Per Abbat: boletín filológico de actualización académica y didáctica, no. 2: 137–48.
                
            
               Underwood, Ted (2014): “Understanding Genre in a Collection of a Million Volumes, Interim Report.”
				
				
         
      
   



      
         
            Einleitung
            Obwohl sich infrastrukturell einiges getan hat, sieht ein typischer Operationsmodus der digitalen Literaturwissenschaft immer noch so aus, dass eine bestimmte Forschungsmethode auf ein oft nur ephemeres Korpus angewandt wird. Im besten Fall ist das Ergebnis 
                    irgendwie reproduzierbar, im schlechtesten Fall gar nicht. Im besten Fall gibt es ein offen zugängliches Korpus in einem Standardformat wie TEI, einer anderen Markup-Sprache oder zumindest als txt-Datei. Im schlechtesten Fall ist das Korpus gar nicht zugänglich, d. h., die Forschungsergebnisse müssen einfach hingenommen werden.
                
            Doch seit kurzem gibt es Anzeichen, dass sich dies ändert. Einige Digital-Humanities-Projekte stellen Schnittstellen zu stabilen Korpora zur Verfügung, über die man mannigfaltige Zugriffsmöglichkeiten bekommt und reproduzierbar arbeiten kann. Eines dieser Projekte ist DraCor, eine offene Plattform zur Dramenforschung, die in diesem Vortrag vorgestellt werden soll (zugänglich unter 
                    bzw. über die Repos und verschiedene Schnittstellen). DraCor transformiert vorliegende Textsammlungen zu ›Programmable Corpora‹ – ein neuer Begriff, den wir mit diesem Vortrag ins Spiel bringen möchten.
                
         
         
            Die Bausteine
            
               Vanillekorpora
               Ähnlich wie die COST Action zu europäischen Romanen (Schöch et al. 2018), versucht das DraCor-Projekt als Basis für eine digitale Komparatistik einen Stamm an multilingualen Dramenkorpora aufzubauen, die in basalem TEI kodiert sind. Ein selbst betriebenes russischsprachiges 
                    () und ein deutschsprachiges Korpus 
                    () dienen dabei als Einstieg. Diese Korpora sind, ähnlich wie die Sammlung »Théâtre classique« von Paul Fièvre, im weitesten Sinne als Vanillekorpora angelegt, die über das notwendige Markup hinaus zunächst kaum weitere spezielle Auszeichnungen enthalten, allerdings frei zur Verfügung stehen und damit fork- und erweiterbar sind. Zur Demonstration, dass auch andere, reicher kodierte Korpora dazugebunden und sofort alle bereits bestehenden Extraktions- und Visualisierungsmethoden der Plattform angeboten werden können, wurden das Shakespeare Folger Corpus sowie das schwedische Dramawebben-Korpus geforkt und angedockt 
                    (bzw. 
                        ). Dramenkorpora in weiteren Sprachen sollen folgen; einzige Voraussetzung dabei ist jeweils, dass diese in TEI vorliegen.
                    
               Die Vorteile von frei auf GitHub gehosteten Korpora liegen auf der Hand. Unabhängig von den letztlich durch die Plattform zur Verfügung gestellten Schnittstellen können die Korpora alternativ durch Klonen oder andere Downloadmethoden, etwa über den SVN-Wrapper von GitHub, direkt bezogen und individuell weiterverarbeitet werden. Ein offen zugängliches GitHub-Repositorium heißt auch, dass Pull Requests zur Fehlerkorrektur und Forks für Erweiterungen möglich und erwünscht sind.
            
            
               XML-Datenbank (eXist-db) und Frontend
               DraCor als Plattform setzt auf die eXist-Datenbank, um die TEI-Dateien zu verarbeiten und Funktionen zur Beforschung der Korpora zur Verfügung zu stellen. Das Frontend wurde mit ReactJS gebaut, ist responsiv und einfach erweiterbar. Der Schwerpunkt liegt aber nicht auf der GUI, sondern auf der API (vgl. generell zur Unterscheidung zwischen beiden Schnittstellenansätzen Bleier/Klug 2018).
            
            
               API und Entwicklungsumgebung
               Um dem Ideal und der Möglichkeit nahe zu kommen, auf einfache Weise »alle Methoden auf alle Texte« anwenden zu können (Frank/Ivanovic 2018), braucht es mehr als offene Korpora. Der zitierte Text von Frank/Ivanovic macht sich hinsichtlich dessen für SPARQL-Endpunkte stark; auch DraCor bietet einen solchen an, besitzt darüber hinaus aber eine reiche API, die über Swagger dokumentiert und erläutert wird 
                    (). In einem Teilbereich der Korpusphilologie, den Digital Scholarly Editions, hat die Diskussion um eine proaktivere Nutzung von APIs bereits begonnen (zur Vorgeschichte vgl. wiederum Bleier/Klug 2018), als Beispiel hierfür diene die Folger Digital Texts API 
                    (), über die man sich spezifische Querys zusammenbauen kann. Der Vorteil einer moderneren Lösung wie Swagger besteht darin, dass API-Querys live und direkt ausgeführt und die Outputs genauer kontrolliert und gesteuert werden können.
                    
               Ein einfaches Use-Case-Szenario sieht dann so aus, dass man etwa im RStudio mit zwei, drei Zeilen Code einen Blick in ein Korpus werfen kann, etwa über die zeitliche Entwicklung der Anzahl der Charaktere im russischen Drama zwischen 1740 und 1940, die in der Metadatentabelle festgehalten sind 
                    (). Diese Datei, beziehbar im JSON- oder CSV-Format, wird in eine Data.Table eingelesen, woraufhin die Werte zweier Spalten (Erscheinungsjahre und Number of Speakers) einfach über ggplot visualisiert werden können (Abb. 1).
                    
               
                  
                     
                     Abbildung 1: Anzahl der Charaktere pro Drama in chronologischer Ordnung (Quelle: RusDraCor).
                  Anhand dieses sehr simplen Beispiels zeigt sich dann recht deutlich, dass sich mit Puschkins an Shakespeare angelehntem historischen Drama »Boris Godunow« (1825), in dem Sprechakte von 79 Charakteren vorkommen, eine strukturelle Diversifizierung der russischen Dramenlandschaft Bahn bricht.
                    
               Die Möglichkeiten beschränken sich aber nicht darauf, vorgefertigte API-Funktionen zu benutzen. Neue Forschungsideen zeitigen immer auch neue Bedarfe an einfach bezieh- und reproduzierbaren Daten und Metriken; die API kann dementsprechend erweitert werden. Dies wird dadurch erleichtert, dass über Apache Ant die gesamte Entwicklungsumgebung auf dem eigenen System nachgebaut werden kann.
               Durch bereits implementierte Funktionen können neben Struktur- und Metadaten etwa auch Volltexte ohne Markup bezogen werden (auch Untermengen von Volltexten wie Regieanweisungen), etwa wenn Methoden wie die Stilometrie oder das Topic Modeling der Endzweck sind, also Methoden, die nach dem »bag of words«-Prinzip arbeiten, für das kein Markup vonnöten ist.
               Insgesamt wird durch den Aufbau und die Dokumentation offener APIs die bisher oft aufwendige Reproduzierbarkeit von Forschungsergebnissen erheblich erleichtert.
            
            
               Shiny App
               Ein Beispiel für die vielseitigen Nutzungsmöglichkeiten der DraCor-API ist die Shiny App, die Ivan Pozdniakov aufgesetzt hat 
                    (). Shiny ist ein auf R basierendes Framework, das es ermöglicht, interaktive Visualisierungen im Browser darzustellen. Die DraCor-Shiny-App tut genau dies und setzt dabei vollkommen auf die DraCor-API für den Datenbezug. So kann zu Lehr- und Forschungszwecken, aber auch zur einfacheren Datenkorrektur, auf Visualisierungen des aktuellen Datenbestandes zugegriffen werden.
                    
            
            
               Didaxe
               Das Markup oder andere Formalisierungen literarischer Texte sind nicht selbsterklärend. Zwar gibt es einige Standards, aber die jeweilige Operationalisierungslösung hängt von der Forschungsfrage ab. Allein das Extrahieren von Figurennetzwerkdaten ist auf viele Arten und Weisen möglich, was dazu führt, dass etwa alle von verschiedenen Forschungsgruppen extrahierten Netzwerke aus Shakespeares »Hamlet« zu leicht verschiedenen Ergebnissen kommen. Selbst für Dramen ist dies also schon ein nicht-trivialer Akt, von Romanen dann ganz zu schweigen (beispielhaft seien Grayson et al. 2016 genannt, die verschiedene Extraktionsmethoden für Romane durchtesten und die Ergebnisse vergleichen). Um diese Erkenntnis schon in der Lehre zu fördern, wurde das Tool »Easy Linavis« 
                    () entwickelt und in die DraCor-Toolchain integriert. Per Hand können Netzwerkdaten aus Texten extrahiert und dabei das Bewusstsein für die Kontingenz dieses Vorgangs geschärft werden, eine wichtige Vorstufe zur Operationalisierung.
                    
               Neben einem Ansatz zur Gamifizierung des TEI-Korrekturvorgangs (Göbel/Meiners 2016) haben wir für Lehrzwecke auch ein Dramenquartett entwickelt, um spielerisch das Verständnis von Netzwerkwerten zu trainieren (Fischer at al. 2018).
               Die aufgezählten, um die Plattform herumgruppierten didaktischen Mittel sind integraler Bestandteil des ganzen Projekts, da sie auf dessen Daten und Operationalisierungen aufsetzen. Wichtig dabei war die Erkenntnis, dass Daten mehrere Gestalten annehmen und für Forschung und Lehre gleichermaßen von Bedeutung sein können.
            
            
               Linked Open Data (LOD)
               Im TEI-Code sind PND- bzw. Wikidata-Identifier sowohl für Autor*innen als auch für die Werke hinterlegt. Auf diese Weise lassen sich verschiedene Realien, die außerhalb der eigenen Korpusarbeit liegen, hinzufügen. Eine automatisch erstellte Autor*innengalerie hat dabei noch eher illustrativen Charakter (de la Iglesia/Fischer 2016).
               Darüber hinaus kann man aber zum Beispiel feststellen, ob es nicht einen unbewussten regionalen Bias im Korpus gibt. Dafür lässt man sich über die Wikidata-Identifier die Verteilung der Geburts- und Sterbeorte der Autor*innen auf einer Karte anzeigen. So konnte dann für das deutschsprachige Korpus GerDraCor ausgeschlossen werden, dass es einen solchen Bias gibt, da sich die Orte relativ gleichmäßig über die (historisch) deutschsprachigen Gebiete verteilen (Göbel/Fischer 2015).
               Ebenso lässt sich über die Wikidata-ID der Stücke herausfinden, wo diese uraufgeführt worden sind (Beispiel-Query: 
                        ), d. h., Aspekte der Aufführungsgeschichte lassen sich zuschalten, obwohl diese gar nicht im Fokus des Kernprojekts liegen. Programmable Corpora verbinden sich also auch mit der Welt um sie herum, was sie u. a. von den nach innen gerichteten Workbenches der Korpuslinguistik unterscheidet.
                    
            
            
               Infrastruktur statt Rapid Prototyping
               Projekte wie DraCor versuchen nichts anderes als den digitalen Literaturwissenschaften eine verlässliche und ausbaufähige Infrastruktur zu geben, damit sie sich stärker auf eigentliche Forschungsfragen konzentrieren und reproduzierbare Ergebnisse hervorbringen können.
               Eine wichtige Folgerung für uns war, dass wir die Weiterentwicklung unserer seit vier Jahren entwickelten all-in-one Python-Skriptsammlung 
                        dramavis aufgeben und uns lieber der Arbeit an der API widmen. 
                        Dramavis (Kittel/Fischer 2014–2018 sowie Fischer et al. 2017) folgte dem in den Digital Humanities nicht untypischen Rapid Prototyping mit direkter Verarbeitung literarischer XML-Daten (Trilcke/Fischer 2018) und einer mittlerweile stark gewachsenen Codebasis, die alles auf einmal kann, deren Maintenance aber immer schwieriger geworden ist und oft genug von den eigentlichen Forschungsfragen weggeführt hat.
                    
            
         
         
            
               Fazit
                
            In Anlehnung an das Projekt »ProgrammableWeb« – das eine Datenbank von offenen APIs unterhält und dessen Slogan lautet: »APIs, Mashups and the Web as Platform« (zugänglich unter 
                    ) – schlagen wir für infrastrukturell-forschungsorientierte, offene, erweiterbare und LOD-freundliche Korpora den Begriff ›Programmable Corpora‹ vor.
                
            Programmable Corpora erleichtern es, Forschungsfragen auf viele Arten und Weisen um Korpora herum programmieren zu können. Es steht zu erwarten, dass sich infrastrukturelle Anstrengungen dieser Art für die gesamte Community auszahlen mit Effekten, wie sie John Womersley in seiner Präsentation auf der ICRI2018 in Wien aufgezählt hat: a) dramatically increase scientific reach; b) address research questions of long duration requiring pooled effort; c) promote collaboration, interdisciplinarity, interaction.
            Der Anschlussmöglichkeiten sind viele, egal ob man gar nicht programmieren möchte, sondern nur eine GEXF-Datei für Gephi benötigt, ob ein Korpus über seine Verbindungen zur Linked Open Data Cloud beforscht oder einfach aus R oder Python heraus bestimmte Daten bezogen werden sollen, ohne dass man sich mit dem Korpus und dessen Maintenance und Reproduzierbarkeit selbst kümmern muss (all dies bleibt natürlich aber eine Option). Programmable Corpora erleichtern die Entscheidung, auf welcher Ebene der eigene Forschungsprozess einsetzt.
         
      
      
         
            
               Bibliographie
               
                  Bleier, Roman / Klug, Helmut W. (2018): Discussing Interfaces in Digital Scholarly Editing. In: Digital Scholarly Editions as Interfaces. BoD, Norderstedt, S. V–XV. URL: 
               
               
                  de la Iglesia, Martin / Fischer, Frank (2016): The Facebook of German Playwrights. URL: 
               
               
                  Fischer, Frank / Dazord, Gilles / Göbel, Mathias / Kittel, Christopher / Trilcke, Peer (2017): Le drame comme réseau de relations. Une application de l‘analyse automatisée pour l’histoire littéraire du théâtre. In: Revue d'historiographie du théâtre. № 4. URL: 
               
               
                  Fischer, Frank / Kittel, Christopher / Milling, Carsten / Schultz, Anika / Trilcke, Peer / Wolf, Jana (2018): Dramenquartett – Eine didaktische Intervention. In: Konferenzabstracts zur DHd2018, Universität zu Köln. S. 397 f. DOI: 
               
               
                  Göbel, Mathias / Fischer, Frank (2015): The Birth and Death of German Playwrights. URL: 
               
               
                  Göbel, Mathias / Meiners, Hanna-Lena (2016): Play(s): Crowdbasierte Anreicherung eines literarischen Volltext-Korpus. In: Konferenzabstracts zur DHd2016, Bern/CH. S. 140–143. URL: http://www.dhd2016.de/abstracts/vortr%C3%A4ge-007.html
               
               
                  Grayson, Siobhán / Wade, Karen / Meaney, Gerardine / Greene, Derek (2016): The Sense and Sensibility of Different Sliding Windows in Constructing Co-Occurrence Networks from Literature. In: 2nd IFIP International Workshop on Computational History and Data-Driven Humanities. Trinity College Dublin 2016. PDF: 
               
               
                  Kittel, Christopher / Fischer, Frank (2014–2018): dramavis. Python-Skriptsammlung. Repo: 
               
               
                  Schöch, Christoph et al. (2018): Distant Reading for European Literary History. A COST Action [Poster]. In: DH2018: Book of Abstracts / Libro de resúmenes. Mexico: Red de Humanidades Digitales A. C. URL: https://dh2018.adho.org/en/?p=11345
               
               
                  Trilcke, Peer / Fischer, Frank (2018): Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen. In: Wie Digitalität die Geisteswissenschaften verändert: Neue Forschungsgegenstände und Methoden. Hrsg. von Martin Huber und Sybille Krämer (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 3). DOI: 
               
            
         
      
   



      
         
            Einleitung
            Das Ziel dieses Tutorials ist es, den Teilnehmerinnen und Teilnehmern konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt.
            Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den "Maschinenraum" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden.
            Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im "Center for Reflected Text Analytics" 
                (CRETA) in den letzten drei Jahren annotiert wurden, und deckt Texte verschiedener Disziplinen und Sprachstufen ab.
                
         
         
            
               Entitätenreferenzen
                
            Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke).
            Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. "Peter"), zum anderen Gattungsnamen (z.B. "der Bauer"), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert.
            In 
                    literarischen Texten sind vor allem Figuren und Räume als grundlegende Kategorien der erzählten Welt von Interesse. Über die Annotation von Figurenreferenzen können u.a. Figurenkonstellationen und -relationen betrachtbar gemacht sowie Fragen zur Figurencharakterisierung oder Handlungsstruktur angeschlossen werden. Spätestens seit dem 
                    spatial turn rückt auch der Raum als relevante Entität der erzählten Welt in den Fokus. Als "semantischer Raum" (Lotmann, 1972) übernimmt er eine strukturierende Funktion und steht in Wechselwirkung mit Aspekten der Figur.
                
            In den 
                    Sozialwissenschaften sind politische Parteien und internationale Organisationen seit jeher zentrale Analyseobjekte der empirischen Sozialforschung. Die Annotation der Entitäten der Klassen ORG, PER und LOC in größeren Textkorpora ermöglicht vielfältige Anschlussuntersuchungen, unter anderem zur Sichtbarkeit oder Bewertung bestimmter Instanzen, beispielsweise der Europäischen Union.
                
         
         
            
               Textkorpus
                
            Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (vgl. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches 
                Teilkorpus.
            
            Der 
                    Parzival
                Wolframs von Eschenbach ist ein arthurischer Gralroman in mittelhochdeutscher Sprache, entstanden zwischen 1200 und 1210. Der 
                    Parzival zeichnet sich u.a. durch sein enormes Figureninventar und seine komplexen genealogischen Strukturen aus, wodurch er für Analysen zu Figurenrelationen von besonderem Interesse ist. Der Text ist in 16 Bücher unterteilt und umfasst knapp 25.0000 Verse.
                
            
               Johann Wolfgang von Goethes 
               Die Leiden des jungen Werthers ist ein Briefroman aus dem Jahr 1774. Unsere Annotationen sind an einer überarbeiteten Fassung von 1787 vorgenommen und umfassen die einleitenden Worte des fiktiven Herausgebers sowie die ersten Briefe von Werther an seinen Freund Wilhelm.
                
            Das 
                    Plenardebattenkorpus des deutschen Bundestages besteht aus den von Stenografinnen und Stenografen protokollierten Plenardebatten des Bundestages und umfasst 1.226 Sitzungen zwischen 1996 und 
                    2015. Unsere Annotationen beschränken sich auf Auszüge aus insgesamt vier Plenarprotokollen, die inhaltlich Debatten über die Europäische Union behandeln. Hierbei wurde pro Protokoll jeweils die gesamte Rede eines Politikers bzw. einer Politikerin annotiert.
                
         
         
            
               Ablauf
                
            Der Ablauf des Tutorials orientiert sich an sog. 
                    shared tasks aus der Computerlinguistik, wobei der Aspekt des Wettbewerbs im Tutorial vor allem spielerischen Charakter hat. Bei einem traditionellen 
                    shared task arbeiten die teilnehmenden Teams, oft auf Basis gleicher Daten, an Lösungen für eine einzelne gestellte Aufgabe. Solch eine definierte Aufgabe kann z.B. 
                    part of speech-tagging sein. Durch eine zeitgleiche Evaluation auf demselben Goldstandard können die entwickelten Systeme direkt verglichen werden. In unserem Tutorial setzen wir dieses Konzept live und vor Ort um.
                
            Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann.
            Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Ähnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen („brute force'“) zeitlich Grenzen gesetzt sind.
            Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (vgl. Reiter et al., 2017b) – stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden.
            Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit.
         
         
            
               Lernziele
                
            Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmenden des Workshops bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar.
            Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können.
                    
            
            
               Zeitplan:
               (Dauer in Minuten, ca.)
                
            Im Vorfeld der Veranstaltung: Installationsanweisungen und Support
            
               (10) Lecture
                        
                     Intro & Ablauf
                  
               
               (15) Hands-On
                        
                     Test der Installation bei allen
                  
               
               (50) Lecture
                        
                     Einführung in Korpus und Annotationen
                     Grundlagen maschinellen Lernens
                     Überblick über das Skript (where can you edit what?)
                                
                           Grundlagen Python Syntax
                           Bereitgestellte Features
                        
                     
                  
               
               (15) Hands-On
                        
                     Erste Schritte
                  
               
               (30) Kaffeepause
               (60) Hands-On
                        
                     Hack
                  
               
               (30) Evaluation
            
         
         
            Beitragende (Kontaktdaten und Forschungsinteressen)
            Der Workshop wird ausgerichtet von Mitarbeitenden des "Center for Reflected Text Analytics" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine "black box" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.
                    
            
            
               Gerhard Kremer
               
                  gerhard.kremer@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
                        Pfaffenwaldring 5b
                        70569 Stuttgart
                    
               Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen.
                        
               
            
            
               Kerstin Jung
               
                  kerstin.jung
                  @ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
                        Pfaffenwaldring 5b
                        70569 Stuttgart
                    
               Kerstin Jungs Forschungsinteressen liegen im Bereich der Nachhaltigkeit von (computer)linguistischen Ressourcen und Abläufen sowie der Verlässlichkeitsbeschreibung von automatisch erzeugten Annotationen. Dabei verfolgt sie einen aufgabenbasierten Ansatz und arbeitet an der Schnittstelle zwischen Computerlinguistik und anderen sprach- und textverarbeitenden Disziplinen.
            
         
         
            
               Zahl der möglichen Teilnehmerinnen und Teilnehmer
                
            Zwischen 15 und 25.
         
         
            
               Benötigte technische Ausstattung
                
            Es wird außer einem Beamer keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem es möglich ist, den Teilnehmenden über die Schulter zu blicken und durch die Reihen zu gehen.
         
      
      
         
            
               www.creta.uni-stuttgart.de
            
            
                    Aus urheberrechtlichen Gründen wird das Tutorial ohne das Teilkorpus zu Adornos ästhetischer Theorie stattfinden, das in den Publikationen erwähnt wird.
                
            
                    Die Texte wurden im Rahmen des PolMine-Projekts verfügbar gemacht: http://polmine.sowi.uni-due.de/polmine/
                
         
         
            
               Bibliographie
               
                  Kuhn, Jonas / Reiter, Nils (2015): "A Plea for a Method-Driven Agenda in the Digital Humanities" in: Digital Humanities 2015: Conference Abstracts, Sydney.
                    
               
                  Reiter, Nils / Blessing, André / Echelmeyer, Nora / Koch, Steffen / Kremer, Gerhard / Murr, Sandra / Overbeck, Maximilian / Pichler, Axel (2017a): "CUTE: CRETA Unshared Task zu Entitätenreferenzen" in Konferenzabstracts DHd2017, Bern.
                    
               
                  Reiter, Nils / Kuhn, Jonas / Willand, Marcus (2017b): "To GUI or not to GUI?" in Proceedings of INFORMATIK 2017, Chemnitz.
                    
               
                  Blessing, André / Echelmeyer, Nora / John, Markus / Reiter, Nils (2017): "An end-to-end environment for research question-driven entity extraction and network analysis" in Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, Vancouver.
                    
               
                  Lotman, Juri (1972): Die Struktur literarischer Texte, München.
                    
            
         
      
   



      
         Das Zedlersche 
                Universal-Lexicon (17-1754) stellt mit seinen 68 Bänden eine wichtige Quelle für die Erforschung des 18. Jahrhunderts dar. Durch das Portal „Zedler-Lexikon.de“ wurde dieses „bedeutendste Monument des enzyklopädischen Schreibens im Zeitalter der Aufklärung“ (Schneider 2013, S. 9) in einem Kooperationsprojekt zwischen der Herzog August Bibliothek Wolfenbüttel und der Bayerischen Staatsbibliothek München von 2004 bis 2007 digital erschlossen (siehe: Dorn et al. 2008). Entsprechend wurde dieser Webauftritt zu einer wichtigen digitalen Quelle, was sich in hohen Zugriffszahlen widerspiegelt (Dorn et al. 2008, S. 100). Daher erschien es grundlegend angebracht, diesen digitalen Zugriff auf die analoge Quelle einer gründlichen Kritik zu unterziehen.
            
         Das vorgeschlagene Poster präsentiert die Ergebnisse dieser Quellenkritik. Bei einem Abgleich der Anzahl der digitalen Scanbilder mit der gedruckten Seitenzählung zeigte sich, dass teilweise bis zu 180 Seiten pro Band „fehlten“. Die fehlende Seitenzahl geht nach Überprüfung am Original jedoch auf Fehler im Buchdruck zurück und bildet ein deutliches Muster ab, dass die Tätigkeit der verschiedenen Redakteure (Jakob August Franckenstein 1731-32, Paul Daniel Longolius 1733-35 und Carl Günther Ludovici 1738-54) abbildet. 
         
            
         
         So zeigen deutliche Variationen in der Seitenzählung (Auslassung und Doppelzählung von Seiten) die anfänglichen Schwierigkeiten im verteilten Druck zwischen Halle, Leipzig und Delitzsch (1731-36) und die deutliche Professionalisierung des Drucks nach der Übernahme der Redaktion durch Carl Günther Ludovici ab 1739.
         Dies ist für die Forschung interessant, da die Entstehungsbedingungen trotz bereits beachtlicher Erfolge (Calov 2007, Haug 2007, Löffler 2007, Prodöhl 2005, Quedenbaum 1977) immer noch viele Fragen aufwerfen. Zusätzlich zeigt das Poster Ergebnisse einer Analyse von Metadaten, die mittels eines Python Scripts von Zedler-Lexikon.de abgerufen wurden:
         Kamen beispielsweise im ersten Band auf jeden Artikel nur etwa 0,3 Verweise, so stieg diese bis zum letzten Band auf rund 2 Verweise pro Artikel an. Darin zeigt sich ein zunehmender Anspruch an die Nutzerfreundlichkeit des Universal-Lexicon von Seiten der Redakteure. Weiters zeigt eine Untersuchung der Länge der einzelnen Artikel einen deutlichen Wandel vom knappen Konversationslexikon zur umfangreichen Enzyklopädie. Der Begriff »Enzyklopädie« bezeichnet in diesem Kontext enzyklopädische Lexika, eine Bedeutung, die sich ab der zweiten Hälfte des 18. Jahrhunderts durch die Vorbildwirkung der Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers herausbildete. Die ältere Bedeutung von »Enzyklopädie« als systematische Wissensordnung trifft auf das UL nicht zu. (Vergleiche hierzu ausführlich Dierse 1977).
         Klar erkennbar wird hier jedoch, dass die durchschnittliche Länge der Artikel pro Band von 1732 bis 1754 stetig zunimmt und um 1754 durchschnittlich die 8-fache Länge gegenüber 1732 erreicht. 
         
            
         
         Hier zeigt sich vor allem in den sehr langen Artikeln der letzten Bände (wie Wien mit 134 Seiten, Wolfische Philosophie mit 174 Seiten oder Zunft mit 54 Seiten) nicht nur der Anspruch, Themen nun in größerem Detailgrad darzustellen, sondern auch ein finanzielles Motiv der Herausgeber. So erschienen von 1747-1750 noch acht Bände zum Buchstaben „W“ und nochmal fünf Bände zum Buchstaben „Z“, wohl um die profitablen Zahlungen der Subskribenten noch möglichst lange zu nutzen.
         Die Zusammenführung dieser und weiterer erhobenen Metadaten mit der wechselvollen Geschichte des 
                Universal-Lexicon, wie sie die bisherige Forschungsliteratur rekonstruieren konnte, ermöglichte es, viele Ergebnisse der historischen Forschung weitgehend zu bestätigen. Gleichzeitig mahnen die Erkenntnisse damit zu einer kritischen Verwendung des 
                Universal-Lexicons, da dieses in den 23 Jahren seiner Entstehung großen inhaltlichen Wandlungsprozessen unterzogen war und daher Artikel aus dem Band A-Am (1732) anders bewertet werden müssen als Artikel aus dem Band Zm-Zz (1750) oder gar aus den vier Supplementbänden (1751-54).
            
         Der Einsatz digitaler Quellenkritik an Zedler-Lexikon.de führte in diesem Fall nicht nur zu einer positiven Bestätigung der digitalen Repräsentation des Werkes, sondern lieferte auch Erkenntnisse zum analogen Werk, die ohne die vorhergehende Digitalisierung und den Einsatz entsprechender Methoden nicht realisierbar gewesen wäre.
      
      
         
            
               Bibliographie
               
                  Calov, Carla (2007): Quellen zu Johann Heinrich Zedler und seinem Lexikon im Stadtarchiv Leipzig. In: Leipziger Jahrbuch zur Buchgeschichte 16, S. 203–244.
					
               
                  Dierse, Ulrich: Enzyklopädie. Zur Geschichte eines philosophischen und wissenschaftstheoretischen Begriffs. Band. 2. Bonn 1977.
					
               
                  Dorn, Nico / Oetjens, Lena / Schneider, Ulrich Johannes (2008): Die sachliche Erschließung von Zedlers "Universal-Lexicon". Einblicke in die Lexicographie des 18. Jahrhunderts. In: Das achtzehnte Jahrhundert 32 (1), S. 96–125.
					
               
                  Haug, Christine (2007): Das "Universal-Lexicon" des Leipziger Verlegers Johann Heinrich Zedler im politischen Konfliktfeld zwischen Sachsen und Preußen. In: Leipziger Jahrbuch zur Buchgeschichte 16, S. 301–331.
					
               
                  Jürgens, Hanco / Lüsebrink, Hans-Jürgen (2017): Enzyklopädismus und Ökonomie im Aufklärungszeitalter. Zur Einführung. In: Das achtzehnte Jahrhundert 41 (2), S. 197–202.
					
               
                  Löffler, Katrin (2007): Wer schrieb den Zedler? Eine Spurensuche. In: Leipziger Jahrbuch zur Buchgeschichte 16, S. 265–284.
					
               
                  Lohsträter, Kai / Schock, Flemming (Hg.) (2013): Die gesammelte Welt : Studien zu Zedlers "Universal-Lexicon" ; [Ergebnisse einer internationalen Tagung, die im November 2010 in der Herzog-August-Bibliothek Wolfenbüttel stattgefunden hat]. Wiesbaden: Harrassowitz (Schriften und Zeugnisse zur Buchgeschichte).
					
               
                  Prodöhl, Ines (2005): "Aus denen besten Scribenten." Zedlers "Universal Lexicon" im Spannungsfeld zeitgenössischer Lexikonproduktion. In: Das achtzehnte Jahrhundert 29 (1), S. 82–94.
					
               
                  Quedenbaum, Gerd (1977): Der Verleger und Buchhändler Johann Heinrich Zedler 1706-1751. Ein Buchunternehmer in den Zwängen seiner Zeit ; ein Beitrag zur Geschichte des deutschen Buchhandels im 18. Jahrhundert. Hildesheim u.a.: Olms.
					
               
                  Schneider, Ulrich Johannes (2013): Die Erfindung des allgemeinen Wissens : enzyklopädisches Schreiben im Zeitalter der Aufklärung. Berlin: Akademie-Verlag.
					
            
         
      
   



      
         Das Projekt „Paleocoran“ untersucht Koran-Handschriftenfragmente aus dem siebten bis zehnten Jahrhundert aus der Amr ibn al-ʿĀs- Moschee in al-Fuṣtāt (Alt-Kairo), die heute in verschiedenen Sammlungen weltweit verteilt aufbewahrt werden. Paleocoran sammelt kodikologische (buchgeschichtliche) und paläographische (schriftgeschichtliche) Daten zu den Handschriftenfragmenten, um die zu rekonstruieren, welche Fragmente ursprünglich einen Kodex gebildet haben. Die digitale Rekonstruktion der Kodizes – einem Puzzle mit ca. 25.000 Teilen zu vergleichen – wird durch IIIF digital ermöglicht.
         Das von F. Déroche (Paris) und M. Marx (Potsdam) geleitete DFG-ANR-Projekt „Paleocoran“ greift inhaltlich und methodisch an das Akademievorhaben „Corpus Coranicum“ der BBAW an: Datensätze aus den Corpus-Coranicum-Datenbanken zu Handschriften und Koran-Textvarianten werden in einem System zur wortgenauen Textstellenverortung von Koranpassagen verwendet. Die in „Paleocoran“ und „Corpus Coranicum“ generierten philologischen Daten werden in einem web-basierten System auf Grundlage des PHP-Frameworks Laravel in einer MySQL-Datenbank aufgezeichnet.
         Für die Rekonstruktion der Korankodizes aus Alt-Kairo werden die Seiten der einzelnen Fragmente in der Datenbank erfasst und mit Textstellenkoordinaten (Sure-Vers-Wort) ausgezeichnet. Derzeit befinden sich ca. 1100 Koranhandschriften mit insgesamt über 25.000 Manuskriptseiten in der aus mehr als 40 Sammlungen Corpus Coranicum-Datenbank. Die hier vorgehaltenen Bilder werden über den IIIF-kompatiblen Bildserver digilib ausgeliefert.
         Anhand der Textstellenangaben wird der Text auf der Manuskriptseite mit dem Text der Koran-Druckausgabe Kairo 1924 nach Schreibvarianten, Textvarianten und Verszählung untersucht. In vielen Fällen enthalten die verschiedenfarbigen Tinten der Handschrift Vokalzeichen, die Textvarianten in die Handschriften eintragen. Auch diese werden als unterschiedliche Lesarten verzeichnet.
         Außerhalb der philologischen Daten stellen Illuminationen und Ornamente eine wichtiges Kennzeichen für Manuskriptfragmente dar, die ursprünglich zusammengehörten. Die Form und farbliche Gestaltung der funktionalen Ornamente (Verstrenner und Kapitelüberschriften). Form und Layout (z.B. Pflanzenornamente oder geometrische Muster) der Ornamente oder deren farbliche Gestaltung lassen dabei auf eine gemeinsame Herkunft der Fragmente schließen.
         Für den Vergleich und die Darstellung von Orthographiedifferenzen im Vergleich zum Text der Druckausgabe Kairo 1924 wurde im Rahmen des Paleocoran-Projekts die Programmierbibliothek „Rasmify“ entwickelt. Rasmify entfernt sämtliche buchstabendifferenzierenden Zeichen (Diakritika) und Vokalzeichen aus arabischen Zeichenketten, sodass nur das Konsonantenskelett (arabisch: rasm) der verarbeiteten Strings übrig bleibt. Dies ist wichtig, da die frühen Koranhandschriften sehr häufig Buchstaben undifferenziert schreiben. Durch die reduzierte Wortform wird es einfacher, Unterschiede zwischen einzelnen Koranhandschriftenfragmenten und der Kairiner Druckausgabe zu identifizieren. Das Programm „Rasmify“ wurde in PHP, Python 3 und JavaScript als freie Software veröffentlicht und kann einfach über die jeweiligen Abhängigkeitsverwaltungen composer, pip und npm nachgenutzt werden.
         Ähnliche Muster bei Abweichungen in Textvarianten (Lesarten), Schreibvarianten- und Verssegmentierung weisen darauf hin, dass die betreffenden Fragmente ursprünglich demselben Korankodex stammen. Bei genügend vorliegenden Indizien werden die einzelnen Handschriftenfragmente bzw. Teile der Fragmente einem virtuellen Kodex zugeordnet. Durch die den einzelnen Handschriftenseiten zugeordneten Textstellen werden dann die dem virtuellen Kodex zugeordneten Handschriftenseiten nach Textkoordinate sortiert, sodass letztendlich ein IIIF-Manifest für den virtuellen Kodex erstellt werden kann.
         Auf der Projektwebseite paleocoran.eu kann mittels des IIIF-Manifests der virtuell rekonstruierte Korankodex in seiner ursprünglichen Form im IIIF-Viewer Mirador digital abgebildet werden. Mirador bietet darüber hinaus Lichttischfunktionalitäten, sodass sowohl einzelne Seiten desselben virtuell rekonstruierten Korankodex als auch unterschiedliche virtuell rekonstruierte Korankodizes miteinander gezielt verglichen werden können.
         Zusätzlich werden Metadaten der zugeordneten Manuskriptfragmente sowie Metadaten zur kodikologischen und paleographischen Einordnung des rekonstruierten Kodex angezeigt. Weiterhin werden die Lesarten- und Orthographievarianten sowie Ornamente samt Wortkoordinate und zitierfähigem IIIF-Bildausschnitt angegeben, sodass Forschende die virtuelle Rekonstruktion nachvollziehen können.
         Zum aktuellen Zeitpunkt wurden 338 virtuell rekonstruierte Kodizes bzw. Kodexteile angelegt und über 1500 Lesartenvarianten sowie über 2500 Orthographieunterschiede identifiziert. Durch die Anbindung an das Corpus Coranicum Projekt ist die langfristige Sicherung und Nachnutzung gewährleistet. Der Launch der Paleocoran-Projektwebseite soll Ende 2018 erfolgen.
      
   



      
         
            Beschreibung
            
               
               Briefeditionen sind ein Typus der digitalen Edition, in dem die Vorteile des digitalen Mediums bereits mit am intensivsten fruchtbar gemacht werden.
               1
                
               In der alltäglichen Arbeit des Edierens sowie der Software-Entwicklung richtet sich der Blick zum großen Teil meist auf den einzelnen Brief und seine Tiefenerschließung, weniger auf die Menge an Briefen eines Korrespondenzkorpus. Weiterführende quantitative Analysen auf Basis der Tiefenerschließung (vollständige Transkription, Modellierung in XML/TEI, Normdaten etc.) und mit digitalen Methoden, die gerade auch für korpusübergreifende Untersuchungen den Weg ebnen würden, sind traditionellerweise in Editionsprojekten (noch) nicht vorgesehen.
               2
                Mit dem 
               eintägigen 
               Workshop „Distant Letters“ möchten wir ein Panorama an quantitativ orientierten Analysemethoden und -praktiken für Daten digitaler Briefeditionen vorstellen, 
               vermitteln und diskutieren, um so neue Perspektiven 
               auf
                Briefkorpora 
               zu
                erproben.
               3
                
               Der Workshop gliedert sich in vier Abschnitte:
            
            
               Auswertung von Metadaten und Entitäten
               
                  
                  Auf der Grundlage von standardisiert kodierten Briefmetadaten in XML/TEI sollen mit der Abfragesprache XQuery zunächst Fragen formuliert werden wie: „Wie viel hat Sender A an Empfänger B insgesamt geschrieben? Wie viel in einem bestimmten Jahr?“ Im Anschluss sollen vergleichende Untersuchungen angestellt werden, denen Fragen wie „Wie viel hat Sender A an Empfänger B und Empfänger C geschrieben?“ oder „Wie gestaltet sich das Verhältnis von gesendeten und empfangenen Briefen in der Korrespondenz von A und B?“ zugrunde liegen. Auch Entitäten aus dem Brieftext können in die Untersuchung mit einbezogen werden („Wie häufig wird Person X im Verlauf der Korrespondenz erwähnt?“). Das Ergebnis von derlei Fragen sind statistische Werte, die, um sie interpretatorisch zugänglicher zu machen, weiter aufbereitet werden müssen, z.B. als Visualisierungen in Diagrammen, Kreisen und Kurven.
               
            
            
               Analyse linguistischer Merkmale
               
                  
                  Im zweiten Teil wendet sich die Untersuchung den Volltextdaten zu. In den Blick genommen werden dabei linguistische Merkmale auf Token-Ebene (z.B. Lemma und Wortart), die einfachen oder komplexen Abfragen (z.B. nach typischen Adjektiv-Anbindungen bestimmter Substantive, Häufungen einer bestimmten Wortart, festen Wendungen, Kollokationen) an den Text zugrunde gelegt werden können und so u.a. Aufschluss über inhaltliche und stilistische Gegebenheiten ermöglichen. Im Workshop werden Werkzeuge gezeigt und benutzt, die zum einen die automatische linguistische Analyse von Texten, z.B. deren Lemmatisierung und POS-Tagging, erlauben und zum anderen Möglichkeiten der Auswertung annotierter linguistischer Merkmale bieten, z.B. mittels leistungsstarker Suchanfragesprachen oder Möglichkeiten der Visualisierung. Genauer in den Blick genommen und z.T. benutzt werden TXM, Corpus Workbench, DTA und WebLicht.
                  4
               
            
            
               Topic Modeling
               
                  
                  Im dritten Teil des Workshops rücken die Inhalte der Briefkommunikation stärker in das Zentrum des Interesses, wenn Fragen aufgegriffen werden wie „Welche Themen werden 
                  behandelt und wie sind diese zeitlich verteilt?“ oder „Gibt es bestimmte Themen, die in einer bestimmten Personengruppe stärker verhandelt werden als in einer anderen?“. Analysiert wird dabei der Volltext der Briefe, zusätzlich können jedoch auch die Briefmetadaten in die Interpretation der Analyseergebnisse einfließen. Für die Modellierung 
                  der Topics
                   wird das Tool „Mallet“
                   
                  verwendet,
                  5
                   und es wird im Workshop gemeinsam ein Topic Model für ein Briefkorpus erstellt. Für die Auswertung in Kombination mit Metadaten und Visualisierungen wird der „Topic Modeling Workflow“ 
                  (TMW) verwendet.
                  6
                  
                  Diskutiert werden soll 
                  außerdem, wie sich die Konzepte ‚Topic‘ und ‚Thema‘ zueinander verhalten.
                  7
               
            
            
               Stilometrie
               
                  Im letzten Teil des Workshops soll mit Methoden und Tools der Stilometrie der Sprach- bzw. Schreibstil eines Briefkorpus genauer untersucht werden. Analysiert wird dabei erneut der Volltext, diesmal in orthografisch normalisierter Form. Mögliche Fragestellungen der Analyse sind: „Welche Rückschlüsse erlauben stilometrische Analysen hinsichtlich Sender und Empfänger der Briefe?“, „Korrelieren die stilistische Nähe bzw. Distanz mit Faktoren wie Zeit, Raum oder Empfänger?“. Auch Stilvergleiche werden beispielhaft auf Grundlage der Fragen „Ändert sich der Stil von Sender A in seinen Briefen an die Empfänger B und C?“ und „Variiert der Stil zwischen Geschäfts- und Privatkorrespondenz?“ unternommen. Für die stilometrischen Analysen nutzen wir das „Stylo“-Paket für R.8
                  
                  Auch für die stilistischen Analysen ist zu diskutieren, welches Konzept von Stil hinter den gewählten Methoden steht und wie es sich zu anderen Definitionen von Stil verhält.
                  9
               
            
         
         
            Ziele
            
               Ziel des Workshops ist es, ein Panorama quantitativer Analysemöglichkeiten für Briefkorpora vorzustellen, das eine Ergänzung zu den traditionellen ‚close reading‘-Verfahren in wissenschaftlichen Editionen darstellt und die Digitalität der Editionsdaten mit Methoden der Digital Humanities noch stärker für quellenimmanente Forschungsfragen fruchtbar macht. Die Teilnehmerinnen und Teilnehmer sollen den Workshop am Ende des Tages mit einem Set an Skripten und Tools verlassen und in der Lage sein, diese auf andere (ggf. eigene) Datensätze anzuwenden. Neben der Vermittlung von technischen Fertigkeiten ist die Diskussion der Methoden und Ergebnisse mit den Teilnehmerinnen und Teilnehmern fester Bestandteil des Workshops. 
                    
               Es soll dabei gemeinsam eruiert werden, auf welchen theoretischen Annahmen die Methoden jeweils basieren, wo ihre Stärken und Schwächen liegen und auch inwieweit die vermittelten Praktiken eine Chance haben könnten, zukünftig ein Bestandteil bei der Erstellung und Nutzung wissenschaftlicher digitaler Briefeditionen zu werden. 
            
         
         
            Daten
            Die Organisatorinnen und Organisatoren stellen XML/TEI und Plain Text Datensätze aus zwei verschiedenen Briefeditionen für den Workshop bereit: ca. 5500 Brieftexte und ebenso viele Metadatensätze aus „Jean Paul - Sämtliche Briefe digital“ (Bernauer / Miller / Neuber 2018) sowie ca. 400 Brieftexte und 3000 Metadatensätze der „edition humboldt digital“ (Ette 2017-). Darüber hinaus steht es den Teilnehmerinnen und Teilnehmer frei, ihre eigenen Datensets (XML/TEI-kodiert und Plain Text) zu verwenden.
         
         
            Teilnehmerzahl und Vorkenntnisse
            Die Anzahl der Teilnehmerinnen und Teilnehmer ist auf 25 begrenzt. Gewisse Grundkenntnisse in der Programmierung (z.B. XSLT/XQuery, Python, R) sind von Vorteil, die im Workshop verwendeten Skripte werden jedoch so vorbereitet, dass sich die Arbeit daran auf Modifikationen und Erweiterungen unter Anleitung der Lehrenden beschränkt. Im Vorfeld des Workshops werden Installationshinweise für die verwendeten Werkzeuge gegeben und die Übungsdaten zum Download bereitgestellt.
         
         
            Lehrende
            
               
               Stefan Dumont: Wissenschaftlicher Mitarbeiter bei der TELOTA-Initiative der Berlin-Brandenburgischen Akademie der Wissenschaften, dort u.a. zuständig für die „edition humboldt digital”. Wissenschaftlicher Koordinator des DFG-Projekts „correspSearch - Briefeditionen vernetzen“. Co-Convener der TEI Special Interest Group „Correspondence“. Expertise u.a. mit Standardisierung von Briefkodierung und -metadaten und X-Technologien.
                
            
               Susanne Haaf: Wissenschaftliche Mitarbeiterin im Projekt CLARIN-D an der Berlin-Brandenburgischen Akademie der Wissenschaften, u.a. beteiligt am Auf- und Ausbau des Deutschen Textarchivs. Doktorandin im Bereich korpusbasierter Untersuchung von Textsortenspezifika. Spezialisierung in Korpusaufbau, Korpuslinguistik, Standards für Text- und Metadaten (insbes. TEI) sowie Textedition.
                
            
               Ulrike Henny-Krahmer: Wissenschaftliche Mitarbeiterin im Projekt „Computergestützte Literarische Gattungsstilistik” (CLiGS) an der Universität Würzburg. Studium der Regionalwissenschaften Lateinamerika in Köln und Lissabon, Doktorandin in Digital Humanities mit dem Thema „Topic and Style in Subgenres of the Spanish American Novel (1830-1910)“.
                
            
               Benjamin Krautter: Wissenschaftlicher Mitarbeiter im Projekt “Quantitative Drama Analytics” (QuaDramA) an der Universität Stuttgart. Studium der Literaturwissenschaft (Germanistik) und Politikwissenschaft in Stuttgart und Seoul (Südkorea), Doktorand im Bereich Digital Literary Studies mit dem Thema “Quantitative Dramenanalyse - Operationalisierung aristotelischer Kategorien” (Arbeitstitel).
                
            
               Frederike Neuber: Wissenschaftliche Mitarbeiterin bei der TELOTA-Initiative der Berlin-Brandenburgischen Akademie der Wissenschaften, dort u.a. zuständig für die Briefedition “Jean Paul - Sämtliche Briefe digital”. Studium der Italianistik und Editionswissenschaft in Berlin und Rom, Doktorandin in Digital Humanities. Spezialisierung in Editionsphilologie, Datenmodellierung und Programmierung mit X-Technologien.
                
         
      
      
         
            
                Der webservice „correspSearch“ etwa illustriert die Bedeutung von standardisierter Metadatenerfassung mit Normdaten zur Vernetzbarkeit von Korrespondenzen, . 
            
                Vereinzelt werden quantitative Analysemethoden bereits auf Editionsdaten angewandt: Etwa wird im Kontext des Projekts “Mapping the Republic of Letters” (Stanford University 2013) zur Erschließung der Briefkommunikation und -verbreitung mit verschiedenen statistisch- und/oder netzwerkanalytisch-basierten Visualisierungen experimentiert; Andorfer (2017) erprobt Topic Modelling mit dem Korrespondenzkorpus Leo von Thun-Hohensteins.
            
                Nicht Teil dieses Panoramas ist die Netzwerkanalyse, auch wenn diese Form der Auswertung bzw. Visualisierung für Briefdatensätze oft die am naheliegendste scheint. Grundkompetenzen zur Netzwerkanalyse bzw. -visualisierung werden mittlerweile regelmäßig in Workshops vermittelt, z.B. im Rahmen der „Historical Network Research-Community“ (http://historicalnetworkresearch.org/). Der Fokus des Workshops richtet sich daher auf bisher weniger berücksichtigte Formen der Analyse von Briefkorpora.
            
               
                http://textometrie.ens-lyon.fr, , , .
            
               
               
            
            
               
               
            
            
                Zwar ist das Verfahren für die Ermittlung von Schlüsselwörtern und Themen entwickelt worden, je nach verwendetem Korpus ergeben sich aber auch andere Arten von Topics, z.B. sprachspezifische oder motivische. Vgl. dazu u.a. Rhody (2012) und Schöch (2017).
            
               
               
            
            
                Für einen Überblick zu verschiedenen Stilbegriffen in der Literatur- und Sprachwissenschaft siehe Herrmann et al. (2015).
         
         
            
               Bibliographie
               
                  
                  Andorfer, Peter (2017):
                  Turing Test für das Topic Modeling. Von Menschen und Maschinen erstellte inhaltliche Analysen der Korrespondenz von Leo von Thun-Hohenstein im Vergleich, 
                        in: Zeitschrift für digitale Geisteswissenschaften; doi: 
                        
                     10.17175/2017_002
                  [zuletzt abgerufen 7. Januar 2019].
                    
               
                  Bernauer, Markus / Miller, Norbert / Neuber, Frederike (eds.) (2018):
                  Jean Paul – Sämtliche Briefe digital. 
                        In der Fassung der von Eduard Berend herausgegebenen 3. Abteilung der Historisch-kritischen Ausgabe (1952-1964), im Auftrag der Berlin-Brandenburgischen Akademie der Wissenschaften überarbeitet und herausgegeben von Markus Bernauer, Norbert Miller und Frederike Neuber; 
                        
                     http://jeanpaul-edition.de
                   [letzter Zugriff 7. Januar 2019].
                    
               
                  
                  Burrows, John (2002):
                  Delta’: a Measure of Stylistic Difference and a Guide to Likely Authorship, 
                        in: Literary and Linguistic Computing 17/3, S. 267–287.
                    
               
                  Dumont, Stefan (2016):
                  correspSearch – Connecting Scholarly Editions of Letters, 
                        in: Journal of the Text Encoding Initiative [Online], Issue 10; 
                         [letzter Zugriff 7.Januar 2019].
                    
               
                  Eder, Maciej / Rybicki, Jan / Kestemont, Mike (2016):
                  Stylometry with R: A Package for Computational Text Analysis, 
                        in: The R Journal 8/1 (2016), S. 107–121.
                    
               
                  Ette Ottmar (eds.) (seit 2016):
                  edition humboldt digital. 
                        Berlin-Brandenburgische Akademie der Wissenschaften, Berlin. Version 3 vom 14.09.2018; 
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  Graham, Shawn / Weingart, Scott / Milligan, Ian (2012):
                  Getting Started with Topic Modeling and MALLET, 
                        in: The Programming Historian 1;
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  
                  Heiden, Serge (2010):
                  The TXM Platform: Building Open-Source Textual Analysis Software Compatible with the TEI Encoding Scheme, 
                        24th Pacific Asia Conference on Language, Information and Computation, Nov 2010, Sendai, Japan. Institute for Digital Enhancement of Cognitive Development, Waseda University, S.389–398. 
                    
               
                  
                  Herrmann, Berenike J. / van Dalen-Oskam, Karina / Schöch, Schöch (2015):
                  Revisiting Style, a Key Concept in Literary Studies, 
                        in: Journal of Literary Theory 9/1, S. 25–52.
                    
               
                  
                  Rhody, Lisa M. (2012):
                  Topic Modeling and Figurative Language, 
                        in:  Journal of Digital Humanities 2/1; 
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  
                  Schöch, Christof (2017):
                  Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama, 
                        in: Digital Humanities Quarterly 11/2; 
                        
                        [letzter Zugriff 7. Januar 2019].
                    
               
                  Walmsley, Priscilla (2009):
                  XQuery: Search Across a Variety of XML Data.
                        O’Reilly Media.
                    
            
         
      
   



      
         Zu den Verfahren der digitalen Textanalyse, die in den vergangenen Jahren in den textbasierten digitalen Geisteswissenschaften etabliert wurden, gehört das LDA (Latent Dirichlet Allocation) Topic Modeling (Blei 2012; Steyvers und Griffiths 2006). Diese Methode eignet sich zur Analyse der Verteilung semantischer Wortgruppen in Textsammlungen, und kann sowohl für die computergestützte Textklassifikation als auch für die explorative Betrachtung der Inhalte eines Corpus herangezogen werden. Um dem zunehmenden Interesse am Topic Modeling von Seiten der DH-Community Rechnung zu tragen, entwickelt DARIAH-DE seit 2017, basierend auf den Python-Bibliotheken "LDA" von Allan Riddell und "DARIAHTopics" (Jannidis et al. 2017), den DARIAH-TopicsExplorer, eine Software, mit der interessierte Forschende Topic Modeling auf ihren eigenen Rechnern an ihren eigenen Texten ausprobieren können, und die den gesamten Analyseprozesses, vom unverarbeiteten Text bis zum Ergebnis, durch eine graphische Nutzeroberfläche (GUI) unterstützt.
         Der TopicsExplorer bietet nicht die Leistungsfähigkeit und vor allem die Flexibilität bisheriger Lösungen, die entweder, wie das weit verbreitete MALLET (McCallum 2002), als Kommandozeilenprogramme, oder aber, wie Gensim (Rehurek und Sojka 2010), als Bibliothek für eine Programmiersprache konzipiert wurden. Dafür kann er aber ohne Kenntnis irgendeiner Programmier- oder Skriptsprache eingesetzt werden und muss nicht einmal über die Kommandozeile aufgerufen werde. Damit schließt der TopicsExplorer eine wichtige Lücke: Forschende, die selbst nicht, oder noch nicht, programmieren, können sich hier einen Eindruck davon verschaffen, wie die Methode funktioniert und was sie theoretisch leisten kann. Das befähigt auf der einen Seite, Forschungsarbeiten, die auf Topic Modeling basieren, und ihre Ergebnisse informiert einzuschätzen. Auf der anderen Seite kann das Werkzeug für einfache Fragestellungen, die Topic Modeling erfordern, direkt eingesetzt werden, und bei komplizierteren Ansätzen eine informierte Entscheidung darüber ermöglichen, ob sich die Aneignung der notwendigen technischen Fähigkeiten für die Verwendung einer fortgeschrittenen Lösung für die jeweilige Forschungsfrage überhaupt lohnt.
         Der TopicsExplorer wurde in einer ersten Version 2017 und 2018 im Rahmen mehrerer Workshops und Konferenzen verschiedenen Gruppen von Forschenden und Studierenden vorgestellt. Die Erfahrungen aus dem Umgang mit Nutzerinnen und Nutzern in solchen Workshops und vor allem ihr direktes Feedback sind seither umfangreich in die Weiterentwicklung eingeflossen und die daraus resultierenden Änderungen gehen weit über die Beseitigung von Bugs und die Sicherstellung der nachhaltigen Funktionalität hinaus. Aus einem anfänglichen "GUI-Demonstrator" (Simmler et al. 2018), der noch die Installation einer Python-Bibliothek erforderte, auf einem lokalen Server lief und im Browser angezeigt wurde, ist eine vollwertige Stand-Alone-Software geworden, die nach dem Herunterladen ohne weitere Vorbereitung auf gängigen Windows-, MacOS- und Linux-Systemen gestartet werden kann. Die Visualisierungen können interaktiv manipuliert, und die Ergebnisse im csv-Format exportiert werden. Zahlreiche kleinere, von der Testcommunity gewünschte Features, wie z.B. eine Fortschrittsanzeige mit Abbruchbutton (Abb. 1), haben die Usability wesentlich verbessert.
         Zur Zeit wird eine Version in grundlegend überarbeitetem Design vorbereitet, deren Ziel es ist, auch komplizierteren, aus den Testcommunies heraus formulierten Ansprüchen an die Interaktivität der Software gerecht zu werden. Auf technischer Ebene wird dabei die Visualisierung der Ergebnisse statt mit der bisher verwendeten Python-Bibliothek "Bokeh" direkt in Javascript realisiert, um zusätzliche Flexibilität für die Umsetzung neuer Funktionalitäten zu gewinnen. Äußerlich wird es damit möglich, Ergebnisse auch nachträglich interaktiv umzusortieren und in mehreren Fenstern darzustellen. Mit dem Ziel, die User-Experience im explorativen Umgang mit den erzeugten Modellen zu verbessern, wird darüber hinaus ein völlig neues Visualisierungskonzept auf Basis der Vorschläge von Chaney und Blei (2012) umgesetzt. Dieses Konzept erlaubt es nicht nur, einzelne Dokumente im Corpus mitsamt den dazugehörigen Analyseergebnissen zu betrachten, es werden darüber hinaus automatisch andere Texte mit ähnlichen inhaltlichen Schwerpunkten vorgeschlagen (Abb. 2).
         Wir hoffen, dass der TopicsExplorer mit den angestrebten Verbesserungen und Erweiterungen dazu beiträgt, eine mittlerweile doch recht verbreitete Forschungsmethode aus der Nische derjenigen DH-Verfahren heraus zu holen, die nur von Programmiererinnen und Programmierern verwendet, verstanden und kritisch diskutiert werden. Die neuen Features, die für das kommende Release entwickelt werden, sollten dazu einen wesentlichen Beitrag leisten.
         
            
               
               
                  Abbildung 1. Fortschrittsanzeige für die laufende Modellierung im aktuellen TopicsExplorer
					
            
         
         
            
               
               
                  Abbildung 2. Übersicht für ein Dokument im Prototypen der Version 2
					
            
         
      
      
         
            
               Bibliographie
               
                  Blei, David M. (2012):
                  Probabilistic Topic Models, in: 
                        Communication of the ACM55, Nr. 4 (2012): 77–84. doi:10.1145/2133806.2133826.
                    
               
                  Chaney, Allison J.B. / Blei, David M. (2012):
                  The Visualizing Topic Models, in: 
                        Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media 419-422.
                    
               
                  Jannidis, Fotis/ Pielström, Steffen / Schöch, Christof / Vitt, Thorsten (2017):
                  Making topic modeling easy: a programming library in Python, in: 
                        Proceedings of the Digital Humanities 2017 Conference.
                    
               
                  McCallum, Andrew K. (2002): 
                  MALLET : A Machine Learning for Language Toolkit.
                        
                     http://mallet.cs.umass.edu.
                  
               
               
                  Rehurek, Radim/ Sojka, Petr (2010):
                  Software framework for topic modelling with large corpora. 
                        In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks.
                    
               
                  Simmler, Severin / Vitt, Thorsten / Pielström. Steffen (2018):
                  LDA Topic Modeling über eine graphisches Interface, in: 
                        Konferenzabstracts der 5. Tagung des Verbands Digital Humanities im deutschsprachigen Raum e.V. 428-429.
                    
               
                  Steyvers, Mark/ Griffiths, Tom (2006):
                  Probabilistic Topic Models, in:
                        Latent Semantic Analysis: A Road to Meaning, herausgegeben von T. Landauer, D. McNamara, S. Dennis, und W. Kintsch. Laurence Erlbaum.
                    
            
         
      
   



      
         
            Die 
                Faustedition versammelt 573 Textzeugen mit ca. 730 datierbaren Objekten zu Goethes „Faust“ in einem digitalen Archiv. Für die Benutzer sollen sich die repräsentierten historischen Objekte nicht isoliert voneinander, sondern in einem sinnvollen Zusammenhang darstellen. Eine solcher Zusammenhang kann dadurch entstehen, dass die Objekte genetisch geordnet werden. Genetische Einordnung bedeutet zuallererst, Objekte zeitlich zu situieren. Dabei geht es teils um die Ermittlung von Kalenderdaten, teils um die Bestimmung des relativen zeitlichen Verhältnisses mehrerer Objekte. Die Ermittlung chronologischer Systeme ist eine Grundfrage in vielen Disziplinen. Je nach Sachlage können dabei etwa statistische Verfahren (z.B. Bayliss 2015 zur Archäologie) oder evolutionäre Modelle (Trovato 2014: 189–200 zur mediävistischen Philologie) eingesetzt werden; die Verwendung disziplinfremder Ansätze kann aber auch zu Problemen führen (Pereltsvaig/Lewis 2015 zur Glottochronologie). Beim derzeitigen Stand scheint es am aussichtsreichsten, das in einzelnen Disziplinen oder bei einzelnen Projekten geübte Vorgehen zu formalisieren, um potentiell generalisierbare Verfahren zu entwickeln.
            
         Besonders schwierig ist es, nicht bloß einzelne Objekte zu datieren, sondern eine große Menge in eine chronologische Ordnung zu bringen, wenn die Objekte genetisch voneinander abhängig sind, nur wenige absolute Daten zur Verfügung stehen und sonst nur lokale Anhaltspunkte für relative Chronologien gegeben sind (klassisches Beispiel: die antike Chronographie; Grafton 1993, Burgess/Witakowski 1999). Dies ist auch bei umfangreichen genetischen Handschriftendossiers neuzeitlicher Autoren und Werken mit komplexer Entstehungsgeschichte der Fall. Hier können einzelne Teilentwürfe in relativer zeitlicher Beziehung stehen, vereinzelt sind Datierungen verfügbar; doch die Rekonstruktion der 
                Makrogenese, d.h. der chronologischen Ordnung des Gesamtbestands (zum Begriff Van Hulle 2018: 47–48), kann sich als außerordentlich komplex erweisen.
            
         Und so liegen die Dinge auch bei „Faust“. Nur wenige der makrogenetischen Objekte sind genau datierbar; stattdessen gibt es eine große Menge relativer, aber strikt lokaler Chronologien für jeweils nur einige Objekte. Den bislang einzigen Versuch, Einzelaussagen zu aggregieren und alle Objekte in zeitlich-stemmatische Beziehung zueinander zu setzen, macht Fischer-Lamberg für zwei Akte des “Faust II”. Ihre Stemmata (Fischer-Lamberg 1955: 150–166) markieren die praktische Grenze dessen, was an Einzelinformationen mit menschlichen Mitteln aggregiert werden kann. Die Rekonstruktion einer (theoretisch beliebig großen) Makrogenese verlangt nach maschineller Verarbeitung und visueller Aufbereitung vorhandener Einzelinformationen.
         
            Datengrundlage
            Um dies zu ermöglichen, wurde der Informationsgehalt der verfügbaren einschlägigen Aussagen zur Datierung1 in XML erfasst:
                
            
               bibliographische Quelle
               datierter Textzeuge
               absolute Datierung
               relative Datierung
               (ungefähre) Gleichzeitigkeit von Textzeugen
            
            Eine 
                    Relative Datierung setzt Zeugen2 in eine zeitliche Reihenfolge. Es wird ausgesagt, dass ein Zeuge vor einem anderen entstanden ist.
                
            
               
                  
                  
                     Abbildung 1. Kodierte Aussage: Laut Fischer-Lamberg 1955: 160 (source), ist 2 III H.5 vor 2 III H.8 (temp-pre) (vereinfacht).
						
               
            
            Eine 
                    Absolute Datierung ordnet einen oder mehrere Textzeugen konkreten Datumsangaben zu. Tagesgenaue Datierungen bilden die Ausnahme; häufig sind Aussagen, dass ein Zeuge nicht vor oder nicht nach einem Zeitpunkt entstanden sei. Typisch sind unscharfe Angaben wie 1800/1801 oder Frühsommer. Solche Angaben werden nach dokumentierten Regeln auf Aussagen der Form nicht vor und nicht nach  mit jeweils normierten tagesgenauen Angaben abgebildet.
                
            
               
                  
                  
                     Abbildung 2. Laut Bohnenkamp 1994: 208, ist 2 III H.5 auf den Zeitraum vom 26. Februar zum 5. April 1825 zu datieren (vereinfacht).
						
               
            
         
         
            Modellierung als Graph
            Die verschiedenen Aussagen werden in einem gerichteten Graphen modelliert3: Die Zeugen bilden Knoten, die relativen Datierungen Kanten des Graphen. Zur Integration der absoluten Datierungen werden die Datumsangaben (Tage) ebenfalls als Knoten in den Graphen integriert: Aus einer Datierung wie 
                    2 III H.5 wurde nicht vor dem 26. Februar 1825 und nicht nach dem 5. April 1825 geschrieben wird so ein Teilgraph 
                    1825-05-25 → 2
                
               III
                
               H.5 → 1825-04-06, in dem die Knoten für Ereignisse (Daten oder Zeugen) und die Kanten für 
                    zeitlich vor stehen. Ergänzt wird der Graph durch »Zeitstrahlkanten«, die von jedem Datum zum nächstfolgenden führen.
                
            
               
                  
                  
                     Abbildung 3. Aus Einzelaussagen gebildeter Graph zu 2 III H.8, unmittelbar benachbarten Zeugen und Datierungen (vereinfacht).
						
               
            
            Aus diesem Graphen lassen sich Informationen ableiten, die sich erst aus dem Zusammenspiel der Einzelaussagen ergeben: Betrachtet man einen Zeugen 
                    z, so sind alle von 
                    z aus entlang gerichteter Kanten erreichbaren Zeugen (hier: 2 III H.1) nach 
                    z entstanden, und die von 
                    z aus erreichbaren Daten bilden Grenzen des 
                    terminus ante quem (des Zeitpunkts, vor welchem ein Zeuge entstand). Der Graph bietet damit die Grundlage für eine ungefähre Datierung auch derjenigen Zeugen, für die keine direkte absolute Datierung vorliegt.
                
            Ist die Gesamtheit der Aussagen nicht widerspruchsfrei, so ergeben sich Zyklen im Graphen. Dies induziert einen Teilgraphen, in dem (vgl. Abb. 4 mit relativen Datierungen einiger Handschriften) jeder Knoten von jedem anderen erreichbar ist (der Teilgraph ist 
                    stark zusammenhängend).
                
            
               
                  
                  
                     Abbildung 4. Relative Datierungen für einige Handschriften des 4. Akts. Erst die Entfernung aller rotgestrichelten Kanten führt zu einem zyklenfreien Graphen.
						
               
            
            Um den Graphen aus Abb. 4 zyklenfrei zu machen, ist die Entfernung von wenigstens drei Kanten notwendig (gestrichelt). Der komplette Makrogenesegraph enthält eine stark zusammenhängende Komponente mit 477 Dokumenten und 2136 Kanten – zu umfangreich, um die Konflikte manuell zu eliminieren.
         
         
            Konfliktbehandlung
            Eine möglichst kleine Menge von Kanten zu entfernen, um einen zyklenfreien Graphen zu erhalten, ist ein als 
                    Minimum Feedback Arc Set bekanntes, NP-vollständiges (Karp 1972) Problem der Graphentheorie; um eine optimale Lösung zu finden, ist die größte der stark zusammenhängenden Komponenten zu groß (und die aus graphentheoretischer Sicht optimale Lösung muss auch nicht die philologisch korrekte sein, es wäre nur diejenige, die die wenigsten Datierungsaussagen verwirft). Verwendet wird stattdessen eine Heuristik, z.Z. das Verfahren von Eades (1993, Implementierung von igraph4), wobei zuviel entfernte Kanten nach Möglichkeit wieder hinzugefügt werden.
                
            Entfernt man alle Konfliktkanten, so erhält man einen zyklenfreien gerichteten Graphen (DAG), der die Basis für die automatisierte Weiterverarbeitung ist. Dessen Knoten können in eine 
                    topologische Ordnung gebracht werden, d.h. eine Reihenfolge, die mit allen Kanten des Graphen konsistent ist (Manber 1989: 199).
                
            Um die aus einer Vielzahl teils widersprüchlicher Aussagen mechanisch gezogenen Schlüsse nachvollziehbar und verbesserbar zu machen, werden die Daten in einer Reihe verlinkter Darstellungen mit GraphViz (Gansner/North 2000) visualisiert. Die Grundlage bildet der Gesamtgraph mit allen Aussagen, in dem algorithmisch entfernte Aussagen (Konflikte) rotgestrichelt visualisiert werden.
            Zu jedem Zeugen gibt es einen Teilgraphen, der seine Nachbarn, die nächsten erreichbaren absoluten Datierungen und alle Aussagen dazwischen visualisiert. Darunter werden die Aussagen tabellarisch aufgelistet. Zu jeder (entfernten) Konfliktkante zeigt eine separate Visualisierung einen Pfad in der Gegenrichtung, mit dem die Kante in Konflikt stand, so dass der Konflikt erkennbar wird und zu den beteiligten Zeugen weiternavigiert werden kann.5 Daneben gibt es aufbereitete Darstellungen für jede Szene und jede Quelle.
                
            Anhand der Visualisierungen können die vorliegenden Reihenfolgeentscheidungen nachvollzogen, aber auch Datierungskontroversen und -lücken identifiziert undin den Quelldateien behoben werden:
            
               Aussagen können als 
                        zu ignorieren markiert werden, um sie bei der Bildung des Graphen auszuschließen.
                    
               Quellen können nach ihrer Zuverlässigkeit bewertet werden, um zu beeinflussen, wie leicht oder schwer entsprechende Kanten als Konfliktkanten entfernt werden.
               Eigene Erkenntnisse können mit entsprechend hohem Gewicht einbezogen werden.
            
            Die visualisierten Ergebnisse erfüllen so einen mehrfachen Zweck: Sie dienen zur systematischen Erschließung der einschlägigen Forschung, zur Klärung der genetischen Verhältnisse für den gesamten Faust und als Hilfsmittel zur Überprüfung, Vervollständigung und Verbesserung der Datenlage, d.h. zur Erweiterung des Forschungsstand. Darüber hinaus wird die ermittelte Reihenfolge in der Faustediton verwendet, um die Zeilensynopse sowie das Balkendiagramm zu sortieren.
         
         
            Ausblick
            Neben der manuellen Nachbearbeitung der Daten kommen zur Verbesserung des Verfahrens alternative Heuristiken für das Minimum-Feedback-Arc-Problem in Frage (z.B. Even et al. 1998). 
            
               Bei der vorgestellten Methode werden in der relativen Chronologie die Entstehungsintervalle nur in eine disjunkte vor-nach-Beziehung gesetzt. Möglich wäre es, hier zu prüfen, ob eine weitere Ausdifferenzierung der Beziehungen zwischen Entstehungsintervallen realisierbar ist, so dass Beziehungen der Form „wurde begonnen vor Fertigstellung von“ ausgedrückt werden können. Komplexere Relationen als eine einfache zeitliche Abfolge werden bereits von Allen (1983) in einem Graphmodell beschrieben, allerdings ist hier die globale Konfliktfreiheit ein Problem.
                
            Eine Alternative zu der oben beschriebenen Abbildung unscharfer Aussagen auf scharf begrenzte Intervalle ist etwa die Modellierung über Fuzzy-Mengen (vgl. z.B. Barro et al. 1994). Dies erfordert jedoch auch die Neudefinition der Relationen (Schockaert/De Cock 2008) und der darauf aufbauenden Verfahren etwa zur Konfliktauflösung.
            Der vorgestellte Ansatz basiert auf bereits vorhandenen, mit traditionellen philologischen Mitteln gewonnenen Datierungsaussagen. In Wissenbach/Pravida/Middell (2012) wird ein Verfahren vorgestellt, mit der kodierte, textinhärente Eigenschaften von Fassungen für die regelbasierte Bildung genetischer Hypothesen genutzt werden, um auf diesem Weg generelle Hypothesen zur Arbeitsweise des Autors zu verifizieren. 
                    Nachdem die genetische Analyse des Korpus nun weiter vorangeschritten ist, kann dieser Ansatz auf einer breiteren Datenbasis evaluiert werden.
                
         
      
      
         
            
               Zu den ausgewerteten Quellen siehe faustedition.net/macrogenesis/sources.
            
               Das Datenmodell der Faustedition sieht eine konzeptuelle Unterscheidung zwischen Zeugen und Inskriptionen (Niederschriften) vor: Die eigentlichen Objekte der Datierung sind Inskriptionen. Ein Zeuge kann eine oder mehrere unterschiedlich datierte Inskriptionen enthalten. Im folgenden wird der Einfachheit halber nur von Zeugen gesprochen.
            
               Mit der Graphbibliothek NetworkX (Hagberg/Schult/Swart 2008).
             http://igraph.org/
            
               Beispiel: faustedition.net/macrogenesis/H_P93--1825-01-01.
         
         
            
               Bibliographie
               
                  
                  Allen, James F. (1983):
                  Maintaining knowledge about temporal intervals, 
                        in: Communication of ACM 21(11): 832–843 doi: 
                        
                     10.1145/182.358434.
                  
               
               
                  Barro, Senén / Marín, Roque / Mira, José / Patón, Alfonso R. (1994):
                  A model and a language for the fuzzy representation and handling of time, 
                        in: Fuzzy Sets and Systems 61: 153–175. doi: 
                        
                     10.1016/0165-0114(94)90231-3.
                  
               
               
                  Bayliss, Alex (2015):
                  Quality in Bayesian chronological models in archaeology, 
                        in: World Archaeology 47(4): 677-700. doi:
                        
                     10.1080/00438243.2015.1067640
                  
               
               
                  Bohnenkamp, Anne (1994): 
                  ... das Hauptgeschäft nicht außer Augen lassend.
                        Die Paralipomena zu Goethes
                  Faust.
                         Frankfurt am Main / Leipzig: Insel.
               
               
                  Burgess, Richard W. / Witakowski, Witold (1999):
                  Studies in Eusebian and Post-Eusebian Chronography 
                        (= Historia. Einzelschriften; 135). Stuttgart: Steiner.
                    
               
                  Eades, Peter / Lin, Xue-Min / Tamassia, Roberto (1993):
                  A fast and effective heuristic for the feedback arc set problem, 
                        in: Information Processing Letters 47(6): 319–323. doi:
                        
                     10.1016/0020-0190(93)90079-O.
                  
               
               
                  Even, Guy / Naor, Joseph / Schieber, Baruch / Sudan, Madhu (1998):
                  Approximating Minimum Feedback Sets and Multicuts in Directed Graphs, 
                        in: Algorithmica 20(2): 151–174. doi:
                        
                     10.1007/PL00009191.
                  
               
               
                  Fischer-Lamberg, Renate (1955):
                  Untersuchungen zur Chronologie von Faust II 2 und 3. Diss. phil. (masch.), 
                        Humboldt-Universität Berlin.
                   
               
                  Gansner, Emden R. / North, Stephen C. (2000):
                  An open graph visualization system and its applications to software engineering, 
                        in: Software: Practice and Experience 30(11): 1203–1233. doi:
                        
                     10.1002/1097-024X(200009)30:113.0.CO;2-N.
                  
               
               
                  Grafton, Anthony (1993):
                  Joseph Scaliger. A Study in the History of Classical Scholarship. Vol. II: Historical Chronology
                        (= Oxford-Warburg Studies). Oxford: Clarendon.
                    
               
                  Hagberg, Aric A. / Schult, Daniel A. / Swart, Pieter J. (2008):
                  Exploring Network Structure, Dynamics, and Function using NetworkX, 
                        in: 
                        Varoquaux, Gael / Vaught, Travis / Millman, Jarrod (eds): 
                        Proceedings of the 7th Python in Science Conference (SciPy2008) in Pasadena, CA
                        
                        [letzter Zugriff 14. Oktober 2018].
                    
               
                  Karp, Richard M. (1972):
                  Reducibility Among Combinatorial Problems, 
                        in: 
                        Miller, Raymond Edward / Thatcher, James W. (eds.):
                  Complexity of Computer Computations. 
                        New York: Plenum 85–103. doi: 10.1007/978-3-540-68279-0_8.
                    
               
                  Manber, Udi (1989):
                  Introduction to Algorithms: A Creative Approach. 
                        Reading, MA: Addison-Wesley.
                    
               
                  Pereltsvaig, Asya / Lewis, Martin (2015):
                  The Indo-European Controversy. Facts and Fallacies in Historical Linguistics. 
                        Cambridge: Cambridge University Press.
                    
               
                  Schockaert, Steven / De Cock, Martine (2008):
                  Temporal Reasoning about Fuzzy Intervals, 
                        in: Artificial Intelligence 172(8): 1158–1193. doi:
                        
                     10.1016/j.artint.2008.01.001.
                  
               
               
                  Trovato, Paolo (2014):
                  Everything You Always Wanted to Know About Lachmann's Method. A Non-Standard Handbook of Genealogical Textual Criticism in the Age of Post-Structuralism, Cladistics, and Copy-Text. 
                        Padova: libreriauniversitaria.it
                    
               
                  Van Hulle, Dirk (2018):
                  Editing the Wake’s Genesis: Digital Genetic Criticism, 
                        in: 
                        Sartor, Genevieve (ed.):
                  James Joyce and Genetic Criticism. Genesic Fields 
                        (= European Joyce Studies; 28). Leiden, Boston: Brill Rodopi 37–54.
                    
               
                  Wissenbach, Moritz / Pravida, Dietmar / Middell, Gregor (2012):
                  Reasoning about Genesis or The Mechanical Philologist, 
                        in: 
                        Meister, Jan Christoph (ed.):
                  Digital Humanities 2012. Conference Abstracts. 
                        Hamburg: Hamburg University Press 418–422
                        
                        [letzter Zugriff 14. Oktober 2018].
                    
            
         
      
   



      
         
            Die Digital Humanities haben sich im Verlauf der letzten zehn Jahre aus einem randständigen Thema an den deutschen Universitäten zu einem etablierten Ausbildungsbereich verwandelt. Die seit Jahren anhaltende Diskussion um konvergente Curricula zeugt von dieser Entwicklung (Sahle 2013). Digitale Editionen waren vor zehn Jahren im deutschsprachigen Raum selten anzutreffen. In ihren Ausprägungsformen waren sie noch sehr unterschiedlich und trugen den Charakter vereinzelter Leuchtturmprojekte, die die Grenzen neuer Verfahren in den Geisteswissenschaften ausloteten. Mittlerweile ist die “digitale Editorik” ein eigener Forschungsbereich. Während Fachkenntnisse in diesem Bereich vor zehn Jahren nur in außeruniversitären Sonderveranstaltungen wie Summer Schools und Workshops erworben werden konnten, gibt es heutzutage an einigen deutschsprachigen Universitäten regelmäßige Lehrveranstaltungen zum Thema, die im Kontext der bisher entstandenen Lehrstühle
            1
             der Digital Humanities verortet sind.
         
         
            Obwohl sich die universitäre Ausbildung in den letzten Jahren merklich und kontinuierlich verbessert hat, ist dennoch der Bedarf nach den Schools des Instituts für Dokumentologie und Editorik (IDE) ungebrochen; dies ist ein deutliches Zeichen, dass die Digital Humanities weiterhin stärker an den Universitäten verankert werden müssen. Daneben bieten die IDE-Schools ein gutes Angebot für InteressentInnen sowohl des außeruniversitären, als auch des postdoktoralen Sektors.
         
         
            Deshalb bleiben komprimierte Angebote jenseits von Studiengängen wie die Veranstaltungsreihe ESU Leipzig
            2
             oder eine Vielzahl vereinzelter Workshops oder Summer Schools
            3
             die einzige Möglichkeit, sich grundlegende Kompetenzen für die von individuellen Forschungsfragen angetriebene Arbeit in den Digital Humanities anzueignen. Daneben wurden auch verschiedene Online-Angebote für E-Learning entwickelt. Für den Bereich der digitalen Editorik seien hier beispielsweise Kurse bei #dariahTeach, Schulungsmaterialien von DiXiT und DARIAH-DE oder Dokumentationen auf Webseiten oder GitHub genannt.
            4
         
         
            Das IDE bietet seit 2008 regelmäßig einwöchige Schools an, die sich auf Themen rund um digitale Editionen konzentrieren. Bis 2018 wurden insgesamt 13 Schools in Wien (4), Köln (2), Chemnitz (2), Graz (2), Berlin (1), Weimar (1) und Rostock (1) mit insgesamt fast 300 TeilnehmerInnen durchgeführt.
            5
             Sie vermitteln wesentliche Kenntnisse für AnfängerInnen und Fortgeschrittene auf dem Gebiet der XML-basierten digitalen Editorik. In der Regel organisieren dabei lokale InteressentInnen die finanziellen und örtlichen Rahmenbedingungen und können grobe inhaltliche Vorgaben machen. Das IDE übernimmt die inhaltliche Ausgestaltung und die Auswahl des Lehrpersonals. Dabei wird in die Planung neuer Schools immer die Auswertung von Evaluationsbögen der vorangegangenen Schools einbezogen.
         
         Das IDE legt Wert darauf, dass die TeilnehmerInnen an ihren eigenen Editionsprojekten arbeiten, um die Motivation, die eigene Arbeit konsequent auf den neuen Methoden aufzubauen, zu erhöhen. Es hält jedoch auch eigene Übungsmaterialien bereit, um den Einstieg durch gemeinsames Erarbeiten der jeweils neuen Lernstoffe sowohl an der “Tafel” und zeitgleich am eigenen Arbeitsgerät zu erleichtern.
         
            Der erfolgreiche Besuch einer School wird stets durch ein Zertifikat bescheinigt, das in manchen Fällen als “credit points” in Studiengängen angerechnet werden konnte. Besonderes Augenmerk wird auf eine gute personelle Betreuung der TeilnehmerInnen durch zusätzliche TutorInnen und einen hohen Praxisanteil für Übungen gelegt. Die Kurse behandeln Basistechnologien wie XML, XSL, XQuery, Python, kontrollierte Vokabularien und Normdaten, editionsrelevante Kapitel der TEI, Metadaten, Text Mining, sowie allgemeine Webtechnologien wie HTML und JavaScript oder neuere Ansätze wie Graphentechnologien. Die konsequente Online-Bereitstellung von Vortragsfolien und Übungsaufgaben auf der Website des IDE ermöglicht auch nachträglich, sich Inhalte der Schools anzueignen und fügt das Angebot in die wachsende Zahl von online verfügbaren Tutorials ein (s.o.). Im Zuge der Schools wurden essentielle Technologien in Flyerform
            6
             kurz zusammenzufassen. Das auf den Schools vermittelte Wissen lässt sich so einerseits direkt nachnutzen, andererseits können diese Angebote auch zeitlich versetzt in andere Schools eingebunden werden.
         
         
            Das Poster wird die mit den Schools gewonnenen Erfahrungen der letzten Jahre zusammenfassen. Es vergleicht die IDE-Schools mit thematisch benachbarten Veranstaltungen,
            7
             analysiert Trends und Konstanten der curricularen Struktur, erhebt statistische Angaben über die BesucherInnen, präsentiert die Sicht der TeilnehmerInnen durch die Auswertung einer umfassenden Befragung und verortet das Angebot im Gesamtfeld der digitalen Editorik bzw. der Digital Humanities im Allgemeinen. Es leistet damit einen Beitrag für die Untersuchung der Vermittlungsformen und Lehrinhalte in der Ausbildungslandschaft der Digital Humanities außerhalb ordentlicher Studiengänge und die Auswirkungen dieser Ausbildungsformen auf Forschung und Karriere.
         
      
      
         
            
               https://dhd-blog.org/?p=6174
            
            
               http://www.culingtec.uni-leipzig.de/ESU_C_T/node/97
            
             Siehe z.B. Digital Humanities at Oxford Summer School (https://digital.humanities.ox.ac.uk/dhoxss/), Digital Humanities Summer Institute der University of Victoria (http://www.dhsi.org/courses.php).
             Siehe https://teach.dariah.eu/; zu den Schulungsmaterialien im Rahmen des Marie Sklodowska Curie Doktorandenprogramms DiXiT: http://dixit.uni-koeln.de/programme/materials/; zu den Lehrmaterialien von DARIAH-DE siehe exemplarisch “Digitale Textedition mit TEI” von Christof Schöch: https://de.dariah.eu/tei-tutorial; eine Workshop-Dokumentation unter https://www.lib.ncsu.edu/workshops/introduction-to-xml-and-digital-scholarly-editing-using-the-text-encoding-initiative-tei-1, ein Github-Repository unter https://github.com/slstandish/lrbs-scholarly-editing.
             Zur Dokumentation der Schools siehe https://www.i-d-e.de/aktivitaeten/schools/.
            
               https://www.i-d-e.de/publikationen/weitereschriften/xml-kurzreferenzen/.
             Zu digitalen Editionen siehe z.B. die Reihe „Edirom“ in Paderborn 2013-2018 (https://ess.uni-paderborn.de/) oder als Einzelveranstaltungen Madrid "Edición digital académica" 2015 (https://extension.uned.es/actividad/idactividad/9408) und 2016 (https://formacionpermanente.uned.es/tp_actividad/idactividad/8680, München “Digital Humanities” 2017 (https://dhmuc.hypotheses.org/summerschool-2017), Prag 2017 (https://praguebeast.hypotheses.org/program) und Grenoble 2018 (https://edeen.sciencesconf.org/).
         
         
            
               Bibliographie
               
                    Digital Humanities als Beruf. Fortschritte auf dem Weg zu einem Curriculum. Akten der DHd-Arbeitsgruppe 
                        
                     "
                  Referenzcurriculum Digital Humanities
                        
                     "
                  . Graz 2015.
                    
               Digital Humanities Course Registry. Dariah/Clarin 2014-2018. 
                        https://registries.clarin-dariah.eu/courses/
               
               
                  Fritze, Christiane / Rehbein, Malte (2012):
                  Hands-On Teaching Digital Humanities: A Didactic Analysis of a Summer School Course on Digital Editing, 
                        in: 
                        Hirsch, Brett D. (ed.): 
                  Digital Humanities Pedagogy: Practices, Principles and Politics [Online]. 
                        Cambridge: Open Book Publishers. 
                        
                     http://books.openedition.org/obp/1617
                  
               
               
                  Henny, Ulrike (2012): 
                  Digitale Editionen – Methoden und Technologien für Fortgeschrittene 
                        [Tagungsbericht zur IDE-School, Chemnitz 2012], in: 
                        H-Soz-Kult, 11.12.2012, 
                        www.hsozkult.de/conferencereport/id/tagungsberichte-4540
               
               
                  Locke, Brandon T. (2017): 
                  Digital Humanities Pedagogy as Essential Liberal Education: A Framework for Curriculum Development, 
                        in: DHQ 11.3 (2017). 
                        
                     http://www.digitalhumanities.org/dhq/vol/11/3/000303/000303.html
                  
               
               
                  Neuber, Frederike (2015): 
                  Spring in Graz – Sunshine and X-technologies 
                        [Bericht zur IDE-School Graz 2015], in: DiXiT Blog 26.4.2015. 
                        https://dixit.hypotheses.org/633
               
               
                  Sahle, Patrick (2008):
                  Digitale Editionen – Methodische und technische Grundfertigkeiten 
                        [Tagungsbericht zur IDE-School, Köln 2008], in: H-Soz-Kult, 21.11.2008, 
                        www.hsozkult.de/conferencereport/id/tagungsberichte-2353
               
               
                  Sahle, Patrick (2013): 
                  DH studieren! Auf dem Weg zu einem Kern- und Referenzcurriculum der Digital Humanities. 
                        (= DARIAH-DE Working Papers Nr. 1). Göttingen: GOEDOC. 
                        http://nbn-resolving.de/urn.nbn.de.gbv:7-dariah-2013-1-5
               
            
         
      
   



      
         
            Das OCR-Programm ocropy
            Die optische Zeichenerkennung (engl. Optical Character Recognition – OCR) von historischen Texten weißt oftmals niedrige Erkennungsraten auf. Mit einem gekonnten Preprozessing und ocropy (auch ocropus), einem modular aufgebauten Kommandozeilenprogramm auf Basis eines neuronalen long short-term memory Netzes, ist es möglich, deutlich bessere Ergebnisse zu erzielen. (Springmann 2015, S. 3; Vanderkam 2015) Ocropy ist in Python geschrieben und enthält u. a. Module zur Binarisierung (Erzeugung einer Rastergrafik), zur Segmentierung (Dokumentaufspaltung in Zeilen), zur Korrektur fehlerhafter Erkennungstexte, zum Training neuer Zeichen und natürlich zur Erkennung von Dokumenten (siehe Abbildung 1). Ein bedeutender Vorteil dabei ist, dass jedes Modul eine Reihe von nachvollziehbaren Einstellungsmöglichkeiten hat, um auf die individuellen Herausforderungen jedes Dokumentes einzugehen. Zusätzlich besteht die Möglichkeit ocropy auf die Erkennung einer bestimmten Schriftart, bzw. eines Zeichensatzes zu trainieren.
            
               
               Abbildung 1. Überblick zum Prozessablauf der Texterkennung mit den grundlegenden Software-Modulen
            
            Die Benutzung von ocropy als Kommandozeilenprogramms setzt jedoch den Umgang mit einer Consolen-Umgebung und eine grundlegende Kenntnis von Bash-Kommandos voraus: Für viele potenzielle NutzerInnen stellt dies eine erste Einstiegshürde dar, denn der NutzerInnenanteil von Linuxderivaten beträgt nur 3% (statista 2018), wobei die Gruppe an ShelluserInnen noch kleiner sein dürfte. Im Workshop wird diese Hürde abgebaut, indem alle Schritte „from zero to recognised textfile“ nachvollziehbar und zum Mitmachen aufzeigt wird. Insgesamt werden sechs Themengebiete behandelt, damit die TeilnehmerInnen des Workshops alle benötigten Informationen erhalten, um selbstständig Frakturschriften (oder andere Schriftarten) durch ocropy erkennen zu lassen.
         
         
            Ubuntu in der VirtualBox
            Für bisher ausschließliche NutzerInnen des Betriebssystem Windows oder Mac OS, ist es unverhältnismäßig, allein wegen ocropy Linux als Zweit- oder sogar Hauptsystem zu installieren. Durch die Verwendung einer 
                    VirtualBox und des Linux-Derivats 
                    Ubuntu kann dieser Schritt umgangen werden. Mit Hilfe einer virtuellen Maschine lässt sich ein Betriebssystem innerhalb eines anderen Betriebssystems emulieren. Das bringt den Vorteil mit sich, keine größeren Änderungen am System vornehmen zu müssen und die Software in einem geschützten virtuellen Rahmen testen zu können. Das Einrichten einer virtuellen Maschine ist daher für die meisten NutzerInnen das Fundament (und vielleicht auch der Einstieg) in Unix-basierte Entwicklerumgebungen. Dabei sind diverse kleinere Einstellungen zu beachten, vom Einschalten der Virtualisierung im BIOS bis hin zur Installation von gemeinsam genutzten Ordnern zwischen Host und Gast. Ubuntu als „Einstiegslinux“ eignet sich hervorragend für die ersten Schritte, da es eine hohe Benutzerfreundlichkeit aufweist und trotzdem alle wichtigen Features mitbringt, die benötigt werden.
                
         
         
            Repositorien für brauchbare Digitalisate
            OCR-Software erzielt bessere Ergebnisse mit hochauflösenden und fehlerfreien Digitalisaten. Bilddateien sollten mindestens eine Auflö
                  zentrale Verzeichnis digitalisierter Drucke
                oder das 
                    
                  Münchener DigitalisierungsZentrum
                bieten exzellente Anlaufstellen zur Beschaffung digitalisierter Drucke; aber auch Sammlungen wie das 
                    
                  Verzeichnis der im deutschen Sprachbereich erschienen Drucke des 16. - 19. Jahrhunderts
                der Universität- und Landesbibliothek Sachsen-Anhalt verfügen über frei zugängliche Digitalisate mit einer Auflösung bis zu 600 DPI. 
                
         
         
            Installation von ocropy
            Ocropy ist nicht in den nativen Quellen von den bekanntesten Linux-Derivaten enthalten, sondern muss von 
                    Github heruntergeladen und über ein Script installiert werden. Dabei ist die Version der Programmiersprache Python 2.7 zu beachten und die Abhängigkeiten einiger benötigter Module. Im Workshop wird die Installation begleitet und ein bereits auf Drucke des 18. Jahrhundert trainiertes Erkennungsmodul zur Verfügung gestellt.
                
         
         
            Preprocessing mit ScanTailor
            Eine Texterkennung ist nur so gut wie das Preprocessing des Digitalisates. Bilder, Initiale oder Flecken im Bild stören die Texterkennung und müssen entfernt werden. Darüber hinaus benötigt ocropy binarisierte (schwarz/weiß gerasterte) oder normalisierte Graustufenbilder zur Verarbeitung. Obwohl ocropy mit dem Modul ocropus-nlbin eine eigene Lösung zur Binarisierung von Bilddateien anbietet, hilft dies nicht in Bezug auf Nicht-Text-Elemente, wie Bilder oder schräge Spaltenlinien. Bearbeitungssoftware wie Gimp beinhaltet zwar alle benötigten Funktionen, ist jedoch in Bezug auf die serielle Verwendung bei Textdigitalisaten ineffizent. Im Workshop wird die Software 
                    ScanTailor als passgenaues Preprocessing-Tool zur Vorbereitung der Digitalisate favorisiert. ScanTailor ist wie dafür gemacht gescannte Texte in eine einheitliche Form zu bringen und beinhaltet (zum Teil vollständig automatisierte) Funktionen wie 
                
            
               der Aufsplittung von Spalten oder Seiten
               das Ausrichten der Seite
               des Auswählens des Inhalts
               der Möglichkeit Bereich zu füllen
               der Entzerrung gekrümmter Seiten und
               der Anpassung des Schwellwertes (threshold) bei der Binarisierung.
            
            Außerdem werden Hinweise zu den grundlegenden Eigenschaften eines guten Eingangsbildes gegeben, z. B. in Bezug auf Schwellwert oder DPI-Zahl.
         
         
            Entwicklung einer Pipeline zur Texterkennung
            Die ocropy-Module funktionieren am effizientesten innerhalb einer Pipeline. Ausgehend von der Konvertierung unpassender Dateiformate der Roh-Digitalisate bis hin zur Erstellung einer Korrektur-HTML für die Verbesserung der falsch erkannten Zeichen bietet die Linux-Shell zusammen mit ocropy und dem Programm 
                    ImageMagick alle benötigten Werkzeuge. So lassen sich auch große Mengen an Bilddateien stapelweise verarbeiten. In einem Script werden Befehle zur Bildkonvertierung, Zeilenauftrennung, Texterkennung und Textkonvertierung in Reihe geschaltet, um eine stapelhafte Verarbeitung zu ermöglichen. Der Workshop bietet zwei vorgefertigte Scripte zum Gebrauch an und erklärt ihren Ablauf, um eventuelle Anpassungen an die eigenen Bedürfnisse vornehmen zu können.
                
         
         
            Training unbekannter Schriftarten
            Die eigentliche Stärke von ocropy ist die Möglichkeit Erkennungsmodule für Schriftarten zu trainieren. Die dazu bereitgestellte Ground Truth Data bestimmt maßgeblich die Leistungsfähigkeit der Erkennungsmodule. Dabei stellt sich die Frage, wie eine gute Ground Truth im wörtlichen Sinne auszusehen hat? Wie „schmutzig“ dürfen die Daten sein? Sind abgeschnittene Serifen, fehlende Bögen oder i-Punkte ein Problem? Welche Zeichen sollten verwendet werden, um Abbreviationen oder Abkürzungszeichen zu kodieren? Darüber hinaus trainiert ocropy sich nicht permanent besser, sondern baut das neurale Netz zeitweise mit negativen Auswirkungen für die Erkennungsraten um (siehe Abbildung 2). Im Workshop wird ein Script zur Identifikation des besten Trainingsmoduls vorgestellt, um das Beste aus ocropy herauszuholen.
            
               
               
                  Abbildung 2. Trainingsprozess von ocropus-rtrain mit Ground Truth von Zedlers Universallexikon. training = Ground Truth anhand derer das Modul trainiert wurde, testing = unbekanne Ground Truth zum Test der Performance.
					
            
         
         
            Ablauf
            Der Workshop richtet sich vorrangig an Anfänger und leicht fortgeschrittene NutzerInnen im Umgang mit Linux und der Console. Es werden keine Vorkenntnisse in Bash oder Python benötigt und alle im Kurs vorgestellte Software, 
                    Scripte und Daten stehen frei zur Verfügung. Der Workshop möchte alle an Interessierten da abholen, wo sie stehen und versucht durch ein schrittweises Vorgehen an die Vorzüge der Consolen-Benutzung und kommandozeilenbasierte Software heranzuführen. Teilnehmer sollten ihr eigenes Notebook mitbringen, auf dem sie auch Administrator-Rechte besitzen. Des Weiteren wird ein Internetzugang benötigt, um fehlende Software oder Abhängigkeiten herunterladen zu können. Größere Softwarepakete (VirtualBox, Ubuntu) werden auch auf USB-Sticks zur Verfügung gestellt, sollten aber nach Möglichkeit vorher selbstständig heruntergeladen werden. Es können je nach Erfahrungsstand der TeilnehmerInnen mit Console und Linux 20 bis 25 Personen betreut werden. Der Workshop dauert drei bis vier Stunden.
                
         
      
      
         
            
               Bibliographie
               
                  ImageMagick (2018):
                  Convert, Edit, Or Compose Bitmap Images @ ImageMagick, 
                        URL: 
                        https://www.imagemagick.org/, [zuletzt besucht am 14.10.2018].
                    
               
                  MDZ (2018):
                  Münchner DigitalisierungsZentrum, 
                        Bayerische Staatsbibliothek, München, URL: 
                        https://www.digitale-sammlungen.de/, [zuletzt besucht am 12.10.2018].
                    
               
                  ocropy (2018):
                  Python-based tools for document analysis and OCR, 
                        URL: 
                        https://github.com/tmbdev/ocropy, [zuletzt besucht am 14.10.2018].
                    
               
                  ScanTailor (2018):
                  ScanTailor, 
                        http://scantailor.org/, [zuletzt besucht am 14.10.2018].
                    
               
                  Springman, Uwe (2015):
                  Ocrosis. A high accuracy OCR method to convert early printings into digital text, Center for Information and Language Processing (CIS), 
                        Ludwig-Maximilians-University, Munich, URL: 
                        http://cistern.cis.lmu.de/ocrocis/tutorial.pdfm [zuletzt besucht am 14.10.2018].
                    
               statista, Marktanteile der führenden Betriebssysteme in Deutschland von Januar 2009 bis Juli 2018, URL: https://de.statista.com/statistik/daten/studie/158102/umfrage/marktanteile-von-betriebssystemen-in-deutschland-seit-2009/, [zuletzt besucht am 10.10.2018].
               
                  Vanderkam, Dan (2015):
                  Extracting text from an image using Ocropus, 
                        URL: http://www.danvk.org/2015/01/09/extracting-text-from-an-image-using-ocropus.html, [zuletzt besucht am 10.10.2018].
               
                  VD (2018):
                  Digitale Sammlungen des 16. bis 19. Jahrhunderts, 
                        Universitäts- und Landesbibliothek Sachsen-Anhalt, Halle (Saale), URL: 
                        http://digitale.bibliothek.uni-halle.de/, [zuletzt besucht am 14.10.2018].
                    
               
                  VirtualBox (2018):
                  Oracle VM VirtualBox, 
                        URL: 
                        https://www.virtualbox.org/, [zuletzt besucht am 14.10.2018].
                    
               
                  Ubuntu (2018):
                  The leading operating system for PCs, IoT devices, servers and the cloud | Ubuntu, 
                        URL: 
                        https://www.ubuntu.com/, [zuletzt besucht am 14.10.2018].
                    
               
                  ZVDD (2018):
                  Zentrales Verzeichnis Digitalisierter Drucke, 
                        Georg August Universität Göttingen, Niedersächsische Staats- und Universitätsbibliothek Göttingen, Göttingen, URL: 
                        http://www.zvdd.de/, [zuletzt besucht am 12.10.2018].
                    
            
         
      
   



      
         
            EINLEITUNG
            In den vergangenen 30 Jahren ist ein beträchtlicher Teil des in Deutschland gedruckten Materials aus der Zeit von 1500 bis ca. 1850 in mehreren, durch die Deutsche Forschungsgemeinschaft (DFG) geförderten Kampagnen in den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke des 16.-18. Jahrhunderts (VD16, VD17, VD18) zunächst nachgewiesen und seit 2006 digitalisiert worden. Zusätzlich vorliegender Volltext wird mittlerweile auf breiter disziplinärer Front als Schlüssel zu einer ganzen Reihe von geistes- und kulturwissenschaftlichen Forschungsfragen gesehen und gilt zunehmend als elementare Voraussetzung für die Weiterentwicklung der transdisziplinär arbeitenden Digital Humanities. Deshalb werden bereits an verschiedenen Stellen OCR-Verfahren angewendet; viele dieser Unternehmungen haben allerdings noch sehr starken Projektcharakter. Die informationswissenschaftliche Auseinandersetzung mit OCR kann an der großen Zahl wissenschaftlicher Studien und Wettbewerbe ermessen werden, die Möglichkeiten zur Verbesserung der Textgenauigkeit sind in den letzten Jahrzehnten enorm gestiegen. Der Transfer der auf diesem Wege gewonnenen, oftmals sehr vielversprechenden Erkenntnisse in produktive Anwendungen ist jedoch häufig nicht gegeben: Es fehlt an leicht nachnutzbaren Anwendungen, die eine qualitativ hochwertige Massenvolltextdigitalisierung aller historischen Drucke aus dem Zeitraum des 16. bis 19. Jahrhundert ermöglichen. 
                    Auf dem DFG-Workshop „Verfahren zur Verbesserung von OCR-Ergebnissen“ (Deutsche Forschungsgemeinschaft 2014) im März 2014 formulierten Expertinnen und Experten daher folgende Desiderate um die Weiterentwicklung von OCR-Verfahren zu ermöglichen. Es bestehe eine dringende Notwendigkeit für freien Zugang zu historischen Textkorpora und lexikalischen Ressourcen zum Training von vorhandener Software zur Texterkennung bestehe. Ebenso müssen Open-Source-OCR-Engines zur Verbesserung der Textgenauigkeit weiterentwickelt werden, wie auch Anwendungen für die Nachkorrektur der automatisch erstellten Texte. Daneben sollten Workflow, Standards und Verfahren der Langzeitarchivierung mit Blick auf zukünftige Anforderungen an den OCR-Prozess optimiert werden. Als zentrales Ergebnis dieses Workshops stand fest, dass eine koordinierte Fördermaßnahme der DFG notwendig ist. Die „Koordinierte Förderinitiative zur Weiterentwicklung von Verfahren der Optical Character Recognition (OCR)“, kurz OCR-D, begann im September 2015 und versucht seitdem einen Lückenschluss zwischen Forschung und Praxiseinsatz, indem für die Entwicklungsbedarfe Lösungen erarbeitet und der aktuelle Forschungsstand zur OCR mit den Anforderungen aus der Praxis zusammengebracht werden. 
                
         
         
            ARBEITEN IM PROJEKT OCR-D
            Das Vorhaben hat zum Ziel, einerseits Verfahren zu beschreiben und Richtlinien zu erarbeiten, um einen optimalen Workflow sowie eine möglichst weitreichende Standardisierung von OCR-bezogenen Prozessen und Metadaten zu erzielen, andererseits die vollständige Transformation des schriftlichen deutschen Kulturerbes in digitale Forschungsdaten in (xml-strukturierter Volltext) konzeptionell vorzubereiten. Am Ende des Gesamtvorhabens (d.h. unter Einschluss der Modulprojektphase) sollte ein in allen Aspekten konsolidiertes Verfahren zur OCR-Verarbeitung von Digitalisaten des schriftlichen deutschen Kulturerbes stehen und eine Dokumentation, die Antworten auf die damit verbundenen technischen, informationswissenschaftlichen und organisatorischen Probleme und Herausforderungen gibt sowie Rahmenbedingungen formuliert.
            Das Projekt ist in zwei Phasen geteilt: In der ersten Phase hat das Koordinierungsgremium von OCR-D Bedarfe für die Weiterentwicklung von OCR-Technologien analysiert und sich intensiv mit den Möglichkeiten und Grenzen der Verfahren zur Text- und Strukturerkennung auseinandergesetzt. Zahlreiche Gespräche mit ExpertInnen aus Forschungseinrichtungen und Bibliotheken sowie Sichtung vorhandener Werkzeuge aber auch Betrachtung vorhandener Textsammlungen sowie aktueller und geplanter Digitalisierungsvorhaben mündeten in der Erkenntnis, dass der Lückenschluss zwischen Wissenschaft und Praxis das primäre Desiderat im Bereich der Textdigitalisierung darstellt. Zudem hat sich im Lauf der ersten Projektphase eine technologische Wende auf dem Gebiet der Zeichenerkennung vollzogen - an die Stelle traditioneller Verfahren der Mustererkennung, die auf einer Segmentierung von Textabschnitten in Zeilen, Wörter und schließlich einzelne Glyphen basieren, die anschließend aufgrund charakteristischer Merkmale (z.B. Steigung an Kanten) erkannt werden, ist eine zeilenorientierte Sequenzklassifizierung auf Basis statistischer Modelle, insbesondere verschiedener Arten neuronaler Netze (sog. 
                    Deep Learning), getreten. Grund für diesen Technologiewechsel ist die vielfach nachgewiesene Überlegenheit segmentierungsfreier Erkennungsverfahren bezüglich der resultierenden Textgenauigkeit. Diese Überlegenheit gilt insbesondere für schwierige, historische Vorlagen. Dieser Technologiewandel hat sich bisher nicht oder nur äußerst begrenzt auf die Digitalisierungspraxis ausgewirkt. Der Grund dafür liegt vor allem in den bisher bestehenden Hürden beim Einsatz verfügbarer OCR-Lösungen auf Basis neuronaler Netze. Ohne weitreichende projektspezifische Anpassungen ist ein produktiver Einsatz derzeit nicht möglich. Das betrifft unter anderem die Erstellung passender Erkennungsmodelle, die durch das Trainieren eines neuronalen Netzes auf Basis ausgewählter Ground-Truth-Daten generiert werden. Dafür sind zum einen hochqualitativer und umfangreicher Ground Truth aber auch Erfahrungen bzgl. freier Parameter wie z.B. Anzahl der Trainingsschritte, Lernrate, Modelltiefe unabdingbar. Aus OCR-D heraus ist daher ein Datenset mit Trainings- und Ground-Truth-Daten entstanden, welches für Trainings und Qualitätsanalysen im Vorhaben selber genutzt wird aber auch durch andere Forschungsprojekte nachgenutzt werden kann. Neben der Qualität der Zeichenerkennung sind es vor allem Umfang und Korrektheit der strukturellen Annotationen, die die Utilität eines Volltexts für wissenschaftliche Kontexte determinieren. Auch im Bereich der automatischen Layouterkennung (OLR) gab es innerhalb des bisherigen Projektzeitraums vielversprechende Forschungsergebnisse durch den Einsatz innovativer statistischer Verfahren. Der Übertrag in die Praxis in Form nachnutzbarer Software ist hier jedoch noch nicht gegeben. Kommerzielle OCR-Lösungen ignorieren diesen Bereich weitestgehend und bieten nur minimale Strukturinformationen auf Seitenebene (Text, Tabelle, Abbildung etc.) an. Tiefergehende strukturelle Auszeichnungen (Kapitelstruktur, Bildunterschriften, Inhaltsverzeichnisse) werden daher manuell erfasst und in METS/MODS repräsentiert. Eine Verknüpfung zwischen Struktur und Volltext findet, obwohl technisch möglich, in vielen Digitalisierungsvorhaben nicht statt. Für die philologische, editorische oder linguistische Wissenschaftspraxis bedeutet das eine massive Einschränkung die bspw. eine sinnvolle Transformation in hochstrukturierte Formate wie TEI verhindert. 
                
            Die Erkenntnisse dieser Bedarfsanalyse mündeten in einem OCR-D-Funktionsmodell, welches den Rahmen für die Modulprojekt-Ausschreibung der DFG im März 2017 bot. Vor diesem Hintergrund wurden acht Modulprojekte bewilligt die seit 2018 an Lösungen zur Bildvorverarbeitung, Layouterkennung, Textoptimierung (inkl. Nachkorrektur), zum Modelltraining und zur Langzeitarchivierung der OCR-Daten arbeitet. Die Entwicklungen schöpfen dabei das Potential innovativer Methoden für den gesamten Bereich der automatischen Texterkennung für die Massenvolltextdigitalisierung von historischen Drucken aus. Sie werden anschließend nahtlos in den OCR-D-Workflow zur optimierten OCR-basierten Texterfassung integriert. Das so entstehende OCR-D-Softwarepaket steht damit Kultureinrichtungen wie Forschenden für die automatische Texterkennung als Open-Source-Software zur Verfügung.
            Die meisten Arbeiten werden im Sommer 2019 abgeschlossen sein, aber bereits Anfang des Jahres wird die Alpha-Version einen Einblick in die zu erwartende Gesamtlösung bieten können.
         
         
            ZIEL DES WORKSHOPS
            Der Workshop soll neben der Vorstellung des Projektes und der Software die Gelegenheit bieten selber die Software zu testen und zugleich über Optimierungen und Anforderungen seitens der Wissenschaft an diese Technologien zu diskutieren. Teilnehmende erhalten somit einen exklusiven Einblick in die Entwicklungsarbeit und haben die Möglichkeit proaktiv auf die Arbeiten Einfluss zu nehmen, die Ihren späteren Forschungsalltag begleiten und verbessern soll.
                
         
         
            PROGRAMM
            Der Workshop gliedert sich in drei Abschnitte:
            
               Vorstellung des Projekts OCR-D, des Ground-Truth-Datensets und der Guidelines (30min)
               Demonstration der Eigenentwicklung und eines Test-Workflows (120min)
               Diskussion zu Anforderungen und Optimierungen aus Sicht der Digital Humanities (30min)
            
            Der erste Abschnitt stellt die Hintergründe zum Vorhaben vor und geht auf Besonderheiten der Volltextdigitalisierung von historischen Beständen ein. Anschließend wird das Trainings- und Ground-Truth-Datenset präsentiert, das im Rahmen von OCR-D auf- und weiter ausgebaut wird. Besonders die dazu entwickelten Guidelines geben Hinweise für eine spätere Nachnutzung und die Erstellung eigener Ground-Truth-Daten in anderen Projekten. Der Fokus des Workshops liegt auf dem zweiten Abschnitt, in welche der derzeitige Entwicklungsstand präsentiert wird. Die benötigten Test-Dateien werden auf 
                GitHub1 veröffentlicht. Abgerundet wird der Workshop durch eine Diskussionsrunde zu Anforderungen aus der Wissenschaft heraus an OCR-Techniken und die dafür eingesetzte Software.
                
         
         
            VORAUSSETZUNG
            Teilnehmende benötigen einen eigenen Laptop mit Internetanbindung und Ubuntu 18.04 als Betriebssystem. Alternativ kann auch Windows/Mac OSX mit der Software VirtualBox verwendet werden. Die VM wird den Teilnehmenden vom OCR-D-Projekt vor Ort zur Verfügung gestellt. Die Anzahl der Teilnehmenden ist auf 20-25 begrenzt. Python- und Linux-Kommandozeilen-Kenntnisse sind wünschenswert
         
      
      
         
             OCR-D Git-Hub: https://github.com/OCR-D/
         
         
            
               Bibliographie
               
                  Deutsche Forschungsgemeinschaft (2014):  
                        Workshop 
                        Verfahren zur Verbesserung von OCR-Ergebnissen. 
                        Protokoll zu den Ergebnissen und Empfehlungen des Workshops. 
                        
                     http://www.dfg.de/download/pdf/foerderung/programme/lis/140522_ergebnisprotokoll_ocr_workshop.pdf
                   [Zuletzt abgerufen 07.01.2019]
                    
            
         
      
   



      
         Gesellschaften fügen sich aus Individuen zusammen. Das gilt auch für die Vergangenheit, aus der die Mehrzahl der Individuen nur schlecht bis gar nicht dokumentiert ist. Es hat sich deshalb ein eigenständiger historischer Forschungsbereich entwickelt, die “Prosopographie”, die sich der Aggregation von Einzelinformationen zu Individuen aus historischen Quellen und ihrer Auswertung widmet (Keats-Rohan 2007). Dieses Forschungsgebiet hat früh digitale Methoden eingesetzt. Der Beitrag widmet sich der Frage, ob die Methoden vergleichbar zu IIIF (International Image Interoperability Framework) in ein „International Proposography Interoperability Framework“ (IPIF) integriert werden können.
         Ein IPIF muss von den Personendatenbanken abweichen, die sich als kontrollierte Vokabularien und Referenzen für Linked Open Data in den Digital Humanities etabliert haben (GND/VIAF, deutsche-biographie), bzw. im Begriff sind, sich zu etablieren (wikidata). Diese berücksichtigen nämlich nicht den Vorgang, mit dem Informationen über eine Person aus historischen Quellen aggregiert werden. Der Ansatz weicht damit auch von der „personography“ der TEI ab, die, wie die Linked-Data-Ressourcen, eine Person mit einer Liste an Eigenschaften beschreiben. Ein IPIF muss dagegen ein Modell realisieren, für das Bradley/Short (2005) die Bezeichnung „Factoid“-Model eingeführt haben. Es geht von drei Informationseinheiten aus: Quelle, Individuum und Aussagen der Quelle über das Individuum. John Bradley hat das Modell mehreren Projekten des King’s College London zu Grunde gelegt (PASE, DPRR, CCEd). Auch das Persondendatenrepositorium (PDR) der Berlin-Brandenburgischen Akademie der Wissenschaften (Neumann et al. 2011) und Projekte, die die Software der BBAW weitergenutzt haben, verwenden das gleiche Modell, auch wenn das PDR nicht explizit auf Bradley referenziert. Ebenso verwendet das Repertorium Academicum Germanicum ein solches dreiteiliges Modell (Andresen 2008). 
         Das dreiteilige Modell impliziert auch, dass (auch widersprechende) Aussagen über dasselbe Individuum aus verschiedenen Quellen an verschiedenen Orten publiziert werden können. Es erscheint also als ein Paradebeispiel für das 
                Web of Data des W3C. Das 
                Web of Data ist die Fortführung der Semantic-Web-Aktivitäten des W3C. Es konzentriert sich auf die Öffnung von Datenbanken und erhebt insbesondere den Anspruch, individuelle kleine Datenmengen als RDF über das Semantic Web abfragbar zu machen. Technisch ist RDF, die Grundlage des 
                Web of Data, eine weit verbreitete und gut unterstützte Technologie. Es ist deshalb auch eine Technologie, mit deren Hilfe immer häufiger Maschinen auf prosopographische Datenbanken zugreifen können. Deshalb haben Bradley/Pasin (2015) eine CIDOC-CRM basierte Version des Factoid-Modells vorgeschlagen und entsprechende Ontologien veröffentlicht (Bradley 2017). Das Basismodell ist aber auch mit anderen Vokabularien realisiert worden: SNAP verwendet z.B. Vokabularien aus dem 
                Linking-Ancient-Wisdom-Projek
               1
            ). Das King’s Digital Lab hat jüngst mit Hilfe von 
                Ontop
            
               2
             die prosopographische Datenbank zur römischen Republik als LOD-Ressource incl. eines SPARQL-Endpoints 
                veröffentlicht.
               3
            
         
         Diese Strategie teilt jedoch das Problem vieler RDF-Ressourcen: Die technische Pflege eines SPARQL-Endpoints ist sehr anspruchsvoll. SPARQL-Endpoints sind hä
               4
             und Core 
            API
               5
             liegen auch Vorschläge vor, derartige API-Definition standardisiert zu beschreiben, so dass die Implementation von einschlägigen API-Anbietern und API-Konsumenten teilweise sogar automatisiert werden 
            kann.
               6
             Aus Sicht des Software-Engineering erscheint es also angemessen, auf eine eigene API-Definition statt auf einen SPARQL-Endpoint zurückzugreifen. Gleichzeitig wird es damit erschwert, Daten aus verschiedenen Datenquellen zu aggregieren, da für jeden Datenanbieter ein eigener API-Konsument programmiert werden müsste. Im Bereich der Bibliotheken hat sich deshalb für die Bereitstellung von Bildern von Büchern mit IIIF eine Kombination aus einem Datenstandard und einer Adressierungs-API durchgesetzt. Es ist an der Zeit, auch für personenbezogene Daten über einen solchen technischen Standard nachzudenken, der die Implementation von Anwendungen erleichtert und die Daten auch praktisch interoperabel macht.
            
         Ein solcher Standard muss von konkreten Anwendungsszenarien ausgehen. Sie können unter den Überschriften „Biographical Lexicon“, „Careers”, „Source Editing“, „Fact Checking“, „New Interpretation“, „Publish a Database”, „Integrate Other Databases“, „Analysis“, „Tool User“, „Tool Builder” zusammengefasst werden. Die Szenarien bilden sowohl Forschung mit prosopographischen Daten wie die Erzeugung solcher Daten ab. Zusätzlich achten die Szenarien darauf, nicht nur explizit prosopographische Workflows zu berücksichtigen, sondern schließen auch wissenschaftliches Edieren als Szenario mit ein, in dem der edierte Text als Beleg für eine Person betrachtet werden kann. In einem Workshop in Wien im Februar 2017 haben Forscher aus dem Themengebiet der Prosopographie religiöser Orden solche Anwendungsszenarien diskutiert und einen Entwurf für eine API entwickelt.
         Ein Ergebnis dieser Arbeit ist eine nach den Standards von OpenAPI beschriebenen Definition einer prosopographischen 
            API.
               7
             Die API baut auf dem dreiteiligen Factoid-Modell auf und erlaubt den Zugriff auf Personen, Aussagen, Quellen und ihr Aggregat, einem „Factoid“. Für alle diese Objekte gibt es eigene Pfade zur Suche und Ausgabe der Daten über die zu ihnen abgelegten IDs. Im Kern der API steht deshalb der Zugriff auf Factoide 
            (/factoid). Sie können individuell über bekannte IDs adressiert werden 
            (/factoid/id). Wichtiger sind aber inhaltliche Filtermöglichkeiten. Sie ergeben sich einfach aus den Eigenschaften des Factoids, als Aussage über eine Person. Die Parameter 
                s
            , 
            st und 
                f lassen also die Suche in den Inhalten der mit dem Factoid verknüpften Quellen 
                (source), Aussagen 
                (statement) und den Metadaten des Factoids selbst 
                (factoid) zu. Dabei ist der Standard eine Volltextsuche. Ebenso lassen sich die Quellen und Personen abfragen. Als Parameter können aber auch Identifikatoren für die einzelnen Informationsgruppen übergeben werden, also z.B. mit 
                /statement/?p_id=Placidus_Seiz alle Aussagen über die Person mit einem Identifikator „Placidus_Seiz“ in einem beliebigen Kontext. Die Anwendung liefert dann ein JSON-Objekt zurück, in dem diese Aussagen formalisiert sind. Zu jeder Aussage gehört eine ID, mit der Entwickler z.B. über die API überprüfen können, woher die jeweilige Aussage stammt.
            
         Als Rückgabewert der API-Definition sind JSON-Serialisierungen vorgesehen. Die Statements können Daten als Text (z.B. der Quelle) ebenso wie strukturiert als Graph enthalten. Die Graphen sollen den Spezifikationen von JSON-LD folgen. Damit können zwei Ziele erreicht werden: Erstens ist damit die Ausgabe der API direkt in Linked-Open-Data-Umgebungen nutzbar, kann prinzipiell auch in einer FROM-Klausel einer SPARQL-Abfrage integriert werden oder in Caching-Mechanismen wie im 2011 als Linked Data Middleware von Virtuoso vorgeschlagenen URI-Burner verwendet werden. Zweitens wird damit ein Standard verwendet, der die Referenzierung der verwendeten Vokabularien und ihre formale Beschreibung mit RDFS und OWL ermöglicht.
         Der Workshop in Wien hat als Kernproblem eines echten Datenaustausches die divergierenden Datenmodelle für die Einzelaussagen über die Individuen identifiziert. Während die Individuen selbst im Factoid-Modell keine beschreibenden Metadaten tragen und damit kaum Probleme beim Datenaustausch erzeugen, sind für die Aussagen über die Individuen je nach Projekt, Verwendungszweck und Forschungsdomäne eine Vielfalt von Vokabularien im Einsatz. Einen Ausweg aus dieser Situation bietet die 2017 gegründete 
            dataforhistory-Initiative.
               8
             Die Initiative arbeitet daran projekt- und domänenspezifische Modellierungen zu erleichtern, die zum CIDOC CRM kompatibel sind. Die derzeitige API-Definition sieht deshalb vor, dass die zurückgegebenen Daten eine Referenz auf ein Schema (in JSON-LD als 
                @context) enthalten müssen, das die verwendeten Klassen und Eigenschaften auf Definitionen im CIDOC CRM abbildet, der es der die API konsumierenden Anwendung erlaubt, die Daten als CIDOC CRM zu interpretieren und darauf aufbauende Operationen durchzuführen. Ergänzend dazu ist ein Parameter 
                format=json/cidoc-crm vorgesehen, bei dem die Transformation serverseitig stattfindet. Die Abbildung auf CIDOC CRM soll insbesondere die grundlegenden Suchoperationen ermöglichen, die Katerina Tzompanaki und Martin Doer 2012 formuliert haben und die im Projekt 
                researchspace
               9
             realisiert werden. Die API definiert die Objekteigenschaft 
                graph für die strukturierte Repräsentation der Daten über Personen.
            
         John Bradley und Michele Pasin haben 2015 eine OWL basierte Ontologie vorgestellt, in der eine „temporal entity documented“ (TED) als Ereignis (E4 und E5 im CIDOC-CRM) oder als eine zeitliche Einheit oder klare zeitliche Grenzen (E3: condition, state) modelliert sind. Das entspricht dem Stand der Diskussion über prosopographische Datenmodelle (Lind 1994, Andresen 2008, Tuominen / Hyvönen / Leskinen 2018).
         Nicht zuletzt der Erfolg von IIIF belegt, dass eine solche API aber auch Referenzimplementationen benötigt. Dabei ist entsprechend der oben beschriebenen Benutzungsszenarien sowohl an Ressourcen zu denken, die Daten bereitstellen, als auch an Anwendungen, die diese Daten konsumieren. Die Nachnutzung des „Archiveditors“, eines zunächst projektinternen Werkzeugs der BBAW, in anderen Projekten zeigt, dass dabei nicht nur an Datenextraktion und –anzeige sondern auch an Datengenerierung zu denken ist. Im Rahmen der Arbeit an der Personendatenbank der Österreichischen Akademie der Wissenschaften ist deutlich geworden, dass gerade automatische Informationsextraktion von „Personenrelationen“ (Schlögl et al. forthcoming, Schlögl et al. 2018) von einer solchen API profitieren kann. Die automatisch generierten Aussagen können als eigenständige Factoide in die Personendatenbanken eingehen. Die Metadaten des Factoids und die Referenz auf die verwendete Quelle stellen sicher, dass sie als automatisch generierte Daten identifizierbar bleiben. Der Vortrag wird Beispiele für Datenangebote aus dem Umfeld mittelalterlicher Urkunden (Register der Urkundenempfänger von Papsturkunden nach den Regesten von August Potthast, Daten aus monasterium.net) und Steuererhebungen (England) vorstellen, und Prototypen für Anwendungen benennen, welche die mit der API bereitgestellten Daten konsumieren können.
      
      
         
            
                http://lawd.info/ontology/
            
             https://github.com/ontop/ontop
            
               
                  http://romanrepublic.ac.uk/rdf
               , Dokumentation von John Bradley: 
                  http://romanrepublic.ac.uk/rdf/doc
               
            
             https://www.openapis.org/ 
             http://www.coreapi.org
             z.B. das Python-Framework Flask in Verbindung mit 
                  https://github.com/zalando/connexion
               , vgl. weitere Tools: 
                  https://swagger.io/tools/open-source/open-source-integrations/
               
            
             https://github.com/GVogeler/prosopogrAPhI
             http://dataforhistory.org
             https://www.researchspace.org/
         
         
            
               Bibliographie
               
                  Andresen, Suse (2008):
                  Das 'Repertorium Academicum Germanicum'. Überlegungen zu einer modellorientierten Datenbankstruktur und zur Aufbereitung prosopographischer Informationen der graduierten Gelehrten des Spätmittelalters, 
                        in: 
                        Sigrid Schmitt u. Michael Matheus (eds.):
                  Städtische Gesellschaft und Kirche im Spätmittelalter (Geschichtliche Landeskunde 62). Stuttgart: Steiner 17-26.
                    
               
                  Bradley, John (2017):
                  Factoids. A site that introduces Factoid Prosopograph, 
                        
                     http://factoid-dighum.kcl.ac.uk/
                   und 
                        
                     https://github.com/johnBradley501/FPO
                  
               
               
                  Bradley, John / Pasin, Michele (2015):
                  Factoid-based Prosopography and Computer Ontologies. Towards an integrated approach, 
                        in: DSH 30,1: 86-97.
                    
               
                  Bradley, John / Short, Harold (2005):
                  Texts into databases. The Evolving field of New-style Prosopography, 
                        in: LLC 20, suppl. 1: 3-24.
                    
               
                  CCEd: 
                  Clergy of the Church of England Database, King’s College London 
                        
                     http://theclergydatabase.org.uk/
                  
               
               
                  DPRR: 
                  Digital Prosopography of the Roman Republic, King’s College London 
                        
                     http://romanrepublic.ac.uk/
                  
               
               
                  Keats-Rohan, Katherine S.B. (ed.) (2007): 
                  Prosopography. Approaches and Applications. A Handbook 
                        (Prosopographica et genealogica 13). Oxford: P&G.
                    
               
                  Lind, Gunner (1994):
                  Data Structures for Computer Prosopography, 
                        in: Yesterday: Proceedings from the 6th International Conference of the Association of History and Computing, Odense 1991. Odense: University Press of Southern Denmark. 77-82.
                    
               
                  Neumann, Gerald / Körner, Fabian / Roeder, Torsten / Walkowski, Niels-Oliver (2011):
                  Personendaten-Repositorium, 
                        in: Berlin-Brandenburgische Akademie der Wissenschaften. Jahrbuch 2010: 320-326.
                    
               
                  PASE:
                  Prosopography of Anglo-Saxon England, King’s College London, URL: 
                        
                     http://www.pase.ac.uk/jsp/index.jsp
                  
               
               
                  Schlögl, Matthias / Katalin Lejtovicz (2018):
                  A Prosopographical Information System (APIS), 
                        in: 
                        Antske Fokkens / ter Braake Serge / Sluijter, Ronald / Arthur, Paul / Wandl-Vogt, Eveline (eds.): 
                  BD-2017. Biographical Data in a Digital World 2017. Proceedings of the Second Conference on Biographical Data in a Digital World 2017. 
                        Linz, Austria, November 6-7, 2017. Budapest: CEUR (CEUR Workshop Proceedings 2119): 53-58.
                    
               
                  Schlögl, Matthias / Lejtovicz, Katalin / Bernád, Ágoston Zénó / Kaiser, Maximilian / Rumpolt, Peter (2018):
                  Using deep learning to explore movement of people in a large corpus of biographies. 
                        Zenodo. 
                        
                     http://doi.org/10.5281/zenodo.1149023
                  
               
               
                  Tuominen, Jouni / Hyvönen, Eero / Leskinen, Petri (2018):
                  Bio CRM. A Data Model for Representing Biographical Data for Prosopographical Research, 
                        in: BD-2017. Biographical Data in a Digital World 2017, hg. v. Antske Fokkens, Serge ter Braake, Ronald Sluijter, Paul Arthur, Eveline Wandl-Vogt, Budapest: CEUR (CEUR Workshop Proceedings 2119): 59-66. 
                    
               
                  Tzompanaki, Katerina / Doerr Martin (2012):
                  Fundamental Categories and Relationships for intuitive querying CIDOC-CRM based repositories, 
                        Technical Report ICS-FORTH/TR-429, April 2012, 
                    
            
         
      
   



      
         Geographische Angaben können in historischen Kontexten nicht als simple 2-dimensionale Daten (Längen- und Breitengrade) verstanden werden. Punkte die auf der Karte nur wenige Kilometer voneinander entfernt sind waren vor 500 Jahren wegen geographischer Hürden (Berge, Schluchten, Flüsse etc.) vielleicht ewig weit voneinander entfernt. Ähnliches gilt für politische Grenzen: Vor 30 Jahren waren Orte in Deutschland die heute wenige Autominuten voneinander entfernt liegen durch den eisernen Vorhang voneinander getrennt. Kamzelak (2018) hat es so formuliert: “Orte haben eine historisch-politische Dimension, die bei einer übergreifenden Registererfassung erst sichtbar zu einem Problem wird. [...] Für die Visualisierung von Briefen etwa sind historische Karten ein Desiderat; generell auch Geodaten für Flächen. Und alle mit Geodaten versehenen Einträge müssen mit einem Zeitstempel kombiniert sein, denn beispielsweise die Altstadt von Jerusalem ist eben heute nicht am selben Ort wie vor 2.000 Jahren.”.
         Trotzdem arbeitet eine Vielzahl an Digital Humanities Projekten auch heute noch mit 2-dimensionalen geographischen Angaben. Unserer eigenen Erfahrung nach liegt das Hauptsächlich an der Verfügbarkeit von Daten und Services. Moderne Punkt-Daten können einfach als geonames, openstreetmap etc. dump heruntergeladen werden bzw. über API Schnittstellen abgefragt werden. Für historische Daten existieren diese Services noch nicht vollumfänglich. Mit Pelagios gibt es ein Ökosystem an Services für Ortsdaten in der antiken Welt (http://commons.pelagios.org/), das im Zuge des “World-Historical Gazetteer” Projektes (http://whgazetteer.org/) auch in jüngere Zeiten ausgedehnt wird. Abgesehen von diesen notwendigen Initiativen und sehr wertvollen Datensätzen fehlen den Digital Humanities immer noch Polygondaten zu politischen Entitäten im Verlauf der Zeit.
         HistoGIS hat sich zum Ziel gesetzt genau diese Lücke zu füllen in dem:
         
            ein Repository historischer Polygondaten mit einheitlichen Metadaten aufgebaut und zum Download angeboten wird. Für dieses Repository werden zunächst schon existierende Polygondaten eingesammelt, aufbereitet und erst in einem zweiten Schritt für historisch wichtige Zeitspannen neue Polygone erstellt. Dabei setzt sich HistoGIS zunächst zum Ziel die Periode zwischen dem ausgehenden 18. Jhdt. und 1918 für das Gebiet der KuK Monarchie und des deutschen Bundes abzudecken.
            und RestAPI Services zur einfachen Anreicherung historischer Daten mit Hilfe des Repositories angeboten werden. Z.B.: Wo war Punkt X/Y zum Zeitpunkt 
                Z? Die API antwortet mit den Metadaten zu den einzelnen überlappenden Polygonen die für den Zeitraum (oder Zeitpunkt) Gültigkeit haben. Nehmen wir an: Ein Projekt hat Reiseberichte in seiner Datenbank. Eine Station war Bolzano 1910. Die Geokoordinaten (46.49067, 11.33982) wurden über Geonames gefunden. Schickt man diese mit dem Jahr an die API bekommt man die Metadaten für Bozen Stadt, An der Etsch, Tirol und Österreich-Ungarn 
                zurück.
                
         
         Eine Fokussierung auf die politischen Verwaltungseinheiten und ihre Grenzen erlaubt es in vielen Bereichen - zumindest für jüngere Zeiten - einen Mix aus modernen Daten/Methoden und historischen zu verwenden ohne historisch falsche Daten zu generieren. So können bei oben angeführten Beispiel Bozen die Google Maps API, Geonames oder Openstreetmap zur Geolokalisation verwendet werden (die Geokoordinaten von Bozen haben sich ja nicht geändert) und die HistoGIS API um die Eingliederung in die Verwaltungshierarchien zu verbessern (Bozen war 1910 Teil der KuK Monarchie). Dieses Vorgehen reduziert nicht nur den Aufwand für die Erstellung/Kuratierung der Daten drastisch, es ermöglicht es auch leichter Punkte denen schon Längen- und Breitengrade zugewiesen wurden politisch/historisch zu verorten.
         
            Datenmodell, Technische Grundlage und Workflow
            Die Modellierung historischer Verwaltungsräume hinsichtlich ihrer räumlichen und zeitlichen Ausdehnungen erfolgte bewusst in einer äußerst vereinfachten Art und Weise. Das mittels Python (bzw. GeoDjango) definierte und als Postgresql implementierte Datenmodell besteht in seinem Kern aus den drei Hauptklassen bzw. Tabellen “TempSpatial”, “Source” und “TempSpatialRel”
            Ein historischer Verwaltungsraum (Temporalized Spatial Entity oder eben “TempSpatial”) definiert sich über die im gesamten Datenset einzigartige Kombination der Eigenschaften zeitliche Ausdehnungen (“start_date” und “end_date”, Datumsfelder), räumliche Ausdehnung (“geom”; Multipolygon) sowie einem Feld “date_accuracy”, welches Auskunft über den Grad der Genauigkeit der angegebenen Datumswerte gibt. Ergänzt wird diese Klasse um die Eigenschaften “name” für, entsprechend den Projektkonventionen einen zeitgenössischen Namen der Entität, “alt_names” für alternative Namen sowie einem Feld “additional_data”, welches das Speichern arbiträrer weiterer Daten im JSON Format ermöglicht.
            Das Feld “administrativ_unit” zeigt auf eine Hilfsklasse (“SkosConcept”) für kontrolliertes Vokabular, welche in weiten Teilen das SKOS Datenmodell implementiert. 
            Die Quelle jeder Instanz der Klasse TempSpatial bzw. jeder im Datenset erfassten historischen Verwaltungseinheit wird mit Hilfe der Klasse “Source” beschrieben. Darin werden URLs zu verwendeten Daten anderer Projekte gespeichert wie auch eine Beschreibung der (weiteren) Datenerhebung und Kuration im Rahmen des Projektes sowie ein Zitationsvorschlag. Jedes Source Objekt ist außerdem auch mit einem ESRI-Shapefile verbunden welches Projektintern als primäres Datenformat dient. Mehr dazu im Abschnitt Workflow. 
            Die Modellierung beliebiger Relationen zwischen beliebigen historischen Verwaltungsräumen ist im Datenmodell durch die Klasse TempSpatialRelation grundgelegt. Hier können jeweils zwei TempSpatial Objekte (“instance_a” und “instance_b”) für eine Zeitspanne (“start_date” und “end_date”) in eine typisierte (Verweis auf die bereits erwähnte Hilfsklasse “SkosConcept”) Relation gebracht werden. Hierbei ist jedoch weniger an die in Gazetteern üblichen “part of” Beziehungen gedacht, sondern an Relationen wie beispielsweise “ist Vorgänger von” oder “wurde zusammengelegt mit”. Allerdings muss darauf hingewiesen werden, dass im derzeitigen Status des Projektes noch keine derartigen Beziehungen erfasst werden.
            Die Art und Weise wie eben erwähnte “part of” Beziehungen erfasst werden sollen, wurde im Projekt ausgiebig diskutierte, wobei hier neben formal- konzeptionellen Argumenten vor allem auch die konkreten Arbeitsvoraussetzungen und -bedingungen im Projekt berücksichtigt werden mussten. 
            Schlussendlich wurde eine explizite Modellierung hierarchischer Strukturen der erfassten Verwaltungseinheiten verzichtet, sprich im Datenmodell wird nicht ausdrücklich festgehalten, dass z.B. die Verwaltungseinheit A für einen gegebenen Zeitraum, Teil der Verwaltungseinheit B und diese Teil der Verwaltungseinheit D war. Dies erscheint deshalb als zulässig, weil im Projekt von der Prämisse ausgegangen wird, dass, zumindest für den für das Projekt primär interessante (Zeit)Raum, die Fläche der übergeordnete Einheit stets die Summe aller ihr untergeordneten Einheiten bildet. Dies ermöglicht es, dass part-of Beziehungen zwischen TempSpatial Objekten ‘on the fly’ mit Hilfe von spatial queries und unter Berücksichtigung der jeweiligen Start- und Enddatumswerten berechnet werden können. Und dies wiederum erleichtert einerseits die Arbeit der DatenkuratorInnen im Projekt, da diese keine expliziten Verbindungen pflegen müssen, andererseits ermöglicht dieses Flexibilität die Integration anderer Datensatz mit relativ geringem Aufwand.
            An dieser Stelle muss jedoch betont werden, dass die bis dato kuratierte Menge an Daten noch nicht ausreicht um konkrete Aussagen hinsichtlich der Belastbarkeit des hier vorgestellten Datenmodells treffen zu können. Erste Tests diesbezüglich fielen jedoch durchwegs positiv aus, sowohl was die Genauigkeit der spatial queries, vor allem aber auch was deren Performanz betrifft.
            Die technischen Komponenten des Projektes sind überschaubar. Als Storage Layer fungiert eine Postgresql Datenbank mit PostGIS erweiterung. Die Interaktion damit erfolgt über einen mittels dem Python basierten Webframework (Geo)Django implementierten Applikation Layer, wobei mit (Geo)Django, respektive django-rest-framework sowohl die HistoGIS-Webapplikation als auch ein entsprechender REST Webservice implementiert wurde bzw. wird.
            Die eigentlich Datenkuration erfolgt davon völlig unabhängig und unter Verwendung der open source Software Qgis. Bis dato wurden damit vorwiegend bereits existierende Daten harmonisiert und als ESRI shapefiles gespeichert. Das Schema dieser Dateien entspricht dabei weitgehend dem oben skizzierten Datenmodell. Als ‘fertig’ erachtete Datensätze (gezippte Shapefiles) werden dann über ein Webformular in HistoGIS als sogenannte “Source” objekte hochgeladen, entpackt, die Features der Shapfiles als TempSpatial Objekte gespeichert und mit dem Source Objekt verknüpft. 
            Neben der Kuration und Harmonisierung bereits bestehender “Vektordaten” werden im Projekt aber auch selbst Daten erzeugt. Dazu wählt ein Historiker im Team verwertbare (historische) Karten aus, welche idealerweise bereits digitalisiert (gescannt) sind. Die Datakuratorinnen georeferenziert diese Scans (geotiffs) und extrahieren die darin auffindbaren Informationen zu historischen Verwaltungsgrenzen als Vektordaten. 
         
         
            Kartenmaterial bis dato und Ausblick
            Bis dato befinden sich knapp 4000 Polygone in der Production Instanz des Systems. Das Anpassen und Einspielen schon vorhandener Polygone wurde mit Daten aus dem Census mosaic Projekt (https://censusmosaic.demog.berkeley.edu/) und dem HGis Archiv (http://www.hgis-germany.de/) begonnen. Damit können große Teile des 19. Jhdts für die Gebiete Österreich-Ungarns und des Deutschen Bundes (inkl. der jeweiligen Nachfolgeentitäten) bereits abgedeckt werden. Die Daten werden nicht nur technisch aufbereitet, sondern auch inhaltlich von einem Verwaltungshistoriker überprüft. HistoGIS implementiert dafür ein Ampelsystem. Vom HistoGIS-Team technisch wie inhaltlich überprüfte Daten werden mit Grün markiert, vom HistoGIS-Team ausgewählte Daten die noch nicht überprüft wurden mit Gelb und von Usern zur Verfügung gestellte, nicht überprüfte Daten mit Rot (momentan befinden sich lediglich gelbe und grüne Daten im System).
            Die Aufbereitung der Daten, wie auch die Entwicklung des technischen Systems schreitet erfreulicher Weise schneller voran als geplant. Es wurde deshalb erst kürzlich beschlossen in HistoGIS schon während der Projektlaufzeit auch Daten außerhalb der geplanten räumlich-zeitlichen Grenzen aufzunehmen.
            In unserer Präsentation werden wir vor allem das Datenmodell und die RestAPI Schnittstellen des Systems diskutieren und vorstellen.
         
      
      
         
            
                    Eine auf diese API aufbauende Abfragemaske findet sich hier: https://histogis.acdh.oeaw.ac.at/shapes/where-was/
                
            
                    Beispielhaft das Polygon für Tirol: https://histogis.acdh.oeaw.ac.at/shapes/shape/detail/3352
                
         
         
            
               Bibliographie
               
                  Kamzelak, Roland S. (2018): “Von der Raupe zum Schmetterling oder Wie fliegen lernen – Editionsphilologie zwischen Infrastruktur und Semantic Web.” In: Kamzelak, Roland S / Steyer, Timo (eds.): Digitale Metamorphose: Digital Humanities und Editionswissenschaft. (= Sonderband der Zeitschrift für digitale Geisteswissenschaften, 2). text/html Format. DOI: 10.17175/sb002_004 [letzter Zugriff 28. September 2018]
					
               
                  Nüssli, Marc-Antoine / Nüssli, Christos (2017): “A formal model for historical atlases and historical knowledge”, http://www.academia.edu/35853762 [letzter Zugriff 28. September 2018]
					
            
         
      
   



      
         
            Einleitung
            Unter dem Begriff des 
                     Semantic Web (Berners-Lee, Hendler, Lassila 2001) werden Techniken, Standards und Methoden zusammengefasst, mit deren Hilfe im Internet verfügbare Daten der semantischen Verarbeitung durch Maschinen zugänglich gemacht werden können. Durch die Einführung und Nutzung von offenen Standards wie z. B. RDF (Schreiber & Raimond 2014) soll hierbei die Interoperabilität unterschiedlicher Datenquellen sichergestellt werden. Diese Standards beziehen sich auf die Art, wie Informationen repräsentiert werden und wie Verknüpfungen mit anderen Informationen hergestellt werden können. Daher wird oftmals auch der Begriff der 
                     Linked Data verwendet (Bizer, Heath, Berners-Lee 2009). In einer Visualisierung der Linked-Data-Cloud von 2017 (Freyberg 2017: 29) sind die Geisteswissenschaften als eigener Bereich nicht explizit aufgeführt, was die geringe Veröffentlichung geisteswissenschaftlicher semantischer Daten widerspiegelt bzw. vermuten lässt, wenngleich z. B. im Bereich der Graphentechnologien durchaus einige Projekte existieren (Kuczera 2017). 
                  
                   
            
               Metadaten als Basis literaturwissenschaftlicher Forschung
               Dabei sind solche Daten Basis vieler (literatur-)wissenschaftlicher Fragestellungen: Soll bspw. eine quantitative Textanalyse einer großen Anzahl von Romanen durchgeführt werden, müssen zunächst einmal die in Frage kommenden Werke ermittelt und ausgewählt werden. Die Erstellung solcher möglichst repräsentativen Samples ist allerdings ohne eine Kenntnis der gesamten Romanproduktion einer Epoche, der dort behandelten Themen und Motive und weiterer Angaben über die inhaltliche Ausgestaltung der zu betrachtenden Textproduktion nicht ohne Weiteres möglich.
               Hierbei helfen können Nachschlagewerke wie z. B. Fachbibliographien, in denen bibliographische Metadaten verzeichnet sind. Teilweise liegen solche Metadaten bereits als Linked Data vor, da Bibliothekskataloge (retro-)digitalisiert wurden. Diese Metadaten sind als Basis literaturhistorischer Arbeit jedoch häufig nicht ausreichend, da für eine zielgerichtete Auswahl relevanter Literatur oftmals mehr als die üblicherweise erschlossenen bibliographischen Angaben notwendig sind.
               Einen weiteren, großen Anteil an der prinzipiell verfügbaren Literatur haben jedoch auch Werke, die nicht digitalisiert, sondern nur in gedruckter Form vorliegen. Die 
                        Bibliographie du genre romanesque français 1751-1800 (Martin, Mylne, Frautschi 1977) fasst alle von den Autoren auffindbaren französischsprachigen Romane aus der zweiten Hälfte des 18. Jahrhunderts zusammen. Neben bibliographischen Daten zu Autoren, Werktiteln, Verlegern u. a. sind, soweit möglich, auch Angaben zu weiteren Auflagen (Reeditionen) und zum Inhalt der Werke zusammengetragen worden. Die Bibliographie enthält somit inhaltliche Informationen zu den einzelnen Romanen, die weit über eine Auflistung bibliographischer Metadaten hinausgehen. Solche Informationen sind wie o. g. notwendige Voraussetzung für die Erstellung repräsentativer Samples, u. a. zur weiteren literaturhistorischen Untersuchung der Textproduktion einer Sprache bzw. Epoche.
                     
            
            
               Zielsetzung
               Im Rahmen des hier präsentierten Vorhabens – einer Masterarbeit im Studiengang Digital Humanities an der Universität Trier – wurde die o. g. Bibliographie eingescannt und mittels 
                        Optical Character Recognition (OCR) in maschinenlesbaren Text umgewandelt. Auf dieser Grundlage wurden mithilfe eines Verfahrens des überwachten maschinellen Lernens die einzelnen Einträge extrahiert, in ein selbst entwickeltes semantisches Modell überführt und mit externen Daten verknüpft, sodass die Bibliographie nunmehr als RDF-Datensatz vorliegt und weiterverwendet werden kann. Zielsetzung der Arbeit war es, die in der Bibliographie enthaltenen Informationen unter Nutzung bibliographischer Standards und aktueller, verbreiteter Datenmodelle auf eine Art und Weise zu repräsentieren, die zukünftig weitere Verarbeitungen und Anreicherungen ermöglicht. Die so entstandene digitale Bibliographie kann darüber hinaus als Basis für buchwissenschaftliche, literaturhistorische und verwandte Forschungen dienen, da in ihr sowohl formale als auch inhaltliche Metadaten zur französischsprachigen Romanproduktion eines definierten Zeitraums enthalten sind.
                     
            
         
         
            Metadatenextraktion
            
               Ablauf
               Der Ablauf der Metadatenextraktion ist in Abbildung 1 dargestellt.
               
                  
                     
                     Abbildung 1: Ablauf der Metadatenextraktion
                  Nach dem Einscannen der gedruckten Vorlage, der OCR, der Vorverarbeitung (Korrektur von Fehlern, Entfernen von Vorwort und Abbildungen, einheitliche Zeichenkodierung etc.) wurden die einzelnen Jahreslisten der Bibliographie und innerhalb dieser die einzelnen Einträge/Romane durch XML-Markup voneinander getrennt (Segmentierung).
                     
               Anschließend wurde ein Trainingsset erstellt, mit welchem der verwendete Algorithmus trainiert werden konnte. Für die Trainingsdaten wurde aus jedem Jahrzehnt ein Jahr ausgewählt und die Metadaten der in diesem Jahr erschienenen Romane wurden manuell mit XML-Markup ausgezeichnet. Zur Evaluation der Modelle wurde ein Teil der Daten als Testset zurückgehalten.
               Das maschinelle Lernen verlief iterativ, sodass jeweils Modelle für unterschiedlich „tiefe“ Metadatenebenen gelernt wurden, da eine mehrstufige Anwendung mehrerer Modelle oftmals bessere Ergebnisse als die Verwendung eines einzigen Modells für die gesamten Daten erzielt (Kovacevic et al. 2011: 388) und simpler strukturierte Modelle weniger Trainingsdaten benötigen (Candeias 2011: 28). Ein erstes Modell wurde bspw. zur Bestimmung der Makrostruktur der Metadaten verwendet (Titel, Autor, Publikationsdetails etc.), weitere Modelle verfeinerten jeweils die Auszeichnung innerhalb einer dieser Gruppen (z. B. Differenzierung der Publikationsdetails: Ort, Verleger, Jahr, Format, Seitenangabe). Insgesamt wurden sechs Modelle trainiert, die durch stichprobenartige Analyse der erzeugten Daten sukzessive angepasst wurden, bis keine Verbesserungen mehr möglich waren. Das jeweils beste Modell einer Iteration wurde dann auf die restlichen, noch nicht im Trainings- bzw. Testset enthaltenen Jahreslisten angewendet.
            
            
               Algorithmus und Features
               Zur Modellbildung wurden 
                        Conditional Random Fields (CRF), ein Verfahren des überwachten maschinellen Lernens, verwendet (Lafferty, McCallum, Pereira 2001), das sich in den letzten Jahren zu einem wesentlichen Verfahren im Rahmen der Informationsextraktion entwickelt hat (vgl. z. B. Groza, Grimnes, Handschuh 2012). CRF kombinieren die Vorteile von 
                        Hidden-Markov-Modellen (HMM) und 
                        Support Vector Machines (SVM), zwei weiteren gut untersuchten Verfahren (Peng, McCallum 2004: 329).
                     
               Die in den Algorithmus eingespeisten Daten (hier: Wörter bzw. Token) werden als Sequenzen von Zuständen modelliert und auf Grundlage dieser beobachteten Zustände werden Label für die einzelnen Elemente vergeben. Im Gegensatz zu HMM berücksichtigen CRF jedoch mögliche Beziehungen der Elemente untereinander – im vorliegenden Fall also der Metadatenfelder bzw. der berücksichtigten Features. Da die Einträge der Bibliographie einem definierten Schema folgen (z. B. steht immer zuerst die Autorenangabe, dann folgt der Titel), ist dieser Algorithmus zur Modellierung der vorliegenden Daten besonders geeignet.
               
                  
                     
                  Damit ein CRF-Modell trainiert werden kann, müssen Features erhoben werden, die den Inhalt der einzelnen Metadatenfelder repräsentieren. Tabelle 1 gibt die genutzten Features wieder. Diese Features wurden nicht nur für das jeweilige Wort, sondern auch für das vorherige und das nachfolgende Wort erhoben. So kann im Modell bspw. gelernt werden, dass auf ein bestimmtes Wort stets eine Zahl folgt. 
                     
               Die genutzten Features wurden ausgehend von einer manuellen Analyse der Einträge in der Bibliographie und basierend auf den ausführlichen Erläuterungen der Autoren zur Sammlung und Strukturierung der Daten im Vorwort der Bibliographie ausgewählt. In der gedruckten Vorlage wurde Großschreibung bspw. zur Hervorhebung von Familiennamen verwendet und Angaben zum Inhalt eines Romans folgten fest definierten einleitenden Begriffen. 
               Eine ausführliche Evaluation unterschiedlicher Feature-Kombinationen fand im Rahmen der Arbeit nicht statt, da bereits die o. g. simplen Features zu ausreichend hoher Genauigkeit der Metadatenextraktion führten. Weitere Optimierungen hätten überdies vom eigentlichen Ziel der Arbeit weggeführt. Die zur Unterscheidung der einzelnen Metadatenfelder günstigsten Features wurden jedoch erhoben, um die Wirksamkeit und innere Struktur der gelernten Modelle zu überprüfen. Hierbei zeigte sich z. B., dass die einleitenden Wendungen zur inhaltlichen Beschreibung der Romane auch vom Algorithmus als solche gelernt und zur Auszeichung neuer Daten verwendet wurden. 
               Um auch weniger strukturierte Datengrundlagen als Bibliographien mit dem entwickelten Workflow verarbeiten zu können, bestünde hier ein möglicher, näher zu untersuchender Ansatzpunkt für eine genauere Analyse hilfreicher Features und die eventuelle Einführung weiterer Features.
                  
            
            
               Evaluation
               Das maschinelle Lernen wurde mithilfe der Programmiersprache 
                        Python und der dort verfügbaren Bibliothek 
                        sklearn-crfsuite
                   implementiert. Die Evaluation der Modelle geschah mit der zu 
                        sklearn-crfsuite kompatiblen Bibliothek für wissenschaftliche Programmierung
                         scikit-learn
                  .  In der folgenden Tabelle sind die gängigen Maße Precision, Recall und der F1-Score für die sechs gelernten Modelle angegeben.
                     
               
                  
                     
                  
               
               Für alle Metadatenfelder konnte eine sehr hohe Genauigkeit erreicht werden. Der so erzeugte Datensatz mit allen Einträgen aus der Bibliographie ist somit nahezu vollständig korrekt ausgezeichnet.
            
         
         
            Semantische Modellierung
            Zurzeit existiert kein einheitlicher, akzeptierter Standard, der in der Bibliothekswelt für die semantische Repräsentation bibliographischer Daten verwendet wird. Stattdessen orientieren sich diejenigen Bibliotheken, die bereits Linked Data zur Verfügung stellen, an unterschiedlichen Datenmodellen, Schemas und Ontologien. Es existieren jedoch Versuche, die bereits entwickelten Modelle in ein möglichst generisches und von vielen Bibliotheken nachnutzbares Modell zu integrieren (Suominen, Hyvönen 2017).
            
               Vorhandene Ontologien
               Vor allem die folgenden Datenmodelle sind für die semantische Modellierung der Metadaten aus der Bibliographie relevant, da sie entweder bereits weit verbreitet sind oder spezifische Elemente enthalten, die nachgenutzt werden können.
               
                  
                     FRBR: Functional Requirements for Bibliographic Records  und
                           RDA: Resource Description and Access (IFLA 2009)
                        
                  
                     DCTerms: 
                     Dublin Core Metadata Terms (Dublin Core Metadata Initiative 2012)
                        
                  
                     PRISM: Publishing Requirements for Industry Standard Metadata (IDEAlliance 2008)
                        
                  
                     SPAR
                     Ontologies (Peroni, Shotton 2018)
                        
               
               Die Entwicklung der SPAR-Ontologien wird von den Autoren u. a. damit begründet, dass bisherige Systeme uneinheitlich seien und deutliche Schwächen aufwiesen. PRISM und FRBR seien bspw. „top-level vocabularies rather than something specifically developed to characterise specific aspects of scholarly publishing“ (Peroni, Shotton 2018). Gleichzeitig benutzen die SPAR-Ontologien jedoch Elemente aus den anderen o. g. Vokabularen, um Redundanzen und doppelte Element-Definitionen zu vermeiden. In der hier beschriebenen Arbeit wurde daher ebenfalls versucht, aus den o. g. Datenmodellen vorrangig diejenigen Elemente zu verwenden, die bereits im Bibliothekswesen etabliert und nicht zu spezifisch, gleichzeitig aber ausreichend detailliert sind.
            
            
               Modellentwicklung
               Nach einer eingehenden Analyse der in der Bibliographie vorhandenen Metadaten wurden aus den o. g. Ontologien diejenigen Elemente zur weiteren Berücksichtigung ausgewählt, die zur möglichst genauen und eindeutigen Modellierung der einzelnen Einträge der Bibliographie (siehe Abbildung 2) benötigt werden. Hierbei wurde darauf geachtet, nicht bloß die einzelnen Romane mit ihren Metadaten abzubilden, sondern auch den Aufbau und die Struktur der Bibliographie an sich. Dadurch konnte das gesamte zu erzeugende Modell an den bereits im Linked-Data-Service der 
                        Bibliothèque nationale de France (BnF) vorhandenen Eintrag für die 
                        Bibliographie du genre romanesque français angebunden werden.
               
               
                  
                     
                     Abbildung 2: Beispieleinträge in der gedruckten Bibliographie
                  
               
               Durch die im Vorfeld bereits erfolgte Extraktion der einzelnen Metadatenfelder aus den OCR-Daten konnten diese schließlich direkt auf die entsprechenden Elemente in dem erstellten RDF-Modell abgebildet werden. Dies geschah überwiegend mithilfe der Programmiersprache 
                        Java und der dort verfügbaren Bibliothek 
                        Apache Jena
                  .
                     
            
            
               Verknüpfung mit anderen Ressourcen
               Um die Möglichkeit der Anreicherung der Daten mit Informationen aus externen Ressourcen beispielhaft darzustellen, wurden die Namen der Autoren der einzelnen Romane aus dem RDF-Modell extrahiert und mithilfe von Apache Jena an die API der 
                        Virtual 
                  International Authority File (VIAF) gesendet. Von dort wurden – sofern vorhanden – die VIAF-IDs extrahiert und dem RDF-Modell hinzugefügt. Weitere externe Ressourcen könnten auf ähnliche Weise integriert werden. Voraussetzung für die erfolgreiche Nutzung der API ist, dass die Einträge im RDF-Modell keine Schreibfehler oder OCR-Fehler aufweisen. Dies kommt allerdings relativ häufig vor (Gründe sind u. a.: kleine Schrift in der Vorlage, viele Eigennamen, kurze Wörter mit wenig Kontext) und ist eines der wesentliche Probleme des Datensatzes.
                     
            
         
         
            Fazit
            Sowohl die Extraktion der einzelnen Metadaten aus den OCR-Texten als auch die Erstellung und anschließende Überführung in ein RDF-Modell ließen sich mit gutem Erfolg umsetzen. Die Erkennungsgenauigkeit des CRF-Algorithmus war mit einem F1-Score von durchschnittlich 0,964 (0,908–0,997) außerordentlich hoch. Grund hierfür war sicherlich vor allem die bereits stark strukturierte Datengrundlage. Fehlende einheitliche Standards zur Repräsentation bibliographischer Metadaten und Fehler in den Textdaten sind jedoch Schwachstellen, die eine genauere Analyse und evtl. umfangreiche Bereinigung/Korrektur der zu repräsentierenden Daten nötig machen.
            Das vorgestellte Projekt hat durch die Kombination von modernen Verfahren zur Informationsextraktion und die Zusammenstellung von aktuellen Ontologien zur Repräsentation bibliographischer Metadaten einen für die Datengrundlage passenden Ansatz entwickelt, der als Standard-Workflow für ähnliche Projekte verwendet werden könnte und in solchen überprüft und verfeinert werden sollte. Denkbar wären z. B. die Digitalisierung und Metadatenextraktion weiterer Bibliographien, um den erzeugten Datenbestand zu ergänzen, zu erweitern oder anzureichern. Auch die Überprüfung des hier beschriebenen Vorgehens in verwandten Kontexten (andere Nachschlagewerke, andere Sprachen, andere Epochen) unter Nutzung weiterer oder anderer Features wäre sinnvoll. 
            Der Workflow und die Daten werden daher am 
                     Trier Center for Digital Humanities im Rahmen des von der Forschungsinitiative Rheinland-Pfalz geförderten Projektes „MiMoText – Mining and Modeling Text“ weiterverwendet und erweitert. Ziel ist hier der Aufbau eines „aus unterschiedlichen Quellen gespeisten Informationsnetzwerks für die Geisteswissenschaften, das durch die Bereitstellung als Linked Open Data nicht nur frei verfügbar und mit anderen Wissensressourcen des Semantic Web verknüpfbar ist, sondern auch neuartige und effiziente Zugriffsmöglichkeiten auf fachwissenschaftliche Informationen bietet“. Die beschriebene Arbeit liefert hierfür eine geeignete Grundlage.
                  
         
      
      
         
             Der Datensatz ist verfügbar unter 
                      (Lizenz: CC-BY).
                  
            
               
            
            
               
            
            
               
            
            
               
            
            
               
            
            
               https://kompetenzzentrum.uni-trier.de/de/projekte/projekte/m/
            
         
         
            
               Bibliographie
               
                  Berners-Lee, Tim / 
                  Hendler, 
                  James / 
                  Lassila, 
                  Ora (2001): 
                        "The Semantic Web", in: 
                        Scientific American 284.5: 29–37.
                     
               
                  Bizer, Christian / 
                  Heath,
                   Tom / 
                  Berners-Lee, 
                  Tim (2009): 
                        "Linked Data – The Story So Far", in: 
                        International Journal on Semantic Web and Information Systems 5.3: 1–22 
                        http://tomheath.com/papers/bizer-heath-berners-lee-ijswis-linked-data.pdf
                        [letzter Zugriff 03. Januar 2020]. 
                     
               
                  Candeias, Ricardo Pereira (2011): 
                        Metadata Extraction from Scholarly Articles. Master Thesis, Universidade Técnica de Lisboa. 
                         [letzter Zugriff 03. Januar 2020].
                     
               
                  Dublin Core Metadata Initiative (2012): 
                        DCMI Metadata Terms. DCMI Recommendation. 
                         [letzter Zugriff 03. Januar 2020],
                     
               
                  Freyberg, Linda (2017): 
                        "Density of Knowledge Organization Systems", in: 
                        Knowledge Organization for Digital Humanities. Proceedings of the 15th Conference on Knowledge Organization WissOrg '17 of the German Chapter of the International Society for Knowledge Organization (ISKO) 25–30.
                     
               
                  Groza, T. / Grimnes, A. / Handschuh, S. (2012): 
                        "Reference Information Extraction and Processing Using Conditional Random Fields", in: 
                        Information Technology and Libraries 31.2: 6–20.
                     
               
                  IDEAlliance – International Digital Enterprise Alliance (2008): 
                        The PRISM Namespace – Final 
                  http://www.prismstandard.org/specifications/2.0/PRISM_prism_namespace_2.0.pdf
                        [letzter Zugriff 03. Januar 2020].
                     
               
                  IFLA Study Group on the Functional Requirements for Bibliographic Records (2009): 
                        Functional Requirements for Bibliographic Records – Final Report. (IFLA Series on Bibliographic Control, Vol. 19) 
                         [letzter Zugriff 03. Januar 2020].
                     
               
                  Kovacevic, Aleksandar / Ivanovic, Dragan / Milosavljevic, Branko / Konjovic, Zora / Surla, Dusan (2011): 
                        "Automatic extraction of metadata from scientific publications for CRIS systems“, in: 
                        Program 45.4: 376–396.
                     
               
                  Kuczera, A. (2017): 
                        "Graphentechnologien in den Digitalen Geisteswissenschaften", in: 
                        ABI Technik, 37.3: 179–196.
                     
               
                  Lafferty, John D. / 
                  McCallum, 
                  Andrew / 
                  Pereira, 
                  Fernando C. N (2001): 
                        "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data", in: 
                        Proceedings of the Eighteenth International Conference on Machine Learning (ICML '01) 282–289.
                     
               
                  Martin, Angus / 
                  Mylne,
                   Vivienne / 
                  Frautschi, 
                  Richard (1977): 
                        Bibliographie du genre romanesque français 1751-1800. London, Paris: Mansell, France expansion.
                     
               
                  Peng, Fuchun / McCallum, Andrew (2004): 
                        "Accurate Information Extraction from Research Papers using Conditional Random Fields", in: 
                        Proceedings of Human Language Technology Conference and North American Chapter of the Association for Computational Linguistics (HLTNAACL) 329–336 
                        [letzter Zugriff 03. Januar 2020].
                     
               
                  Peroni, Silvio / 
                  Shotton, 
                  David (2018): 
                        "The SPAR Ontologies", in: Vrandečić D. et al. (eds.): 
                        The Semantic Web – ISWC 2018. Lecture Notes in Computer Science. Cham: Springer 119–136.
                     
               
                  Schreiber, Guus / 
                  Raimond, 
                  Yves (2014): 
                        RDF 1.1 Primer. W3C Note. 
                         [letzter Zugriff 03. Januar 2020].
                     
               
                  Suominen, Osma / Hyvönen, Nina (2017): 
                        "From MARC silos to Linked Data silos?", in: 
                        o-bib. Das offene Bibliotheksjournal 4.2: 1–13.
                     
            
         
      
   



      
         
            Die Infrastuktur und das Projekt
            Seit 2010 kooperieren das Wittgenstein Archiv der Universität Bergen und das Centrum für Informations- und Sprachverarbeitung der Ludwig-Maximilians Universität München in der Forschungsgruppe „Wittgenstein Advanced Search Group“ (WAST). Die Forschungsgruppe entwickelt Web-Frontends (FinderApps) und spezielle Suchwerkzeuge, die sich gut für die Forschung und Lehre im Bereich der Digital Humanities eignen. Ihre erste Suchmaschine, die FinderApp WiTTFind (wittfind.cis.lmu.de, siehe Abb. 1), die den von der UNESCO zum Weltkulturerbe (im Jahr 2017) erhobenen (Schmidt 2018) Nachlass von Ludwig Wittgenstein durchsucht, gewann im Jahre 2014 der EU-Open-Humanity Award. Der Preis zeichnet Gruppen aus, die herausragende Technologie im Bereich der Humanities entwickelt haben. Die in der Forschergruppe programmierte FinderApp WiTTFind erlaubt es, mit hochqualifizierten, computerlinguistisch orientierten Suchwerkzeugen Nachlasstrans-kriptionen zu durchsuchen. Die Transkriptionen entstammen der 
                    Bergen Normalized Edition, die die Grundlage der Wittgenstein Edition bildet. Neben den gefundenen Treffern der Suchmaschine, werden in den Suchergebnissen von WiTTFind die Faksimile-Extrakte aus den Originaldokumenten angezeigt. So kann der Nutzer die „Aura“ der gefundenen Textstelle im Original studieren und nicht nur den transkribierten Text sehen.
                
            
               
                  
                   Abbildung 1: WiTTFind (http://wittfind.cis.lmu.de)
               
            
            Damit derNutzer auch den seitenweisen Kontext des Suchtreffers im Original studieren kann, wurde am CIS eine weitere WEB-Applikation entwickelt, der doppelseitige Reader. Dieser Reader ermöglicht es, vom Suchtreffer direkt an die entsprechende Stelle im entsprechenden Dokument des Originals zu springen. Im doppelseitigen Lesemodus kann der Nutzer in den Faksimile des originalen Dokuments blättern. Eine symmetrische Autovervollständigung gibt während der Suchanfrage einen statistischen und lexikalischen Zugang zu den Wörtern, die in der Edition vorkommen. Im Zentrum der Suche steht die selbstprogrammierte C++ Suchmaschine wf, die mit Hilfe von Vollformlexika (WiTTlex), verbessertem POS-Tagging und weiteren Metainformationen regelbasiertes Suchen erlaubt. Zum Aufspüren semantisch ähnlicher Textpassagen in der Edition gibt es das NLP-Tool WiTTSim.
            Die thematisch getrennten Aufgaben innerhalb der Infrastruktur der WAST-Tools (siehe Abb. 2) werden über REST-API’s von einzelnen Microservices realisiert, deren zentrale Datenhaltung über eine mongo Datenbank realisiert wird. Die Oberflächen der FinderApps werden mit HTML5, Javascript und Bootstraptechniken für WEB-Browser programmiert und möglichst browserunabhängig gehalten. 
            
               
                  
                   Abbildung 2: Infrastruktur der WAST-Tools (http://gitlab.cis.lmu.de)
               
            
            Alle Programme, Schnittstellen und Entwicklungen werden dokumentiert (siehe Abb. 3) und Tutorials für Anschlussprojekte entwickelt. So ist gewährleistet, dass die Tools und Suchmaschinen nachhaltig verwendet und auch für die Forschung und Lehre eingesetzt werden können. Als Versionskontrollsystem wird git verwendet.
            
               
                  
                   Abbildung 3: Dokumentation der WAST-Tools: http://wittfind.cis.uni-muenchen.de/wast/infrastruktur/index.html
                  
               
            
            Bei der Entwicklung der Infrastruktur der WAST-Tools wurden die strengen Vorgaben des EU-Open-Humanity Awards eingehalten: Forderungen nach Open-Source, interdisziplinäre Öffnung und Nachhaltigkeit. Diese Offenheit ermöglichte es weitere FinderApps für andere Wissenschaftsbereiche zu implementieren: GoetheFind (Faust-I und Faust-II Edition, Deutsches Textarchiv Berlin (XML-TEIP5, DTA Basis Format)), HistoFind (Briefwechsel Erzherzog Leopold Wilhelms an Kaiser Ferdinand III. aus dem Reichsarchiv Stockholm; Kooperation mit Historikern) und den OdysseeReader (Schreibprozess der zur Logisch-Philosophischen-Abhandlung führte; Kooperation mit Philosophen).
            In diesem Workshop werden die verwendeten Softwaretechnologien und computerlinguistischen Methoden im konkreten Einsatz vorgestellt. Den Teilnehmer*innen wird ein Debian-10 Container mit allen notwendigen Programmen, Tools und Dokumentation der gesamten Softwareinfrastruktur zur Verfügung gestellt. Innerhalb dieses Containers können die Teilnehmer*innen die einzelnen Tools der WAST-Projektgruppe kennenlernen und bekommen von den Projektmitarbeiter*innen kleine Aufgaben gestellt, die sie dann mit ihnen bearbeiten. So können sie die Arbeitsweise der WAST Infrastruktur konkret kennenlernen.
         
         
            Im Workshop werden folgende Datenformate, Tools und Programmierkonzepte vorgestellt und geübt
            Gitlab Projektmanagement und Continuous Integration, XML TEI-P5 Edition CISWAB, Faksimilestrukturierung und Texterkennung, lexikalische Arbeit, WEB-Oberfläche der FinderApps und Einsatz mit Micorservices, doppelseitiger Faksimilereader mit MongoDB, NLP-Tools zur semantischen Ähnlichkeitssuche, Vorstellung und Programmierung einer regelbasierten Suchmaschine und die Erstellung eines Dokumentationssystems mit Sphinx.
            
               Voraussetzungen an die Kursteilnehmer*innen
               Programmierkenntnisse (Grundkenntnisse): LINUX (Arbeit mit der UNIX-Shell), Python, XML, HTML, git, javascript, POS-Tagging.
               Da beim Workshop einige Entwickler der WAST-Tools anwesend sein werden, gibt es die Möglichkeit auch vertieft in die jeweilige Thematik einzusteigen.
            
            
               Gitlab Projektmanagement und Continuous Integration (Hadersbeck, Still)
               Im gesamten Projekt wird als Versionierungssystem git verwendet. Die Projektrepositories werden auf zwei unterschiedlichen Rechnern ausgerollt: Dem preview-Server für Tests und einem Projektserver für die offizielle Onlineversion. Es wird das in der Praxis bewährte „git branching model“ kombiniert mit einer „continuous integration“ Technik eingesetzt. Mit einer Feedbackapp können Nutzer Fehler melden oder Implementierungswünsche äußern, die in Issues innerhalb der Projektrepositories bearbeitet werden.
            
            
               XML TEI-P5 Edition CISWAB (Hadersbeck)
               Als Datenbasis für das WiTTFind Projekt wird die „Bergen Nachlass Edition“ (BNE) verwendet, die sich an den Richtlinien der Text Encoding Initiative (TEI-P5) orientiert. Im Workshop werden die wichtigen TEI-XML-Elemente der BNE vorgestellt.
                  
            
            
               Faksimilestrukturierung und Erkennung (Eisterhues, Landes)
               Da in den FinderApps neben den gefunden Textstellen auch die zugehörigen Faksimileextrakte aus der Edition dargestellt werden, sind Kenntnisse der Bildkoordinaten der Textstellen nötig. Diese Koordinaten werden mit Hilfe einer Kette von Bildverarbeitungstools ermittelt. Da bei Manuskripten und bei manuellen Änderungen in Dokumenten die automatische Zeichenerkennung unbrauchbare Ergebnisse liefert, wurden eigene Strategien entwickelt, die die Informationen aus der BNE nutzen. Im Workshop werden die eingesetzten Tools und Optimierungsstrategien vorgestellt.
            
            
               Lexikalische Arbeit (Lokale Grammatiken, Semantik) (Röhrer)
               Zur lemmatisierten Suche, Partikelverberkennung und semantischen Wortfeldern wurden spezielle Projektlexika entwickelt (Röhrer 2017). Die Lexika enthalten alle Wörter der zu durchsuchenden Edition und sind mit grammatischen Angaben und zum Teil mit zusätzlichen semantischen Informationen versehen. Diese Lexika und ein nachgestelltes optimiertes Part-of-Speech Tagging ist die Grundlage für die computerlinguistischen Methoden, die bei der regelbasierten Suche im Nachlass von Ludwig Wittgenstein eingesetzt werden.
            
            
               Regelbasierte Suchmaschine (Babl)
               Im Zentrum der FinderApps steht die Suchmaschine wf, ein multithreaded C++ Programm, das viele Anfragemöglichkeiten zur Suche implementiert: Einwort und Mehrwortsuche (mit internem Rankingverfahren) und reguläre Ausdrücke kombiniert mit linguistischen Anfragen (Morphologische Eigenschaften, POS-Tags, semantische und syntaktische Tags). Für das Rankingverfahren wird für jeden Suchtreffer die Relevanz zur Suchanfrage berechnet. Die Qualität für jeden Suchtreffer, die Distanz zwischen den einzelnen Wörtern und unterschiedlichen Belohnungs- und Bestrafungsparametern, gehen in die Berechnung der Relevanz ein. Die Treffer werden dann nach dieser sortiert und auf der Website ausgegeben. Durch dieses neuartige Ranking kann nun auch nach verschiedenen Wörtern gesucht werden, die im Text nicht direkt hintereinander stehen müssen.
            
            
               NLP-Tool Semantische Ähnlichkeitssuche (Ullrich)
               Zur Extraktion von semantisch ähnlichen Bemerkungen wurde das Analysetool WiTTSim (Ullrich 2018) entwickelt, welches anhand von semantischen und syntaktischen Features ähnliche Texte identifiziert. Da die enorm hohe Anzahl von etwa 100.000 Features in Kombination mit den zu vergleichenden 54.000 Bemerkungen eine effiziente Suche unmöglich macht, wurde ein semantisches Clustering-Verfahren vorgeschaltet (Ullrich 2019), welches durch Dimensionsreduktion und Gruppierung der Texte die Rechenzeit der Ähnlichkeitssuche um den Faktor 100 beschleunigt.
            
            
               WEB-Oberfläche der FinderApps und Micorservices (Hadersbeck, Still)
               Zur Arbeit mit WiTTFind wird dem User eine WEB-basierte FinderApp zur Verfügung gestellt, die über REST-APIs und „internet microservices“ mit den WAST-Tools kommuniziert. HTML5, Javascript und Bootstrap-css erlauben den Aufbau der WEB-page, die nahezu browserunabhängig die Schnittstelle zum Anwender darstellt. 
            
            
               Doppelseitiger Faksimilereader und MongoDB (Lindinger)
               Der doppelseitiger Faksimilereader ist eine komplett eigenständige Anwendung mit Suchschlitz und Investigate Mode zur gleichzeitigen Betrachtung von Faksimile und Transkription. Außerdem gibt es zahlreiche weitere Features, die es den Nutzern sehr bequem erlauben, die gefunden Treffer der Suchmaschine im Kontext einer doppelseitigen Darstellung der Faksimile zu sehen und gleichzeitig durch die Dokumente der Forschungsdomäne zu blättern. Sämtliche Informationen bzgl. Edition und Faksimile sind in einer MongoDB gespeichert und werden über HTTP-Schnittstellen abgefragt.
            
            
               Dokumentationssystem Sphinx (Babl) (siehe Abb.2)
               Für jedes Teilprojekt der Wittgenstein Advanced Search Tools (WAST) wird im entsprechenden Gitlab Ordner eine README.md Datei erstellt, das in einer Dokumentation, die alle Projekte umspannt mithilfe der Software Sphinx zusammengefasst und online auf ansprechende Art und Weise darstellt. Die Dokumentation hilft, neuen Studierenden einen schnelleren Einstieg in das Projekt zu finden und ermöglicht es, das gesamte WAST-Projekt schnell nach bestimmten Fachbegriffen zu durchsuchen. 
            
         
         
            Programm des Workshops (ganztages Workshop)
            
               Überblick/Einführung/Vorstellungsrunde 
               Digitaler Zugang zum Nachlass von Ludwig Wittgenstein, das Projekt WAST (Dr. Max Hadersbeck)
               Fragen/ Diskussion/ gewünschte Schwerpunkte der Teilnehmer*innen des Workshops
            
            
               
                  WAST-Spezialthemen (jeweils ca. 15 Min. Theorie / 20 Min. Praxis)
               
                   Gitlab Projektmanagement und Continuous Integration mit git production / testing server (Hadersbeck, Still) 
      
                   XML TEI-P5 Edition CISWAB (Hadersbeck): Bergen Normalized Edition und xslt-Transformationen und Investigate-Mode von WiTTFind
                   Faksimilestrukturierung und OCR Erkennung (Eisterhues, Landes) 
                   Lexikalische Arbeit (Röhrer): Lemmatisierte Suche, Lexika, Lokale Grammatiken, Query Beispiele
                   WEB-Oberfläche der FinderApps und Microservices (Hadersbeck, Still): Flask server, Javascript
                   Doppelseitiger Faksimilereader und mongodb (Lindinger) 
                   NLP-Tool Semantische Ähnlichkeitssuche (Ullrich): NLP-Python Libraries, Funktionalitäten
                   Regelbasierte Suchmaschine (Babl): Programmierung C++, make/cmake, client-server Programmierung mit C++
                   Dokumentationssystem Sphinx (Babl): Markdown, Sphinx Installation, 2HTML, 2PDF
               
            
            
               Arbeitsgruppen: Diskussionen/Spezialfragen
               Je nach Interesse der Teilnehmer*innen unter der Leitung der einzelnen Dozent*innen.
            
         
         
            Kurzbiographie der Dozent*innen
            
               Florian Babl (CIS)
               Bachelorarbeit: Entwicklung eines Rankingverfahrens der Suchtreffer für die FinderApp WiTTfind im Nachlass Ludwig Wittgensteins 
               Forschungsschwerpunkte: verschiedene Rankingalgorithmen und ihre Funktionalität mit dem Ziel der Rankingverbesserung.
            
            
               Marcel Eisterhues (CIS)
               Forschungsschwerpunkte: Der momentane Forschungsschwerpunkt ist die automatische Seitensegmentierung von handgeschriebenen Texten.
            
            
               Max Hadersbeck (CIS)
               Projektleiter und Dozent am CIS
               Forschungsschwerpunkte: Digitaler Zugang zum Nachlass von Ludwig Wittgenstein, FinderApp WiTTFind, Wittgenstein Advanced Search Tools, Programmierung: C++, Python, XML
            
            
               Florian Landes (Kommission für bayerische Landesgeschichte bei der Bayerischen Akademie der Wissenschaften) 
               Bachelorarbeit: Optical Character Recognition (OCR) – Optische Zeichenerkennung (OZE) Ein Werkzeug zur Verknüpfung von digitaler Edition und Faksimile? Semiautomatische Ermittlung von Bildkoordinaten für WiTTFind
               Forschungsschwerpunkte: OCR, OZE, Bavarikonprojekt Ortsnamen des Regierungsbezirks Schwaben
                  
            
            
               Ines Röhrer (CIS)
               Masterarbeit: Lexikon, Syntax und Semantik - computerlinguistische Untersuchungen zum Nachlass Ludwig Wittgensteins
               Forschungsschwerpunkte: Digitales Speziallexikon WiTTLex für den Nachlass von Ludwig Wittgenstein
            
            
               Sebastian Still (CIS)
               Masterarbeit: Ludwig Wittgenstein: 100 Jahre Traktatus. Der Odyssee-Reader, ein web-basiertes Tool zur textgenetischen Suche im Traktatus
               Forschungsschwerpunkte: moderne Frontend Programmierung, NLP (Backend)
            
            
               Sabine Ullrich (CIS)
               Masterarbeit: Clustering zur Verbesserung der Performanz einer Ähnlichkeitssuche
               Forschungsschwerpunkte: Natural Language Processing, Data Mining, semantische Ähnlichkeitserkennung im Nachlass von Ludwig Wittgenstein
            
         
      
      
         
            
               Bibliographie
               
                  Babl, Florian (2019): 
                        Entwicklung eines Rankingverfahrens der Suchtreffer für die FinderApp WiTTFind im Nachlass Ludwig Wittgensteins. Bachelor‘s thesis. LMU.
                    
               
                  Landes, 
                  Florian (2019): 
                        Optical Character Recognition (OCR) – Optische Zeichenerkennung (OZE). Ein Werkzeug zur Verknüpfung von digitaler Edition und Faksimile? Semiautomatische Ermittlung von Bildkoordinaten für WiTTFind, Bachelorarbeit, LMU.
                    
               
                  Lindinger, Matthias (2013): 
                        Highlighting von Treffern des Suchmaschinentools 
                  WiTTFind im zugehörigen Faksimile. Bachelor‘s thesis, LMU.

               
                  Lindinger, 
                  Matthias (2015): 
                        Entwicklung eines WEB-basierten Faksimileviewers mit Highlighting von Suchmaschinen-Treffern und Anzeige der zugehörigen Texte in unterschiedlichen Editionsformaten. Master's thesis, LMU.
                    
               
                  Pichler, Alois (2017): 
                        Wittgenstein Archives at the University of Bergen (WAB): Open Access to Wittgenstein's Nachlass. XML based Interactive Dynamic Presentation (IDP) of WAB's Nachlass transcriptions. 16. Mai 2017. http://wab.uib.no/transform/wab.php?modus=opsjoner [letzter Zugriff 20.09.2019].
                    
               
                  Hadersbeck, 
                  Maximilian / 
                  Pichler, 
                  Alois / 
                  Fink, 
                  Florian /
                   Gjesdal, 
                  Øyvind L. (2014): „Wittgenstein's Nachlass: WiTTFind and Wittgenstein advanced search tools (WAST)“
                        , in: 
                        Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage, 91-96. ACM.
                    
               
                  Hadersbeck , 
                  Maximilian /
                   Pichler, 
                   Alois /
                   Bruder, Daniel / Schweter, Stefan (2016): 
                        New (re)search 
                  possibilities for Wittgenstein's Nachlass II: Advanced Search, Navigation and Feedback with the FinderApp WiTTFind. http://wab.uib.no/alois/Hadersbeck_Pichler%20Kirchberg2016.pdf [letzter Zugriff 20.09.2019].
                    
               
                  Röhrer, 
                  Ines / 
                  Ullrich, 
                  Sabine / 
                  Hadersbeck, 
                  Maximilian (2019): 
                        Weltkulturerbe international digital: Erweiterung der Wittgenstein Advanced Search Tools durch Semantisierung und neuronale maschinelle Übersetzung. multimedial multimodal. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 25. - 29.03.2019 an den Universitäten zu Mainz und Frankfurt.
                    
               
                  Röhrer, 
                  Ines (2017): 
                        Musik und Ludwig Wittgenstein: Semantische Suche in seinem Nachlass. Bachelor‘s thesis, LMU.
                    
               
                  Schmidt, 
                  Alfred (2018): „Ludwig Wittgenstein’s Nachlass in the UNESCO Memory of the World register.“, in: 
                        Nordic Wittgenstein Review 7(2):209–213.
                    
               
                  Ullrich, 
                  Sabine /
                   Bruder, 
                  Daniel /
                   Hadersbeck, 
                  Maximilian (2018): Aufdecken von „versteckten" Einflüssen: Teil-Automatisierte Textgenetische Prozesse mit Methoden der Computerlinguistik und des Machine Learning. Kritik der digitalen Vernunft. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 26.02.-02.03. 2018 an der Universi
                        tät zu Köln, veranstaltet vom Cologne Center for eHumanities (CceH).
                    
               
                  Ullrich, Sabine (2019):
                         Boosting Performance of a Similarity Detection System using State of the Art Clustering Algorithms. Master‘s thesis. LMU.
                    
            
         
      
   



      
         
            Einleitung
            
  Seit 2010 kooperieren das Wittgenstein Archiv an der Universität Bergen (WAB, Alois Pichler) und das Centrum für Informations- und Sprachverarbeitung der Ludwig-Maximilians Universität München
  (CIS, Max Hadersbeck et. al.) in der Forschungsgruppe „Wittgenstein Advanced Search Tools” (WAST). Die WAST-Projektgruppe entwickelt die web-basierte 
  FinderApp WiTTFind (), die einen computerlinguistisch gestützten digitalen Zugang zu WABs Wittgenstein-Edition erlaubt. Nach einer kompletten Neuscannung des Nachlasses und intensiven Verhandlungen des WAB mit den Rechteinhabern, dürfen seit 2018 WABs Edition auf der WiTTFind-Webseite durchsucht und Faksimileextrakte dargestellt werden. Nun konnten wir uns einer zentralen Frage der Wittgensteinforscher widmen: Wo finden sich in seinem Nachlass semantisch ähnliche Bemerkungen und, retroperspektivisch betrachtet, wann fanden diese Änderungen statt? 

            Wir entwickelten das Analysetool WiTTSim (Ullrich, 2018), das semantisch ähnliche Bemerkungen in der Edition aufspürt, zusammen mit einem vorgeschalteten semantischem Clusterverfahren (Ullrich, 2019), welches die Rechenzeit der Ähnlichkeitssuche um den Faktor 100 verkürzte. Zur retroperspektivischen Analyse der Edition entwickelten wir ein zeitorientiertes, textgenetisches Datenmodell, das die Spielräume der Interpretation der bisher dokumentorientierten Edition auf zugelassene Lesarten reduziert.
            In unserem Vortrag stellen wir die Verfahren unserer Ähnlichkeitssuche mit vorgeschaltetem semantischen Clustering und ein neues mehr textgenetisch- als dokumentorientiertes Modell einer Edition vor, das im Web-Frontend des OdysseeReaders (www.odysseereader.wittfind.cis.lmu.de) implementiert ist und auch die Frage beantwortet: „Wann gibt es semantisch ähnliche Bemerkungen“.
         
         
            Die Datenbasis: Dokument- und Zeitorientierte Modelle
            Die bei uns verwendete Datenbasis BNE 2015- und IDP 2016-, die am Wittgensteinarchiv an der Universität Bergen (Pichler, WAB) erstellt werden, enthalten Faksimile und Transkriptionen (auf der Basis von XML-TEI-P5) des Nachlasses von Ludwig Wittgenstein. Dieser Nachlass umfasst ca. 20.000 Seiten, welche vom WAB in Dokumente und diese wiederum in logische Textabschnitte unterteilt sind. Jeder der 54.930 Textabschnitte – eine sogenannte Bemerkung – wird mit einer eindeutigen Bezeichnung, dem sogenannten Siglum, versehen und wird in unserer Ähnlichkeitssuche als einzelnes Textobjekt definiert und semantisch analysiert. 
            Betrachtet man die Annotationen der BNE unter dem Aspekt der Retroperspektive, taucht folgendes Problem auf: Die BNE liefert nur auf der Ebene der Bemerkungen Informationen über ihren Erstellungszeitpunkt bzw. -zeitrahmen. Die Änderungen auf Wort und Zeichenebene sind zwar akribisch annotiert, allerdings fehlt die zeitliche Information wann diese Änderungen vorgenommen wurden. Um textgenetische Metainformationen auf Wort- bzw. Zeichenebene in das “ordered hierarchy of content objects model data” (OHCO) einer XML-Edition, wie das der BNE zu integrieren, schlägt das TEI-P5 Konsortium Fragmentierungs-, Milestone oder Standoff-Markup Annotationen vor (Jörg Hornschemeyer, 2013), die am WAB bisher nicht durchgeführt wurden. Von Geisteswissenschaftlern, deren wissenschaftliches Kerngebiet im Allgemeinen weit entfernt von der XML-Programmierung liegt, würde großer programmtechnischer Editionsaufwand verlangt. Eine Folge ist, dass von „Nachverwertern“ der Edition zur Generierung der textlichen Varianten algorithmisches Ausmultiplizierten der annotierten Varianten implementiert wird, was z.B. in der Wittgenstein-Edition bei einzelnen Bemerkungen eine vierstellige Anzahl von Lesarten generiert. Betrachtet man die so automatisch generierten Lesarten, sind die meisten syntaktisch und semantisch falsch, was fatale Auswirkungen auf semantische Analysen der Textobjekte hat. Ohne zusätzliche, fein granulierte Metainformation in den annotierten Varianten sind die Spielräume der automatisierten Lesartengenerierung jedoch nicht einzugrenzen.
            Im Umfeld der Wittgensteinforschung gibt es eine Edition, die bis auf Zeichenebene zeitliche Informationen zur Textgenese liefert: Die Prototractatus-Tools (PTT 2016) von Martin Pilch (Pilch 2018). Sie dokumentieren den 
Nutzern Ludwig Wittgensteins Schreibprozess, beginnend mit einem leeren Notizbuch im Jahre 1915 und bis zum endgültigen Diktat des Ts-204 im Sommer 1918, das zu seiner einzigen philosophischen Veröffentlichung zu Lebzeiten, der „Logisch-Philosophischen Abhandlung“ führte. Leider konnten wir die Daten und Metainformationen der PTT-Edition in unserer FinderApp Infrastruktur nicht direkt analysieren, da unsere WiTTFind Infrastruktur zum einen auf das dokumentorientierte XML-TEI-P5 Datenformat aus Bergen zugeschnitten ist, und zum anderen die PTT-Edition im inkompatiblen Microsoft Word-97 Format vorliegt. Alle verfügbaren XML-TEI Importtools erfassten nur Bruchteile der Annotationen, sodass z.B. die Zeitinformationen der PTT überhaupt nicht erkannt und transformiert wurden. Um möglichst viel von der PTT-Textedition weiterzuverwenden, und damit der PTT-Hg. die Edition in seiner gewohnten Microsoft-Office Umgebung weiter optimieren kann, entwickelten wir eine mit Microsoft EXCEL leicht zu bedienende mehrdimensionale Tabellenstruktur. Die Editionsdaten und Metainformation der Word-97 Edition konnten wir größtenteils mit eigenen Programmen und Office-Macrotechniken transferieren. Zur Integration der Tabellen in die Infrastruktur unserer FinderApp verwendeten wir LibreOffice-Tools und selbst geschriebene Python Programme, die die Daten, sobald sie in das git-Repository des Projekts kopiert werden, mit Hilfe der continuous Integration automatisch transformieren und importieren. Zur Web-Präsentation werden sie an unsere neu entwickelte FinderApp, den 
OdysseeReader (siehe Abb. 1,
odysseereader.wittfind.cis.lmu.de), übergeben. Dieses Vorgehen trennt zwar das Daten- und Repräsentationsmodell, jedoch entwickelten wir ein positionsinvariantes Siglensystem, bestehend aus dem Tupel (Zeitstempel, Dokument, Seite, Zeile, Zeichenposition), das die beiden Modelle eineindeutig verknüpft. Diese bijektive Relation zwischen den beiden Modellen definiert dem Hg., wo er in seinem Datenmodell Änderungen vornehmen muss um sie an eine bestimmte Stelle, zu einem bestimmten Zeitpunkt im Repräsentationsmodell zu platzieren.

            
               
                  
                   Abbildung 1: Der OdysseeReader odysseereader.wittfind.cis.lmu.de
                  
               
            
         
         
            Ähnlichkeitssuche mit vorgeschaltetem Semantic Clustering
            Die Ähnlichkeitssuche WiTTSim berechnet mit Hilfe
computerlinguistischer Methoden für jede Bemerkung einen
„charakteristischen” Vektor, oder, intuitiv gesprochen: Man bestimmt
einen “Fingerabdruck”. Dieser automatisierte Prozess wird unabhängig
im Voraus berechnet, was spätere Prozesse vereinfacht und
beschleunigt. Dieser „Fingerabdruck“ beinhaltet linguistische
Informationen, wie beispielsweise Wörter, deutsche und englische
Synonyme (aus Germanet und Wordnet), Wortarten (Treetagger) und
Lemmata  (WiTTLex, Röhrer 2019). Diese Informationen werden in binäre Vektoren übersetzt, welche insgesamt etwa 115.000 Features umfassen. Zusätzlich zur Datenbasis wurden 471 Bemerkungen bereits gruppiert, also mit Ground Truth Labels versehen. Die Gruppen bestehen dabei aus 2-15 Bemerkungen und das gelabelte das Korpus umfasst 1.670 Bemerkungen, was ca. 3% des gesamten Nachlasses entspricht.

            Zur Semantischen Ähnlichkeitsberechnung ist allerdings eine Reduktion des Feature Raumes zwingend nötig, da die Vektoren mit so hoher Dimensionalität semantisch „weit voneinander entfernt“ sind und keine semantischen Gruppierungen auszumachen sind. Dieses Phänomen ist auch bekannt als 
Curse of Dimensionality. Daher werden die Vektoren zunächst auf eine angemessene Anzahl von Features skaliert, um sie anschließend clustern zu können. Verwendete Reduktionstechniken umfassen Singular Vector Decomposition (SVD), Principal Component Analysis (PCA), Sparse Random Projection (SRP) und Uniform Manifold Approximation and Projection (UMAP). Auf unseren Daten zeigte eine SVD Reduktion zu 1.600 Dimensionen die besten Ergebnisse, zusammen mit UMAP, welches darüber hinaus die Daten im zweidimensionalen Raum klar gruppiert. Letzteres erlaubt nur eine Zieldimension von 2 bis 100 Dimensionen, weshalb zum Erhalt der Varianz die maximale Dimensionsanzahl von 100 gewählt wurde, um einen bestmöglichen Erhalt der gespeicherten Information zu gewährleisten.

            Nach erfolgter Reduktion der Dimension können die Datenpunkte, also alle Bemerkungen, geclustert werden. Verwendete Clustering Techniken umfassen den klassischen K-Means Ansatz (Mac-Queen 1967, Ball and Hall 1956, Lloyd 1982, Steinhaus 1955), aber auch Dichte-basierte Ansätze wie Mean-Shift (Duda und Hart 1973) und DBSCAN (Ester et al. 1996), das statistische Gaussian Mixture Modell (Redner und Walker 1984) und das hierarchische Ward Clustering (Ward 1963). Beste Ergebnisse konnten mit einer Kombination von SVD und K-Means mit einer Anzahl an k=150 Clustern erzielt werden. Evaluiert wurde anhand der drei unüberwachten Metriken Silhouette Score, Davies Bouldin Index, und Calinski-Harabasz Index. Zusätzlich konnte durch die verfügbaren Ground Truth Labels auch der Recall berechnet werden, welcher in den Experimenten einen maximalen Wert von 1,0 erreicht. Dies zeigt, dass alle der gelabelten Daten richtig zugeordnet werden konnten. Wird eine Suchanfrage zum Auffinden ähnlicher Bemerkungen gestartet, muss nur der charakteristische Vektor der eingegebenen Bemerkung berechnet werden und das nächstliegende Cluster bestimmt werden. Letzteres erfolgt durch eine Bestimmung des am nächsten gelegenen Cluster Mittelpunkts (Zentroids). Anschließend werden die Abstände zu allen Bemerkungen des bestimmten Clusters gemessen, welche zuletzt dem Philologen zur genaueren Prüfung „gerankt“ vorgeschlagen werden.
         
         
            Zusammenfassung und Ausblick
            Unsere zeitgesteuerte textgenetische Edition kann von einem Wissenschaftler ohne XML Kenntnisse innerhalb einer Office Umgebung erstellt werden. Das continuous Integration System von git transferiert die Edition automatisch in unser WEB-basiertes Repräsentationssystem, den OdysseeReader. Über das von uns entwickelte eineindeutige Siglensystem verliert der Hg. niemals den klaren Zusammenhang zwischen Editions- und Präsentationsmodell.

            Das von uns entwickelte Ähnlichkeitstool mit vorgeschaltetem Semantic Clustering könnte auch zur Ähnlichkeitsbestimmung zwischen zwei gegebenen Texten verwendet werden: Der Nutzer könnte einen Text eingeben, und es werden potentiell ähnliche Textpassagen in einer Sammlung von Texten gesucht, die dann „gerankt“ nach Ähnlichkeiten in einer Art Hitliste ausgegeben werden. Eine derartige Sortierung nach Textähnlichkeiten könnte es dem Philologen zum Beispiel besonders erleichtern, potentielle Zitate, Einflüsse und Verweise eines Autors innerhalb seines Werkes und im Bezug auf die Literatur seiner Zeit aufzuspüren.
         
      
      
         
            
               Bibliographie
               
                  Ball, 
                  Geoffrey H.
                   / Hall 
                  David J. (1965): 
                        Isodata, a novel method of data analysis and pattern classification. Technical report, Stanford research inst Menlo Park CA.
                    
               
                  Duda, Richard
                   O. / Hart, 
                  Peter E. (1973): "Pattern analysis and scene classification." 
                        J. Wiley 1:73.
                    
               
                  Ester, 
                  Martin
                   / Kriegel 
                  Hans-Peter /
                   Sander, 
                  Jörg
                   / Xu, 
                  Xiaowei
                   et al. (1996): „A density-based algorithm for discovering clusters in large spatial databases with noise.“, in 
                        KDD, volume 96, pages 226–231.
                    
               Hadersbeck, Maximilian / Pichler, Alois / Fink, Florian / Gjesdal, Oyvind (2014): Wittgenstein’s Nachlass: WiTTFind and Wittgenstein Advanced Search Tools (WAST), DATeH, Madrid.
               
                  Hadersbeck, 
                  Maximilian
                   / Still, 
                  Sebastian
                   (2018): 
                  Investigating Wittgenstein’s Nachlass: WiTTFind, WiTTReader, OdysseeReader and Wittgenstein Advanced Search Tools, im Katalog zur Ausstellung „DIE TRACTATUS ODYSSEE“ S.127-137, Wittgenstein Initiative, Wien.
                    
               
                  Lloyd, 
                  Stuart P. (1982): „Least squares quantization in pcm“, in: 
                        IEEE transactions on information theory, 28(2):129–137.
                    
               
                  MacQueen, 
                  J. B. (1967): „Some methods for classification and analysis of multivariate observations.“, in: 
                        Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, volume 1, pages 281–297. Oakland, CA, USA, 1967
                    
               
                  Pichler, Alois / Krüger, Heinz W. / Smith, D. / Bruvik, Tone / Lindebjerg, Anne / Olstad, Vemund (Hrsg.) (2009): Wittgenstein Source Bergen Facsimile (BTE). Wittgenstein Source Bergen.
                    
               
                  Redner, 
                  Richard A. /
                   Walker, 
                  Homer F. (1984): Mixture densities, maximum likelihood and the em algorithm. SIAM review, 26(2):195–239.
                    
               
                  Röhrer, 
                  Ines / 
                  Ullrich, 
                  Sabine / 
                  Hadersbeck, 
                  Maximilian (2019): 
                        Weltkulturerbe international digital: Erweiterung der Wittgenstein Advanced Search Tools durch Semantisierung und neuronale maschinelle Übersetzung. multimedial multimodal. Abstracts zur Jahrestagung des Verbandes Digital Humanities im deutschsprachigen Raum, 25. - 29.03.2019 an den Universitäten zu Mainz und Frankfurt.
                    
               
                  Steinhaus, 
                  Hans (1955): Quelques applications des principes topologiques à la géométrie des corps convexes. Fund. Math, 41:284–290.
                    
               
                  Ullrich, Sabine /
                   Bruder, Daniel /
                   Hadersbeck, Maximilian (2018): “Aufdecken von “versteckten” Einflüssen: Teil-Automatisierte Textgenetische Prozesse mit Methoden der Computerlinguistik und des Machine Learning”, 5. Tagung Digital Humanities im deutschsprachigen Raum 26.2.-2.3. (Köln).
                    
               
                  Ullrich, Sabine (2019):
                         Boosting Performance of a Similarity Detection System using State of the Art Clustering Algorithms. Master‘s thesis. LMU.
                    
               
                  Pilch, Martin (2018): 
                        Frontverläufe im Prototractatus – Zur gedanklichen Entwicklung von Krakau bis Sokal (1914/1915), Wittgenstein-Studien 9 (S.101-154), Internationale Ludwig Wittgenstein Gesellschaft (ILWG).
                    
               
                  Still, Sebastian (2018): 
                        Ludwig Wittgenstein: 100 Jahre Traktatus. Der Odyssee-Reader, ein web-basiertes Tool zur text-genetischen Suche im Traktatus, Masterthesis, Ludwig-Maximilians-Universität München.
                    
               
                  Feldweg, Birgit /
                   Feldweg, Helmut (1997): „GermaNet - a Lexical-Semantic Net for German.", in: 
                        Proceedings of the ACL workshop Automatic Information Extraction and Building of Lexical Semantic Resources for NLP Applications. Madrid.
                    
               
                  Henrich, Verena / Hinrichs, Erhard (2010): „GernEdiT - The GermaNet Editing Tool", in: 
                        Proceedings of the Seventh Conference on International Language Resources and Evaluation (LREC 2010). Valletta, Malta, pp. 2228-2235.
                    
               
                  Hörnschemeyer, Jörg / Thaller, Manfred / Förtsch, Reinhard (2017): 
                        Textgenetische Prozesse in Digitalen Editionen, Köln Universitäts- und Stadtbibliothek Köln 2017, https://www.worldcat.org/title/textgenetische-prozesse-in-digitalen-editionen/oclc/1002260195
                    
               
                  Schmidt, 
                  Alfred (2018): „Ludwig Wittgenstein’s Nachlass in the UNESCO Memory of the World register.“, in: 
                        Nordic Wittgenstein Review 7(2):209–213.
                    
               
                  UNESCO (2017): UNESCO-Weltdokumentenerbe - Zwei
  Neuaufnahmen. URL:
  https://www.unesco.at/presse/artikel/article/unesco-weltdokumentenerbe-zwei-neuaufnahmen/
  [letzter Zugriff 19. Juni 2018].

               
                  Ward, John H. (1963): „Hierarchical grouping to optimize an objective function." 
                        Journal of the American statistical association 58.301: 236-244.
                    
            
         
      
   



      
         
            Einleitung
            Das Ziel dieses Tutorials ist es, den Teilnehmenden konkrete und praktische Einblicke in einen Standardfall automatischer Textanalyse zu geben. Am Beispiel der automatischen Erkennung von Entitätenreferenzen gehen wir auf allgemeine Annahmen, Verfahrensweisen und methodische Standards bei maschinellen Lernverfahren ein. Die Teilnehmerinnen und Teilnehmer können beim Bearbeiten von lauffähigem Programmiercode den Entscheidungsraum solcher Verfahren ausleuchten und austesten. Es werden dabei keinerlei Vorkenntnisse zu maschinellem Lernen oder Programmierkenntnisse vorausgesetzt.
            Es gibt keinen Grund, den Ergebnissen von maschinellen Lernverfahren im Allgemeinen und NLP-Tools im Besonderen blind zu vertrauen. Durch die konkreten Einblicke in den "Maschinenraum" von maschinellen Lernverfahren wird den Teilnehmenden ermöglicht, das Potenzial und die Grenzen statistischer Textanalysewerkzeuge realistischer einzuschätzen. Mittelfristig hoffen wir dadurch, den immer wieder auftretenden Frustrationen beim Einsatz automatischer Verfahren für die Textanalyse und deren teilweise wenig zufriedenstellender Ergebnis-Daten zu begegnen, aber auch die Nutzung und Interpretation der Ergebnisse von maschinellen Lernverfahren (d.h. in erster Linie von automatisch erzeugten Annotationen) zu fördern. Zu deren adäquater Nutzung, etwa in hermeneutischen Interpretationsschritten, ist der Einblick in die Funktionsweise der maschinellen Methoden unerlässlich. Insbesondere ist die Art und Herkunft der Trainingsdaten für die Qualität der maschinell produzierten Daten von Bedeutung, wie wir im Tutorial deutlich machen werden.
            Neben einem Python-Programm für die automatische Annotierung von Entitätenreferenzen, mit und an dem während des Tutorials gearbeitet werden wird, stellen wir ein heterogenes, manuell annotiertes Korpus sowie die Routinen zur Evaluation und zum Vergleich von Annotationen zu Verfügung. Das Korpus enthält Entitätenreferenzen, die im "Center for Reflected Text Analytics" (CRETA) annotiert wurden, und deckt Texte verschiedener Disziplinen und Sprachstufen ab.
                
         
         
            
               Entitätenreferenzen
                
            Als empirisches Phänomen befassen wir uns mit dem Konzept der Entität und ihrer Referenz. Das Konzept steht für verschiedene linguistische und semantische Kategorien, die im Rahmen der Digital Humanities von Interesse sind. Es ist bewusst weit gefasst und damit anschlussfähig für verschiedene Forschungsfragen aus den geistes- und sozialwissenschaftlichen Disziplinen. Auf diese Weise können unterschiedliche Perspektiven auf Entitäten berücksichtigt werden. Insgesamt werden in den ausgewählten Texten fünf verschiedene Entitätenklassen betrachtet: PER (Personen/Figuren), LOC (Orte), ORG (Organisationen), EVT (Ereignisse) und WRK (Werke).
            Unter Entitätenreferenzen verstehen wir Ausdrücke, die auf eine Entität in der realen oder fiktiven Welt referieren. Das sind zum einen Eigennamen (Named Entities, z.B. "Peter"), zum anderen Gattungsnamen (z.B. "der Bauer"), sofern diese sich auf eine konkrete Instanz der Gattung beziehen. Dabei wird als Referenzausdruck immer die maximale Nominalphrase (inkl. Artikel, Attribut) annotiert. Pronominale Entitätenreferenzen werden hingegen nicht annotiert.
            In 
                    literarischen Texten sind vor allem Figuren und Räume als grundlegende Kategorien der erzählten Welt von Interesse. Über die Annotation von Figurenreferenzen können u.a. Figurenkonstellationen und -relationen betrachtbar gemacht sowie Fragen zur Figurencharakterisierung oder Handlungsstruktur angeschlossen werden. Spätestens seit dem 
                    spatial turn rückt auch der Raum als relevante Entität der erzählten Welt in den Fokus. Als "semantischer Raum" (Lotmann, 1972) übernimmt er eine strukturierende Funktion und steht in Wechselwirkung mit Aspekten der Figur.
                
            In den 
                    Sozialwissenschaften sind politische Parteien und internationale Organisationen seit jeher zentrale Analyseobjekte der empirischen Sozialforschung. Die Annotation der Entitäten der Klassen ORG, PER und LOC in größeren Textkorpora ermöglicht vielfältige Anschlussuntersuchungen, unter anderem zur Sichtbarkeit oder Bewertung bestimmter Instanzen, beispielsweise der Europäischen Union.
                
         
         
            
               Textkorpus
                
            Die Grundlage für (überwachte) maschinelle Lernverfahren bilden Annotationen. Um die Annotierung von Entitätenreferenzen automatisieren zu können, bedarf es Textdaten, die die Vielfalt des Entitätenkonzepts abdecken. Bei diesem Tutorial werden wir auf Annotationen zurückgreifen, die im Rahmen von CRETA an der Universität Stuttgart entstanden sind (vgl. Blessing et al., 2017; Reiter et al., 2017a). Das Korpus enthält literarische Texte aus zwei Sprachstufen des Deutschen (Neuhochdeutsch und Mittelhochdeutsch) sowie ein sozialwissenschaftliches Teilkorpus.
            
            Der 
                    Parzival
                Wolframs von Eschenbach ist ein arthurischer Gralroman in mittelhochdeutscher Sprache, entstanden zwischen 1200 und 1210. Der 
                    Parzival zeichnet sich u.a. durch sein enormes Figureninventar und seine komplexen genealogischen Strukturen aus, wodurch er für Analysen zu Figurenrelationen von besonderem Interesse ist. Der Text ist in 16 Bücher unterteilt und umfasst knapp 25.0000 Verse.
                
            
               Johann Wolfgang von Goethes 
               Die Leiden des jungen Werthers ist ein Briefroman aus dem Jahr 1774. Unsere Annotationen sind an einer überarbeiteten Fassung von 1787 vorgenommen und umfassen die einleitenden Worte des fiktiven Herausgebers sowie die ersten Briefe von Werther an seinen Freund Wilhelm.
                
            Das 
Plenardebattenkorpus des deutschen Bundestages besteht aus den von Stenografinnen und Stenografen protokollierten Plenardebatten des Bundestages und umfasst 1.226 Sitzungen zwischen 1996 und 2015. Unsere Annotationen beschränken sich auf Auszüge aus insgesamt vier Plenarprotokollen, die inhaltlich Debatten über die Europäische Union behandeln. Hierbei wurde pro Protokoll jeweils die gesamte Rede eines Politikers bzw. einer Politikerin annotiert.
                
         
         
            
               Ablauf
                
            Der Ablauf des Tutorials orientiert sich an sog. 
                    shared tasks aus der Computerlinguistik (s. a. Willand et al., 2019 zu dieser Form in den DH), wobei der Aspekt des Wettbewerbs im Tutorial vor allem spielerischen Charakter hat. Bei einem traditionellen 
                    shared task arbeiten die teilnehmenden Teams, oft auf Basis gleicher Daten, an Lösungen für eine einzelne gestellte Aufgabe. Solch eine definierte Aufgabe kann z.B. 
                    part of speech-tagging sein. Durch eine zeitgleiche Evaluation auf demselben Goldstandard können die entwickelten Systeme direkt verglichen werden. In unserem Tutorial setzen wir dieses Konzept live und vor Ort um.
                
            Zunächst diskutieren wir kurz die zugrundeliegenden Texte und deren Annotierung. Annotationsrichtlinien werden den Teilnehmerinnen und Teilnehmern im Vorfeld zur Verfügung gestellt. Im Rahmen der Einführung wird auch auf die konkrete Organisation der Annotationsarbeit eingegangen, so dass das Tutorial als Blaupause für zukünftige Tätigkeiten der Teilnehmenden in diesem und ähnlichen Arbeitsfeldern dienen kann.
            Die Teilnehmerinnen und Teilnehmer versuchen selbständig und unabhängig voneinander, eine Kombination aus maschinellen Lernverfahren, Merkmalsmenge und Parametersetzungen zu finden, die auf einem neuen, vom automatischen Lernverfahren ungesehenen Datensatz zu den Ergebnissen führt, die dem Goldstandard der manuellen Annotation am Ähnlichsten sind. Das bedeutet konkret, dass der Einfluss von berücksichtigten Features (z.B. Groß- und Kleinschreibung oder Wortlänge) auf die Erkennung von Entitätenreferenzen empirisch getestet werden kann. Dabei sind Intuitionen über die Daten und das annotierte Phänomen hilfreich, da simplem Durchprobieren aller möglichen Kombinationen („brute force“) zeitlich Grenzen gesetzt sind.Zusätzlich werden bei jedem Testlauf Informationen über die Entscheidungen protokolliert, um die Erklärbarkeit der Ergebnisse zu unterstützen. 
            Wir verzichten bewusst auf eine graphische Benutzerschnittstelle (vgl. Reiter et al., 2017b) – stattdessen editieren die Teilnehmerinnen und Teilnehmer das (Python)-Programm direkt, nach einer Einführung und unter Anleitung. Vorkenntnisse in Python sind dabei nicht nötig: Das von uns zur Verfügung gestellte Programm ist so aufgebaut, dass auch Python-Neulinge relativ schnell die zu bearbeitenden Teile davon verstehen und damit experimentieren können. Wer bereits Erfahrung im Python-Programmieren hat, kann fortgeschrittene Funktionalitäten des Programms verwenden.
            Wie am Ende jedes maschinellen Lernprozesses wird auch bei uns abschließend eine Evaluation der automatisch generierten Annotationen durchgeführt. Hierfür werden den Teilnehmerinnen und Teilnehmern nach Ablauf einer begrenzten Zeit des Experimentierens und Testens (etwa 60 Minuten) die finalen, vorher unbekannten Testdaten zur Verfügung gestellt. Auf diese Daten werden die erstellten Modelle angewendet, um automatisch Annotationen zu erzeugen. Diese wiederum werden dann mit dem Goldstandard verglichen, wobei die verschiedenen Entitätenklassen sowie Teilkorpora getrennt evaluiert werden. Auch das Programm zur Evaluation stellen wir bereit.
         
         
            
               Lernziele
                
            Am hier verwendeten Beispiel der automatischen Annotation von Entitätenreferenzen demonstrieren wir, welche Schritte für die Automatisierung einer Textanalyseaufgabe mittels maschinellen Lernverfahren nötig sind und wie diese konkret implementiert werden können. Die Teilnehmenden des Workshops bekommen einen zusammenhängenden Überblick von der manuellen Annotation ausgewählter Texte über die Feinjustierung der Lernverfahren bis zur Evaluation der Ergebnisse. Die vorgestellte Vorgehensweise für den gesamten Ablauf ist grundsätzlich auf ähnliche Projekte übertragbar.
            Das Tutorial schärft dabei das Verständnis für den Zusammenhang zwischen untersuchtem Konzept und den dafür relevanten Features, die in ein statistisches Lernverfahren einfließen. Durch Einblick in die technische Umsetzung bekommen die Teilnehmerinnen und Teilnehmer ein Verständnis für die Grenzen und Möglichkeiten der Automatisierung, das sie dazu befähigt, zum einen das Potenzial solcher Verfahren für eigene Vorhaben realistisch(er) einschätzen zu können, zum anderen aber auch Ergebnisse, die auf Basis solcher Verfahren erzielt wurden, angemessen hinterfragen und deuten zu können. 
         
         
            Abgrenzung zur Einreichung 
                    „Vom Phänomen zur Analyse – ein CRETA-Workshop zur reflektierten Operationalisierung in den DH“
                
            Neben diesem CRETA-Hackatorial befindet sich noch ein weiterer Workshop des Stuttgarter DH-Zentrums CRETA in Begutachtung. Auch wenn es eine gewisse Schnittmenge zwischen den Workshops gibt (Textgrundlagen, Anwendungsfälle), ist die jeweilige Zielsetzung grundsätzlich verschieden: Während es beim hier vorgestellten CRETA-Hackatorial um Verfahren des Maschinellen Lernens geht, konzentriert sich der parallel ausgearbeitete CRETA-Workshop auf den grundlegenderen Schritt der Operationalisierung – es geht also darum, Ansätze aufzuzeigen, wie ein Untersuchungsvorhaben oder theoretisches Konzept überhaupt für die computergestützte Analyse “vor- bzw. aufbereitet” werden kann. Beide Workshops ergänzen einander sinnvoll, was die Teilnahme an beiden oder an nur einem der Workshops möglich macht.
         
         
            Anhang 
            
               Zeitplan (Dauer in Minuten, ca.)
               Im Vorfeld der Veranstaltung: Installationsanweisungen und Support
               
                  (10) Lecture
    
                        Intro & Ablauf
                     
                  
                  (15) Hands-On
    
                        Test der Installation bei allen
                     
                  
                  (50) Lecture
    
                        Einführung in Korpus und Annotationen
                        Grundlagen maschinellen Lernens
                        Überblick über das Skript (where can you edit what?)
      
                              Grundlagen Python Syntax
                              Bereitgestellte Features
                           
                        
                     
                  
                  (15) Hands-On
    
                        Erste Schritte
                     
                  
                  (30) Kaffeepause
                  (60) Hands-On
    
                        Hack
                     
                  
                  (30) Evaluation
               
            
            
               
            
            
               Beitragende (Kontaktdaten und Forschungsinteressen)
               Der Workshop wird ausgerichtet von Mitarbeitenden des "Center for Reflected Text Analytics" (CRETA) an der Universität Stuttgart. CRETA verbindet Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung. Hauptaufgabe von CRETA ist die Entwicklung reflektierter Methoden zur Textanalyse, wobei wir Methoden als Gesamtpaket aus konzeptuellem Rahmen, Annahmen, technischer Implementierung und Interpretationsanleitung verstehen. Methoden sollen also keine "black box" sein, sondern auch für Nicht-Technikerinnen und -Techniker so transparent sein, dass ihr reflektierter Einsatz im Hinblick auf geistes- und sozialwissenschaftliche Fragestellungen möglich wird.
  
                 
               
                  Gerhard Kremer
                  gerhard.kremer@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
  Pfaffenwaldring 5b
  70569 Stuttgart
  
               Der Interessenschwerpunkt Gerhard Kremers ist der reflektierte Einsatz von Werkzeugen der Computerlinguistik für geistes- und sozialwissenschaftliche Fragestellungen. Damit zusammenhängend gehören die Entwicklung übertragbarer Arbeitsmethoden und die angepasste, nutzerfreundliche Bedienbarkeit automatischer linguistischer Analysetools zu seinen Forschungsthemen.
  
                 
               
                  Kerstin Jung
                  kerstin.jung@ims.uni-stuttgart.de
                  Institut für Maschinelle Sprachverarbeitung
    Pfaffenwaldring 5b
    70569 Stuttgart
  
               Kerstin Jungs Forschungsinteressen liegen im Bereich der Nachhaltigkeit von (computer)linguistischen Ressourcen und Abläufen sowie der Verlässlichkeitsbeschreibung von automatisch erzeugten Annotationen. Dabei verfolgt sie einen aufgabenbasierten Ansatz und arbeitet an der Schnittstelle zwischen Computerlinguistik und anderen sprach- und textverarbeitenden Disziplinen.
            
            
               
                  Zahl der möglichen Teilnehmerinnen und Teilnehmer
                    
               Zwischen 15 und 25. 
            
            
               
                  Benötigte technische Ausstattung
                    
               Es wird außer einem Beamer und ausreichend Stromanschlüssen für die Laptops der Teilnehmenden keine besondere technische Ausstattung benötigt. Es sollte sich um einen Raum handeln, in dem genügend Platz ist, durch die Reihen zu gehen und den Teilnehmenden über die Schulter zu blicken.
            
         
      
      
         
            
               www.creta.uni-stuttgart.de
            
             Aus urheberrechtlichen Gründen wird das Tutorial ohne das Teilkorpus zu Adornos ästhetischer Theorie stattfinden, das in den Publikationen erwähnt wird.
             Die Texte wurden im Rahmen des PolMine-Projekts verfügbar gemacht: http://polmine.sowi.uni-due.de/polmine/
         
         
            
               Bibliographie
               
                  Blessing, André / Echelmeyer, Nora / John, Markus / Reiter, Nils (2017): "An end-to-end environment for research question-driven entity extraction and network analysis" in 
Proceedings of the Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, Vancouver.

               
                  Kuhn, Jonas / Reiter, Nils (2015): "A Plea for a Method-Driven Agenda in the Digital Humanities" in: 
  Digital Humanities 2015: Conference Abstracts, Sydney. 

               
                  Lotman, Juri (1972): 
  Die Struktur literarischer Texte, München.

               
                  Reiter, Nils / Blessing, André / Echelmeyer, Nora / Koch, Steffen / Kremer, Gerhard / Murr, Sandra / Overbeck, Maximilian / Pichler, Axel (2017a): "CUTE: CRETA Unshared Task zu Entitätenreferenzen" in 
  Konferenzabstracts DHd2017, Bern.

               
                  Reiter, Nils / Kuhn, Jonas / Willand, Marcus (2017b): "To GUI or not to GUI?" in 
  Proceedings of INFORMATIK 2017, Chemnitz.

               
                  Willand, Marcus / Gius, Evelyn / Reiter, Nils (2019): “Ein neues Format für die Digital Humanities: Shared Tasks. Zur Annotation narrativer Ebenen.” in 
  Abstracts of DHd: multimedial und multimodal, Frankfurt. 

            
         
      
   



      
         
            Einleitung
            Der Workshop adressiert eine der großen Herausforderungen für Arbeiten in den Digital Humanities – die Operationalisierung geisteswissenschaftlicher Konzepte und Fragestellungen für computergestützte Methoden (vgl. Jannidis 2010, 109–132; Moretti 2013; Flanders, Jannidis 2015; Jacke 2014, 118–139). Während Geisteswissenschaftler vor allem mit komplexen, häufig textübergreifenden Phänomenen arbeiten und als relevant erachtete Kontexte der behandelten Themen heranziehen, ist die computergestützte Arbeit an identifizierbare Phänomene auf der Textoberfläche gebunden. Die hieraus erwachsende Diskrepanz zwischen Erwartungen und Ergebnissen gilt es über eine adäquate Operationalisierung, 
                    also eine Messbarmachung theoretischer Konzepte, zu überbrücken. Mit unserem Workshop wollen wir genau diese Schnittstelle in den Fokus rücken. Anhand dreier Anwendungsfälle zeigen wir auf, welche Herausforderungen sich aus dem Einsatz computergestützter Methoden für geisteswissenschaftliche Zwecke ergeben und wie mit ihnen umgegangen werden kann. In einem praktischen Teil haben die Teilnehmenden die Möglichkeit, selbst an der Operationalisierung eines Phänomens zu arbeiten; hierfür stellen wir Anwendungsfälle mit geeigneten Tools und Technik-„Baukästen“ zur Verfügung. Programmierkenntnisse werden dabei nicht vorausgesetzt. Ziel des Workshops ist es, das Bewusstsein für die Differenzen zwischen geisteswissenschaftlicher und computergestützter Arbeitsweise zu schärfen, typische Herausforderungen zu adressieren und Herangehensweisen zur Operationalisierung geisteswissenschaftlicher Phänomene aufzuzeigen. Denn nur durch die reflektierte Auseinandersetzung mit den Operationalisierungsannahmen kann ein angemessener (also reflektierter) Umgang mit den Ergebnissen gewährleistet werden.
                
         
         
            
               Use Cases
                
            Als Anwendungsfälle stellen wir drei unterschiedliche literatur- und sozialwissenschaftliche Phänomene vor, zu denen wir im Rahmen des Stuttgarter „Center for Reflected Text Analytics“ (CRETA)
umfangreiche Erfahrungen gesammelt haben. Die gewählten Beispiele decken verschiedene Aufgabentypen ab: Wir behandeln erstens die Extraktion bestimmter Instanzen aus einem Text, zweitens die Segmentierung eines Textes und drittens ein holistisches Textphänomen.
                
            
               
                  Entitäten und Entitätenreferenzen
                    
               Zum einen befassen wir uns mit dem Konzept der Entität und ihrer Referenz in literatur- und sozialwissenschaftlichen Texten (vgl. Reiter u.a. 2017, 19–22; Blessing u.a. 2017). Als Entitätenreferenzen gelten alle Ausdrücke, die auf eine Entität der realen oder fiktiven Welt referieren. Dazu zählen Personen/Figuren, Orte, Organisationen sowie Ereignisse, so dass das Konzept der Entität bewusst weit gefasst und für verschiedene Forschungsfragen anschlussfähig ist. Auf Entitäten kann auf verschiedene Weise referiert werden, u.a. über Eigen- und Gattungsnamen (z.B. “Angela Merkel”, “die Kanzlerin”). Um Entitäten in einem Text zu extrahieren, müssen folglich die Entitätenreferenzen annotiert und kookkurrente Ausdrücke aufgelöst werden. Die Herausforderungen bestehen vor allem in der Festlegung der Referenzausdrücke (welche Ausdrücke werden berücksichtigt?), in der Abgrenzung von Entitätenreferenzen gegenüber Generika sowie im Umgang mit Verschachtelungen, Metonymien und textspezifischen Besonderheiten. Am Beispiel zweier Textsorten (mhd. Artusroman und Bundestagsdebatten) stellen wir das Phänomen und Möglichkeiten der Umsetzung vor.
            
            
               
                  Erzählebenen
                    
               Des Weiteren beschäftigen wir uns mit der Annotation von Erzählebenen.
Hierbei geht es formal darum, einen Text in sinnvolle Segmente zu zerlegen, die seriell aneinandergereiht oder ineinander verschachtelt sein können. Auch wenn das narratologische Konzept ‘Erzählebene’ recht klar definiert erscheint, wird das Phänomen je nach theoretischer Grundlage unterschiedlich aufgefasst und analysiert (vgl. Genette 1988 [1983]; Ryan 1991). Um eine intersubjektive Annotation von Erzählebenen zu erreichen, gilt es deshalb zunächst, einen gemeinsamen Konsens zu theoretischen Grundannahmen zu finden. Ferner macht es die Operationalisierung von Erzählebenen notwendig, das vage Konzept akkurat zu formalisieren und distinktive Merkmale zu bestimmen, die das Phänomen sinnvoll abgrenzen können.
                    
            
            
               
                  “Wertherness”
                    
               Als dritten Anwendungsfall stellen wir die sog. “Wertherness” vor, womit eine Sammlung von Texteigenschaften gemeint ist, die Texte als “Wertheriaden” identifizieren können. Die Veröffentlichung von Goethes “Die Leiden des jungen Werthers” 1774 zog eine Reihe an literarischen Adaptationen nach sich, die sich durch verschiedene Bezugnahmen auf den Originaltext als sog. Wertheriaden ausweisen. Die Referenzen können dabei sowohl formaler (z.B. Briefroman, Dreiecksbeziehung) als auch inhaltlicher (z.B. Rolle der Natur, Verhältnis Subjekt-Gesellschaft) Art sein. Für eine computergestützte Analyse solcher Referenztexte müssen einerseits die einzelnen formalen und semantischen Kategorien operationalisiert und in den Texten identifiziert werden, andererseits ist zu untersuchen, welche Kriterien in bekannten Wertheriaden in Kombination miteinander auftreten.
            
         
         
            
               Ansätze zur Operationalisierung
                
            Im Workshop stellen wir zwei Ansätze zur Operationalisierung vor, die sich – in verschiedenen Phasen des Forschungsprozesses – sehr gut gegenseitig ergänzen. Der erste Ansatz besteht dabei in der Schärfung von 
                    Konzeptdefinitionen durch Annotationen und richtet sich an Menschen. Die Ergebnisse sind also keine Skripte oder Funktionen, sondern klare(re) Definitionen der fraglichen Konzepte, die von Menschen mit größerer intersubjektiver Übereinstimmung umgesetzt werden können, aber auch die theoretische Diskussion bereichern (vgl. Gius/Jacke, 2017; Pagel et al., 2018; Reiter et al., im Erscheinen). Daneben führt der Annotationsprozess auch zu einer intensiven und kritischen Beschäftigung mit dem Material und den textuellen Instanzen des Konzeptes und liefert damit auch Ideen für eine computergestützte Operationalisierung.
                
            Als zweiten Ansatz stellen wir die Idee vor, Zielphänomene 
                    indirekt zu operationalisieren. Hierbei werden pro Phänomen mehrere messbare Eigenschaften in den Blick genommen, die mit dem Zielkonzept verwandt, aber nicht deckungsgleich sind. Aufschlussreich ist dabei in erster Linie nicht die Inspektion einzelner Eigenschaften, sondern die Gesamtschau der verschiedenen Einflussfaktoren (vgl. “instrumental variables” in Sack, 2011; “indirekte Operationalisierung” in Reiter/Willand, 2018). Bei textbasierten Phänomenen können so insbesondere linguistische und strukturelle Eigenschaften betrachtet werden, die größtenteils mit großer Reliabilität automatisch extrahierbar sind.
                
         
         
            
               Ablauf
                
            In einem Theorieteil führen wir in die Problematik der Operationalisierung von geisteswissenschaftlichen Phänomenen für die computergestützte Analyse ein. Anhand der drei oben genannten Beispiele aus der CRETA-Praxis thematisieren wir die Problematik und stellen die Ansätze der Operationalisierung im Detail vor. Je nach Interesse kann anschließend einer dieser Anwendungsfälle ausgewählt und bearbeitet werden.
            Im praktischen Teil des Workshops haben die Teilnehmenden die Möglichkeit, beide Operationalisierungsansätze an ihrem gewählten Anwendungsfall zu erproben. Hierfür befassen sie sich zunächst mit dem Phänomen, indem sie es anhand eines Textauszugs manuell annotieren und parallel stichpunktartig die Richtlinien schärfen. In einer ersten Diskussionsrunde werden die verschiedenen Ergebnisse gesammelt und diskutiert. Zur Erprobung des zweiten Ansatzes stellen wir für jeden Anwendungsfall einen Operationalisierungs-„Baukasten“ vor. Dieser besteht aus einer Sammlung von Python-Skripten in einem Jupyter-Notebook, die auf das jeweilige Untersuchungsvorhaben zugeschnitten ist und den Teilnehmenden die Möglichkeit gibt, sich dem zu untersuchenden Phänomen über computergestützte Verfahren anzunähern. Die Teilnehmenden können in Kleingruppen in diesem Baukasten verschiedene Parameter einstellen sowie manuell Eigenschaften an- oder abwählen, wobei sie auf ihr Vorwissen über den Untersuchungsgegenstand aus der ersten Praxisrunde zurückgreifen (können). Nachdem die Teilnehmenden die Eigenschaften ausgewählt und ggf. parametrisiert haben, können sie die Ergebnisse visualisieren und mit den Texten abgleichen. Damit erhalten die Teilnehmenden ein direktes Feedback zu den ausgewählten Parametern und können prüfen, ob das Untersuchungsvorhaben mit den festgelegten Einstellungen angemessen umgesetzt wird. Der Baukasten ist zur iterativen Nutzung vorgesehen, so dass der Einfluss verschiedener verwandter Eigenschaften auf die Ausgaben sichtbar wird und die Teilnehmenden sich einer geeigneten technischen Umsetzung sukzessiv annähern können. In einer abschließenden Diskussion werden die Ergebnisse gesammelt und es wird ausgewertet, wie adäquat sich die jeweiligen Zielphänomene mittels der gewählten Annahmen haben abbilden lassen.

         
         
            
               Lernziele
                
            Ziel unseres Workshops ist es, die Teilnehmenden für die
Wichtigkeit der Operationalisierung in den Digital Humanities zu
sensibilisieren und ihnen Lösungsangebote vorzustellen. Durch die
interdisziplinäre Ausrichtung von DH-Arbeiten kommt der
Operationalisierung eine Schlüsselposition zu, indem diese eine Brücke
zwischen geisteswissenschaftlichem Phänomen und computergestützter
Umsetzung schlägt. Mit den gewählten Anwendungsfällen wollen wir den
Teilnehmenden ein “Repertoire” für die Operationalisierung
verschiedener Aufgabentypen mitgeben. Wir zeigen zum einen, dass die
Annotation eines Phänomens als Methode seiner Operationalisierung
dienen kann (vgl. Gius, Jacke 2017, 233–254); zum anderen führen wir
für textbasierte Phänomene eine approximative Operationalisierung ein
(vgl. Reiter/Willand, 2018). Beide Verfahrensweisen sind auf andere
Anwendungsfälle übertragbar. Gleichzeitig möchten wir deutlich machen,
dass es für jedes Untersuchungsvorhaben nicht nur eine, sondern
verschiedene Wege der Operationalisierung gibt. Die Spielräume, die
bei der Operationalisierung geisteswissenschaftlicher Fragestellungen
entstehen, machen es notwendig, Entscheidungen reflektiert zu treffen,
sie offenzulegen und ihren Einfluss auf die Ergebnisse als
Voraussetzung für eine angemessene Interpretation zu bedenken.
               
            
               Abgrenzung zum CRETA-Hackatorial “Maschinelles Lernen lernen”
               Neben diesem Workshop zur Operationalisierung wird noch ein weiterer Workshop des Stuttgarter DH-Zentrums CRETA während der diesjährigen DHd-Konferenz stattfinden (Gerhard Kremer, Kerstin Jung: “Maschinelles Lernen lernen: Ein CRETA-Hackatorial zur reflektierten automatischen Textanalyse”). Auch wenn es eine gewisse Schnittmenge zwischen den Workshops gibt (Textgrundlagen, Anwendungsfälle), ist die jeweilige Zielsetzung grundsätzlich verschieden: Während es beim CRETA-Hackatorial um Verfahren des Maschinellen Lernens geht, konzentriert sich der hier vorgestellte Workshop auf den grundsätzlicheren Schritt der Operationalisierung. Es geht also darum, Ansätze aufzuzeigen, wie ein Untersuchungsvorhaben oder theoretisches Konzept überhaupt für die computergestützte Analyse “vor- bzw. aufbereitet” werden kann. Beide Workshops ergänzen einander sinnvoll, was die Teilnahme an beiden oder an nur einem der Workshops möglich macht.
            
         
         
            
               Anhang
  
            
               
                  Zeitplan
    
               (insgesamt 3 Stunden + 30 Min. Pause)
               
                  Einführung und Ablauf (10 Min.)
                  Theoretischer Teil (insgesamt 40 Min.)
      
                        Erläuterung der Problemstellung 
                        Vorstellung der drei Anwendungsfälle
                     
                  
                  Praktischer Teil
      
                        Einführung in die Primärtexte und Tools, Ausgabe der skizzierten Guidelines (10 Min.)
                        Erste Praxisrunde (Kleingruppen): Manuelle Annotation eines Phänomens, parallele Erweiterung/Überarbeitung der Guidelines, iterativ (30-40 Min.)
	  - Kaffeepause (30 Min.) -
                        
                        Sammeln der Ergebnisse und Diskussion der Herangehensweisen (20 Min.)
                        Zweite Praxisrunde (Kleingruppen): Arbeit am Operationalisierungsbaukasten, Feedback über Ausgabedatei, iterativ (30-40 Min.)
                     
                  
                  Abschlussdiskussion: Sammeln der „Ergebnisse“, Diskussion der Erfahrungen und Lernziele (30 Min.)
               
            
            
               
                  Zahl der möglichen Teilnehmer
    
               Zwischen 15 und 25.
            
            
               
                  Angaben zur technischen Ausstattung
    
               Abgesehen von Beamer und ausreichend Steckdosen ist keine besondere technische Ausstattung erforderlich. Die Teilnehmenden arbeiten im praktischen Teil an ihrem eigenen PC. Informationen zu eventuellen Vorab-Installationen werden rechtzeitig mitgeteilt.
            
            
               
                  Beitragende
  
               Der Workshop wird von Mitarbeitenden des “Center for Reflected Text Analytics” (CRETA) der Universität Stuttgart veranstaltet, die bereits erfahrene Workshop-Leiter/-innen im DH-Bereich sind (DHd 2017, DH 2017, DHd 2018, ESU 2018, DHd 2019, HCH 2019). 
               Das BMBF-geförderte eHumanities-Zentrum CRETA ist auf die interdisziplinäre Zusammenarbeit von Literaturwissenschaft, Linguistik, Philosophie und Sozialwissenschaft mit Maschineller Sprachverarbeitung und Visualisierung ausgerichtet. Die übergreifende Zielsetzung besteht in der Erarbeitung systematischer und transparenter Workflows, in denen die Entwicklung komputationeller Modelle und Methoden kritisch reflektiert und adäquat auf die unterschiedlichen geistes- und sozialwissenschaftlichen Forschungsfragen angepasst wird.
                
               
                  Nora Ketschik
                  
                  nora.ketschik@ilw.uni-stuttgart.de
                  Universität Stuttgart
    Institut für Literaturwissenschaft, Abt. für Germ. Mediävistik
    Keplerstraße 17
    70174 Stuttgart
  
               Nora Ketschik ist Promotionsstudentin in der Abteilung für
  Germanistische Mediävistik. Im Rahmen von CRETA führt sie
  Netzwerkanalysen zu ausgewählten mittelhochdeutschen Romanen durch
  und setzt sich dabei kritisch mit der Verwendung computergestützter
  Methoden für literaturwissenschaftliche Analysezwecke
  auseinander.
                
               
                  Benjamin Krautter
                  
                  Benjamin.Krautter@ilw.uni-stuttgart.de
                  Keplerstraße 17
    70174 Stuttgart
  
               Benjamin Krautter ist Promotionsstudent in der Abteilung für
  Neuere Deutsche Literatur II und Mitarbeiter im Projekt QuaDramA -
  Quantitative Drama Analytics. Dort arbeitet er an der
  Operationalisierung Aristotelischer Kategorien für die quantitative
  Dramenanalyse. Er beschäftigt sich zudem mit der Integration
  quantitativer Methoden in literaturwissenschaftliche Fragestellungen
  (scalable reading).
  
                
               
                  Sandra Murr
                  
                  sandra.murr@ts.uni-stuttgart.de
                  Universität Stuttgart
  Institut für Literaturwissenschaft, Abt. für Neuere Deutsche Literatur I 
  Keplerstraße 17
  70174 Stuttgart
  
               Sandra Murr ist Promotionsstudentin in der Abteilung für Neuere Deutsche Literatur I. In CRETA arbeitet sie an der digitalen Analyse des “Wertheriaden-Korpus”, Texte, die in der Folge von Goethes “Werther” seit 1774 erschienen sind. Mittels computergestützter Verfahren wird sich mit der Frage auseinandergesetzt, anhand welcher charakteristischer Kriterien eine “Wertheriade” als solche definiert wird und wie sich entsprechende strukturelle und inhaltliche Kriterien operationalisieren, in den Texten automatisch identifizieren und reflektiert vergleichen lassen.
                  
               
                  Janis Pagel
                  
                  janis.pagel@ims.uni-stuttgart.de
                  Universität Stuttgart
  Institut für Maschinelle Sprachverarbeitung
  Pfaffenwaldring 5b
  70569 Stuttgart
  
               Janis Pagel ist Promotionsstudent am Institut für Maschinelle Sprachverarbeitung und Mitarbeiter im QuaDramA-Projekt. Er forscht zu Anwendungen von computerlinguistischen Methoden auf literaturwissenschaftliche Fragestellungen und innerhalb von CRETA hauptsächlich zu Koreferenzresolution für literarische Texte.
                
               
                  Nils Reiter
                  
                  nils.reiter@uni-koeln.de
                  Institut für Digital Humanities
    Universität zu Köln
    Albertus-Magnus-Platz
    50931 Köln
  
               Nils Reiter hat Computerlinguistik/Informatik an der Universität des Saarlandes studiert, wurde 2013 an der Uni Heidelberg promoviert und ist seit 2014 Post-Doc am Institut für Maschinelle Sprachverarbeitung. Seit seiner Promotion ist er im Bereich Digital Humanities unterwegs, mit einem besonderen Interesse an Fragen der Operationalisierung, und zwar sowohl im Hinblick auf Automatisierung wie auch auf manuelle Annotation. Er arbeitet dabei auch an praktischen Fragen der Kooperation zwischen Geistes- und Computerwissenschaftler*innen, und organisiert einen shared task zur Erkennung von Erzählebenen. Derzeit ist er Vertretungsprofessor für Sprachliche Informationsverarbeitung/Digital Humanities an der Universität zu Köln.
            
         
      
      
         
            
               www.creta.uni-stuttgart.de
            
            
    Bei der Umsetzung des Konzepts wurde auf Vorarbeiten des Shared Tasks “SANTA” (Systematic Analysis of Narrative Texts through Annotation) zurückgegriffen, 
    . Das Material ist veröffentlicht in Reiter u.a. (2019).
  
            
               
            
         
         
            
               Bibliographie
               
                  Blessing, André / Echelmeyer, Nora / John, Markus / Reiter, Nils (2017): „An end-to-end environment for research question-driven entity extraction and network analysis“ in 
  Proceedings of the Joint SIGHUM Workshop on
  Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature, Vancouver.

               
                   Julia Flanders / Fotis Jannidis (2015):
  Knowledge Organization and Data Modeling in the Humanities, urn:nbn:de:bvb:20-opus-111270.

               
                   Gérard Genette (1988 [1983]: Narrative Discourse Revisited. (Translated by Jane E. Lewin), Ithaca. 
               
                   Marie-Laure Ryan (1991): Possible Worlds, Artificial Intelligence and Narrative Theory, Bloomington, Indianapolis.
               
                   Evelyn Gius / Janina Jacke (2017): The Hermeneutic Profit of Annotation. On Preventing and Fostering Disagreement in Literary Analysis, in: International Journal of Humanities and Arts Computing 11, S. 233–254.
               
                   Janina Jacke (2014): Is There a Context-Free Way of Understanding Texts? The Case of Structuralist Narratology, in: Journal of Literary Theory 8, S. 118–39.
               
                   Fotis Jannids (2010): Methoden der computergestützten Textanalyse, in: Methoden der literatur- und kulturwissenschaftlichen Textanalyse. Ansätze – Grundlagen – Modellanalysen, hg. v. Vera Nünning, Ansgar Nünning und Irina Bauder-Begerow, Stuttgart, Weimar, S. 109–132.
               
                   Franco Moretti (2013): “Operationalizing”: or, the function of measurement in modern literary theory, in: Literary Lab 6, S. 1–13.
               
                   Janis Pagel / Nils Reiter / Ina Rösiger / Sarah
Schulz (2018): A Unified Text Annotation Workflow for Diverse Goals, in: Proceedings of the Workshop for Annotation in Digital Humanities (annDH), hg. v. Sandra Kübler und Heike Zinsmeister, Sofia, Bulgaria, August 2018, S. 31–36.
               
                   Nils Reiter / Evelyn Gius / Marcus Willand
(Hrsg.) (2019): A Shared Task for the Digital Humanities. Special issue of Cultural Analytics. November 2019.
               
                   Nils Reiter / Marcus Willand (2018): Poetologischer Anspruch und dramatische Wirklichkeit: Indirekte Operationalisierung in der digitalen Dramenanalyse, in: Quantitative Ansätze in den Literatur- und Geisteswissenschaften: Systematische und historische Perspektiven, hg. v. Toni Bernhart, Marcus Willand, Sandra Richter und Andrea Albrecht, Stuttgart, S. 45–76.
               
                   Nils Reiter /   André Blessing / Nora Echelmeyer /
Gerhard Kremer / Steffen Koch / Sandra Murr / Maximilian Overbeck /
Axel Pichler (2017): CUTE: CRETA Unshared Task zu Entitätenreferenzen, in: DHd 2017 Bern, Conference Abstracts, S. 19–22.
               
                   Graham Alexander Sack (2011): Simulating Plot: Towards a Generative Model of Narrative Structure, in: Papers from the AAAI Fall Symposium (FS-11-03).
            
         
      
   



      
         
            Einleitung
            In diesem Beitrag stellen wir eine Methode vor, um Informationen über Figurenrelationen in dramatischen Texten, die innerhalb der 
dramatis personae (Figurenverzeichnis) sprachlich kodiert sind, zu extrahieren und maschinenlesbar im TEI/XML vorzuhalten. Das Figurenverzeichnis kann als Paratext (Genette 1993) dem Nebentext zugerechnet werden, ist jedoch literaturwissenschaftlich, von Einführungswerken abgesehen, noch so gut wie nicht erschlossen. Das Figurenverzeichnis steht zwar unabhängig vom eigentlichen Text am Anfang, kann jedoch bereits Figuren- bzw. Textwissen vermitteln, indem die Figuren nach sozial-politischem Stand, Familienzugehörigkeit oder nach anderen Gruppierungen geordnet sind (vgl. Abbildung 1). Häufig lässt sich an der Positionierung eines Names im Figurenverzeichnis auch die Wichtigkeit der betreffenden Figur im Drama ablesen (Pangallo 2015, 91). Durch diese Strukturierung ist es teilweise möglich, schon vorab auf zentrale Konfliktpotentiale des Textes zu schließen (Jeßing 2015, 79–80). Darüberhinaus kann das Figurenverzeichnis laut Pfister und Asmuth auch der Ort erster auktorialer Bewertungen oder Hinweise sein und dient somit nicht nur der reinen Vorstellung der Figuren und ihrer Strukturen untereinander (Pfister 2001, 95; Asmuth 2016, 85).

            
               
                  
                   Abbildung 1: Figurenverzeichnis in Die Räuber (Friedrich Schiller, 1781)
               
            
            Das Verfahren – und dessen Implementierung in einem Python-Skript – ist auch für in Zukunft digitalisierte Dramen anwendbar, und wird von uns als quelloffene Software zur Verfügung gestellt. Es ist vergleichsweise einfach auf neue Sprachstufen oder Genres anpassbar und liefert – auch bei nicht-perfekten Ergebnissen – eine gute Vorlage. Eine Evaluation des Verfahrens erfolgt auf ungesehenen Testdaten. Außerdem veröffentlichen wir einen Datensatz mit extrahierten Figurenrelationen aus deutschsprachigen Dramen, die manuell validiert und korrigiert wurden. Diese Daten werden zur einfachen und breiten Nutzung im TEI-Format in das GerDraCor eingespeist. Schlussendlich beschreiben wir beispielhaft zwei Analyseszenarien in denen die Daten neue Einblicke bieten (können).

         
         
            Automatische Extraktion von Figurenrelationen
            Unsere Methode unterscheidet zwischen sieben Kategorien von Figurenrelationen (Tabelle 1). Ausschlaggebend für die Zuordnung zu einer der Kategorien sind Signalwörter wie “Vater”, “Kammerdiener”, “Geschwister” etc. Diese Signalwörter werden in einer kontextfreien Grammatik der entsprechenden Kategorie zugeordnet.
            
                Tabelle 1: Figurenrelationen
               
                  Relationen Label
                  gerichtet/ungerichtet
                  Beschreibung
               
               
                  parent_of
                  directed
                  Eine Figur ist Elternteil einer anderen
               
               
                  lover_of
                  directed
                  Liebesbeziehungen (unverheiratet)
               
               
                  related_with
                  directed
                  Familienbeziehungen (außer Eheleute)
               
               
                  associated_with
                  directed
                  Figuren, die miteinander anderweitig verbunden sind (z.B. Diener, Kindermädchen etc.)
               
               
                  siblings
                  undirected
                  Figuren, die mindestens ein gemeinsames Elternteil haben
               
               
                  spouses
                  undirected
                  verheiratete oder verlobte Figuren
               
               
                  friends
                  undirected
                  Freundschaftsbeziehungen
               
            
            Kontextfreie Grammatiken bezeichnen in der Informatik eine Sammlung aller syntaktisch korrekten Programme einer Programmiersprache (Böckenhauer und Hromkovič 2013, 177). Die formalisierte Art, in der die Grammatik alle Regeln einer Programmiersprache enthält, erlaubt es, automatisierte Syntaxanalysen von Programmen durchzuführen (Böckenhauer und Hromkovič 2013, 177). Die Regeln werden mit Hilfe zweier Alphabete beschrieben: Das Terminalalphabet enthält alle Wörter einer Sprache, wohingegen das Nichtterminalalphabet Variablen enthält, die vorgeben, auf welche Art und Weise die Wörter kombiniert werden können (Böckenhauer und Hromkovič 2013, 178).
            Wir nutzen eine solche Grammatik, um drei verschiedene Zeilenarten im Figurenverzeichnis zu unterscheiden, bei denen es sich um Nichtterminale handelt. Alle in den Sätzen vorkommenden Tokens sind Terminale, deren Kombination und Anzahl Aufschluss darüber gibt, um was für eine Art von Zeile es sich jeweils handelt. Auf diese Weise können auch zeilenübergreifende Relationen erkannt werden.
            Zu Beginn des Programmablaufs werden die in GerDraCor vorhandenen Figuren-IDs zusammen mit dem Figurenverzeichnis ausgelesen und gespeichert. Da wir die Beziehungen zwischen den Figuren ausschließlich anhand der Angaben im Figurenverzeichnis konstruieren, muss der Dramentext nicht extra eingelesen werden. Daraus ergibt sich die Beschränkung, dass jegliche Beziehungen, die nicht im Figureverzeichnis explizit gemacht werden, vom Programm auch nicht erkannt werden können. Es geht demnach ausschließlich darum, das Personenverzeichnis maschinenlesbar und -interpretierbar zu machen. So ignoriert das Programm beispielsweise auch alle Zeilen, die eine Gruppe von Figuren als Kollektiv einführt, da diese als “Nummern oder als anonyme Angehörige von Untergruppen” (Schlaffer 1972, 11) meistens keine eigenen Namen haben und auch keine explizit gemachten Beziehungen.
            
            Anschließend werden alle Tokens jeder Zeile des Figurenverzeichnisses daraufhin untersucht, ob es sich dabei um Figurennennungen oder Signalwörter handelt und die Grammatik einem Parser übergeben, der die Zeilen des Figurenverzeichnisses in Baumstrukturen überführt (Abbildung 2).
            
               
                  
                  Abbildung 2: Zwei reduzierte Baumstrukturen für Figuren aus Nathan der Weise. 
               
            
            Aus den erstellten Baumstrukturen werden einzelne Informationen ausgelesen, die grundlegend für die Erkennung der Figurenrelationen sind. Zuerst wird überprüft, wie viele IDs sich in einer Zeile befinden. Die erste oder einzige wird zur Erstellung späterer Relationen abgespeichert. Befindet sich in einer Zeile zusätzlich zu einer ID noch ein Signalwort für eine Figurenrelation, bezieht sich die Zeile in der Regel auf die vorangegangene, wie beispielsweise in 
                    Nathan der Weise:
                
            Sultan Saladin.
            Sittah, seine Schwester.
            Die zweite Zeile enthält neben dem Namen noch das Signalwort “Schwester”, das auf die Beziehungsart siblings hinweist, eine ungerichtete Relation. Da keine zweite Figurenbezeichnung in der Zeile vorkommt, entnimmt das Programm als zweiten Part für die Geschwisterbeziehung den Namen bzw. die daraus abgeleitete ID saladin aus der vorherigen Zeile:
            
            Wenn die beiden benötigten IDs für das Erstellen der Figurenrelation feststehen, wird die Art der Relation durch das Auslesen des Signalworts aus der Baumstruktur festgestellt. Danach werden daraus die Zeilen mit den Figurenrelationen erstellt und diese anschließend in die jeweilige TEI-Version des Textes geschrieben.
            Befindet sich in einer Zeile eine zweite Figuren-ID, bezieht sich die Zeile nicht auf eine vorangegangene, sondern stellt selbst den zweite Bezugspunkt der Relation. Das ist beispielsweise bei der Figur “Camillo Rota” in 
                    Emilia Galotti der Fall:
                
            Camillo Rota, einer von des Prinzen Räten.
            Die erste erkannte ID ist camillo_rota, die zweite der_prinz, abgeleitet aus “des Prinzen”. Die IDs werden in gerichtete Relationen mit aktivem und passivem Part überführt:
            
            Das Programm arbeitet dabei ausschließlich mit den IDs. Dafür ist es nicht nötig, dass Figurennamen explizit als Namen oder Adelstitel als Titel erkannt werden. Es geht ausschließlich darum aus den einzelnen Wörtern einer Zeile im Figurenverzeichnis Namen bzw. Namensteile und Titelangaben herauszufiltern, die den IDs entsprechen, um die Zeilen einer oder mehreren Figuren zuordnen zu können.
            Um auch IDs zu erkennen, die sich geringfügig von den Namensnennungen im Figurenverzeichnis unterscheiden, überprüft das Programm pro Wort eine Reihe an Varianten. So trennt es beispielsweise vom oben gennannten Wort “Prinzen” das Suffix ab und überprüft, ob ein Artikel Teil der ID ist. So kann “des Prinzen” der ID “der_prinz” zugeordnet werden. In manchen Fällen funktioniert diese Abwandlung aber nicht so reibungslos. In 
                    Der Eheteufel auf Reisen wird eine Figur im Figurenverzeichnis mit dem Namen “Gustel” eingeführt, wohingegen die ID „gustchen“ lautet. Die ID orientiert sich hier an der Namensform, die im Stück tatsächlich verwendet wird und nicht an der Bezeichnung im Figurenverzeichnis. Das führt dazu, dass das Programm die ID “gustchen” nicht dem Wort “Gustel” zuordnen kann, da sie sich zu stark unterscheiden.
                
         
         
            Evaluation
            Um die Methode zu evaluieren, wurden die automatisch erzeugten Relationen manuell nachkorrigiert und so ein Goldstandart erzeugt. Im Schnitt bearbeiteten die Korrektoren 12 Texte pro Stunde. Beim Abgleich der automatisch erzeugten Ergebnisse mit dem Goldstandart lag der Macro-Average-Recall Wert bei 0,3 (Standardabweichung: 0,3) und der Wert von Macro-Average-Precision bei 0,55 (Standardabweichung: 0,4), was einen Macro-Average-F-Score von 0,49 (Standardabweichung: 0,25) ergibt.
         
         
            Korpus
            GerDraCor ist ein deutsches Dramenkorpus, das nach TEI-P5 Standarts kodiert ist und im Dezember 2019 474 Dramen enthält, die im Zeitraum von 1730 bis 1940 veröffentlicht wurden (Fischer u. a. 2019). Es ist Teil des größeren DraCor (Fischer u. a. 2019), das als 
                    Programmable Corpus darauf ausgelegt ist, durch Community-Anstrengungen korrigiert und verbessert werden zu können (Fischer u. a. 2019, 195). Da auf einem Fork von GerDraCor gearbeitet wurde, können die automatisch erzeugten Figurenrelationen dem Korpus unproblematisch hinzugefügt werden. Zusätzlich wurden die Relationen, wie bereits beschrieben, manuell nachkorrigiert, um eine erhöhte Qualität für die Nachnutzung zu gewährleisten.
                
            Im Rahmen der manuellen Nachkorrektur wurden außerdem interessante Fälle identifiziert. So wird etwa eine Gruppe von Figuren in dem oben abgebildeten Figurenverzeichnis von Schillers 
                    Die Räuber als “Libertiner,
		    nachher Banditen” bezeichnet, wodurch
		    Informationen aus der späteren Handlung des
		    Stückes vorweggenommen werden. Diese Art der
		    Vorwegnahme findet sich außerdem in Stücken von
		    Grabbe
		    (Herzog Theodor von
		    Gothland, Panizza
		    (Das Liebeskonzil) und
		    Uhland
		    (Ludwig der Bayer). In Kaisers 
                    Stadt und Land hingegen wird mit der Zeile “Erster Bergmann, später Michael” keine Entwicklung in der Handlung, sondern eine Veränderung der Sprecherbezeichnung markiert. Vorwegnahmen mit Bezug auf veränderliche Beziehungen zwischen Figuren konnten nicht festgestellt werden.
                
         
         
            Analyseszenarien
            Wir stellen im folgenden zwei Analysen vor, in denen von den automatisch extrahierten Relationen Gebrauch gemacht wird, sowohl eine Einzeltext- als auch eine Korpusanalyse. Diese illustrieren Möglichkeiten, die Relationen in der Textanalyse zu berücksichtigen.
            Im ersten Beispiel betrachten wir Shakespeares 
                    Romeo and Juliet in der derzeit auf dracor.org verfügbaren Fassung. Zunächst können die Relationen visualisiert werden. Abbildung 3 zeigt das Figurennetzwerk nach Kopräsenz auf der linken und das Netzwerk, das sich aus den sozialen Beziehungen ergibt auf der rechten Seite. Zur besseren Lesbarkeit wurde ein geeigneter Layout-Algorithmus angewendet. Dabei ist zunächst interessant, dass die beiden Familien keineswegs unverbunden sind: Über Mercutio (Freund von Romeo) und Paris (Verlobter von Julia) sind beide mit dem Prinzen verbunden.
                
            
               
                  
                  Abbildung 3: Figurennetzwerke nach Kopräsenz (oben) und Relationen (unten). Zur besseren Übersichtlichkeit wurden die Figuren auf feste Positionen gesetzt, die oben und unten gleich sind. Bezeichnungen werden nur gezeigt wenn der Grad groß genug ist (oben) oder sie an einer Beziehung beteiligt sind (unten). 
               
            
            Auch wenn Abbildung 3 eine gewisse Symmetrie suggeriert, ist diese keineswegs gegeben wenn wir die Redeanteile nach Familien aufschlüsseln, wie es aus den Annotationen ebenfalls direkt möglich ist. Abbildung 4 zeigt die aggregierten Redeanteile der Figuren, wobei Figuren, die durch Verwandtschaft oder Arbeitsverhältnis zu einer der Familien gehören, zusammengefasst wurden (mit Ausnahme von Mercutio und Paris, die beide mit dem Prinzen verwandt sind). Es zeigt sich, dass Angehörige der Familie Capulet etwas weniger als doppelt so viele Wörter äußern als Angehörige der Familie Montague.
            
               
                  
                  Abbildung 4:  Redeanteile nach Familie 
               
            
            
               
                  
                  Abbildung 5: Verteilung der Relationen im Gesamtkorpus
               
            
            Betrachtet man das annotierte Gesamtkorpus stellt man fest, dass die Relationen ungleich verteilt sind. Während Ehen/Verlobungen, Elternschaft und sonstige Assoziationen relativ häufig vorkommen, spielen Geliebte, sonstige Verwandtschaften, Freundschaften und Geschwister eine vergleichsweise kleine Rolle.
            
            In Abbildung 6 sehen wir die Anzahl der Relationen bestimmter Typen
ins Verhältnis gesetzt zur Großgattung (Komödie/Tragödie). Dabei
wurden die Angaben auf den Titeln der Dramen übernommen und leicht
vereinheitlicht (z.B. Bürgerliches Trauerspiel → Tragödie oder Zauberlustspiel → Komödie). Dabei ist zu konstatieren, dass Median und erstes Quartil bei 0 für alle Dramen bei 0 liegen: Viele Dramen weisen keine Beziehungsdefinition auf (oder sie konnten nicht automatisch identifiziert werden, siehe Fußnote ). Größere oder signifikante Abweichungen zwischen den Gattungen gibt es nicht, egal welche Relation betrachtet wird. Lediglich die Relation spouses scheint im Figurenverzeichnis von Komödien häufiger genannt zu werden.
            
               
                  
                  Abbildung 6:  Anzahl typisierter Relationen nach Gattung 
               
            
            
               
                  
                  Abbildung 7: Anzahl typisierter Relationen nach Autor. Zur besseren Übersicht wurden nur Autoren berücksichtigt, die mindestens durch fünf Dramen vertreten sind
               
            
            Eine Verteilung der genannten Relationen nach Autor zeichnet jedoch ein anderes Bild (Abbildung 7). Bestimmte Autoren, vor allem Ludwig Anzengruber (1839-1889) und Johann Nestroy (1801-1862), haben klare Tendenzen dazu, mehr Relationen im Figurenverzeichnis zu nennen. Beide verfassen tendenziell Possen und Komödien.
         
         
            Fazit
            Mit den von uns bereitgestellten maschinenlesbaren Informationen ermöglichen wir Analysen dramatischer Figuren, die die als bekannt vorausgesetzten Informationen im Figurenverzeichnis mit berücksichtigen können. Neben den oben skizzierten Analysen können die Informationen auch in inhaltliche Analysen einfließen und etwa die soziale Nähe mit der Bühnennähe korrelieren o.ä.
            Kontextfreie Grammatiken haben sich hier – trotz der bekannten Schwächen im Bezug auf natürliche Sprache – als effizienter Formalismus herausgestellt, um die Figurenverzeichnisse maschinenlesbar zu machen. Wir halten dieses Verfahren für geeignet, um auch in anderen Kontexten mit semi-strukturierten Textdaten zu arbeiten, wo aufgrund der begrenzten Menge ein maschinelles Lernverfahren nur bedingt zum Einsatz kommen kann.
         
      
      
         
            
      Beispielsweise  spielt das Figurenverzeichnis im kürzlich erschienenen
      (Tonger-Erk, Werber, und Baum 2018), aber auch in (Genette 1993)
      quasi keine Rolle.
    
            
               https://dracor.org
            
            
      Für mehr     Informationen vergleiche (Schlaffer 1972, 11)
    
            
               
            
            
      Die konkreten Ergebnisse wurden auf den vollautomatisch erzeugten Relationen erzielt.
    
         
         
            
               Bibliographie
               
                   Asmuth, Bernhard.  (2016). 
  Einführung in die Dramenanalyse. Stuttgart: J.B. Metzler Verlag.

               
                   Böckenhauer, Hans-Joachim /  Juraj Hromkovič  (2013): 
  Formale Sprachen: Endliche Automaten, Grammatiken, lexikalische und syntaktische Analyse. Zürich: Springer.

               
                   Fischer, Frank / Ingo Börner / Mathias Göbel /
  Angelika Hechtl  / Christopher Kittel / Carsten Milling /  Peer
  Trilcke
  (2019):  „Programmable Corpora – Die digitale Literaturwissenschaft zwischen Forschung und Infrastruktur am Beispiel von DraCor“. In 
  Proceedings of DHd. 
  .

               
                   Genette, Gérard  (1993): 
  Palimpseste. Die Literatur auf zweiter Stufe. Frankfurt am Main: Suhrkamp.

               
                   Jeßing, Benedikt (2015): 
  Dramenanalyse. Eine Einführung. Berlin: Erich Schmidt Verlag.

               
                   Pangallo, Matteo (2015):  „‚I will keep and character that name‘: Dramatis Personae Lists in Early Modern Manuscript Plays“. 
  Early Theatre 18 (2): 87–118. .

               
                   Pfister, Manfred (2001): 
  Das Drama. München: Wilhelm Fink.

               
                   Schlaffer, Hannelore (1972):
  Dramenform und Klassenstruktur. Eine Analyse der dramatis persona "Volk". Stuttgart: J.B. Metzler Verlag.

               
                   Tonger-Erk, Lily  /  Nils Werber /  Constanze Baum (Hrsg.) (2018):  „Hauptsache Nebentext. Regiebemerkungen im Drama“. 
  Zeitschrift für Literaturwissenschaft und Linguistik 48 (3).

            
         
      
   



      
         
            Einleitung
            Mit der Einführung des Konzepts des „Distant Reading“ von Moretti (2002) wurde in den digitalen Literaturwissenschaften in den letzten Jahren ein Trend angestoßen, den Einsatz von computergestützten quantitativen Methoden zur Analyse und Visualisierung von sehr großen Mengen von Texten zu explorieren. Während dieses Konzept und der Einsatz digitaler Methoden in den Literaturwissenschaften umstritten ist, sind computergestützte und quantitative Verfahren in den Musikwissenschaften schon länger etabliert und werden meist als statistische Musikwissenschaften bezeichnet (Nettheim, 1997). In Anlehnung an den Distant Reading-Begriff aus den Literaturwissenschaften wurde in den letzten Jahren versucht ähnliche Begriffe für die Musikwissenschaft einzuführen, um die computergestützte quantitative Analyse und Visualisierung von größeren Mengen an Musikstücken zu beschreiben. In der jüngsten Forschung findet man diesbezüglich die Begriffe: 
                    Distant Audition (Abdallah et al., 2017), 
                    Distant Listening (Cook, 2013) aber auch 
                    Distant Hearing (Burghardt, 2018). Diese Begriffe werden in Abgrenzung des jeweiligen Close-Konzepts, also 
                    Close Audition/Listening/Hearing betrachtet, womit die etablierte individuelle Analyse einzelner oder sehr weniger Stücke mittels hermeneutischer und qualitativer Methoden bezeichnet wird. Die genannten Begriffe sind nicht in gleicher Weise etabliert wie der Distant Reading-Begriff. Auch wird mit Distant Reading mittlerweile eine Vielzahl komplexer Methoden wie Sentiment Analysis und Topic Modeling beschrieben. Im Folgenden werden wir jedoch den Begriff Distant Hearing verwenden und bezeichnen damit die computergestützte quantitative Analyse und Visualisierung von mehreren Musikstücken. Wir berichten im vorliegenden Beitrag über den momentanen Stand der Entwicklung des neuen Tools 
                    BeyondTheNote, welches Konzepte des Distant Hearing integriert.
                
         
         
            Tools und Programme in der digitalen Musikwissenschaft
            Unabhängig von der Begriffsverwendung wurden einige Tools und Programme entwickelt, um die computergestützte Analyse im Sinne von Distant Hearing zu unterstützen. Nichtsdestotrotz liegen noch einige Mängel vor, die wir im Folgenden herausarbeiten, um die Entwicklung des neuen Tools 
                    BeyondTheNotes zu motivieren. Bereits in den 1990er Jahren wurde das 
                    Humdrum-Toolkit entwickelt (Huron, 1994; Huron, 2002). Es handelt sich dabei um eine programmiersprachen-unabhängige Sammlung von Kommandozeilen-Tools. Eines der bekanntesten und meistgenutzten Programm-Pakete ist 
                    music21 (Cuthbert & Ariza, 2010). Dies ist eine Python-Bibliothek, die Analyse-Möglichkeiten für Musikstücke bietet, die in digitalen Formaten symbolhaft repräsentierter Musik (z.B. MusicXML) vorliegen. In beiden Fällen sind jedoch fortgeschrittene Programmier- und IT-Kenntnisse notwendig, um die Tools zu verwenden. Speziell für HumDrum findet man aber auch Tools, die versuchen eine grafische Schnittstelle anzubieten, um leichter auf die Funktionen von HumDrum zuzugreifen (Taylor, 1996; Kornstädt, 1996). Die genannten Umsetzungen benötigen jedoch teils aufwendige Installationen und erhebliche Einarbeitungszeit. Wie jedoch Burghardt und Wolff (2014) in ihrem Aufsatz über Humanist-Computer Interaction schreiben, ist eine möglichst einfache Zugänglichkeit und eine geringe Schwelle bezüglich des technischen Vorwissens ein essenzielles Kriterium damit Tools in den Geisteswissenschaften breite Verwendung finden. Ferner wird argumentiert, dass auch Aspekte der Usability und User Experience besonders wichtig sind, um aufwendige Einarbeitungszeiten zu vermeiden. Ein leichter zugängliches Web-Tool ist das 
                    Digital Music Lab VIS (DML-VIS, Abdallah et al., 2017). Das Tool integriert auch Ideen des Konzepts von Distant Hearing und ermöglicht Analysen und Visualisierungen auf vorgefertigten Korpora. Dennoch fehlen einige Analysen wichtiger musikalischer Metriken und es ist auch nicht möglich eigenes Material zu analysieren.
                
            Tools und Programme, die speziell die Bedürfnisse von Geisteswissenschaftlern beachten sind bislang selten. Im Kontext der Digital Humanities findet man aktuell Arbeiten im Kontext von Jazz (Frieler et al., 2018), klassischer Musik (Condit-Schultz et al., 2018) und Volksmusik (Burghardt et al., 2015; Burghardt & Lamm, 2017). Vereinzelt bieten diese auch statistische Analysen an (Burghardt et al., 2015), sind aber insgesamt verstärkt fokussiert auf Retrieval-Aspekte.
         
         
            BeyondTheNotes
            BeyondTheNotes wurde mit dem Ziel entwickelt, die weiter oben genannten Probleme und Mängel bisheriger Software-Pakete aufzugreifen und sich an Bedürfnissen von Musikwissenschaftlern zu orientieren. BeyondTheNotes grenzt sich von bisherigen Tools ab indem die technischen Hürden bezüglich Programmierkenntnissen und aufwendigen Installationsverfahren umgangen werden, da BeyondTheNotes als leicht zugängliches Web-Tool geplant ist, das eine grafische Benutzeroberfläche bietet und in jedem gängigen Browser verwendet werden kann. Um Aspekten der Usability und User Experience gerecht zu werden, integrieren wir Methoden des User Centered Design-Prozesses. Als weitere Abgrenzung zu bisherigen Software-Paketen liegt der Fokus auf Distant Hearing und nicht auf der Einzelanalyse. Im DML-VIS fehlende Funktionen wie der Upload von eigenen Dateien oder die Analyse wichtiger musikalischer Metriken wurden integriert. Zielgruppe des Tools sind Musikwissenschaftler und Studierende mit Interesse an quantitativer computergestützter Musikanalyse.
            
               
               Abbildung 1: Logo von BeyondTheNotes
            
         
         
            Entwicklung
            Für die Entwicklung des Tools wurden Ideen des User Centered Design-Prozesses (UCD) (Vredenburg et al., 2002) integriert. Dabei wird versucht in iterativen Entwicklungszyklen potentielle Nutzer mit Methoden des Usability Engineerings so früh wie möglich in den Entwicklungsprozess einzubeziehen.
            Um den Anforderungen unserer Zielgruppen gerecht zu werden, fand gemäß UCD vor Entwicklungsbeginn eine Anforderungsanalyse statt. Diesbezüglich wurden eine Fokusgruppe mit Studierenden der Musikwissenschaft sowie zwei semi-strukturierte Interviews mit ausgebildeten Musikwissenschaftlern durchgeführt. Dadurch sollte Einblick in die Arbeitsweisen von Musikwissenschaftlern gewonnen und Bedürfnisse an ein computergestütztes Tool identifiziert werden. Die Ergebnisse werden im Folgenden zusammengefasst: 
            Die Teilnehmer unserer Anforderungsanalysen erläuterten, dass es kein festes methodisches Vorgehen bei der Analyse von Musikstücken gibt, jedoch steht im Mittelpunkt stets das Verfassen eines Textes. Für diesen Prozess werden statistische Visualisierungen als nützlich erachtet. Meist wird nur ein Stück oder eine überschaubar große Zahl analysiert. Größere Analysen finden für Genres und Komponisten. Als wichtige Features wurden die Analyse von eigenem Material sowie der Download der Ergebnisse genannt. Interessante Metriken für die Analyse sind aus Sicht unserer Teilnehmer Leittöne, Tonarten, Akkorde, Intervalle, der Tonumfang, Tonhöhen und jegliche Form von Motiven. Die Teilnehmer äußerten selten den konkreten potentiellen Einsatz und Nutzen eines Tools in ihrem Arbeitsworkflow und sehen den meisten Nutzen eines potentiellen Tools eher in der vielseitigen Exploration einer großen Menge an Ergebnisse. Die Ergebnisse der Anforderungsanalyse wurden in greifbare Features übertragen und die Mehrzahl dieser in das Tool eingearbeitet. In der Weiterentwicklung des Tools werden wir den UCD weiter aufgreifen indem z.B. größere Usability-Tests und Redesign-Phasen stattfinden und wir den Einsatz des Tools im konkreten Forschungsworkflow untersuchen. 
            Das Tool wurde in Python mit dem Framework Django implementiert. Für viele musikalische Analysen wurde im Back-End music21 (Cuthbert & Ariza, 2010) eingesetzt. Für die Visualisierung von Notenblätter und Statistiken wurde OpenSheetMusicDisplay, Zingchart und chartist.js genutzt. Andere wichtige Technologien für die Entwicklung schließen PostgreSQL, JavaScript und Jquery ein.
                
         
         
            Funktionen
            Die Funktionen des Tools gliedern sich in zwei Bereiche. Die Analyse von einem einzelnen Stück inklusive seiner Partitur („Individual Analysis“) und die statistische Analyse von einem oder mehreren Werken bezüglich der Verteilungen unterschiedlicher Metriken („Distant Hearing"; Abbildung 2).
            
               
               Abbildung 2: Start-Screen von BeyondTheNotes
            
            Nach Auswahl eines Bereichs kann der Nutzer eine oder mehrerer Dateien für die Analyse hochladen. Es werden alle gängigen Dateiformate symbolhaft repräsentierter Musik akzeptiert z.B. MusicXML, MEI, Midi, ABC usw. Alternativ wird zum Testen der music21-Korpus zur Verfügung gestellt. Es handelt sich dabei um ein freies, überschaubar großes Korpus, das unter anderem Werke von Mozart, Bach und Schubert enthält.
            Über eine Suchfunktion können die hochgeladenen Dateien und das bestehende Korpus gefiltert werden (Abbildung 3).
            
               
               Abbildung 3: Upload und Suche
            
            Wird die Individual Analysis gewählt, wird die Partitur des Stücks angezeigt (Abbildung 4). 
            
               
               Abbildung 4: Anzeige für die Auswahl der „Individual Analysis“
            
            Folgende Analysemöglichkeiten sind hier möglich:
            
               Die Akkordanalyse („Chords“): Hierbei wird die Partitur mit den Akkorden ersetzt und diese in römischen Ziffern oder ihren herkömmlichen Namen angezeigt (Abbildung 5) 
            
            
               
               Abbildung 5: Transformiertes Notenblatt nachdem die Akkordanalyse durchgeführt wurde
            
            
               Die Analyse des Tonumfangs („Ambitus“) (Abbildung 6)
            
            
               
               Abbildung 6: Tonumfang-Analyse (Ambitus)
            
            
               Die Analyse der Tonart: Hierbei werden die vier wahrscheinlichsten Tonarten mit ihren Wahrscheinlichkeitswerten angezeigt (Abbildung 7). Die Kalkulationen basieren auf music21.
            
            
               
               Abbildung 7: Tonart-Analyse
            
            Die Ergebnisse der Akkord- und Tonartanalyse können auch verknüpft werden. Der Nutzer kann eine der ermittelten Tonarten auswählen und je nachdem werden die Akkorde angepasst, wenn römische Ziffern zur Anzeige verwendet werden.
            Für die Distant Hearing-Funktionen muss der Nutzer zunächst die zu analysierenden Gruppen benennen. Es können dann beliebig viele Stücke der Suchleiste einer Gruppe hinzugefügt werden. Nach der Kalkulation der Daten werden fünf Visualisierungsbereiche angezeigt:
            
               Akkordanalyse: Über gepaarte Histogramme werden die Verteilungen der Akkorde in den einzelnen Gruppen dargestellt (Abbildung 8). Neben den Akkordverteilungen werden auch Akkord-Grundton- und Tongeschlechts-Verteilungen der Akkorde angezeigt.
            
            
               
               Abbildung 8: Akkordanalyse – Verteilungen von Akkorden für zwei Gruppen
            
            
               Tonhöhenanalyse: Über gepaarte Histogramme werden die Verteilungen der einzelnen Töne sortiert nach Tonname und Oktave angezeigt.
               Tondaueranalyse: Über gepaarte Histogramme werden die Verteilungen der Tondauern unterteilt in Noten und Pausen angezeigt (Abbildung 9).
            
            
               
               Abbildung 9: Tondaueranalyse – Verteilungen von Tondauern für zwei Gruppen
            
            
               Tonartanalysen: Hier werden über gepaarte Histogramme die Verteilung der Tonarten angezeigt. Auch wird ein Liniengraph angezeigt, der pro Gruppe die Wahrscheinlichkeiten für die einzelnen Tonarten angibt (Abbildung 10).
            
            
               
               Abbildung 10: Tonartanalyse – Liniendiagramm für die Wahrscheinlichkeiten verschiedener Tonarten mehrerer Stücke
            
            
               Tonumfanganalyse: Es wird ein Reichweitendiagramm pro Gruppe angezeigt, welches den Tonumfang pro Stück in Form von horizontalen Balkendiagrammen anzeigt (Abbildung 11). Für die Gesamtgruppe wird die Menge und die Verteilung der genutzten Halbtonschritte auch noch in Form eines Boxplots angezeigt.
            
            
               
               Abbildung 11: Tonumfanganalyse – Reichweitendiagramm für 4 Stücke, die der Gruppe Beethoven hinzugefügt wurden
            
            Alle Graphen sind dabei interaktiv und bieten weiterführende Informationen an, wenn der Mauszeiger über Elemente bewegt wird. Die Diagramme können auch zusammen mit ihren Legenden heruntergeladen werden. Ebenso können die gesammelten Daten zur Weiterverwendung in einem JSON-Format heruntergeladen werden. An zahlreichen Stellen wurden Tutorials und Erklärungen eingebaut, um die Nutzung zu erleichtern.
         
         
            Ausblick
            Die momentane erste Version des Tools ist frei verfügbar und kann über 
                    GitHub heruntergeladen und genutzt werden. Des Weiteren ist ein erster vorläufiger Prototyp auch online verfügbar.
                
            Wir befinden uns am Ende des ersten Entwicklungszyklus und planen momentan die Evaluation des Tools gemäß dem UCD-Prozess. Des Weiteren explorieren wir weiter zusammen mit Musikwissenschaftlern, ob die gelieferten Funktionen den Analyseprozess unterstützen können und wie das Tool konkret in den Forschungsworkflow integriert werden kann. Im gleichen Schritt wollen wir auch erste forschungsrelevante Einsatzbeispiele diskutieren. Als ein Bereich für mögliche Analysen wurde von den Teilnehmern unserer Anforderungsanalyse vor allem der Vergleich von Genres, Komponisten und eigens erstellten Sammlungen bezüglich gängiger musikalischer Metriken genannt (Akkorde, Tonumfang etc.). Als eine komplexere Forschungsidee wurde die Untersuchung von Variationen diskutiert. 
                    La Folia, ein spanisches Motiv aus dem 16. Jahrhundert wurde von zahlreichen Komponisten als Grundlage für Variationen genutzt (Hudson, 1973). Durch die Nutzung eines geeigneten Korpus kann mit BeyondTheNotes untersucht werden, ob die Variationen dieses Motivs sich mehr nach Komponist, Zeitraum oder Ursprungsland unterscheiden. Ebenso wollen wir in den kommenden Iterationen durch die enge Zusammenarbeit mit Musikwissenschaftlern das Konzept und den tatsächlichen Nutzen des Distant Hearing kritisch reflektieren.
                
         
      
      
         
            
      https://opensheetmusicdisplay.org/
    
            
      https://www.zingchart.com/
    
            
      https://gionkunz.github.io/chartist-js/
    
            
      Online verfügbar unter: https://github.com/Maxikilliane/DH_MusicAnalysis (Eine Installationsanleitung findet man im Repository)
    
            
      Online verfügbar unter: https://beyondthenotes.herokuapp.com/ 
    
         
         
            
               Bibliographie
               
                  Abdallah, Samer / Benetos, Emmanouil / Gold, Nicolas / Hargreaves, Steven / Weyde, Tillman / Wolff, Daniel (2017): “The Digital Music Lab: A Big Data Infrastructure for Digital Musicology”, in: 
                        Journal on Computing and Cultural Heritage (JOCCH) 10(1).
                    
               
                  Burghardt, Manuel (2018): “Digital Humanities in
Der Musikwissenschaft – Computergestützte Erschließungsstrategien Und Analyseansätze Für Handschriftliche Liedblätter” in: 
                        Bibliothek Forschung Und Praxis 42(2): 324–32.
                    
               
                  Burghardt, Manuel / Lamm, Lukas (2017): “Entwicklung Eines Music Information Retrieval-Tools Zur Melodic Similarity-Analyse Deutschsprachiger Volkslieder” in: Eibl, Maximilian / Gaedke Martin (eds.): 
                        INFORMATIK 2017. Bonn: Gesellschaft für Informatik 87–99.
                    
               
                  Burghardt, Manuel / Wolff, Christian (2014): “Humanist-Computer Interaction: Herausforderungen für die Digital Humanities aus Perspektive der Medieninformatik” in: 
                        DHd Workshop: Informatik und die Digital Humanities.
               
               
                  Burghardt, Manuel / Lamm, Lukas / Lechler, David / Schneider, Matthias / Semmelmann, Tobias (2015): "MusicXML Analyzer. Ein Analysewerkzeug für die computergestützte Identifikation von Melodie-Patterns" in: 
                        Hildesheimer Evaluierungs- und Retrievalworkshop 2015: 29-42.
                    
               
                  Condit-Schultz, Nathaniel / Ju, Yaolong / Fujinaga, Ichiro (2018): “A Flexible Approach to Automated Harmonic Analysis: Multiple Annotations of Chorales by Bach and Prætorius” in: 
                        19th International Society for Music Information Retrieval Conference 66–73.
                    
               
                  Cook, Nicholas (2013): 
                        Beyond the score: Music as performance. Oxford University Press.
                    
               
                  Cuthbert, Michael Scott / Christopher, Ariza (2010): “Music21: A Toolkit for Computer-Aided Musicology and Symbolic Music Data” in: 
                        11th International Society for Music Information Retrieval Conference (ISMIR 2010) 637–642.
                    
               
                  Frieler, Klaus / Hoger, Frank / Pfleiderer, Martin / Dixon, Simon (2018): “Two Web Applications for Exploring Melodic Patterns in Jazz Solos” in: 
                        19th International Society for Music Information Retrieval Conference 777–83.
                    
               
                  Hudson, Richard (1973): “The Folia Melodies” in: Acta Musicologica 45 (1): 98–119.
                    
               
                  Huron, David (1994): 
                        UNIX Tools for Music Research: The Humdrum Toolkit. Reference manual http://www.humdrum.org/Humdrum/manual07.html [letzter Zugriff 21. September 2019].
                    
               
                  Huron, David. (2002): “Music Information Processing Using the Humdrum Toolkit: Concepts, Examples, and Lessons” in: 
                        Computer Music Journal 26 (2): 11–26. 
                    
               
                  Kornstädt, Andreas (1996): “SCORE-to-Humdrum: A Graphical Environment for Musicological Analysis” in: 
                        Computing in Musicology 10: 105–22.
                    
               
                  Moretti, Franco (2002): “Conjectures on World Literature” in: 
                        New Left Review Jan / Feb: 54–68.
                    
               
                  Nettheim, Nigel (1997): “A Bibliography of Statistical Applications in Musicology” in: 
                        Musicology Australia 20(1): 94–106.
                    
               
                  Taylor, Michael (1996): 
                        Humdrum Graphical User Interface. Belfast, Queen’s University.
                    
               
                  Vredenburg, Karel / Mao, Ji-Ye / Smith, Paul W. / Carey, Tom (2002): “A Survey of User-Centered Design Practice” in: 
                        Proceedings of the SIGCHI Conference on Human Factors in Computing Systems 471–478.
                    
            
         
      
   



      
         
            Einleitung
            Das schriftliche Kulturgut des deutschsprachigen Raums aus dem 16.–18. Jahrhundert wird schon seit Jahrzehnten in den Verzeichnissen der im deutschen Sprachraum erschienenen Drucke (VD) zusammengetragen. Ein signifikanter Anteil der verzeichneten Titel wurde der Forschung bereits durch die Bereitstellung von Volldigitalisaten oder einzelnen Schlüsselseiten leichter zugänglich gemacht. Die Verfügbarmachung von Volltexten ist dagegen noch ein Desiderat der Forschung. Das DFG-Projekt OCR-D nimmt sich seit Oktober 2015 im Rahmen der Koordinierten Förderinitiative zur Weiterentwicklung von Verfahren für die Optical Character Recognition (OCR) dieser Aufgabe an, indem es eine modular aufgebaute Open Source-Software entwickelt, deren Werkzeuge alle für die Texterkennung nötigen Schritte abdecken sollen. Der modulare Ansatz ermöglicht es, die technischen Abläufe und Parameter der Texterkennung stets nachzuvollziehen und maßgeschneiderte Workflows zu definieren, die jeweils optimale Ergebnisse für spezifische Titel aus dem Zeitraum des 16. bis 19. Jahrhunderts liefern. Zudem werden Antworten auf die damit verbundenen konzeptionellen, informationswissenschaftlichen und organisatorischen Fragen gefunden.
            Künftig sollen mithilfe der OCR-D-Software Volltexte generiert werden, die zum einen von Forschenden zur Recherche verwendet werden können. Zum anderen könnten diese zum Ausgangspunkt für Studien im Bereich der Digital Humanities (DH) werden, wobei auch auf diese Texte die textkritische Methode anzuwenden ist. Gerade bei einer automatisierten Weiterverarbeitung der erzeugten Volltexte ist es für Forschende unerlässlich, die Genese der von ihnen verwendeten Daten kritisch zu hinterfragen. Nur so können Eigenheiten der Daten, die Resultat von zuvor genutzten “Spielräumen” sind, von DH-Forschenden erkannt und in ihrem Umgang mit der Datengrundlage berücksichtigt werden. Nicht nur diese interpretatorischen Spielräume sind zu betrachten, sondern auch, welche konkreten Implementierungen den DH die gewünschten “Spielräume“ für die Erkenntnisgenerierung geben. Im Folgenden wird in vier Thesen eine notwendige Begrenzung der Spielräume vorgenommen. Diese Begrenzung ergibt sich aus dem Vergleich mit anderen Projekten und der heute gängigen Praxis. Ziel ist es, den Forderungen der DH nach qualitativ hochwertigen Volltexten gerecht zu werden. 
         
         
            Im Rückblick
            Das Projekt hat sich in den vergangenen vier Jahren mit verschiedenen Themen auf der DHd zur Diskussion gestellt (Boenig et al 2016; Boenig et al 2018; Baierer et al 2019). Zu Beginn standen methodische Fragen, wie die Textqualität erhöht werden kann. Dabei wurden statistische Methoden vorgestellt, die auf Basis eines Vergleichs von mindestens zwei erstellten Textfassungen entwickelt wurden. Im Rahmen des Themas “Kritik der digitalen Vernunft” wurden die DH befragt, wie in den Geisteswissenschaften Ergebnisse ohne Ground Truth und Referenzdaten gewonnen bzw. verifiziert werden. Diesem Desiderat begegnete das Projekt OCR-D mit dem Vorschlag von Transkriptionsrichtlinien für die Erfassung von Ground Truth-Daten und in der Folge mit der Definition von spezifischen Metadaten. Bei dem 2019 veranstalteten Workshop konnten Wissenschaftler und Wissenschaftlerinnen sowie Interessierte Einblicke in den OCR-D-Workflow erhalten. An Beispielen konnten die Möglichkeiten der Software demonstriert und getestet werden. Die Diskussion, Hinweise und Fragen wurden soweit wie möglich in OCR-D umgesetzt. 
                
         
         
            Thesen
            Das Ziel der prototypischen Implementierung des OCR-D-Workflows und damit der Generierung von Forschungsdaten, die sich durch eine erkennbare XML-Strukturierung sowie eine hohe Zeichen- und Textqualität auszeichnen, wird im ersten Quartal 2020 erreicht werden. Dies stellt jedoch nicht das Ende des Weges dar, sondern eher den Beginn der nun folgenden Volltexttransformation. Letztlich besteht die Aufgabe darin, ca. 1 Mio. frühneuzeitliche Titel mit ca. 250 Mio. Seiten, die zum Teil bereits als Bilddigitalisate vorliegen, zu Volltextdigitalisaten zu transformieren.
            1. Die Volltexttransformation der Bestände stellt eine Herausforderung für Bibliotheken und Archive dar. Die vorhandenen institutionellen und interinstitutionellen Vorgehensweisen und Konventionen sind möglichst zentral aufeinander abzustimmen, damit die Aufgabe in absehbarer Zeit gelöst wird.
            
            Es gibt bereits einige Projekte, in denen (Teil-)Bestände und Sammlungen volltextdigitalisiert wurden. Deren Nutzen für die DH wird jedoch v.a. durch zwei Faktoren begrenzt: Zum einen weisen die erstellten Volltexte aufgrund fehlender Standards bzw. Konventionen im Bereich von Text- und Strukturerkennung eine große Bandbreite in der Transkription der Texte und der Benennung von Textstrukturen auf, die deren automatisierte Auswertung und Bearbeitung durch die DH erschweren. Zum anderen gibt es bislang keine zentrale Anlaufstelle, die die Bereitstellung und auch die Erstellung von Volltexten steuert. Dadurch sind die existierenden Volltexte sowohl für die Forschung, als auch für die volltextdigitalisierenden Einrichtungen weniger sichtbar, was die Gefahr aufwändiger und teurer Doppelarbeiten erhöht.
                
            2. Die Volltexttransformation auf Basis von Erkennungssoftware, die neuronale Netze nutzt, setzt Trainingsdaten voraus. Diese fundamental wichtigen Daten sind systematisch aus vorhandenen Ressourcen zu gewinnen und aktiv zu erweitern.
            Mit ihren Förderinitiativen von 2010 und 2013 hat die DFG die Bedeutung der Forschungsdaten und des zugehörigen Managements erkannt. Heute sollten Projekte von Beginn an mit entsprechenden Forschungsdatenmangementplänen aufgesetzt und die entstehenden Daten in den zuvor bereitgestellten Repositorien verwahrt werden. Gerade bei der automatisierten Texterfassung im Rahmen von Editionsprojekten werden in der Regel aber nur die abschließend bearbeiteten und korrigierten Daten veröffentlicht. Eine Nachnutzung dieser Daten ist in vielfacher Hinsicht nur begrenzt möglich. Dabei spielt nicht nur das Format der Daten, sondern auch die Methodik der Datenerfassung eine entscheidende Rolle. Für die Nachnutzung ist eine Transformation dieser Daten nötig, die entweder von den Nutzenden zu leisten ist, oder von den bestandsverwaltenden Einrichtungen angeboten werden könnte. Um eine solche Transformation zu gewährleisten, sind sowohl Richtlinien als auch entsprechende Metadaten zu etablieren, damit vergleichbare und konsistente Daten bereitgestellt werden können.
            
            3. Die Volltexttransformation wird für einen Teil der Dokumente ein Prozess sein, der sich über einen größeren Zeitraum wiederholt.
            Digitale Daten müssen beständig gepflegt und aktualisiert werden. Dies haben auch Bibliotheken als Herausforderung der digitalen Transformation ihrer Bestände erkannt (vgl. Kempf 2015: 277–278). Werden lernende Systeme für die Text- und Strukturerkennung genutzt, können diese in absehbaren Intervallen verbessert werden. Denn die Verbesserung bestehender Algorithmen sowie die Nutzung zusätzlicher oder verbesserter Trainingsdaten führt auch zu besseren Ergebnissen in der Text- und Strukturerkennung, wie sich beispielsweise im GoogleBooks-Projekt zeigt. Diese wiederkehrende Prozessierung muss konzeptionell berücksichtigt werden.
                
            4. Die Volltexttransformation muss in ihrer Qualität von den Nutzenden beurteilbar sein.
            Bibliotheken geben den Nutzenden mit ihrem Bestand und dessen Erschließung ein Qualitätsversprechen. Die Nutzenden können sich auf die vorhandenen Daten verlassen und sie z.B. in Bibliographien verwenden. Das Volltextangebot aus der automatischen Texterkennung kann dagegen häufig nur unpräzise als “schmutzige OCR” bezeichnet werden. Diese pauschale Angabe ermöglicht den DH keine verlässliche Qualitätseinschätzung und führt dazu, dass Volltextbestände oft a priori als minderwertig eingeschätzt werden. Daher besteht die Gefahr, dass projektintern eine erneute Volltextdigitalisierung durchgeführt wird, die nicht immer sinnvoll ist, da die Erkennung teilweise nur durch eine Korrektur verbessert werden könnte. Oder es könnten im umgekehrten Fall auf Grund einer ungenauen bzw. zu groben Einschätzung aufwendige Korrekturen vorgenommen werden. In beiden Fällen werden finanzielle und personelle Ressourcen verschwendet.
                
         
         
            Lösungen und Desiderate des OCR-D-Projekts
            
               Zu 1: Die bisherigen umfassenden Bilddigitalisierungsarbeiten im VD17 wurden über einen Masterplan gesteuert, um die große Anzahl an Titeln effizient, in nachnutzbarer Form verarbeiten zu können und Doppelarbeiten zu vermeiden. Ein ähnliches Vorgehen, bei dem die zu prozessierenden Titel an interessierte Einrichtungen verteilt werden, dürfte auch für die Volltexttransformation der VD zielführend sein. Die Voraussetzungen und Rahmenbedingungen für diese Arbeiten wurden von dem OCR-D-Koordinierungsprojekt um die Jahreswende 2019/2020 durch eine Umfrage mit den VD-Bibliotheken zusammengetragen. OCR-D wird die mehrjährige Projekterfahrung im Austausch mit den verschiedenen Stakeholdern nutzen, um die Nachnutzbarkeit von Daten und Abläufen zu verbessern, sowohl mit technischer Dokumentation und Best Practices, als auch als Katalysator für einen ergebnisorientierten, inklusiven Diskurs zur Etablierung von Standards.
            
               Zu 2: Für die Transkription von Texten gibt es unzählige Richtlinien, die von verschiedenen Fächern, Arbeitskreisen und Forschungsprojekten entsprechend ihrer jeweiligen Anforderungen aufgestellt und wiederum an die spezifischen Erfordernisse bestimmter Transkriptionsprojekte angepasst wurden. Bei diesen Gruppen ist zum einen ein Bewusstsein dafür zu schaffen, ihre Transkriptionen auch mit Blick auf deren Nachnutzbarkeit durch andere Projekte anzufertigen. Zum anderen sind interdisziplinär erarbeitete und gültige Transkriptionsrichtlinien ein großes Desiderat der Forschung. Erste Impulse hierfür könnten große Fördergeber wie bspw. die DFG geben, indem Praxisrichtlinien geschaffen werden, die von Antragstellern zu beachten sind. Das OCR-D-Projekt ist zudem darum bemüht, seine auf Grundlage des DTA-Basisformats erstellten Transkriptionsrichtlinien interdisziplinär zur Nutzung durch weitere Projekte zu kommunizieren.
            
               Zu 3: Modelltraining mit tesstrain und okralact
            Das Projekt ocropy, die Python-Implementierung von Tom Breuels OCRopus-Projekt, brachte neben Werkzeugen für die Text- und Strukturerkennung auch Werkzeuge für das Erstellen von Ground Truth und das Trainieren neuer Modelle mit sich. Mit diesen Werkzeugen und einigen Anpassungen lassen sich auch die auf ocropy basierenden Weiterentwicklungen Calamari und Kraken trainieren. Insbesondere für tesseract, die mit Abstand am meisten genutzte Open Source OCR, gab es bis 2018 kaum Dokumentation oder Tooling für das Training. Daher wurde im Rahmen von OCR-D 
ocrd-train entwickelt, eine Makefile-basierte Lösung zum Trainieren von Tesseracts LSTM-Engine, das inzwischen unter dem Namen 
tesstrain vom Tesseract-Entwicklerteam gepflegt und weiterentwickelt wird. Die Aufrufe zum Training von Texterkennungsmodellen und insbesondere das Inventar an freien Parametern sind allerdings in hohem Maße engine-spezifisch, keineswegs trivial und erfordern zur optimalen Feinadjustierung manuelle Intervention. Daher entwickelt OCR-D seit 2019 das Werkzeug okralact, das über ein komfortables Webinterface und ein skalierbares Backend ein Training aller relevanter Open Source OCR-Engines mit einem einheitlichen Interface ermöglichen wird.
                
            
               Zu 4: Nachkorrektur und Qualitätsanalyse
            Innerhalb des OCR-D-Projektes beschäftigen sich zwei Projekte mit der automatischen, bzw. semi-automatischen Nachkorrektur von OCR-Texten. Das Hauptproblem dabei ist es, historische Schreibweisen und Druckfehler von OCR-Fehlern zu unterscheiden. Für moderne Texte würde eine reine Rechtschreiberkennung genügen, wie sie in jedem Textverarbeitungsprogramm verfügbar ist. Die Projekte kooperieren und haben verschiedene Verfahren entwickelt, basierend auf einem Fehler-Profiler, neuronalen Netzen oder endlichen Automaten. Als trainierbare Algorithmen werden sie, analog zur Struktur- und Texterkennung, mit mehr und besseren Trainingsdaten bessere Ergebnisse liefern. Was "besser" bedeutet ist noch Gegenstand der Forschung. OCR-D bringt sich in die Entwicklung ein und unterstützt tatkräftig Projekte wie dinglehopper (ein Werkzeug zur Fehlervisualisierung). Gerade im Bereich der Ground-Truth-freien Evaluation von Text und der Qualitätsanalyse von Strukturdaten gibt es noch große Lücken im Software-Portfolio, die zu schließen sich OCR-D auch weiterhin befleißigen wird.
                
         
         
            Ausblick
            Ab der ersten Jahreshälfte 2020 werden die entwickelten Software-Komponenten im OCR-D-Workflow verankert sein. Damit tritt diese Software immer mehr aus dem Projektstadium heraus und wird in den produktiven Einsatz überführt. Um kontinuierlich gute Erkennungsergebnisse mit dem aus fast vier Jahrhunderten stammenden Material zu erhalten, sind Optimierungen notwendig. Dabei wird stets darauf abgezielt, Forschungsdaten aus den digitalen Beständen der Bibliotheken zu erzeugen und nicht unstrukturierte Textdaten. So wird die Volltexttransformation in einem umfassenden Maße Grundlagen für datenzentrierte Digital Humanities schaffen.
         
      
      
         
            
      Im Kontext von OCR bezeichnet 
      Ground Truth manuell korrigierte, fehlerfreie Transkriptionen. Diese werden zum einen für das Training von OCR-Engines, zum anderen für die Evaluation der OCR-Ergebnisse benötigt.
    
            
      Der Gedanke folgt der neunten Empfehlung (“Establish an ‘OCR Service Bureau’”) aus dem Report von Smith und Cordell (2018).
    
            
      Vgl. bspw. die folgenden Projekte, die sich auf unterschiedlich große (Teil-)Bestände beziehen: Helmstedter Drucke Online:
      http://www.hab.de/de/home/wissenschaft/forschungsprofil-und-projekte/helmstedter-drucke-online.html;
      Über 14.000 preußische Drucke des 17. Jahrhunderts online verfügbar:
      https://blog.sbb.berlin/ueber-14-000-preussische-drucke-des-17-jahrhunderts-online-verfuegbar/;
      Projekt Digi20
      https://digi20.digitale-sammlungen.de/de/fs1/about/static.html
            
            
      Nachdem im Jahr 2010 der Aufbau von Infrastrukturen für Forschungsdaten von der DFG ausgeschrieben worden war, wurde drei Jahre später das Förderprogramm „Informationsinfrastrukturen für Forschungsdaten“ eingerichtet. Vgl. DFG 2019: 7.
    
            
      Zur aktuellen Situation des Datenmanagements und der Rolle, die Bibliotheken in diesem Bereich einnehmen (könnten), vgl. Neuroth et al 2019.
    
            
      Die Notwendigkeit einheitlicher Richtlinien wird besonders an Projekten wie “Venice Time Machine” deutlich, dessen bereits vorhandenen 8 TB an Daten aufgrund fehlender einheitlicher Richtlinien und Vorgehensweisen bei der Erfassung der Metadaten für die Forschung vermutlich wertlos sind. Vgl. Castelvecchi 2019: 607.
    
            
      Kempf geht davon aus, dass mit OCR-Software nie völlig fehlerfreie Volltexte generiert werden können. Vgl. Kempf 2015: 274.
    
            
      Während die OCR-Ergebnisse im Rahmen des GoogleBooks-Projekts zunächst insgesamt unbefriedigend, für gebrochene Schriften vollkommen unbrauchbar waren, konnten ab dem Jahr 2008 einzelne Frakturtexte in ausreichender Qualität prozessiert werden. In den letzten Jahren konnte die Erkennungsrate noch deutlich gesteigert werden. Vgl. Wikisource: Google Book Search.
    
            
      Bspw. gibt Google die Fehlerquote im Google Books pauschal mit 1,37 % an (vgl. Kempf 2015: 272). Diese für die wissenschaftliche Nutzung hohe Fehlerrate unterscheidet sich, bedingt durch die Vielfalt an Typen und Layouts sowie den großen Publikationszeitraum der digitalisierten Bücher, von Text zu Text deutlich.
    
            
               https://github.com/tesseract-ocr/tesstrain
            
            
               https://github.com/OCR-D/okralact
            
            
               https://github.com/qurator-spk/dinglehopper
            
         
         
            
               Bibliographie
               
                  Baierer, Konstantin / Boenig, Matthias / Hartmann, Volker / Hermann, Elisa / Neudecker, Clemens (2019): 
	„Vom gedruckten Werk zu elektronischem Volltext als Forschungsgrundlage“ (Workshop)
	(, S. 58).
      
               
                  Boenig, Matthias / Würzner, Kay-Michael / Binder, Arne / Springmann, Uwe  (2016): 
	„Über den Mehrwert der Vernetzung von OCR-Verfahren zur Erfassung von Texten des 17. Jahrhunderts.“ 
	Vortrag auf der DHd 2016, 7.12.03.2016 in Leipzig ().
      
               
                  Boenig, Matthias / Federbusch, Maria / Herrmann, Elisa / Neudecker, Clemens / Würzner, Kay-Michael (2018):
	 „Ground Truth: Grundwahrheit oder Ad-Hoc-Lösung? Wo stehen die Digital Humanities?“.
	Vortrag auf der DHd2018, 28.02.2018 in Köln
	().
      
               
                  Castelvecchi, Davide (2019): 
      “Venice ‘Time Machine’ Project Suspended amid Data Row. Disagreements between International Partners Leave Plans to Digitize the Italian City’s History in Limbo” in: 
      Nature 574: 607.
      
               
                  DFG  (2019): 
	„Weiterentwicklung des Förderprogramms ‚Informationsinfrastrukturen für Forschungsdaten‘"
                   [6.3.2019 / 26.9.2019].
      
               
                  Kempf, Klaus (2015): 
      „Data Curation oder (Retro-)Digitalisierung ist mehr als die Produktion von Daten“ in: 
      o-bib 4: 268–278.
      
               
                  Neuroth, Heike / Rothfritz, Laura / Petras, Vivien / Kindling, Maxi  (2019): “Digitales Datenmanagement als neue Aufgabe für wissenschaftliche Bibliotheken” in: Bibliothek. Forschung und Praxis 43: 421–431.
               
                  Smith, David A. / Cordell, Ryan  (2018): 
	“A Research Agenda for Historical and Multilingual Optical Character Recognition”
                   [9.12.2019].
      
               
                  Wikipedia, Die freie Enzyklopädie  (2019): 
	„Google Books“.
	 [16.6.2019 / 25.9.2019].
      
               
                  Wikisource  (2019): 
	“Google Book Search”
                   [22.8.2019 / 26.9.2019].
      
            
         
      
   



      
         Die Web-Applikation 
Schmankerl Time Machine
            
             wurde im Rahmen des Hackathons für offene Kulturdaten, „Coding da Vinci Süd 2019“ (Bergmann, 2019), von einem interdisziplinären Team aus Informatikern, Statistikern und Geisteswissenschaftlern entwickelt (Deck, 2018). Das Projekt basiert auf den digitalisierten Speisekarten Münchner Restaurants, die die 
Monacensia der Stadtbibliothek München für den Hackathon zur Verfügung gestellt hatte. Am Ende der sechswöchigen Sprintphase konnte der Prototyp reüssieren und wurde von der Jury mit dem Preis in der Kategorie „Most Technical“ bedacht (Lehr, 2019). Seitdem lädt die 
Schmankerl Time Machine zu einem lukullischen Streifzug durch die traditionsreiche Münchner Wirtshausgeschichte der vergangenen 150 Jahre ein. Einen ähnlichen Weg schlägt die Plattform „What’s on the Menu?“ ein, die auf dem Speisekartenbestand der 
New York Public Library basiert und die überwiegend US-amerikanische Gastronomie zwischen 1851 und 2008 abbildet. Andere interessante Bestände harren dagegen noch ihrer Digitalisierung aus.
         
         Die Applikation besitzt bereits jetzt ein großes Potenzial für eine breite Öffentlichkeit (Guyton, 2019; Kotteder, 2019) und zeigt damit exemplarisch, wie die 
Digital Humanities über den wissenschaftlichen Raum hinaus zu einer Beschäftigung mit kulturgeschichtlichen Daten anregen können. Das einzureichende Poster möchte die Idee hinter der Applikation, ihre technische Umsetzung und Funktionalität gleichermaßen wie die Nachhaltigkeitsstrategie sowie künftige Entwicklungsmöglichkeiten präsentieren.

         
            Daten und Datenaufbereitung
            375 Speisekarten mit 1.020 Seiten aus den Jahren 1855 bis 2008 wurden inklusive 
  Metadaten durch die 
  Monacensia bereitgestellt. Sie entstammen 144 Münchner Gaststätten, Restaurants, Cafés, Bars, Festzelten und -hallen, die regional größtenteils in den Stadtbezirken Altstadt-Lehel und Ludwigvorstadt-Isarvorstadt zu verorten sind. Aufgrund unterschiedlicher Schriftarten wurde in 
  Transkribus ein komplementärer Ansatz mit 
  Handwritten Text Recognition (HTR) und 
  Optical Character Recognition (OCR) verfolgt. Anschließend erfolgte eine manuelle Fehlerüberprüfung. Zusätzlich wurde ein 
  Tagset entworfen, um die konsistente Annotation von Mengenangaben und Preisen, Gerichten und deren Zusammensetzung zu gewährleisten. Für die kollaborative Projektarbeit, insbesondere die Datenorganisation und -analyse, wurde die Lehr- und Forschungsinfrastruktur 
  Digital Humanities Virtual Laboratory (DHVLab) eingesetzt, die seit 2016 an der IT-Gruppe Geisteswissenschaften der Ludwig-Maximilians-Universität München entwickelt wird (Klinke, 2018: 29–32).
  
         
         
            Technische Umsetzung und Funktionalitäten
            Folgende Frage stand im Vordergrund: Wie kann die enorme Vielfalt der Speisekarten auch von einer technisch wenig versierten Zielgruppe auf möglichst unterschiedliche Art und Weise exploriert werden? Um dies zu erreichen, wurde eine interaktive, responsive Web-Applikation mit der Open-Source-Umgebung 
R und den auf 
R basierenden Paketen 
Shiny und 
Tidyverse entwickelt, die auf Clientseite ergänzt wird durch 
HTML5, 
JavaScript und das 
Frontend-CSS-Framework Bootstrap. Eine Lokalität kann entweder über ein 
Dropdown-Menü oder eine dynamische Karte (basierend auf 
Leaflet und 
LocationIQ) ausgewählt werden (Abbildung 1). Zu jeder Lokalität werden weiterführende Informationen angeboten. Sofern digital vorhanden, wird auf alte Ansichten der Restaurants aus dem Münchner Stadtarchiv verlinkt.

            
               
               Abbildung 1: Auswahl einer Lokalität über eine dynamische Karte. Bildnachweis: 
  Schmankerl Time Machine; lizenziert unter CC BY-SA 4.0.
  
            
            Jede zu einer Lokalität gehörende Speisekarte kann beliebig gezoomt und verschoben werden. Zudem ist jede Annotation, und damit auch jedes Gericht, direkt anwählbar (Abbildung 2). Besonders „exquisite“ Speisen werden algorithmisch über das 0,65-Quantil ausfindig gemacht – und sogar komplette Menüs zusammengestellt; wobei nicht nur die Präferenzen der jeweiligen Nutzerin oder des jeweiligen Nutzers berücksichtigt werden, sondern auch ihr oder sein Budget (Abbildung 3). Ein virtueller Warenkorb unterstützt die Exploration des Fundus weiterhin: Durch die Verknüpfung mit der Rezeptdatenbank des Webportals 
Chefkoch.de können ausgewählte Gerichte nachgekocht werden; Zutatenliste inklusive.

            
               
               Abbildung 2: Speisekarte der Augustiner Gaststätte von 1959. Bildnachweis: 
  Schmankerl Time Machine; lizenziert unter CC BY-SA 4.0.
  
            
            Neben diesem spielerischen Zugang zu den Speisekarten kann die 
Schmankerl Time Machine als Ausgangspunkt für wissenschaftliche und gesellschaftliche Fragestellungen dienen:

            
               In welchem Jahr findet sich erstmals ein bestimmtes Gericht? Wie stellt sich die Preisentwicklung dar?
               Welche Strategien verfolgten die Restaurants, um ihre Kunden zu einer profitablen Speisenauswahl zu animieren?
               Finden sich in der Beschreibung der Gerichte Hinweise auf ein sich veränderndes Ernährungsbewusstsein?
            
            
               
               Abbildung 3: Nutzerpräferenzen-basierte Menüempfehlung. Bildnachweis: 
  Schmankerl Time Machine; lizenziert unter CC BY-SA 4.0.
  
            
            Diese Beispiele zeigen, wie vielfältig sich die Beschäftigung mit den hier erstmals dargebotenen Speisekarten gestalten kann (Roth und Rauchhaus, 2018). Um einen möglichst niederschwelligen Einstieg zu gewährleisten, werden 
Jupyter Notebooks in 
Python 3 zur Verfügung gestellt, die die Daten importieren, bereinigen und exemplarische Statistiken beinhalten. Hierfür werden gängige Bibliotheken im Bereich 
Data Science verwendet (etwa 
pandas, 
NumPy und 
Matplotlib).

         
         
            Nachhaltigkeitskonzept
            Gemäß den 
  FAIR-Prinzipien
  (Findable, Accessible, Interoperable, Re-usable) wurde ein umfassendes Nachhaltigkeitskonzept verfolgt. Der Quelltext der Applikation sowie die Skripte sind auf 
  GitLab verfügbar.
  Die Abbildungen der Speisekarten sowie die im Projekt entstandenen
  Forschungsdaten stehen im Repositorium der
  Ludwig-Maximilians-Universität München
  (Open Data LMU) unter einer offenen Lizenz (CC BY-SA 4.0) dauerhaft und mittels 
  DOI eindeutig referenzierbar zur Nachnutzung bereit. Die Beschreibung des Projekts erfolgt im Metadatenschema 
  DataCite unter Verwendung eines Best-Practice-Guides, der eine standardisierte Anreicherung der Metadaten unterstützt. Dies ermöglicht die Einbindung der Projektdaten in übergeordnete Forschungsdateninfrastrukturen (z. B. 
  GeRDI) und damit ihre leichtere Auffindbarkeit.
  
         
         
            Ausblick
            Bei der 
                    Schmankerl Time Machine handelt es sich um einen fortgeschrittenen Prototyp. Um sein Potenzial sowohl für wissenschaftliche Fragestellungen als auch für eine interessierte Öffentlichkeit zu vergrößern, ist die Integration weiterer Speisekartensammlungen und damit eine wesentliche Erweiterung des Datenbestands vorgesehen. Damit einhergehend wird darauf abgezielt, die Annotation der Karten – unter Einbezug der „Crowd“ – fortzuführen und um weitere Analysekategorien zu erweitern. In Kooperation mit der 
                    Monacensia ist zu diesem Zweck auch ein 
                    Edithaton geplant, bei dem Studierende der Ludwig-Maximilians-Universität München u. a. praktische Kenntnisse im Umgang mit 
                    Transkribus erhalten.
                
            Ein Beispiel, welche Forschungsfragen dadurch eröffnet werden können, stellt das Projekt „Menu Journeys“ dar, das Studierende der 
                    Berkeley School of Information 2015 angestoßen haben. In interaktiven Grafiken wird auf Basis des Speisekartenbestands der 
                    New York Public Library anschaulich dargestellt, wie sich etwa der durchschnittliche Preis eines Gerichts über die Jahrzehnte hinweg und in Relation zur Inflationsrate entwickelt hat. Analysen dieser Art wären auch für die Münchner Gastronomiegeschichte begrüßenswert. Die hier vorgestellte Web-Applikation bietet einen Ausgangspunkt für künftige Entwicklungen in diese Richtung.
                
         
      
      
         
            
      https://dhvlab.gwi.uni-muenchen.de/schmankerltimemachine/ (27.09.2019).
    
            
      http://menus.nypl.org/ (27.09.2019).
    
            
      Größere Bestände mit geographischem Schwerpunkt auf München befinden sich im Münchner Stadtarchiv sowie der Stadtbibliothek München.
    
            
      https://gitlab.com/cds19-team/cds19 (27.09.2019).
    
            
      https://doi.org/10.5282/ubm/data.146 (27.09.2019).
    
            
      Der DataCite-Best-Practice-Guide wurde im Rahmen des Projekts „eHumanities – interdisziplinär“ in Kooperation mit dem Leibniz-Rechenzentrum der Bayerischen Akademie der Wissenschaften entwickelt: https://doi.org/10.5281/zenodo.3559800.
    
            
      Eine Mitarbeit am Projekt „cds19“ ist nach Registrierung möglich: https://transkribus.eu/r/read/projects/ (27.09.2019).
    
            
      http://people.ischool.berkeley.edu/~carlos/menujourneys/ (18.12.2019).

         
         
            
               Bibliographie
               
                   Bergmann, Claudia  (2019): „Kultur-Hackathon Coding da Vinci Süd. Sterbende Jesuiten, visualisierte Theaterdaten und Wirtshausgeschichte zum Nachkochen“, auf: 
      Wikimedia Blog vom 27.05.2019, URL: 
      https://blog.wikimedia.de/2019/05/27/kultur-hackathon-coding-da-vinci-sued-sterbende-jesuiten-visualisierte-theaterdaten-und-wirtshausgeschichte-zum-nachkochen/.
    
               
                   Deck, Klaus-Georg  (2018): „Digital Humanities. Eine Herausforderung an die Informatik und an die Geisteswissenschaften“, in: Huber, Martin / Krämer, Sybille (Hrsg.): 
      Wie Digitalität die Geisteswissenschaften verändert. Neue Forschungsgegenstände und Methoden. Sonderband der Zeitschrift für digitale Geisteswissenschaften 3. DOI: 
      10.17175/sb003_002.
    
               
                   Guyton, Patrick (2019): „Zeitreise durchs kulinarische München“, in: 
      Der Tagesspiegel  vom 26.07.2019, URL: 
      https://www.tagesspiegel.de/gesellschaft/panorama/historische-speisekarten-zeitreise-durchs-kulinarische-muenchen/24843382.html.
    
               
                   Klinke, Harald (2018): „Datenanalyse in der Digitalen Kunstgeschichte. Neue Methoden in Forschung und Lehre und der Einsatz des DHVLab in der Lehre“, in: Ders. (Hrsg.): 
      #DigiCampus. Digitale Forschung und Lehre in den Geisteswissenschaften. München: Universitätsbibliothek der Ludwig-Maximilians-Universität, 19–34, DOI: 
      https://doi.org/10.5282/ubm/epub.42415.
    
               
                   Kotteder, Franz (2019): „Hat’s geschmeckt?“, in: 
      Süddeutsche Zeitung  vom 09.07.2019, URL:
      http://sz.de/1.4516782.
    
               
                   Lehr, Andrea (2019): „Hochkarätig. CDV Süd
      punktet mit Projekten auf hohem Niveau“. 
      Pressemitteilung vom 21.05.2019, URL: 
      https://codingdavinci.de/news/2019/05/21/cdvs-preisverleihung_2.html.
    
               
                   Roth, Tobias / Rauchhaus, Moritz (Hrsg.)  (2018): Wohl bekam’s! In hundert Menus durch die Weltgeschichte. Berlin: Verlag Das Kulturelle Gedächtnis.

            
         
      
   



      
         Kochtraditionen, ob regional oder international, sind eine der herausragendsten Elemente der europäischen Kultur und ein wichtiger Bestandteil der europäischen Identität. Aber die Fragen nach ihrem Ursprung, den Einflüssen und ihrer Entwicklung sind nach wie vor unklar. In den letzten Jahrzehnten kam die Forschung zu zwei wichtigen Schlussfolgerungen, welche mittlerweile die Forschungsbestrebungen prägen: Erstens gibt es keine quantitativen Studien über den Ursprung und die Entstehung der regionalen Küche in Europa; zweitens sind erst ab dem Mittelalter Handschriften mit Tausenden von Kochrezepten überliefert, was wohl als die Geburt der modernen europäischen Küche angesehen werden kann (vgl. Flandrin & Hyman 1988, Laurioux 2005). Auf dem europäischen Kontinent stellen frühneuhochdeutsche, mittelfranzösische und mittellateinische Rezepte den größten Teil der kulinarischen Überlieferung dar, die mehr als 80 Manuskripte und etwa 8000 Rezepte umfasst. Das Projekt “Cooking Recipes of the Middle Ages” bereitet die Kochrezeptüberlieferung von Frankreich und dem deutschsprachigen Raum auf, um ihre Herkunft, ihre Beziehung und ihre Migration innerhalb Europas zu analysieren (vgl. thematisch ähnliche Studien mit unterschiedlicher Fokussetzung: Hieatt 1995 mit linguistischem Fokus, Flandrin 1984, Adamson 1995, Carlin 1989, Adamson 2002, Karg 2007 allgemein und van Winter 1989, Hyman 2005, Laurioux 2002 mit Fokus auf spezielle Gerichte). Die Partner, das Zentrum für Informationsmodellierung der Universität Graz und das Laboratoire CESR (Centre d’Etudes Supérieures de la Renaissance) der Universität Tours werden diese mehrsprachigen Texte mit Hilfe von digitalen Standards aufarbeiten und sie mit aktuellen quantitativen und qualitativen Forschungsmethoden untersuchen. Der Vergleich der französischen und deutschen Ernährungsgeschichte eignet sich besonders gut für diese Aufgabe, da Frankreich einen kulturell prägenden Einfluss auf die deutschsprachigen Völker hatte. 
         Kochrezepte sind kulturell aufgeladene, flüchtige Texte; die erhaltenen Niederschriften stellen daher nur ein punktuelles Zeugnis, eine individuelle Zubereitungsweise eines Gerichts (Hieatt 1985, 26) in Raum und Zeit dar. Das inhaltliche Verständnis dieser Rezepte, ihre möglichen Entstehungs- und Anwendungskontexte und ihre Überlieferung ist zudem kein einfacher Prozess, denn die Fachbegriffe, Zutaten, Utensilien, Verfahren und Bräuche der damaligen Zeit, die in den Rezepten eher öfter implizit als direkt genannt werden, sind auch für Sprach- und Geschichtswissenschaftler, die sich auf das Thema spezialisiert haben, immer wieder eine Herausforderung. Ihre Entwicklung sollte daher am besten diachron und räumlich analysiert werden, was mittlerweile mit digitalen geisteswissenschaftlichen Methoden verhältnismäßig leicht möglich ist – vorausgesetzt, die entsprechenden Daten liegen vor. Im aktuellen Projekt werden die historischen Texte auf mehreren Ebenen erschlossen: So werden die Texte nicht nur neu transkribiert und philologisch-editorisch bearbeitet (vgl. Klug, Kranich 2015), sondern auch in unterschiedlichen Wissensgebieten semantisch angereichert. Das schafft jene Spielräume, die nötig sind, um Analysen wie maschinengestützte Abgleiche von Zutaten, Kochprozessen oder Kochutensilien, die Suche nach Rezepttradition und -migration oder standardisierte philologische Vergleiche, wie z.B. Kollationierungen, durchzuführen. Die Basis unserer Daten sind customized TEI/XML-Dokumente mit einem zusätzlichen adaptierten Schema, das die semantische Annotation von Kochrezepten im Allgemeinen erleichtern soll. 
         Die Rezeptüberlieferung – in Form einzelner Rezeptsammlungen – wird mithilfe einer , die sich an den Handschriftenbeschreibungen renommierter Bibliotheken orientiert und die in Kooperation mit der Abteilung für Sondersammlungen der Grazer Universitätsbibliothek entstanden ist, räumlich und zeitlich fixiert. Besonderes Augenmerk wird dabei auf Informationen zur Handschriftenentstehung (materiell wie auch inhaltlich) und auf die Schriftbeschreibung bzw. den Schreibhandbefund der Rezeptsammlungen gelegt, wobei erstere Informationen meist den Katalogen entstammen, letztere aus der Arbeit mit den Texten kommen. Die Grundlage des Projekts ist die einheitliche Erfassung der überlieferten Texte durch eine hyperdiplomatische Neutranskription der historischen Quellen: Als Arbeitsumgebung fungiert Transkribus, wo das Textlayout automatisch erkannt und die Texte manuell mittels proprietären Codierungen erfasst werden. Mithilfe mehrerer Transformationsschritte wird aus den Rohdaten die Basis für die elektronische Quellenabbildung erstellt, die sich an germanistisch-editorischen Richtlinien orientiert. Die Quellentexttranskription verzeichnet dabei nicht nur das unterschiedliche Schriftzeicheninventar, sondern auch alle textstrukturierenden Elemente. Das gesamte Zeicheninventar ist in einer nach den Richtlinien der TEI erstellten Zeichenbeschreibung erfasst. Die Beschreibung stützt sich dabei auf die theoretischen Ergebnisse zur Beschreibung von Zeichen aus dem DigiPal-Projekt und verwendet außerdem die Zeichenidentifikatoren der Medieval Unicode Font Initiative (vgl. Böhm, Klug 2020). Die so produzierten Daten sind nicht nur der Ausgangspunkt für die wissenschaftlichen Fragestellungen im Projekt, sondern bieten eine solide Grundlage für viele weitere Forschungsfragen aus Germanistik/Linguistik, Paläographie usw. Diese Erarbeitungsstufe wird nach editorischen Richtlinien normalisiert und gibt im Rahmen des Webauftritts in Form einer Text-Bild-Synopse detaillierten Einblick in die historische Quelle.
            
         Aus den Transkriptionsdaten wird außerdem eine auf Zeichenebene normalisierte, in Sinneinheiten untergliederte Textfassung geschaffen, in der die semantischen Informationen annotiert werden. Diese Sinneinheiten umfassen neben dem eigentlichen Rezept und Rezepttitel Eingangs- und Schlussformeln, Handlungsanweisungen, Küchen- und Serviertipps, Hinweise auf medizinische und religiöse Aspekte und selbstverständlich Zutaten, Gerichte und Küchenutensilien. 
         Den Kern der digitalen Forschungsstrategie bildet das Semantic Web beziehungsweise die Anbindung und Integration unserer Daten an Linked Open Data. Wir sind innerhalb der Geisteswissenschaften in der vorteilhaften Position, dass sich unser Projekt zu einem großen Teil mit Lebensmittelzutaten befasst, d.h. mit Tieren, Pflanzen und Pilzen. Das sind Forschungsgebiete, in denen sich bereits eine signifikante Menge an relevanten Ontologien etabliert haben und die gut an die Linked Open Data Cloud, einschließlich der allgemeinen Wissensdatenbanken Wikidata und DBPedia angeschlossen sind. Ontologien werden, wenn auch mit unterschiedlichen Schwerpunkten und Granularität der Daten, außerdem bereits erfolgreich für die Repräsentation von Kochrezepten (Hoehndorf & Lange 2018, Sam et al. 2014, Ribeiro et al. 2006) und in deren Analyse eingesetzt (Chow & Grüniger 2019, Jovanovic et al. 2015, Vadivu & Waheeta Hopper 2010). In unserem digitalen Forschungsansatz setzen wir zwar teilweise auch auf Textähnlichkeiten, der größte Teil unserer Analyse basiert jedoch auf dem Vorkommen von Zutaten, Kochprozessen bzw. Zubereitungshinweisen und Kochutensilien. Weitere Entitäten, die wir für die Analyse der Rezepte heranziehen sind Serviervorschläge sowie medizinische, kulturelle und religiöse Aspekte in den Texten. Die Annotation dieser Entitäten gestaltet sich schon aufgrund ihrer schieren Menge in historischen Kochrezepten als sehr komplex. Neben den Möglichkeiten zuvor unbekannte Beziehungen zwischen den Quellen und deren Entitäten zu finden, war das Arbeiten außerhalb von Sprachbarrieren ein Hauptargument für die Entscheidung, Semantic Web-Technologien in den Mittelpunkt des Projekts zu stellen. 
            
         Durch die Verwendung von 
                Konzepten, im Sinne einer Idee oder eines mentalen Bildes und nicht eines 
                Begriffes, versuchen wir, historische und sprachliche Grenzen zu überwinden. Ein konkretes Beispiel für diese Diskrepanz zwischen Begriff und mentaler Vorstellung liefert uns die österreichische / süddeutsche Variante für Kartoffel: "Erdapfel" ("erdaphel" im Frühneuhochdeutschen) wird etwa in einem Manuskript aus der Zeit um 1488 erwähnt, lange bevor die Kartoffel von Südamerika nach Europa importiert wurde, was uns zeigt, dass das Konzept von "Erdapfel" ein anderes gewesen sein muss (wahrscheinlich jede Art von Rübe) als das heutige Konzept des Erdapfels. Wie oben bereits erwähnt, war es also nötig einen Workflow zu finden, der nicht nur die philologische, sondern auch die semantische Komplexität der Rezepte widerspiegelt. Während die phrasenartigen Informationseinheiten manuell annotiert werden, erfolgt die Annotation auf Wortebene semiautomatisch, indem die Texte mithilfe von XSLT- und Python-Skripten und individuellen Vokabularien, vorgehalten als CSV Dateien, angereichert werden, die alle darauf ausgerichtet sind, die Varianz der historischen Sprachstufen auszugleichen. Für die frühneuhochdeutschen Texte stand uns aus einem früheren Projekt eine Liste mittelalterlicher Pflanzennamen und ihrer Übersetzungen in modernes Englisch und Deutsch sowie ihrer mittelalterlichen Variantendiktionen zur Verfügung. Dies gab uns die Möglichkeit, mit Hilfe der von OpenRefine bereitgestellten Reconciliation Service API, einen teilautomatisierten Prozess zur Annotation von Wikidata-Konzepten zu starten. Die daraus resultierenden Daten bildeten den Grundstock für die zuvor genannten Vokabularien. Ähnliche Listen wurden von den Projektpartnern in Frankreich erstellt, die mithilfe des von den französischen Kollegen entwickelten Tools “Heterotoki” in einem kollaborativen Arbeitsschritt konsolidiert werden können. Sobald jeder Begriff mit einem Konzept verbunden ist, werden diese Konzepte verwendet, um die Zutaten innerhalb der eigentlichen Rezepttexte in den TEI-Dokumenten anzureichern. Ein entscheidender Faktor dieses semiautomatischen Prozesses bleibt jedoch die menschliche Interpretation der angereicherten Einheiten und die Entscheidung für ein konkretes bereits bestehendes Konzept bzw. die Erstellung eines neuen Konzepts in Wikidata.
            
         Wir befinden uns derzeit mitten in dieser semantischen Annotationsphase. Ist diese abgeschlossen, bieten sich mannigfaltige Analysemethoden an. Sobald die Einheiten der einzelnen Rezepte mit Konzepten ausgestattet sind, kann die Analyse des Projekts übereinstimmende oder abweichende Essgewohnheiten, Textmigration sowie den Einfluss der Nachbarländer auf die jeweilige Küche aufzeigen. Die Implementierung von Ontologien aus den Naturwissenschaften wie FoodOn oder SNOMED ermöglicht es uns, Verbindungen von historischen Essgewohnheiten zu modernen Konzepten von Lebensmitteln herzustellen und neues Wissen für den Bereich der Ernährungsgeschichte zu generieren. Die Ontologiedaten werden zusammen mit den Entitäten in einem Triplestore gespeichert und können mit Hilfe von SPARQL Queries befragt werden. Die Ergebnisse dienen als Grundlage für eine räumliche und zeitliche Visualisierung der Daten.
            
          Die Speicherung, Analyse und Dissemination der Projektdaten erfolgt über das vom Zentrum für Informationsmodellierung in Graz entwickelte Repository GAMS (Geisteswissenschaftliches Asset Management). Innerhalb dieser auf Langzeitarchivierung ausgerichteten Infrastruktur wird auf den Triplestore “Blazegraph” über einen Webservice zur Speicherung und Abfrage von RDF-Triples zugegriffen.
            
      
      
         
            
               https://transkribus.eu/Transkribus/
            
            
      Describing Handwriting I-VII; http://www.digipal.eu/blog
            
            
               https://folk.uib.no/hnooh/mufi/
            
            
      Für eine Übersicht an Ontologien in diesen Bereichen siehe: 
      http://www.ontobee.org/, 
      http://aims.fao.org/, 
      https://ndb.nal.usda.gov/ndb/, 
      https://agclass.nal.usda.gov/about.shtml, 
      http://zbw.eu/stw/version/latest/thsys/70498/about.de.html.
      
	Alle sind an die Linked Open Data Cloud angeschlossen indem eine oder mehrere Serialisierungen in OWL und/oder RDF(S) vorliegen. 
      
            
            
               http://medieval-plants.org
            
            
               https://github.com/OpenRefine/OpenRefine/wiki/Reconciliation-Service-API
            
            
               https://github.com/ponchio/heterotoki
            
            
               http://foodon.org/
            
            
               https://browser.ihtsdotools.org/
            
            
               https://gams.uni-graz.at
            
            
               https://www.blazegraph.com/
            
         
         
            
               Bibliographie
               
                  Adamson, M. W. (Ed.)  (1995): Food in the
      Middle Ages. A Book of Essays. New York, London:
      Garland. Adamson, M. W. (Ed.). (2002). Regional Cuisines of
      Medieval Europe: A Book of Essays. New York, London: Routledge.
      
               
                  Amoia, M. / Martínez, J.M.M.  (2019): SaCoCo Diachronic Corpus [WWW Document]. URL 
	http://fedora.clarin-d.uni-saarland.de/sacoco/ (accessed 1.7.20).
      
               
                  Böhm, A.  /  Klug, H.: Quellenorientierte
      Aufbereitung historischer Texte im Rahmen digitaler Editionen:
      Das Problem der Transkription in mediävistischen
      Editionsprojekten. In: [Titel steht noch nicht fest] Hrsg. von
      Ingrid Bennewitz und Martin Fischer (= Bamberger
      interdisziplinäre Mittelalterstudien.) [in Vorbereitung]
      
               
                  Carlin, M. /  Rosenthal, J. T. (Eds.)
      (1998): Food and Eating in Medieval Europe. London: Hambledon
      Press.
      
               
                  Chow, A. E. / Ninger, M. G.  (o. J.): 
	Multimodal Event Recognition with an Ontology For Cooking Recipes. 12.
      
               
                  Dooley, D. M. / Griffiths, E. J. / Gosal, G. S. / Buttigieg, P. L. / Hoehndorf, R. / Lange, M. C.,  / …  / Hsiao,
	W. W. L.  (2018): FoodOn: A harmonized food ontology to increase global food traceability, quality control and data integration. 
	Npj Science of Food, 2(1), 23. 
	https://doi.org/10.1038/s41538-018-0032-6
               
               
                  Flandrin, J.-L. (1984): «Internationalisme,
      nationalisme et régionalisme dans la cuisine des XIVe et XVe
      siècles: le témoignage des livres de cuisine». In Manger et
      boire au Moyen âge. Actes du Colloque de Nice (15-17 octobre
      1982). (pt. 2, p. 75-91). Paris.
      
               
                  Flandrin, J.-L.  /  Hyman, P. (1988):
      “Regional tastes and cuisines: Problems, documents, and
      discourses on food in Southern France in the 16th and 17th
      centuries”. Food and Foodways 1-3, p. 221-251.
      
               
                  Gloning, T.,  (2000): Monumenta Culinaria et Diaetetica Historica. Corpus of culinary & dietetic texts of Europe from the Middle Ages to 1800. Corpus älterer deutscher Kochbücher und Ernährungslehren [WWW Document]. URL 
	http://www.staff.uni-giessen.de/gloning/kobu.htm (accessed 1.7.20).
      
               
                  Hammad, R. /  Hassouna, M.  (2011): 
	Multi-Language Semantic Search Engine. 6.
      
               
                  Hieatt, C.  (1995): Sorting through the
	Titles of Medieval Dishes: What Is, or Is Not, a “Blanc
	manger”. In M. W. Adamson (Ed.), Food in the Middle Ages. A
	Book of Essays. (pp. 25-43). New York, London: Garland.
      
               
                  Hyman, P.  /  M.  (2005): «Les associations de saveurs dans les livres de cuisine français du XVIe siècle». In 
	Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles). Actes du colloque international à la mémoire de Jean-Louis Flandrin (Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 135-150). Saint-Denis: Presses Universitaires de Vincennes.
      
               
                  Karg, S. (Ed.) (2007): Medieval Food
      Traditions in Northern Europe. Copenhagen: National Museum of
      Denmark.
      
               
                  Klug, H. W. /  Kranich, K.  (2015): “Das Edieren von handschriftlichen Kochrezepttexten am Weg ins digitale Zeitalter. Zur Neuedition des Tegernseer Wirtschaftsbuches.” In T. Bein (Ed.), Vom Nutzen der Editionen. Zur Bedeutung moderner Editorik für die Erforschung von Literatur- und Kulturgeschichte. (pp. 121-137). Berlin, Boston: De Gruyter.
               
                  Lamé, M. /  Pittet, P. (2018):
      Heterotoki: Non-st / ructured and heterogeneous terminology alignment for Digital Humanities data producers. 12.
      
               
                  Laurioux, B.  (2005): «Les voyageurs et la gastronomie en Europe à la fin du Moyen âge ». In Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles), Actes du colloque international à la mémoire de Jean-Louis Flandrin (Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 99-117). Saint-Denis, Presses Universitaires de Vincennes.
               
                  Ribeiro, R. / Batista, F. / Pardal, J. P.  / Mamede, N. J. /  Pinto, H. S.  (2006): Cooking an Ontology. In J. Euzenat & J. Domingue (Hrsg.), 
	Artificial Intelligence: Methodology, Systems, and Applications (S. 213–221). Springer Berlin Heidelberg.
      
               
                  Sam, M. / Krisnadhi, A. A. / Wang, C. / Gallagher, J. /  Hitzler, P. (2014): 
	An Ontology Design Pattern for Cooking Recipes – Classroom Created.
      
               
                  Vadivu, G. / Hopper, S. W.  (2010): Semantic Linking and Querying of Natural Food, Chemicals and Diseases. 
	International Journal of Computer Applications, 
	11(4), 35–38. 
	https://doi.org/10.5120/1567-2093
               
               
                  van Winter, J. M. (1989):
	“Kochen und Essen im Mittelalter.” In B. Herrmann
	(Ed.). Mensch und Umwelt im
	Mittelalter. (pp. 88-100). Frankfurt am Main: Fischer
	Taschenbuch Verl.
      
            
         
      
   



      
         
            Opaque – digitale Arbeitsumgebung für die Humanities
            Im Rahmen der DHd 2020 Spielräume möchten wir unsere in aktiver Entwicklung befindliche Webanwendung Opaque vorstellen. Anspruch ist es, Opaque als Arbeitsumgebung für DH-Projekte zu etablieren. Die Entwicklung der Webanwendung, deren Funktionen sukzessive erweitert werden sollen, wird im Rahmen des DFG-geförderten Sonderforschungsbereichs (SFB) 1288 "Praktiken des Vergleichens" im Teilprojekt (TP) INF "Dateninfrastruktur und Digital Humanities" durchgeführt.
            Das TP INF betreut das Forschungsdatenmanagement des SFB und unterstützt dessen Wissenschaftler*innen darüber hinaus bei der Planung, Konzeptionierung und Durchführung von Forschungsprojekten unter Zuhilfenahme digitaler Methoden. Diese beiden Bereiche sollen in Opaque synthetisiert werden. Aufbauend auf den Erfahrungen der Kooperationen entwickeln wir Opaque zur Bündelung und Automatisierung der erprobten Workflows und Best Practices. Eine besondere Schwierigkeit ist hierbei die Heterogenität und Komplexität von Forschungsdaten in den Geisteswissenschaften. Um dieser Schwierigkeit zu begegnen orientiert sich unsere Etablierung von Best Practices an den verschiedenen Stadien des Data Life Cycle, bestehend aus Planung/Beratung, Sammlung, Datenorganisation, Datenanalyse, Dissemination und Nachnutzung, und hat zum Ziel, für alle diese Stadien Best Practices zu entwickeln oder implementieren und so den Forscher*innen verfügbar zu machen. Die einzelnen in Opaque verfügbaren Funktionen werden durch etablierte Open Source-Lösungen realisiert, die durch die modulare Konstruktion der Webanwendung nicht nur gut erweitert sondern auch beständig auf dem neuesten Stand gehalten werden können, sowie reproduzierbare Routinen gewährleisten. Der Fokus auf Nachnutzung bestehender Software ermöglicht es uns, ein breites Spektrum an Funktionalitäten in Opaque zu integrieren.
            
               Opaque: Die Webanwendung
               Opaque bündelt verschiedene Werkzeuge und Services, die Geisteswissenschaftler*innen Methoden der DH an die Hand geben und somit deren verschiedene individuelle Forschungsprozesse unterstützen können. Mittels Opaque können Forschende digitalisiert vorliegende Quellen einer 
Optical Character Recognition (OCR) unterziehen. Die daraus resultierenden Textdateien können anschließend als Datengrundlage zum 
Natural Language Processing (NLP) weiterverwendet werden. Die Texte werden hierbei automatisiert verschiedenen linguistischen Annotationen unterzogen. Die via NLP prozessierten Daten können in der Webanwendung anschließend als Corpora zusammengefasst und mittels eines 
Information Retrieval System durch komplexe Suchanfragen analysiert werden. Der Funktionsumfang der Webanwendung wird zudem anhand der Bedarfe der Forschenden sukzessive erweitert.

               Die Funktionsschwerpunkte von Opaque unterscheiden sich von anderen deutschen DH-Softwareentwicklungen. Hervorzuheben sind 
TextGrid, 
FuD und 
CQPweb, die einen ähnlichen Anspruch als virtuelle Forschungsumgebung verfolgen. Im Unterschied zu Opaque legen 
TextGrid und 
FuD ihre Schwerpunkte auf händische Datenaufbereitung und nachhaltige Speicherung via integrierter Publikationsplattformen, wohingegen 
CQPweb ein Werkzeug zur
Korpusanalyse darstellt, dessen Query
Processor in Opaque übernommen wurde. Opaque
soll demgegenüber keine Publikationsplattform
integrieren, sondern eine automatisierte
Aufbereitung und Informationsanreicherung von
Forschungsdaten mit anschließender Analyse
ermöglichen. Die aufbereiteten Daten und
Analyseergebnisse können mittels
Exportfunktionen anhand gängiger Standards in
offene Dateiformate exportiert und
anschließend auf eigens gewählten
Publikationsplattformen veröffentlicht
werden. Die bereits in Opaque integrierten und
beständig auf dem neuesten Stand gehaltenen
Funktionen im Bereich des NLP und der OCR
grenzen die Plattform von den genannten
bestehenden Lösungen ab.
                  , 
                  
               
               Da Opaque plattformunabhängig konzipiert ist, können die verschiedenen Funktionen von den Wissenschaftler*innen auf beliebigen Endgeräten ohne vorangehende Einrichtung genutzt werden. Alle Funktionen wie z.B. OCR werden innerhalb der Cloud-Infrastruktur ausgeführt, so dass Nutzer*innen selbst keine leistungsfähigen Endgeräte benötigen.
            
            
               Nutzerorientiertes Design
               Die in Opaque implementierten Funktionen und Workflows orientieren sich an den aus unserer Zusammenarbeit im SFB hervorgegangenen Erfahrungen, etablierter Best Practices sowie Vorgaben und Standards des Forschungsdatenmanagements.
               Dies führt nicht nur zu besseren Ergebnissen für die Forscher*innen, sondern auch zu einer besseren Datenorganisation mittels anerkannter Standards.
               Durch eine Gegenüberstellung soll auf dem Poster anhand der verschiedenen Stadien des Data Life Cycle veranschaulicht werden, wie sich Arbeitsprozesse und -schritte durch die Einführung von Opaque verändert haben. Prägnante Beispiele für diese Gegenüberstellung sind Datensammlung und Datenanalyse. Mit Hilfe der Webanwendung können Forscher*innen eigene Quellen und Texte einem OCR-Prozess unterziehen und die Ergebnisse zeitnah selbstständig hinsichtlich der Güte der Texterkennung evaluieren. Diese Automatisierung der Prozesse in Verbindung mit der intuitiven Bedienoberfläche tragen zu einer erhöhten Autonomie der Forschenden bei. Gleichzeitig macht die Echtzeitverfolgung der Jobstatus die Prozessabläufe transparent und nachvollziehbar. Gespräche, die vorher technischer und organisatorischer Natur waren, können nun gezielter für inhaltliche Diskussionen und Planung der Forschung genutzt werden.
               Bezüglich der Qualität der Eingabedateien (z.B. Scans) offerieren
wir Hinweise zur bestmöglichen Digitalisierung von Ausgangsmaterialien
und orientieren uns an gängigen Standards zur Speicherung und
Veröffentlichung von Forschungsdaten (z.B. FAIR), um deren Nachnutzung
zu gewährleisten. Dies schließt neben den Forschungsdaten auch die
Nachhaltung und Bereitstellung von für den Forschungsprozess genutzter
Software in den jeweils genutzten Versionen mit ein, um die
Reproduzierbarkeit von Forschungsergebnissen sicherzustellen.
                      
            
            
               Implementierung
               Die Umsetzung beruht auf 
                        Free Open Source Software und Python. Auf dem Poster werden die Vorteile von Linux Containern in einem skalierbaren Docker-Rechencluster, wie z.B. eine einfache Verwaltung verschiedener Softwareversionen – insbesondere wichtig um Forschungsdaten reproduzieren zu können –, vorgestellt und die einzelnen im Folgenden aufgeführten Module der Plattform näher beleuchtet.
                    
               
                  
                     Webanwendung: Die Webanwendung dient als Schnittstelle zwischen Nutzer*innen und Recheninfrastruktur. Hier können Datenaufbereitungen in Form von Jobs gestartet und in Echtzeit verfolgt werden, dabei werden die Jobs automatisch auf das zugrundeliegende Rechencluster verteilt. Das Webinterface bietet außerdem die Möglichkeit über ein 
                            Information Retrieval System Auswertungen durchzuführen.
                        
                  
                     Daemon: Agiert im Hintergrund, um die von den Nutzer*innen durch die Webanwendung abgesetzten Befehle und Services umzusetzen bzw. zu verwalten.
                        
                  
                     Datenbank: Die Datenbank speichert alle Metadaten, die während der Nutzung der Webanwendung anfallen. Als Datenbanksystem wird 
                            PostgreSQL benutzt.
                        
                  
                     Netzwerkspeicher: Speichert die von den Nutzer*innen hochgeladenen Dateien sowie die daraus generierten Resultate. Die Netzwerkspeicherlösung garantiert den Servern des Cloud-Rechenclusters gleichermaßen Zugriff auf die zu bearbeitenden Dateien.
                        
                  
                     Services: OCR und NLP-Dienste werden mittels der state of the art Software 
                            Tesseract OCR und 
                            spaCy realisiert. Die Korpusanalyse erfolgt durch eine Anbindung an den 
                            CQP query processor der IMS Open Corpus Workbench. Jede Ausführung eines Dienstes ist mit einem Job assoziiert, der in einem eigens dafür erstellten Container bearbeitet wird.
                        
               
               Ein zusätzliches Hands-On von Opaque soll zu einem Erfahrungsaustausch einladen.
            
         
      
      
         
            Eintrag des offiziellen DARIAH-Wikis schildert, dass gängige Funktionen wie ein Lemmatisierer nicht mehr nachinstalliert werden können.
            Der Abschlussbericht des Projekts TextGrid aus dem Jahr 2012 schildert die Implementierung einer OCR Funktion mittels OCRopus, welche in den aktuellen Versionen nicht mehr zu finden ist.
         
      
   



      
         Die kulinarische Tradition ist eine der prägendsten Elemente der europäischen Kultur und sie stellt einen großen Teil der nationalen Identitäten dar. In den letzten Jahrzehnten kam die Forschung zu zwei wichtigen Schlussfolgerungen in Bezug auf dieses Thema: Erstens, es gibt keine quantitativen Studien über die Herkunft und die Bildung von regionalen Küchen in Europa. Zweitens, im Mittelalter entstehen wesentliche Quellen: Manuskripte mit tausenden von Kochrezepten. Damit kann das Mittelalter als die Wiege der modernen europäischen Küche angesehen werden. Auf dem europäischen Kontinent bilden lateinische, mittelfranzösische und frühneuhochdeutsche Rezepte den Großteil der kulinarischen Überlieferung.
         Das vorliegende internationale Projekt (ANR-17-CE27-0019-01, fwf I 3614) zielt darauf ab, die interkulturelle Forschung der mittelalterlichen Kochrezepte und deren Wechselbeziehung mithilfe eines interdisziplinären Ansatzes zu verwirklichen. Das Projekt nimmt die Kochrezeptüberlieferung von Frankreich und den deutschsprachigen Ländern als Korpus – dieses umfasst mehr als 80 Manuskripte und an die 8000 Rezepte – und untersucht sie in Hinblick auf ihre Entstehung, ihre Beziehung untereinander und ihre Migration durch Europa. Der Vergleich der französischen und deutschen Kulinargeschichte eignet sich besonders für diese Aufgabe, da Frankreich seit jeher einen kulturell prägenden Einfluss auf deutschsprachigen Völker hatte!
         Die Partner, das Zentrum für Informationsmodellierung der Universität Graz und das Laboratoire CESR (Centre d’Etudes Supérieures de la Renaissance) der Universität Tours werden diese mehrsprachigen Texte nach modernen Standards aufarbeiten und sie mit aktuellen quantitativen und qualitativen Forschungsmethoden untersuchen. Für eine computergestützte Analyse werden die Rezeptsammlungen und die darin enthaltenen Texte und deren Metadaten in TEI/XML (Digitale Transkription und Edition) modelliert und mit Semantic Web Technologien analysiert (Digitale Annotation und Datenvisualisierung). Die Daten werden einer Langzeitarchivierungsinfrastruktur (GAMS, Zentrum für Informationsmodellierung Graz) zugeführt, in der sie weiter erforscht werden können. Alle Rezepte werden mithilfe von Vokabularien für Zutaten, Kochprozesse und Kochutensilien sowie kulturhistorisch relevanten Metadaten (z. B. in Bezug auf religiöse, kulturelle oder medizinische Aspekte) angereichert. Aufgrund dieser Informationen wird das Projekt über die Sprachgrenzen hinweg konkurrierende oder abweichende Essgewohnheiten, Textmigration sowie den gegenseitigen Einfluss der Nachbarländer auf ihre jeweilige Küche zu Tage fördern. Für die Analyse der deutschsprachigen Texte werden außerdem NLP-Methoden für historische Sprachstufen herangezogen, um Textverwandtschaften innerhalb dieser Überlieferung untersuchen zu können. Die Forschungsdaten und die Auswertungsergebnisse werden die Grundlage für eine räumliche und zeitliche Visualisierung und statistische Auswertung bilden, die neue Ansätze zur Interpretation des historischen und kulturellen Vermögens fördern wird.
         Die im Projekt erarbeiteten Workflows und Daten werden ganz im Sinne des Open Science Gedanken und den FAIR-Prinzipien für die Nachnutzung zur Verfügung gestellt:
         Der Transkriptionsworkflow und die Transkriptionsprinzipien (Theorie und Praxis, in Kooperation mit KONDE) können zur Gänze nachgenutzt werden. Da die Manuskripte mit Transkribus
                 transkribiert wurden, steht ein trainiertes HTR-Modell zur Verfügung mit dem eine automatische Handschriftenerkennung von ähnlichen Texten denkbar ist. Das Annotationsvokabular (Zutaten, Speisen, Küchengeräte, Zubereitungsweisen) wird samt der zugewiesenen semantischen Wikidata-Konzepte zur Verfügung gestellt und stellt somit eine essentielle Basis für die Forschung im Bereich Kulinarhistorik dar. Die Konzepte in Wikidata werden falls vorhanden kontrolliert und gegebenenfalls mit weiteren Daten (wie etwa Links zu relevanten Ontologien wie FoodO oder SNOMED) von unserer Seite angereichert. Noch nicht vorhandene Konzepte werden von uns neu erstellt und mit allen nötigen Daten (Statements) versehen. Die Nutzung von Wikidata verfolgt neben praktischen Überlegungen hauptsächlich das Ziel, die im Projekt gewonnenen Daten auf einfache Art und Weise für die Community bereitzustellen und eine weitere Bearbeitung dieser Daten zu ermöglichen. Überdies hinaus werden von uns auch die Annotationsskripte (Python und XSLT) für die Übertragung der Annotationsvokabularien nach TEI/XML zum Download angeboten. 
            
         Die überlieferten Texte werden durch eine hyperdiplomatische Neutranskription der historischen Quellen einheitlich erfasst und stehen als TEI/XML ebenfalls zur weiteren Nutzung zur Verfügung. Die Quellentexttranskription verzeichnet dabei nicht nur das unterschiedliche Schriftzeicheninventar, sondern auch alle textstrukturierenden Elemente. Das gesamte Zeicheninventar ist in einer nach den Richtlinien der TEI erstellten Zeichenbeschreibung erfasst. Die Beschreibung stützt sich dabei auf die theoretischen Ergebnisse zur Beschreibung von Zeichen aus dem DigiPal-Projekt und verwendet außerdem die Zeichenidentifikatoren der Medieval Unicode Font Initiative (vgl. Böhm, Klug 2020). Die so produzierten Daten sind nicht nur der Ausgangspunkt für die wissenschaftlichen Fragestellungen im Projekt, sondern bieten eine solide Grundlage für viele weitere Forschungsfragen aus Germanistik/Linguistik, Paläographie usw. Die Textdaten werden für eine Nutzung durch NLP Tools auch als Plaintext angeboten und die Handschriftenabbildungen sind je nach Nutzungsbedingungen der Bibliotheken frei verfügbar. 
            
         Darüber hinaus wird aus dem CoReMA-Projekt heraus ein Modell für die Integration weiterer Texte in die Forschungsumgebung bereitgestellt. Das Projekt soll fachliche Impulse für alle betroffenen Disziplinen der mittelalterlichen und frühneuzeitlichen Geschichte, Kulinargeschichte, Digitale Edition und Digital Humanities liefern.
      
      
         
            
               http://www.digitale-edition.at/
            
            
               https://transkribus.eu/Transkribus/
            
            
               https://www.wikidata.org
            
            
               http://foodon.org/
            
            
               https://browser.ihtsdotools.org/
            
            
      Describing Handwriting I-VII; http://www.digipal.eu/blog
            
            
               https://folk.uib.no/hnooh/mufi/
            
         
         
            
               Bibliographie
               
                  Adamson, M. W. (Ed.)  (1995): Food in the Middle Ages. A Book of Essays. New York, London: Garland. Adamson, M. W. (Ed.). (2002). Regional Cuisines of Medieval Europe: A Book of Essays. New York, London: Routledge.
      
               
                  Amoia, M., Martínez, J.M.M.  (2019): SaCoCo Diachronic Corpus [WWW Document]. URL 
	http://fedora.clarin-d.uni-saarland.de/sacoco/
	(accessed 1.7.20).
      
               
                  Böhm, A. & Klug, H.: Quellenorientierte Aufbereitung historischer Texte im Rahmen digitaler Editionen: Das Problem der Transkription in mediävistischen Editionsprojekten. In: [Titel steht noch nicht fest] Hrsg. von Ingrid Bennewitz und Martin Fischer (= Bamberger interdisziplinäre Mittelalterstudien.) [in Vorbereitung]
      
               
                  Carlin, M., & Rosenthal,
	J. T. (Eds.).  (1998): Food and Eating in Medieval Europe. London: Hambledon Press.
      
               
                  Flandrin, J.-L.  (1984): «Internationalisme, nationalisme et régionalisme dans la cuisine des XIVe et XVe siècles: le témoignage des livres de cuisine». In Manger et boire au Moyen âge. Actes du Colloque de Nice (15-17 octobre 1982). (pt. 2, p. 75-91). Paris.
      
               
                  Flandrin, J.-L. & Hyman, P.  (1988): “Regional tastes and cuisines: Problems, documents, and discourses on food in Southern France in the 16th and 17th centuries”. Food and Foodways 1-3, p. 221-251.
      
               
                  Gloning, T.,  (2000): Monumenta Culinaria et Diaetetica Historica. Corpus of culinary & dietetic texts of Europe from the Middle Ages to 1800. Corpus älterer deutscher Kochbücher und Ernährungslehren [WWW Document]. URL http://www.staff.uni-giessen.de/gloning/kobu.htm
	(accessed 1.7.20).
      
               
                  Hieatt, C.  (1995): Sorting through the Titles of Medieval Dishes: What Is, or Is Not, a “Blanc manger”. In M. W. Adamson (Ed.), Food in the Middle Ages. A Book of Essays. (pp. 25-43). New York, London: Garland.
      
               
                  Hyman, P. & M.  (2005). «Les associations de saveurs dans les livres de cuisine français du XVIe siècle». In 
	Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles). Actes du colloque international à la mémoire de Jean-Louis Flandrin
	(Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 135-150). Saint-Denis: Presses Universitaires de Vincennes.
      
               
                  Karg, S. (Ed.).  (2007): Medieval Food Traditions in Northern Europe. Copenhagen: National Museum of Denmark.
      
               
                  Klug, H. W., & Kranich, K. (2015): “Das Edieren von handschriftlichen Kochrezepttexten am Weg ins digitale Zeitalter. Zur Neuedition des Tegernseer Wirtschaftsbuches.” In T. Bein (Ed.), Vom Nutzen der Editionen. Zur Bedeutung moderner Editorik für die Erforschung von Literatur- und Kulturgeschichte. (pp. 121-137). Berlin, Boston: De Gruyter.
      
               
                  Laurioux, B. (2005): «Les voyageurs et la gastronomie en Europe à la fin du Moyen âge ». In Le Désir et le Goût. Une autre histoire (XIIIe-XVIIIe siècles), Actes du colloque international à la mémoire de Jean-Louis Flandrin (Saint-Denis, septembre 2003). Dir. Odile Redon, Line Sallman et Sylvie Steinberg. (p. 99-117). Saint-Denis, Presses Universitaires de Vincennes.
      
               
                  van Winter, J. M.  (1989). “Kochen und Essen im Mittelalter.” In
      B. Herrmann (Ed.). Mensch und Umwelt im
      Mittelalter. (pp. 88-100). Frankfurt am Main: Fischer
      Taschenbuch Verl.
      
            
         
      
   



      
         
            Einleitung
            Die Idee des Distant Reading (Moretti, 2002) ist davon geprägt, durch den Einsatz von Methoden der computergestützten Textanalyse und Textvisualisierung große Mengen an Literatur zu explorieren, um Einsichten zu gewinnen, die mit herkömmlichen Methoden nicht möglich sind. Der Einsatz von Distant Reading wird dabei mittlerweile auch außerhalb der Literaturwissenschaften untersucht wie z.B. in den Religionswissenschaften (Pfahler et al., 2018). Im folgenden Beitrag wird ein Projekt vorgestellt, in dem der Einsatz und Nutzen von Distant Reading in ersten Analysen auf einer größeren Menge deutschsprachiger Songtexte exploriert wird. Ziel des Projekts ist es, mittels Distant Reading Unterschiede in gängigen Genres populärer Musik herauszukristallisieren. 
         
         
            Verwandte Arbeiten 
            Im Bereich des Text Mining wird die
Analyse von Songtexten vor allem im Kontext von Retrieval- und
Recommender-Aufgaben betrieben. Ziel ist meist die automatische
Klassifikation und Vorhersage verschiedener Kategorien, z.B. dem Genre
(Fell & Sporleder, 2014; De Sousa et al., 2016). Außerhalb dieses
Arbeitsgebiets findet man in Bereichen der Kultur- und
Literaturwissenschaften sowie der Psychologie Studien mit Songtexten
als Untersuchungsgegenstand (Cole, 1971; Kuhn,
1999). Forschungsinteressen umfassen dabei Analysen spezifischen
Musikern
(Beatles, West & Martindale, 1996; Whissel, 1996, 
Bob Dylan, Whissel, 2008; Körner, 2012), Epochen (Pettijohn & Sacco, 2009), Emotionen (Napier & Shamir, 2018) oder Erfolg (Riedemann, 2012). Im Bereich der computergestützten Korpus-Analyse findet man vereinzelt Projekte für den englischsprachigen Bereich. Dabei werden beispielsweise quantitative und qualitative Methoden verknüpft, um Stil und historische Eigenheiten zu analysieren (Werner, 2012), Annotations- und Akquisemöglichkeiten von Korpora exploriert (Kreyer & Mukherjee, 2009) oder N-Gramme untersucht (Nishina, 2017). Die Analyse von deutschsprachigen Texten ist jedoch bislang selten und findet vor allem im Bereich von regionalem Rap statt (Hess-Lüttich, 2009) sowie eher qualitativ und hermeneutisch (Stiegler, 2009).
                
         
         
            Korpus-Erstellung
            Als Plattform für die Akquise der Songtexte wurde 
LyricWiki
                gewählt. Ausgehend von aktuellen Umfragen zu den populärsten Genres in Deutschland werden die folgenden vier Genres betrachtet: 
Pop, Rock, Schlager und
 Rap/Hip Hop. Für die Auswahl der Songs wurden manuell durch Analyse der deutschen Charts seit den 60er Jahren eine angemessene Anzahl der wichtigsten deutschsprachigen Genre-Vertreter aufgestellt. Dieser Schritt ist (auch) subjektiv geprägt, der Fokus auf berühmte und „typische“ Vertreter der einzelnen Genres erlaubt jedoch trotzdem erste Analysen. Kritisch sei jedoch anzumerken, dass die Grenzen der Genres für einzelne Interpreten und Songs nicht immer eindeutig sind, insbesondere was Rock, Pop und Schlager betrifft. Wir haben versucht, für das vorliegende Korpus eine Auswahl mit möglichst eindeutigen Zuordnungen zu treffen.
            
            Für jeden gewählten Interpreten wurden über ein Skript alle Songtexte mit Metadaten von LyricWiki akquiriert. Die Akquise des Korpus wurde mittels eines frei verfügbaren angepassten ruby-Skripts durchgeführt.
                
            Abbildung 1 illustriert Eckdaten zum Gesamtkorpus und den Künstlern. In der Spalte „Bekannte Vertreter“ werden einige Künstler beispielhaft angegeben.
            
               
               Abbildung 1: Korpus-Zusammensetzung
            
            Abbildung 2 zeigt die Songverteilung im zeitlichen Verlauf und Genre-Kontext auf.
            
               
               Abbildung 2: Genre und zeitlicher Verlauf des Korpus
            
            Im Bereich des Preprocessing wurden Stoppwörter entfernt und alle Wörter zu Normalisierungszwecken in Kleinschreibung gebracht. 
         
         
            Methoden und Ergebnisse
            Für die allgemeine Textanalyse und das Topic Modeling wurden alle Analysen mittels 
R und unterschiedlichen Bibliotheken wie dem 
NLP
               - und 
topicmodels-package durchgeführt. Die Sentiment Analysis wurde mit 
Python und 
SentiWS (Remus et al., 2010) implementiert.

            
               Allgemeine Textanalyse
               Die Repetition von besonders bedeutenden Wörtern ist ein gängiges Stilmittel bei der Gestaltung von Songtexten. Aus diesem Grund betrachten wir die Analyse der häufigsten Wörter von Songtexten als besonders aufschlussreich. Die folgenden Bilder (Abbildung 3-6) illustrieren die 10 häufigsten Wörter (Most Frequently Used Words; MFWs) der einzelnen Genres.
               
                  
                  Abbildung 3: MFWs für Pop
               
               
                  
                  Abbildung 4: MFWs für Rap
               
               
                  
                  Abbildung 5: MFWs für Rock
               
               
                  
                  Abbildung 6: MFWs für Schlager
               
               Man erkennt, dass es drei Wörter gibt, die in allen vier Genres gleichmäßig stark vertreten sind: „Welt“, „Leben“ und „Zeit“. Diese Konzepte sind demnach konsistenter Inhalt deutschsprachiger Liedtexte unabhängig vom Genre. Die größte Differenzierung zeigen die Genres Rap, in dem Terme der Umgangs- und Jugendsprache enthalten sind, aber auch thematische Schwerpunkte deutlich werden („Geld“) sowie das Genre Schlager, das vor allem von emotionalen Termen wie „Liebe“, „Herz“ oder „Glück“ dominiert wird.
            
            
               Sentiment Analysis
               
                  Sentiment Analysis ist die Methodik zur computergestützten Analyse von Sentiments in Texten, also ob und in welchem Ausmaß Wörter eines Textes eher positiv oder negativ konnotiert ist (Liu, 2016). In den Digital Humanities werden häufig lexikonbasierte Methoden zur Bestimmung von Sentiment-Werten eingesetzt (Mohammad, 2011; Nalisnick & Baird, 2013). Dabei wird durch Summenbildung von Sentiment-Werten von Wörtern die Gesamtpolarität einer Texteinheit ermittelt. Wir verwenden dabei das etablierte Sentiment-Lexikon 
                        SentiWS (Remus et al., 2010). Abbildung 7 illustriert einige Ergebnisse:
                    
               
                  
                  Abbildung 7: Ergebnisse – Sentiment Analysis
               
               Man erkennt, dass für alle 4 Genres insbesondere Varianten von Liebe einen erheblichen Beitrag zur positivien Polarität leisten. Rap grenzt sich deutlich mit für das Genre typischen Themen ab, ausgedrückt durch Wörter wie „reich“ und mit Slang („hart“, „alter“). Alle Genres weisen insgesamt auf eine negative Polarität hin. Entgegen der naiven Intuition sind die Genres „Rap“ und „Rock“ dabei noch am positivsten (gemessen an den normalisierten Werten) bewertet. Erste Analysen machen jedoch auch Probleme der lexikonbasierten Sentiment-Analyse deutlich. Die Wörter „wein“ (weinen) und „feuer“ (das Feuer) sind in SentiWS als negativ markiert, haben aber in unseren Texten oft eher positive Konnotationen. Bei dem Wort „wein“ dann, wenn dieses durch die Normalisierung von „der Wein“ hergeleitet wird. In zukünftigen Arbeiten wollen wir mit einem domänenspezifischen Lexikon arbeiten, das für die jeweilige Anwendungsdomäne optimiert ist.
            
            
               Topic Modeling
               Topic Modeling ist eine Methode, um den Anteil verschiedener Themen in Dokumenten zu analysieren. Ein Thema ist dabei ein selbst definiertes Label für eine Liste von Wörtern, die besonders häufig zusammen auftreten. Als Algorithmus wurde Latent Dirichlet Allocation (LDA) gewählt (Blei et al., 2003). Das Topic Modeling wurde separat für die einzelnen Genres durchgeführt, um Unterschiede und Gemeinsamkeiten zu untersuchen. Wir sind momentan noch am Anfang der Analyse der einzelnen Topics, aber neben Differenzen werden auch Topics gefunden, die ähnliche Konzepte widerspiegeln. Folgende Visualisierungen geben die Wortlisten wider, die wir jeweils als das Topic „Liebe“ in den einzelnen Genres benannt haben. Die Wortgröße gibt die Häufigkeit des Wortes im jeweiligen Sub-Korpus wider (Abbildung 8).
               
                  
                  Abbildung 8: Wortlisten für das Topic „Liebe“
               
               Auffällig ist, dass insbesondere bei Rap familiäre Begriffe wie „Mama“, „Vater“ oder auch „Bruder“ Bestandteil des Topics sind, was traditionellerweise ein häufiger Schwerpunkt im Rap-Genre ist.
            
         
         
            Ausblick
            In unseren zukünftigen Arbeiten wollen wir insbesondere das Korpus systematisch vergrößern und verbessern. Momentane Probleme sind z.B. die Ungleichverteilung in der Menge bezüglich der Genres aber auch ein Fokus auf eher aktuelle Künstler. Wenngleich wir schon erste Eigenheiten der Genres feststellen konnten, wollen wir Methoden wie Sentiment Analysis und Topic Modeling noch weiter explorieren, indem wir beispielsweise die Varianz der Sentiments untersuchen. Des Weiteren wollen wir unsere Arbeit aber auch auf andere Textanalyse-Möglichkeiten wie Kollokationsprofile von Keywords, Named Entity Recognition und Stilometrie ausweiten. Durch die Zusammenarbeit mit Musik- und Literaturwissenschaftlern wollen wir in Zukunft auch explorieren, welche weiteren Forschungsfragen mit Hilfe größerer Korpora und Distant Reading-Methoden beantwortet werden können.
         
      
      
         
            
      https://lyrics.fandom.com/wiki/LyricWiki
    
            
      https://de.statista.com/statistik/daten/studie/171224/umfrage/beliebteste-musikrichtungen/
    
            
      Das Korpus kann auf Anfrage per Mail erhalten werden.
    
            
      https://gist.github.com/siavashs/3556469
    
            
      https://cran.r-project.org/web/packages/NLP/index.html
    
            
      https://cran.r-project.org/web/packages/topicmodels/index.html
    
         
         
            
               Bibliographie
               
                  Blei, David M. / Andrew, Y. Ng / Michael, I. Jordan (2003): "Latent dirichlet allocation", in 
                        Journal of machine Learning research 3: 993-1022.
                    
               
                  Cole, Richard R. (1971): "Top songs in the sixties: A content analysis of popular lyrics", in: 
                        American Behavioral Scientist 14 (3): 389-400.
                    
               
                  De Sousa, Jefferson Martins / Eanes Torres, Pereira / Luciana Ribeiro, Veloso (2016): "A robust music genre classification approach for global and regional music datasets evaluation", in:
                         IEEE International Conference on Digital Signal Processing (DSP).
               
               
                  Fell, Michael / Caroline Sporleder (2014): "Lyrics-based analysis and classification of music", in: 
                        Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics.
               
               
                  Hess-Lüttich, Ernest WB. (2009): "Rap-Rhetorik. Eine semiolinguistische Analyse schweizerischer rap-lyrics", in: 
                        Ars Semeiotica 32.
               
               
                  Körner, Stefan (2012): "Bob, Pop, Bibel-die Spuren der Bibel in den Songtexten Bob Dylans." 
                        Pastoraltheologie 101 (12): 503-521.
                    
               
                  Kreyer, Rolf / Joybrato Mukherjee (2007): "The style of pop song lyrics: A corpus-linguistic pilot study." in: 
                        Anglia-Zeitschrift für englische Philologie 125 (1): 31-58.
                    
               
                  Kuhn, Elisabeth D. (1999): "‘I just want to make love to you’–Seductive strategies in blues lyrics", in: 
                        Journal of pragmatics 31 (4): 525-534.
                    
               
                  Liu, Bing (2016): 
                        Sentiment analysis: Mining opinions, sentiments, and emotions. New York: Cambridge University Press.
                    
               
                  Mohammad, Saif (2011): "From once upon a time to happily ever after: Tracking emotions in novels and fairy tales.", in: 
                        Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities 105-114.
                    
               
                  Moretti, Franco (2002): “Conjectures on World Literature” in: 
                        New Left Review Jan / Feb: 54–68.
                    
               
                  Napier, Kathleen / Lior, Shamir (2018): "Quantitative Sentiment Analysis of Lyrics in Popular Music", in: 
                        Journal of Popular Music Studies 30 (4): 161-176.
                    
               
                  Nalisnick, Eric T. / Baird, Henry S. (2013): "Character-to-character sentiment analysis in shakespeare’s plays.“, in
                        : Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics 479–483.
                    
               
                  Nishina, Yasunori (2017): "A study of pop songs based on the billboard corpus." in: 
                        Int. J. Lang. Linguist 4 (2): 125-134.
                    
               
                  Pettijohn, Terry F. / Donald F. Sacco Jr. (2009): "The language of lyrics: An analysis of popular Billboard songs across conditions of social and economic threat", in: 
                        Journal of Language and Social Psychology 28(3): 297-311.
                    
               
                  Pfahler, Lukas / Elwert, Frederik / Tabti, Samira / Morik, Katharina / Krech, Volker (2018): "Versuche zum distant reading religiöser Online-Foren“: in 
                        Book of Abstracts, DHd 2018.
               
               
                  Remus, Robert / Quasthoff, Uwe / Gerhard, Heyer (2010): "SentiWS-A Publicly Available German-language Resource for Sentiment Analysis.", in: 
                        LREC: 1168-1171.
                    
               
                  Riedemann, Frank (2012): "Computergestützte Analyse und Hit-Songwriting.", in: 
                        Black box pop. Analysen populärer Musik: 43-56.
                    
               
                  Stiegler, Christian (2009): 
                        Nur ein Wort. Dissertation, Universität Wien.
                    
               
                  Werner, Valentin (2012): “Love is all around: A corpus-based study of pop lyrics.” in: 
                        Corpora 7 (1): 19–50.
                    
               
                  West, Alan / Colin Martindale (1996): "Creative trends in the content of Beatles lyrics" 
                        Popular Music & Society 20 (4): 103-125.
                    
               
                  Whissell, Cynthia (1996): "Traditional and emotional stylometric analysis of the songs of Beatles Paul McCartney and John Lennon", in: 
                        Computers and the Humanities 30 (3): 257-265.
                    
               
                  Whissell, Cynthia (2008): "Emotional fluctuations in Bob Dylan's lyrics measured by the Dictionary of Affect accompany events and phases in his life", in
                        : Psychological reports 102 (2): 469-483.
                    
            
         
      
   



      
         
             Prosopographical Interoperability – State of the Art
            In einem wegweisenden Artikel zu Prosopographie aus dem Jahr 1971 schreibt Lawrence Stone (Stone, 1971) “The method employed is to establish a universe to be studied, and then to ask a set of uniform questions...”. Dieses 
    Erstellen des Universums ist einer der Reize der Prosopographie, stellt das Feld aber auch vor besondere Herausforderungen. Ein 
    Universum besteht gemeinhin aus Millionen von Objekten und noch mehr Relationen zwischen diesen Objekten. Projekte, die sich mit prosopographischer Forschung beschäftigen, können oder würden deshalb von weiterverwendbaren Daten besonders profitieren. Ihr Daten
    universum dreht sich nicht nur um für ihre Forschung zentrale Daten (die oft neu erstellt oder überprüft werden), sondern auch um angebundene Daten: Geburtsorte von Personen, Orte in denen sich Institutionen befinden, Lehrer von Personen in der Kerngruppe etc.
    
            Eine logische Konsequenz daraus ist es, zumindest für periphere Daten der eigenen Prosopographie LOD-Daten nachzunutzen. Um dies ohne großen Aufwand tun zu können, bräuchte es kompatible Ontologien/Datenmodelle. In den letzten Jahren wurden mehrere Versuche unternommen, für das Feld der Prosopographie einheitliche Datenmodelle vorzuschlagen (Fokkens and ter Braake, 2018; Tuominen et al., 2017), ohne dass es schon zu einem Konsens gekommen wäre.
            Das Poster wird über den Stand einer Initiative berichten, die seit vergangenem Jahr an der Definition einer RESTful API arbeitet, welche die Veröffentlichung von maschinenlesbaren prosopographischen Daten so erleichtern soll, dass typische Anfragen performant und für Softwareentwickler einfach zu realisieren sind. Dieses “International Prosopography Interoperability Format” (IPIF) hat als Kern die Definition in einer RESTful API, die in OpenAPI beschrieben ist.
            
            Das dort vorgeschlagene Datenmodell deckt zum einen die Notwendigkeit ab, zwischen Person, Quelle und Quelleninterpretation zu unterscheiden (“Factoid”-Modell, Bradley & Short 2005), zum anderen vereinfacht es den Zugriff auf Informationen über eine Person in klassischen Benutzungssenzarien der Prosopographie. In einem “Statement” über eine Person können verbale Beschreibungen oder Quellenzitate ebenso wie strukturierte Informationen enthalten sein. Um die prosopographische Benutzung zu erleichtern, lassen sich die strukturierten Informationen im Modell von IPIF als datierbare Ereignisse verstehen, wenn sie mit einer Property “date” versehen sind. Sie können aber für reine Identifikationszwecke auch einfache Eigenschaften (Name, Geschlecht) abbilden. Properties “relatesToPerson” und “isMemberOf” bedienen ein drittes zentrales Szenario der Nutzung von prosopographischen Daten, nämlich Beziehungen zu anderen Personen. Schließlich sind Ortsangaben zu einer Person mit der Property “place” möglich. Mit diesen Angaben ermöglicht IPIF Anzeigen wie die des DARIAH-Cosmotools und Ego-Netzwerke wie z.B. in der Deutschen Biographie.
    
         
         
             Proof of concepts
            Von Beginn an war eine Grundidee des
  Unterfangens, die Praxistauglichkeit des Datenmodells und der API
  Definition möglichst früh zu testen. Zu diesem Zweck wurden seit
  2018 mehrere “Proof of Concept” Applikationen erstellt. Das Poster
  wird den aktuellen Entwicklungsstand dieser Proof of Concept
  Applikationen darstellen.
         
         
             APIS (Austrian Prosopographical Information System)
            APIS ist ein Web-basiertes System zur
  Arbeit an prosopographischen Daten (Schlögl/Lejtovicz 2018). Es
  bietet Webformulare, aber auch API-Schnittstellen zu den
  Daten. Aufbauend auf die schon vorhandenen APIs wurde ein Renderer
  erstellt, der vorhandene Daten in das IPIF Format
  überführt. Zusätzlich wurden als parallele API die IPIF endpoints
  implementiert und somit compliance level 1 laut API Definition (GET
  requests inklusive Filter) erreicht.
         
         
            API Wrapper
            Eines der Anwendungsszenarien von IPIF
  ist die Suche schon vorhandener Identifier über mehrere Referenz
  Ressourcen hinweg (vgl. Vogeler et al 2020 forthcoming). Um die
  Anwendbarkeit von IPIF für dieses Szenario zu testen, wurde eine
  einfache Applikation erstellt, die als Middle-Layer zwischen der
  IPIF API auf der einen Seite und dem Wikidata SPARQL Endpoint und
  der Lobid GND API auf der anderen Seite fungiert. Die Applikation
  übersetzt dabei Anfragen an die API in eine wikidata kompatible
  SPARQL query bzw. einen Lobid kompatiblen GET request und überführt
  die Antworten in das IPIF format. Die Applikation macht sich für
  diese Übersetzung die Django-Templating Engine zu Nutze und ist
  damit auch für andere APIs einfach konfigurierbar.
         
         
            Papilotte
            Auf der in OpenAPI veröffentlichten Spezifikation aufbauend lässt sich über Frameworks wie das von Zalando als Open Source Software bereitgestellte “Connexion” schnell ein Stand-Alone-Server in Python schreiben, der über flexible Konnektoren den Zugriff auf unterschiedlichste interne oder externe Datenquellen ermöglicht und diese IPIF-konform bereitstellt. Ein solcher Server wurde mit Beispieldaten aus dem Monasterium.net-Projekt erstellt.
            
         
         
            Papi-Cosmotool
            Das DARIAH-Cosmotool bietet eine prototypische Oberfläche für eine prosopographische Datenbank an. Es enthält eine biographische (“Zeitleiste”), eine textuelle (“Ereignis-Detail”) und ein geographische (“Kartendarstellung”) Ansicht. Die Datenanzeige wird mit einem Quellenverweis ergänzt. Als Test für die Verwendbarkeit der API-Definition wird von Sebastian Stoff am Zentrum für Informationsmodellierung eine JavaScript basierte Anwendung erarbeitet, die vergleichbare Funktionalitäten bietet.
            
         
         
            JSON-LD
            Schließlich arbeiten wir an einer Integration des JSON-Outputs der API-Definition in das Semantic Web. Dafür soll eine context.json-Datei bereitgestellt werden, die in den Resultsets der API-Anfragen gültige RDF-Aussagen identifiziert.
            
         
      
      
         
            
               https://github.com/GVogeler/prosopogrAPhI
            
            
               https://cosmotool.de.dariah.eu/
            
            
      z.B. https://www.deutsche-biographie.de/graph?id=sfz53095
            
            
               https://github.com/gvasold/papilotte, mit einer Beispielinstallation: 
      https://ginko.uni-graz.at/illurk/api/ui/
            
            
               http://glossa.uni-graz.at/gamsdev/stoffse/erla/mapp/map/, Code von Sebastian Stoff.
    
            
      Ein erster Entwurf ist unter https://github.com/GVogeler/prosopogrAPhI/blob/master/context.json einsehbar.
    
         
         
            
               Bibliographie
               
                  Bradley, John / Short, Harold
                   (2005): “Texts into databases. The Evolving field of New-style Prosopography.“ in: 
                  LLC
                   20, suppl. 1: 3-24.
               
               
                  Fokkens, A. / ter Braake, S. 
                  (2018): Connecting People Across Borders: a Repository for Biographical Data Models, in: Proceedings of the Second Conference on Biographical Data in a Digital World 2017. Linz, Austria, November 6-7, 2017. CEUR Workshop Proceedings: 83–92.
               
               
                  Stone, Lawrence (1971): "Prosopography." in: 
Daedalus
                   100: 46–79.
               
               
                  Schlögl, Matthias / Katalin Lejtovicz
                  . (2018). “A Prosopographical Information System (APIS).“ In: Antske Fokkens, ter Braake, Serge, Sluijter, Ronald, Arthur, Paul, and Wandl-Vogt, Eveline (eds.). 
                  BD-2017. Biographical Data in a Digital World 2017. Proceedings of the Second Conference on Biographical Data in a Digital World 2017
                  . Linz, Austria, November 6-7, 2017. Budapest: CEUR (CEUR Workshop Proceedings 2119): 53-58.
               
               
                  Tuominen, Jouni / Hyvönen, Eero / Leskinen, Petri 
                  (2018): “Bio CRM. A Data Model for Representing Biographical Data for Prosopographical Research.” In: 
                  BD-2017. Biographical Data in a Digital World 2017
                  , ed. by Antske Fokkens, Serge ter Braake, Ronald Sluijter, Paul Arthur, Eveline Wandl-Vogt, Budapest: CEUR (CEUR Workshop Proceedings 2119): 59-66.
               
               
                  Vogeler, Georg / Vasold, Gunter / Schlögl, Matthias 
                  (2020 forthcoming): Data exchange in practice: Towards a prosopographical API. In: BD2019, Workshop on Biographical Data in Occasion of the RANLP 2019, Varna, ed. by Antske Fokkens et al.
               
            
         
      
   



      
         
            Einleitung
            Digitale Analyseverfahren verändern immer intensiver die Forschungsweise der GeisteswissenschaftlerInnen und mit dem wachsenden Spielraum der Methoden wächst auch die Anzahl an Fragen, die sich vor allem an den Grad der Genauigkeit und wissenschaftliche Relevanz dieser Methoden richtet. Das Topic Modeling gewinnt als eine Methode für automatische Erkennung von versteckten thematischen Strukturen in großen Textmengen (Blei 2012: 8) immer mehr an Beliebtheit, erweckt aber auch Unsicherheiten. Daher beschäftigt sich diese Arbeit mit den Möglichkeiten und Problemen des Topic Modeling am Beispiel von Briefen und stellt unter anderem die Fragen, 1) wie Topic Modeling in der Analyse von Briefkorpora eingesetzt werden kann und 2) wie die Qualität der Ergebnisse dieses Prozesses beeinflusst werden kann. 
         
         
            Forschungsmaterial
            Das Forschungsmaterial besteht aus Briefen des Grazer Sprachwissenschaftlers Hugo Schuchardt (1842-1927). Die umfangreiche und mehrsprachige Korrespondenz dieses schon seinerzeit sehr geschätzten Wissenschaftlers ist seit 2007 Teil des Digitalisierung-Projektes 
                    Hugo Schuchardt Archiv (Hurch 2019). Für die Topic-Modeling-Analyse werden 2261 Briefdateien im TEI-Format in Betracht gezogen, da die restlichen zurzeit noch in keinem entsprechenden Format vorhanden sind. Der Vorteil einer solchen Methode ist es aber, dass das gleiche Modell jederzeit auf eine erweiterte Menge an Daten anwendbar ist. Eine Besonderheit dieses Korpus ist, dass Schuchardt in mehreren Sprachen korrespondiert hat, von denen hier elf repräsentiert sind (Abbildung 1). Daher wird das Modell für einzelne Sprachen separat angewendet. Dies ist insofern eine Herausforderung, weil 1) Vorgänge den jeweiligen Sprachen angepasst werden müssen (wie etwa die Lemmatisierung), 2) der Textumfang bei vielen Sprachen nicht ausreichend ist und daher nicht auf alle Sprachen effektiv angewendet werden kann und 3) die verschiedenen Ergebnisse pro Sprache verglichen werden sollten. Ein weiteres Problem für das Topic Modeling ist die große Diskrepanz in den Textlängen der einzelnen Dateien (Abbildung 2), da die Korrespondenz auch kürzere Formen wie Postkarten und Telegramme beinhaltet. So enthalten etwa die kürzesten deutschsprachigen Dateien etwa drei Tokens, die längste jedoch 3947. Dies ist aber ein Zustand, den viele Briefkorpora in der Realität begegnen, da wir als ForscherInnen selten einem ‚idealen‘ Korpus gegenüberstehen. Die Auseinandersetzung mit solchen Problemen ist ein fester Bestandteil unserer Arbeit.
                
            
               
                Abbildung 1: Anteil der einzelnen Sprachen im Briefkorpus
            
            
               
                Abbildung 2: Menge der deutschsprachigen Briefdateien nach ihrer Anzahl der Tokens
            
         
         
            Methode
            Für die Beantwortung der Forschungsfragen war zuerst die Literaturrecherche nötig, und zwar erstens zum Topic Modeling, zweitens zur Textsorte Brief und drittens zu dieser Korrespondenz. Um eine genauere Vorstellung zum Forschungsstand des Topic Modeling zu bekommen, wurden wissenschaftliche Aufsätze und Anwendungsbeispiele in Betracht gezogen, wie etwa Blei 2010, Jagarlamudi/Daumé 2010, Boyd-Graber/Blei 2012, Riddell 2015, Vulić et al. 2015, Bock et al. 2016, Andorfer 2017, Fechner/Weiß 2017, Schöch 2017, Murakami et al. 2017 und Arora et al. 2018. Zudem wird am genannten Korpus Topic Modeling mit Hilfe der Programmiersprache 
                    Python (Python Software Foundation 2001-2019), der Software MALLET (McCallum 2002-2019) und der Anweisungen der Jupyter-Notebooks von DARIAH-DE (DARIAH-DE 2019) vollzogen. Darüber hinaus werden verschiedene Tools zur Vorverarbeitung evaluiert – z. B. 
                    spaCy (Explosion AI 2019) und DTA::CAB (Berlin-Brandenburgische Akademie der Wissenschaften 2011-2018) für die Lemmatisierung – sowie verschiedene Tools und Parameter für die Topic-Modellierung – z. B. 
                    Topics Explorer (DARIAH-DE 2018) – und die daraus resultierenden Ergebnisse und Erfahrungen verglichen. 
                
         
         
            Ergebnisse
            Obwohl es sich um ein laufendes Projekt handelt, gibt es bereits einige relevante Ergebnisse und Schlussfolgerungen. 
            1) Die Vorverarbeitung stellt einen wichtigen Schritt in der Topic-Modellierung dar und beeinflusst die Ergebnisse. Dabei spielen nicht nur die eingesetzten Tools eine Rolle, sondern auch die gewählte Vorgehensweise.
            2) Die Lemmatisierung, auf die beim Topic Modeling oft verzichtet wird, ermöglicht mehr semantische Differenz in den Topics. 
            3) Der unterschiedliche Textumfang von einzelnen VerfasserInnen kann zu falschen Ergebnissen führen, wenn die Topics pro VerfasserIn analysiert werden.
            4) Entscheidungen über Parameter wie Optimierungsintervall, Topic- und Iterations-Anzahl können die Ergebnisse beeinträchtigen und müssen immer projektspezifisch getestet werden, bis ein sinnvolles Resultat vorliegt. Das ‚Sinnvolle‘ zu erkennen ist eine Herausforderung, die fachwissenschaftliches Verständnis verlangt. 
            Die Inkonsistenz der Topics und manchmal verwirrende Ergebnisse zeigen, dass die naive Anwendung eines Topic-Modeling-Tools nicht immer befriedigend sein kann. Intensivere Beschäftigung mit den einzelnen Schritten und Ergebnissen kann sich jedoch positiv auf den Erfolg der Analyse auswirken. Die weitere Arbeit wird zeigen, ob und welchen Mehrwert Topic Modeling bei der Analyse der Schuchardt-Korrespondenz leisten kann, die durch 
                    close reading nicht erreicht werden können. 
                
         
      
      
         
            
               Bibliographie
               
                  Andorfer, Peter (2017): 
"Turing Test für das Topic Modeling. Von Menschen und Maschinen
erstellte inhaltliche Analysen der Korrespondenz von Leo von
Thun-Hohenstein im Vergleich", in: 
Zeitschrift für digitale Geisteswissenschaften 2. http://zfdg.de/2017_002 [letzter Zugriff 27. September 2019].

               
                  Arora, Sanjeev / Ge, Rong; Halpern, Yoni / Mimno, David / Moitra, Ankur / Sontag, David / Wu, Yichen / Zhu, Michael (2018): 
  "Learning topic models - provably and efficiently",  in: 
  Communications of the ACM 61 / 4: 85–93. 10.1145/3186262.

               
                  Berlin-Brandenburgische Akademie der Wissenschaften (ed.) (2011-2018): 
  Das DTA-Basisformat. http://www.deutschestextarchiv.de/doku/basisformat/ [letzter Zugriff 27. September 2019].

               
                  Blei, David M. (2010): 
 "Introduction to Probabilistic Topic Models", in: 
 Semantic Scholar.
 https://pdfs.semanticscholar.org/5f10/38ad42ed8a4428e395c96d57f83d201ef3b3.pdf
 [letzter Zugriff 27. September 2019].

               
                  Blei, David M. (2012): 
  "Topic Modeling and Digital Humanities", in: 
  Journal of Digital Humanities 2 / 1: 8–11.
  http://journalofdigitalhumanities.org/2-1/topic-modeling-and-digital-humanities-by-david-m-blei/ [letzter Zugriff 27. September 2019].

               
                  Bock, Sina / Du, Keli / Huber, Michael / Pernes, Stefan / Pielström, Steffen (2016): 
  Der Einsatz quantitativer Textanalyse in den Geisteswissenschaften. Bericht über den Stand der Forschung. (= DARIAH-DE working papers 18). Göttingen: GOEDOC – Dokumenten- und Publikationsserver der Georg-August-Universität Göttingen. http://webdoc.sub.gwdg.de/pub/mon/dariah-de/dwp-2016-18.pdf [letzter Zugriff 27. September 2019].

               
                  Boyd-Graber, Jordan / Blei, David (2012): 
  Multilingual Topic Models for Unaligned Text. http://arxiv.org/pdf/1205.2657v1 [letzter Zugriff 27. September 2019].

               
                  DARIAH-DE (2018): 
  Topics Explorer. V. 2.0.1. https://github.com/DARIAH-DE/TopicsExplorer [letzter Zugriff 27. September 2019].

               
                  DARIAH-DE (2019): 
  DARIAH Topics. Easy Topic Modeling in Python. V. 2.0.1. https://github.com/DARIAH-DE/Topics [letzter Zugriff 27. September 2019].

               
                  Explosion AI (2019): 
  spaCy. V. 2.1.6. https://github.com/explosion/spaCy [letzter Zugriff 27. September 2019].

               
                  Fechner, Martin / Weiß, Andreas (2017): 
  "Einsatz von Topic Modeling in den Geschichtswissenschaften: Wissensbestände des 19. Jahrhunderts", in: 
  Zeitschrift für digitale Geisteswissenschaften 2. http://zfdg.de/2017_005 [letzter Zugriff 27. September 2019].

               
                  Hurch, Bernhard (2019): 
  "Hugo Schuchardt Archiv". Institut für Romanistik, Karl-Franzens-Universität Graz (ed.). https://schuchardt.uni-graz.at [letzter Zugriff 27. September 2019].

               
                  Jagarlamudi, Jagadeesh / Daumé, Hal (2010): 
  "Extracting Multilingual Topics from Unaligned Comparable Corpora",  in: Gurrin, Cathal (ed.): 
  Advances in information
  retrieval. Proceedings 444–456. (= Lecture notes in computer science 5993). Berlin / Heidelberg / New York: Springer.

               
                  McCallum, Andrew Kachites (2002-2019): 
  MALLET. A Machine Learning for Language Toolkit. V. 2.0.8. http://mallet.cs.umass.edu [letzter Zugriff 27. September 2019].

               
                  Murakami, Akira / Thompson, Paul / Hunston, Susan / Vajn, Dominik (2017): 
  "‘What is this corpus about?’: using topic modelling to explore a
  specialised corpus", in: 
  Corpora 12 / 2: 243–277. https://www.euppublishing.com/doi/10.3366/cor.2017.0118 [letzter Zugriff 27. September 2019].

               
                  Python Software Foundation (2001-2019): 
  Python. V. 3.7.4. https://github.com/python [letzter Zugriff 27. September 2019].

               
                  Riddell, Allen (2015): 
  Text Analysis with Topic Models for the Humanities and Social Sciences — Text Analysis with Topic Models for the Humanities and Social Sciences. DARIAH-DE Initiative (ed.). https://liferay.de.dariah.eu/tatom/ [letzter Zugriff 27. September 2019].

               
                  Schöch, Christof (2017): 
  "Topic Modeling Genre. An Exploration of French Classical and
  Enlightenment Drama",
  in:  Digital Humanities Quarterly 11 / 2. http://www.digitalhumanities.org/dhq/vol/11/2/000291/000291.html [letzter Zugriff 27. September 2019].

               
                  Vulić, Ivan / Smet, Wim de / Tang, Jie / Moens, Marie-Francine (2015): 
  "Probabilistic topic modeling in multilingual settings. An overview
  of its methodology and applications",  in: 
  Information Processing & Management 51 / 1: 111–147. https://www.sciencedirect.com/science/article/pii/S0306457314000739 [letzter Zugriff 27. September 2019].

            
         
      
   

